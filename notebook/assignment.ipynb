{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a83a267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a11feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb76828",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"./data\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    show_progress=True,\n",
    "    loader_cls=PyPDFLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc658b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:14<00:00,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1724 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b878850d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 1, 'page_label': '2'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 2, 'page_label': '3'}, page_content='Modern Time Series Forecasting with\\nPython\\nCopyright © 2024 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced,\\nstored in a retrieval system, or transmitted in any form or by\\nany means, without the prior written permission of the\\npublisher, except in the case of brief quotations embedded in\\ncritical articles or reviews.\\nEvery eﬀort has been made in the preparation of this book to\\nensure the accuracy of the information presented. However, the\\ninformation contained in this book is sold without warranty,\\neither express or implied. Neither the author, nor Packt\\nPublishing, and its dealers and distributors will be held liable\\nfor any damages caused or alleged to be caused directly or\\nindirectly by this book.\\nPackt Publishing has endeavored to provide trademark\\ninformation about all of the companies and products\\nmentioned in this book by the appropriate use of capitals.\\nHowever, Packt Publishing cannot guarantee the accuracy of\\nthis information.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 3, 'page_label': '4'}, page_content='Early Access Publication: Modern Time Series Forecasting\\nwith Python\\nEarly Access Production Reference: B22389\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK\\nISBN: 978-1-83588-318-1\\nwww.packt.com'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 4, 'page_label': '5'}, page_content='Table of Contents\\nModern Time Series Forecasting with Python, Second Edition:\\nIndustry-ready machine learning and deep learning time\\nseries analysis with PyTorch and pandas\\n1. Introducing Time Series\\n1. Join our book community on Discord\\n2. Technical requirements\\n3. What is a time series?\\n1. Types of time series\\n2. Main areas of application for time series analysis\\n4. Data-generating process (DGP)\\n1. Generating synthetic time series\\n2. Stationary and non-stationary time series\\n5. What can we forecast?\\n6. Forecasting terminology\\n7. Summary\\n8. Further reading\\n2. Acquiring and Processing Time Series Data\\n1. Join our book community on Discord\\n2. Technical requirements\\n3. Understanding the time series dataset\\n1. Preparing a data model'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 5, 'page_label': '6'}, page_content='4. pandas datetime operations, indexing, and slicing – a\\nrefresher\\n1. Converting the date columns into\\npd.Timestamp/DatetimeIndex\\n2. Using the .dt accessor and datetime properties\\n3. Slicing and indexing\\n4. Creating date sequences and managing date oﬀsets\\n5. Handling missing data\\n1. Converting the half-hourly block-level data (hhblock)\\ninto time series data\\n2. Compact, expanded, and wide forms of data\\n3. Enforcing regular intervals in time series\\n4. Converting the London Smart Meters dataset into a time\\nseries format\\n6. Mapping additional information\\n7. Saving and loading ﬁles to disk\\n8. Handling longer periods of missing data\\n1. Imputing with the previous day\\n2. Hourly average proﬁle\\n3. The hourly average for each weekday\\n4. Seasonal interpolation\\n9. Summary\\n3. Analyzing and Visualizing Time Series Data\\n1. Join our book community on Discord'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 6, 'page_label': '7'}, page_content='2. Technical requirements\\n3. Components of a time series\\n1. The trend component\\n2. The seasonal component\\n3. The cyclical component\\n4. The irregular component\\n4. Visualizing time series data\\n1. Line charts\\n2. Seasonal plots\\n3. Seasonal box plots\\n4. Calendar heatmaps\\n5. Autocorrelation plot\\n5. Decomposing a time series\\n1. Detrending\\n2. Deseasonalizing\\n3. Implementations\\n6. Detecting and treating outliers\\n1. Standard deviation\\n2. Interquartile range (IQR)\\n3. Isolation Forest\\n4. Extreme studentized deviate (ESD) and seasonal ESD (S-\\nESD)\\n5. Treating outliers\\n7. Summary'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 7, 'page_label': '8'}, page_content='8. References\\n9. Further reading\\n4. Setting a Strong Baseline Forecast\\n1. Join our book community on Discord\\n2. Technical requirements\\n3. Setting up a test harness\\n1. Creating holdout (test) and validation datasets\\n2. Choosing an evaluation metric\\n4. Generating strong baseline forecasts\\n1. Naïve forecast\\n2. Moving average forecast\\n3. Seasonal naive forecast\\n4. Exponential smoothing (ETS)\\n5. ARIMA\\n6. Theta Forecast\\n7. TBATS\\n8. MSTL\\n9. Evaluating the baseline forecasts\\n5. Assessing the forecastability of a time series\\n1. Coeﬃcient of Variation (CoV)\\n2. Residual variability (RV)\\n3. Entropy-based measures\\n4. Kaboudan metric\\n6. Summary'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 8, 'page_label': '9'}, page_content='7. References\\n8. Further reading\\n5. Time Series Forecasting as Regression\\n1. Join our book community on Discord\\n2. Understanding the basics of machine learning\\n1. Supervised machine learning tasks\\n2. Overﬁtting and underﬁtting\\n3. Hyperparameters and validation sets\\n3. Time series forecasting as regression\\n1. Time delay embedding\\n2. Temporal embedding\\n4. Global forecasting models – a paradigm shift\\n5. Summary\\n6. References\\n7. Further reading\\n6. Feature Engineering for Time Series Forecasting\\n1. Join our book community on Discord\\n2. Technical requirements\\n3. Feature engineering\\n4. Avoiding data leakage\\n5. Setting a forecast horizon\\n6. Time delay embedding\\n1. Lags or backshift\\n2. Rolling window aggregations'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 9, 'page_label': '10'}, page_content='3. Seasonal rolling window aggregations\\n4. Exponentially weighted moving averages (EWMA)\\n7. Temporal embedding\\n1. Calendar features\\n2. Time elapsed\\n3. Fourier terms\\n8. Summary\\n1. Cover\\n2. Table of contents'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 10, 'page_label': '11'}, page_content='Modern Time Series Forecasting with\\nPython, Second Edition: Industry-\\nready machine learning and deep\\nlearning time series analysis with\\nPyTorch and pandas\\nWelcome to Packt Early Access. We’re giving you an exclusive\\npreview of this book before it goes on sale. It can take many\\nmonths to write a book, but our authors have cutting-edge\\ninformation to share with you today. Early Access gives you an\\ninsight into the latest developments by making chapter drafts\\navailable. The chapters may be a little rough around the edges\\nright now, but our authors will update them over time.\\nYou can dip in and out of\\u202fthis book\\u202for\\u202ffollow along\\u202ffrom start to\\nﬁnish; Early Access is designed to be ﬂexible. We hope you\\nenjoy getting to know more about the process of writing a Packt\\nbook.\\n1. Chapter 1: Introducing Time Series\\n2. Chapter 2: Acquiring and Processing Time Series Data\\n3. Chapter 3: Analyzing and Visualizing Time Series Data'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 11, 'page_label': '12'}, page_content='4. Chapter 4: Setting a Strong Baseline Forecast\\n5. Chapter 5: Time Series Forecasting as Regression\\n6. Chapter 6: Feature Engineering for Time Series Forecasting\\n7. Chapter 7: Target Transformations for Time Series\\nForecasting\\n8. Chapter 8: Forecasting Time Series with Machine Learning\\nModels\\n9. Chapter 9: Ensembling and Stacking\\n10. Chapter 10: Global Forecasting Models\\n11. Chapter 11: Introduction to Deep Learning\\n12. Chapter 12: Building Blocks of Deep Learning for Time Series\\n13. Chapter 13: Common Modeling Patterns for Time Series\\n14. Chapter 14: Attention and Transformers for Time Series\\n15. Chapter 15: Strategies for Global Deep Learning Forecasting\\nModels\\n16. Chapter 16: Specialized Deep Learning Architectures for\\nForecasting\\n17. Chapter 17: Probabilistic Forecasting and Other Use Cases\\n18. Chapter 18: Multi-Step Forecasting\\n19. Chapter 19: Evaluating Forecasts – Forecast Metrics\\n20. Chapter 20: Evaluating Forecasts – Validation Strategies'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 12, 'page_label': '13'}, page_content='1 Introducing Time Series'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 13, 'page_label': '14'}, page_content='Join our book community on Discord\\nhttps://packt.link/EarlyAccess/\\nWelcome to Modern Time Series Forecasting with Python! This\\nbook is intended for data scientists or machine learning (ML)\\nengineers who want to level up their time series analysis skills\\nby learning new and advanced techniques from the ML world.\\nTime series analysis is something that is commonly\\noverlooked in regular ML books, courses, and so on. They\\ntypically start with classiﬁcation, touch upon regression, and\\nthen move on. But it is also something that is immensely\\nvaluable and ubiquitous in business. We look at the world in a\\nthree-dimensional perspective. But time is the hidden\\ndimension which we rarely think about, but is all-pervasive.\\nAnd as long as time is one of the four dimensions in the world\\nwe live in, time series data is all-pervasive.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 14, 'page_label': '15'}, page_content=\"Analyzing time series data unlocks a lot of value for a business.\\nTime series analysis isn't new—it's been around since the 1920s.\\nBut in the current age of data, the time series that are collected\\nby businesses are growing larger and wider by the minute.\\nCombined with an explosion in the quantum of data collected\\nand the renewed interest in ML, the landscape of time series\\nanalysis also changed considerably. This book attempts to take\\nyou beyond classical statistical methods such as\\nAutoRegressive Integrated Moving Average (ARIMA) and\\nintroduce to you the latest techniques from the ML world in\\ntime series analysis.\\nWe are going to start with some fundamental concepts and\\nquickly scale up to more complex topics. In this chapter, we're\\ngoing to cover the following main topics:\\nWhat is a time series?\\nData-generating process (DGP)\\nWhat can we forecast?\\nForecasting terminology and notation\\nTechnical requirements\\nYou will need to set up the Anaconda environment following\\nthe instructions in the Preface of the book to get a working\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 15, 'page_label': '16'}, page_content=\"environment with all the libraries and datasets required for the\\ncode in this book. Any additional library will be installed while\\nrunning the notebooks.\\nThe associated code for the chapter can be found at\\nhttps://github.com/PacktPublishing/Modern-Time-Series-\\nForecasting-with-Python-/tree/main/notebooks/Chapter01.\\nWhat is a time series?\\nTo keep it simple, a time series is a set of observations taken\\nsequentially in time. The focus is on the word time. If we keep\\ntaking the same observation at diﬀerent points in time, we will\\nget a time series. For example, if you keep recording the\\nnumber of bars of chocolate you have in a month, you'll end up\\nwith a time series of your chocolate consumption. Suppose you\\nare recording your weight at the beginning of every month. You\\nget another time series of your weight. Is there any relation\\nbetween the two time series? Most likely, yeah. But we can\\nanalyze that scientiﬁcally by the end of this book.\\nA few other examples of time series are the weekly closing\\nprice of a stock that you follow, daily rainfall or snow in your\\ncity, or hourly readings of your heartbeat from your\\nsmartwatch.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 16, 'page_label': '17'}, page_content='Types of time series\\nThere are two types of time series data based on time-intervals,\\nas outlined here:\\nRegular time series: This is the most common type of time\\nseries where we have observations coming in at regular\\nintervals of time, such as every hour or every month. For\\nexample, if we take a time series of temperature in a cuty, we\\nwill get the time series in a regular interval (whichever\\nfrequency we chose for observation).\\nIrregular time series: There are a few time series where we do\\nnot have observations at a regular interval of time. For\\nexample, consider we have a sequence of readings from lab\\ntests of a patient. We see an observation in the time series only\\nwhen the patient heads to the clinic and carries out the lab test,\\nand this may not happen in regular intervals of time.\\nNOTE\\nThis book only focuses on regular time series, which\\nare evenly spaced in time. Irregular time series are\\nslightly more advanced and require specialized\\ntechniques to handle them. A couple of survey papers\\non the topic is a good way to get started on irregular'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 17, 'page_label': '18'}, page_content=\"time series and you can ﬁnd them in the Further\\nreading section of this chapter.\\nMain areas of application for time series analysis\\nThere are broadly three important areas of application for time\\nseries analysis, outlined as follows:\\nTime series forecasting: Predicting the future values of a time\\nseries, given the past values—for example, predict the next\\nday's temperature using the last 5 years of temperature data.\\nThis use-case is one of the most popular and important because\\nany kind of planning we need to do needs some visibility into\\nthe future. For instance, planning how many chocolates to\\nproduce next month needs a forecast of expected demand.\\nTime series classiﬁcation: Sometimes, instead of predicting\\nthe future value of the time series, we may also want to predict\\nan action based on past values. For example, given a history of\\nan electroencephalogram (EEG; tracking electrical activity in\\nthe brain) or an electrocardiogram (EKG; tracking electrical\\nactivity in the heart), we need to predict whether the result of\\nan EEG or an EKG is normal or abnormal.\\nOutlier Detection: There are some situations where we only\\nwant to detect if something is going wrong or if something is\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 18, 'page_label': '19'}, page_content='out of the ordinary. In such cases, we need to use\\nclassiﬁcation or forecasting, but instead we can do outlier\\ndetection. For instance, the wearable tech on your body\\nrecords the accelerometer readings across time and can use\\noutlier detection to identify falls or accidents.\\nInterpretation and causality: Understand the whats and whys\\nof the time series based on the past values, understand the\\ninterrelationships among several related time series, or derive\\ncausal inference based on time series data. For example, we\\nhave a time series of market share for a brand and also another\\ntime series of advertising spends. Using interpretation and\\ncausality techniques, we can start to understand how much\\nadvertising spends are aﬀecting the market share and possibly\\ntake appropriate actions around it.\\nNOTE\\nThe focus of this book is predominantly on time series\\nforecasting, but the techniques that you learn will help\\nyou approach time series classiﬁcation problems also,\\nwith minimal change in the approach. Interpretation is\\nalso addressed, although only brieﬂy, but causality is\\nan area that this book does not address because it\\nwarrants a whole diﬀerent approach.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 19, 'page_label': '20'}, page_content=\"Now that we have an overview of the time series landscape,\\nlet's build a mental model on how time series data is generated.\\nData-generating process (DGP)\\nWe saw that time series data is a collection of observations\\nmade sequentially along the time dimension. Any time series is,\\nin turn, generated by some kind of mechanism. For example,\\ntime series data of daily shipments of your favorite chocolate\\nfrom the manufacturing plant is aﬀected by a lot of factors such\\nas the time of the year, the holiday season, the availability of\\ncocoa, the uptime of the machines working on the plant, and so\\non. In statistics, this underlying process that generated the time\\nseries is referred to as the DGP. Time series data is produced by\\nstochastic and deterministic process. The deterministic\\nprocesses involve quantities that evolve in a predictable\\nmanner over time. An example of this is the radioactive decay\\nof an element, where the remaining quantity diminishes\\naccording to a precise mathematical formula, leading to a\\nconsistent reduction over time. But most of the interesting time\\nseries (from a forecasting perspective) are generated by a\\nstochastic process. A stochastic process is a way to describe how\\nthings change over time in a random but somewhat predictable\\nmanner, like how the weather changes daily with some patterns\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 20, 'page_label': '21'}, page_content=\"and probabilities involved. So, let's discuss more about time\\nseries generated from stochastic processes.\\nIf we had complete and perfect knowledge of reality, all we\\nmust do is put this DGP together in a mathematical form and\\nyou will get the most accurate forecast possible. But sadly,\\nnobody has complete and perfect knowledge of reality. So, what\\nwe try to do is approximate the DGP, mathematically, as much\\nas possible so that our imitation of the DGP gives us the best\\npossible forecast (or any other output we want from the\\nanalysis). This imitation is called a model that provides a useful\\napproximation to the DGP.\\nBut we must remember that the model is not the DGP, but a\\nrepresentation of some essential aspects of reality. For example,\\nlet's consider an aerial view of Bengaluru and a map of\\nBengaluru, as represented here:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 21, 'page_label': '22'}, page_content=\"Figure 1.1 – An aerial view of Bengaluru (left) and a map of\\nBengaluru (right) (Color)\\nThe map of Bengaluru is certainly useful—we can use it to go\\nfrom point A to point B. But a map of Bengaluru is not the same\\nas a photo of Bengaluru. It doesn't showcase the bustling\\nnightlife or the insuﬀerable traﬃc. A map is just a model that\\nrepresents some useful features of a location, such as roads and\\nplaces. The following diagram might help us internalize the\\nconcept and remember it:\\nFigure 1.2 – DGP, model, and time series (Color)\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 22, 'page_label': '23'}, page_content=\"Naturally, the next question would be this: Do we have a useful\\nmodel? Every model has it's own limitations and challenges. As\\nwe saw already, a map of Bengaluru does not perfectly\\nrepresent Bengaluru. But if our purpose is to navigate\\nBengaluru, then a map is a very useful model. What if we want\\nto understand the culture? A map doesn't give you a ﬂavor of\\nthat. So, now, the same model that was useful is utterly useless\\nin the new context.\\nDiﬀerent kinds of models are required in diﬀerent situations\\nand for diﬀerent objectives. For example, the best model for\\nforecasting may not be the same as the best model to make a\\ncausal inference.\\nWe can use the concept of DGPs to generate multiple synthetic\\ntime series, of varying degrees of complexity.\\nGenerating synthetic time series\\nSynthetic time series, or artiﬁcial time series, are excellent tools\\nwhich which you can understand the time series space,\\nexperiment with diﬀerent ways, and even use as ways to test\\nnew models or modelling setup. These time series are designed\\nto be predictable, even though a bit challenging. Let's take a\\nlook at a few practical examples where we can generate a few\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 23, 'page_label': '24'}, page_content='time series using a set of fundamental building blocks. You can\\nget creative and mix and match any of these components, or\\neven add them together to generate a time series of arbitrary\\ncomplexity.\\nWhite and red noise\\nAn extreme case of a stochastic process that generates a time\\nseries is a white noise process. It has a sequence of random\\nnumbers with zero mean and constant variance. This is also\\none of the most popular assumptions of noise in a time series.\\nLet\\'s see how we can generate such a time series and plot it, as\\nfollows:\\n# Generate the time axis with sequential numbers \\ntime = np.arange(200)\\n# Sample 200 hundred random values\\nvalues = np.random.randn(200)*100\\nplot_time_series(time, values, \"White Noise\")\\nHere is the output:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 24, 'page_label': '25'}, page_content=\"Figure 1.3 – White noise process\\nRed noise, on the other hand, has zero mean and constant\\nvariance but is serially correlated in time. This serial\\ncorrelation or redness is parameterized by a correlation\\ncoeﬃcient r, such that:\\nwhere w is a random sample from a white noise distribution.\\nLet's see how we can generate that, as follows:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 25, 'page_label': '26'}, page_content='# Setting the correlation coefficient\\nr = 0.4\\n# Generate the time axis\\ntime = np.arange(200)\\n# Generate white noise\\nwhite_noise = np.random.randn(200)*100\\n# Create Red Noise by introducing correlation bet\\nvalues = np.zeros(200)\\nfor i, v in enumerate(white_noise):\\n    if i==0:\\n        values[i] = v\\n    else:\\n        values[i] = r*values[i-1]+ np.sqrt((1-np\\nplot_time_series(time, values, \"Red Noise Process\\nHere is the output:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 26, 'page_label': '27'}, page_content=\"Figure 1.4 – Red noise process\\nCyclical or seasonal signals\\nAmong the most common signals you see in time series are\\nseasonal or cyclical signals. Therefore, you can introduce\\nseasonality into your generated series in a few ways.\\nLet's take the help of a very useful library to generate the rest of\\nthe time series—TimeSynth. For more information, refer to\\nhttps://github.com/TimeSynth/TimeSynth.\\nThis is a useful library to generate time series. It has all kinds of\\nDGPs that you can mix and match and create authentic\\nsynthetic time series.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 27, 'page_label': '28'}, page_content='NOTE\\nFor the exact code and usage, please refer to the\\nassociated Jupyter notebooks.\\nLet\\'s see how we can use a sinusoidal function to create\\ncyclicity. There is a helpful function in TimeSynth called\\ngenerate_timeseries that helps us combine signals and\\ngenerate time series. Have a look at the following code snippet:\\n#Sinusoidal Signal with Amplitude=1.5 & Frequency\\nsignal_1 =ts.signals.Sinusoidal(amplitude=1.5, fr\\n#Sinusoidal Signal with Amplitude=1 & Frequency=0\\nsignal_2 = ts.signals.Sinusoidal(amplitude=1, fre\\n#Generating the time series\\nsamples_1, regular_time_samples, signals_1, error\\nsamples_2, regular_time_samples, signals_2, error\\nplot_time_series(regular_time_samples,\\n                 [samples_1, samples_2],\\n                 \"Sinusoidal Waves\",\\n                 legends=[\"Amplitude = 1.5 | Freq\\nHere is the output:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 28, 'page_label': '29'}, page_content='Figure 1.5 – Sinusoidal waves\\nNote the two sinusoidal waves are diﬀerent with respect to the\\nfrequency (how fast the time series crosses zero) and amplitude\\n(how far away from zero the time series travels).\\nTimeSynth also has another signal called PseudoPeriodic. This\\nis like the Sinusoidal class, but the frequency and amplitude\\nitself has some stochasticity. We can see in the following code\\nsnippet that this is more realistic than the vanilla sine and\\ncosine waves from the Sinusoidal class:\\n# PseudoPeriodic signal with Amplitude=1 & Freque\\nsignal = ts.signals.PseudoPeriodic(amplitude=1, f\\n#Generating Timeseries\\nsamples, regular_time_samples, signals, errors ='),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 29, 'page_label': '30'}, page_content='plot_time_series(regular_time_samples,\\n                 samples,\\n                 \"Pseudo Periodic\")\\nHere is the output:\\nFigure 1.6 – Pseudo-periodic signal\\nAutoregressive signals\\nAnother very popular signal in the real world is an\\nautoregressive (AR) signal. We will go into this in more detail\\nin Chapter 4, Setting a Strong Baseline Forecast, but for now, an\\nAR signal refers to when the value of a time series for the\\ncurrent timestep is dependent on the values of the time series\\nin the previous timesteps. This serial correlation is a key'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 30, 'page_label': '31'}, page_content='property of the AR signal, and it is parametrized by a few\\nparameters, outlined as follows:\\nOrder of serial correlation—or, in other words, the number of\\nprevious timesteps the signal is dependent on\\nCoeﬃcients to combine the previous timesteps\\nLet\\'s see how we can generate an AR signal and see what it\\nlooks like, as follows:\\n# We have re-implemented the class in src because\\nfrom src.synthetic_ts.autoregressive import AutoR\\n# Autoregressive signal with parameters 1.5 and -\\n# y(t) = 1.5*y(t-1) - 0.75*y(t-2)\\nsignal= AutoRegressive(ar_param=[1.5, -0.75])\\n#Generate Timeseries\\nsamples, regular_time_samples, signals, errors = \\nplot_time_series(regular_time_samples,\\n                 samples,\\n                 \"Auto Regressive\")\\nHere is the output:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 31, 'page_label': '32'}, page_content=\"Figure 1.7 – AR signal\\nMix and match\\nThere are many more components you can use to create your\\nDGP and thereby generate a time series, but let's quickly look at\\nhow we can combine the components we have already seen to\\ngenerate a realistic time series.\\nLet's use a pseudo-periodic signal with white noise and\\ncombine it with an AR signal, as follows:\\n#Generating Pseudo Periodic Signal\\npseudo_samples, regular_time_samples, _, _ = gene\\n# Generating an Autoregressive Signal\\nar_samples, regular_time_samples, _, _ = generate\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 32, 'page_label': '33'}, page_content='# Combining the two signals using a mathematical \\nts = pseudo_samples*2+ar_samples\\nplot_time_series(regular_time_samples,\\n                 ts,\\n                 \"Pseudo Periodic with AutoRegres\\nHere is the output:\\nFigure 1.8 – Pseudo-periodic signal with AR and white noise\\nStationary and non-stationary time series\\nIn time series, stationarity is of great signiﬁcance and is a key\\nassumption in many modeling approaches. Ironically, many (if\\nnot most) real-world time series are non-stationary. So, let\\'s'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 33, 'page_label': '34'}, page_content=\"understand what a stationary time series is from a layman's\\npoint of view.\\nThere are multiple ways to look at stationarity, but one of the\\nclearest and most intuitive ways is to think of the probability\\ndistribution or the data distribution of a time series. We call a\\ntime series stationary when the probability distribution\\nremains the same at every point in time. In other words, if you\\npick diﬀerent windows in time, the data distribution across all\\nthose windows should be the same.\\nA standard Gaussian distribution is deﬁned by two parameters\\n—the mean and the variance. So, there are two ways the\\nstationarity assumption can be broken, as outlined here:\\nChange in mean over time\\nChange in variance over time\\nLet's look at these assumptions in detail and understand them\\nbetter.\\nChange in mean over time\\nThis is the most popular way a non-stationary time series\\npresents itself. If there is an upward/downward trend in the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 34, 'page_label': '35'}, page_content=\"time series, the mean across two windows of time would not be\\nthe same.\\nAnother way non-stationarity manifests itself is in the form of\\nseasonality. Suppose we are looking at the time series of\\naverage temperature measurements in a month for the last 5\\nyears. From our experience, we know that temperature peaks\\nduring summer and falls in winter. So, when we take the mean\\ntemperature of winter and mean temperature of summer, they\\nwill be diﬀerent.\\nLet's generate a time series with trend and seasonality and see\\nhow it manifests, as follows:\\n# Sinusoidal Signal with Amplitude=1 & Frequency=\\nsignal=ts.signals.Sinusoidal(amplitude=1, frequen\\n# White Noise with standard deviation = 0.3\\nnoise=ts.noise.GaussianNoise(std=0.3)\\n# Generate the time series\\nsinusoidal_samples, regular_time_samples, _, _ = \\n# Regular_time_samples is a linear increasing tim\\ntrend = regular_time_samples*0.4\\n# Combining the signal and trend\\nts = sinusoidal_samples+trend\\nplot_time_series(regular_time_samples,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 35, 'page_label': '36'}, page_content='ts,\\n                 \"Sinusoidal with Trend and White\\nHere is the output:\\nFigure 1.9 – Sinusoidal signal with trend and white noise\\nIf you examine the time series in Figure 1.9, you will be able to\\nsee a deﬁnite trend and the seasonality, which together make\\nthe mean of the data distribution change wildly across diﬀerent\\nwindows of time.\\nChange in variance over time'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 36, 'page_label': '37'}, page_content=\"Non-stationarity can also present itself in the ﬂuctuating\\nvariance of a time series. If the time series starts oﬀ with low\\nvariance and as time progresses, the variance keeps getting\\nbigger and bigger, we have a non-stationary time series. In\\nstatistics, there is a scary name for this phenomenon—\\nheteroscedasticity. The Air Passengers dataset, which is the\\n'iris dataset' of time series (most popular, over-used, and use-\\nless) is a classic example of heteroscedastic time series. Let's\\nlook at the plot.\\nFigure 1.10 – Air Passengers Dataset – Example of\\nHeteroscedastic Time Series\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 37, 'page_label': '38'}, page_content=\"In the ﬁgure, you can see that the seasonal peaks keeps getting\\nwider and wider as we move along the time and this is a classic\\nsign that the time series is heteroscedastic. But all\\nheteroscedastic time series aren't easy to spot. We have\\nstatistical tests to check for each of the stationarity cases which\\nwe will cover in Chapter 7 – Target Transformations for Time\\nSeries Forecasting.\\nThis book just tries to give you intuition about stationary versus\\nnon-stationary time series. There is a lot of statistical theory\\nand depth in this discussion that we are skipping over to keep\\nour focus on the practical aspects of time series.\\nArmed with the mental model of the DGP, we are at the right\\nplace to think about another important question: What can we\\nforecast?\\nWhat can we forecast?\\nBefore we move ahead, there is another aspect of time series\\nforecasting that we have to understand—the predictability of a\\ntime series. The most basic assumption when we forecast a time\\nseries is that the future depends on the past. But not all time\\nseries are equally predictable.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 38, 'page_label': '39'}, page_content=\"Let's take a look at a few examples and try to rank these in\\norder of predictability (from easiest to hardest), as follows:\\nHigh tide next Monday\\nLottery numbers next Sunday\\nThe stock price of Tesla next Friday\\nIntuitively, it is very easy for us to rank them. High tide next\\nMonday is going to be the easiest to predict because it is so\\npredictable, the lottery numbers are going to be very hard to\\npredict because these are pretty much random, and the stock\\nprice of Tesla next Friday is going to be diﬃcult to predict, but\\nnot impossible.\\nNOTE\\nHowever, for people thinking that they can forecast\\nstock prices with the advanced techniques covered in\\nthe book and get rich, that won't happen(most likely).\\nAlthough it is worthy of a lengthy discussion, we can\\nsummarize the key points in a short paragraph.\\nShare prices are not a function of their past values but an\\nanticipation of their future values, and this thereby violates our\\nﬁrst assumption while forecasting. And if that is not bad\\nenough, ﬁnancial stock prices typically have a very low signal-\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 39, 'page_label': '40'}, page_content=\"to-noise ratio. The ﬁnal wrench in the process is the eﬃcient-\\nmarket hypothesis (EMH). This seemingly innocent hypothesis\\nproclaims that all known information about a stock price is\\nalready factored into the price of the stock. The implication of\\nthe hypothesis is that if you can forecast accurately, many\\nothers will also be able to do that, and thereby the market price\\nof the stock already reﬂects the change in price that this\\nforecast brought about.\\nThe M6 competition chose to tackle this problem head-on to\\nevaluate if the EMH holds true by conducting a year long\\nforecasting and investment strategy competition. Although not\\nconclusive, the results show that the EMH holds true for the\\nvast majority of the participants, barring a few top teams. And\\neven in that, they found out that in the top teams, there was no\\nsigniﬁcant correlation between forecasting accuracies and the\\nselection of stocks into the portfolio, i.e. the teams weren't\\nchoosing stocks which they were able to forecast better (Full\\nreport is linked in Further Reading).\\nComing back to the topic at hand—predictability—three main\\nfactors form a mental model for this, as follows:\\nUnderstanding the DGP: The better you understand the\\nDGP, the higher the predictability of a time series.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 40, 'page_label': '41'}, page_content='Amount of data: The more data you have, the better your\\npredictability is.\\nAdequately repeating pattern: For any mathematical model\\nto work well, there should be an adequately repeating\\npattern in your time series. The more repeatable the pattern\\nis, the better your predictability is.\\nEven though you have a mental model of how to think about\\npredictability, we will look at more concrete ways of assessing\\nthe predictability of time series in Chapter 3, Analyzing and\\nVisualizing Time Series Data, but the key takeaway is that not all\\ntime series are equally predictable.\\nIn order to fully follow the discussion in the coming chapters,\\nwe need to establish a standard notation and get updated on\\nterminology that is speciﬁc to time series analysis.\\nForecasting terminology\\nThere are a few terminologies that will help you follow the\\nbook as well as other literature on time series. These are\\ndescribed in more detail here:\\nForecasting'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 41, 'page_label': '42'}, page_content='Forecasting is the prediction of future values of a time series\\nusing the known past values of the time series and/or some\\nother related variables. This is very similar to prediction in ML\\nwhere we use a model to predict unseen data.\\nMultivariate forecasting\\nMultivariate time series consist of more than one time series\\nvariable that is not only dependent on its past values but also\\nhas some dependency on the other variables. For example, a set\\nof macroeconomic indicators such as gross domestic product\\n(GDP), inﬂation, and so on of a particular country can be\\nconsidered as a multivariate time series. The aim of\\nmultivariate forecasting is to come up with a model that\\ncaptures the interrelationship between the diﬀerent variables\\nalong with its relationship with its past and forecast all the time\\nseries together in the future.\\nExplanatory forecasting\\nIn addition to the past values of a time series, we might use\\nsome other information to predict the future values of a time\\nseries. For example, for predicting retail store sales,\\ninformation regarding promotional oﬀers (both historical and\\nfuture ones) is usually helpful. This type of forecasting, which'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 42, 'page_label': '43'}, page_content='uses information other than its own history, is called\\nexplanatory forecasting.\\nBacktesting\\nSetting aside a validation set from your training data to\\nevaluate your models is a practice that is common in the ML\\nworld. Backtesting is the time series equivalent of validation,\\nwhereby you use the history to evaluate a trained model. We\\nwill cover the diﬀerent ways of doing validation and cross-\\nvalidation for time series data later.\\nIn-sample and out-sample\\nAgain, drawing parallels with ML, in-sample refers to training\\ndata and out-sample refers to unseen or testing data. When you\\nhear in-sample metrics, this is referring to metrics calculated on\\ntraining data, and out-sample metrics is referring to metrics\\ncalculated on testing data.\\nExogenous and endogenous variables\\nExogenous variables are parallel time series variables that are\\nnot modeled directly for output but used to help us model the\\ntime series that we are interested in. Typically, exogenous\\nvariables are not aﬀected by other variables in the system.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 43, 'page_label': '44'}, page_content='Endogenous variables are variables that are aﬀected by other\\nvariables in the system. A purely endogenous variable is a\\nvariable that is entirely dependent on the other variables in the\\nsystem. Relaxing the strict assumptions a bit, we can consider\\nthe target variable as the endogenous variable and the\\nexplanatory regressors we include in the model as exogenous\\nvariables.\\nForecast combination\\nForecast combinations in the time series world are similar to\\nensembles from the ML world. It is a process by which we\\ncombine multiple forecasts by using some function, either\\nlearned or heuristic-based, such as a simple average of three\\nforecast models.\\nThere are a lot more terms speciﬁc to time series, some of\\nwhich we will be covering throughout the book. But to start\\nwith a basic familiarity in the ﬁeld, these terms should be a\\ngood starting point.\\nSummary\\nWe had our ﬁrst dip into time series as we understood the\\ndiﬀerent types of time series, looked at how a DGP generates a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 44, 'page_label': '45'}, page_content='time series, and saw how we can think about the important\\nquestion: How well can we forecast a time series? We also had a\\nquick review of the terminology and notation required to\\nunderstand the rest of the book. In the next chapter, we will be\\ngetting our hands dirty and will learn how to work with time\\nseries data, how to preprocess a time series, how to handle\\nmissing data and outliers, and so on. If you have not set up the\\nenvironment yet, take a break and put some time into doing\\nthat.\\nFurther reading\\nA Survey on Principles, Models and Methods for Learning from\\nIrregularly Sampled Time Series: From Discretization to\\nAttention and Invariance by S.N. Shukla and B.M. Marlin\\n(2020): https://arxiv.org/abs/2012.00168\\nLearning from Irregularly-Sampled Time Series: A Missing\\nData Perspective by S.C. Li and B.M. Marlin (2020), ICML:\\nhttps://arxiv.org/abs/2008.07599\\nThe M6 forecasting competition: Bridging the gap between\\nforecasting and investment decisions by Spyros Makridakis et\\nal. (2023) https://arxiv.org/abs/2310.13357'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 45, 'page_label': '46'}, page_content='2 Acquiring and Processing Time Series\\nData'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 46, 'page_label': '47'}, page_content=\"Join our book community on Discord\\nhttps://packt.link/EarlyAccess/\\nIn the previous chapter, we learned what a time series is and established\\na few standard notations and terminologies. Now, let's switch tracks from\\ntheory to practice. In this chapter, we are going to get our hands dirty\\nand start working with data. Although we said time series data is\\neverywhere, we are still yet to get our hands dirty with a few time series\\ndatasets. We are going to start working on the dataset we have chosen to\\nwork on throughout this book, process it in the right way, and learn\\nabout a few techniques for dealing with missing values.\\nIn this chapter, we will cover the following topics:\\nUnderstanding the time series dataset\\npandas datetime operations, indexing, and slicing – a refresher\\nHandling missing data\\nMapping additional information\\nSaving and loading ﬁles to disk\\nHandling longer periods of missing data\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 47, 'page_label': '48'}, page_content=\"Technical requirements\\nYou will need to set up the Anaconda environment following the\\ninstructions in the Preface of the book to get a working environment with\\nall the libraries and datasets required for the code in this book. Any\\nadditional library will be installed while running the notebooks.\\nThe code for this chapter can be found at\\nhttps://github.com/PacktPublishing/Modern-Time-Series-Forecasting-\\nwith-Python-/tree/main/notebooks/Chapter02.\\nHandling time series data is like handling other tabular datasets, but\\nwith a focus on the temporal dimension. As with any tabular dataset,\\npandas is perfectly equipped to handle time series data as well.\\nLet's start getting our hands dirty and work through a dataset from the\\nbeginning. We are going to use the London Smart Meters dataset\\nthroughout this book. If you have not downloaded the data already as\\npart of the environment setup, go to the Preface and do that now.\\nUnderstanding the time series dataset\\nThis is the key ﬁrst step in any new dataset you come across, even before\\nExploratory Data Analysis (EDA), which we will be covering in\\nChapter 3, Analyzing and Visualizing Time Series Data. Understanding\\nwhere the data is coming from, the data generating process behind it,\\nand the source domain is essential to having a good understanding of the\\ndataset.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 48, 'page_label': '49'}, page_content=\"London Data Store, a free and open data-sharing portal, provided this\\ndataset, which was collected and enriched by Jean-Michel D and\\nuploaded on Kaggle.\\nThe dataset contains energy consumption readings for a sample of 5,567\\nLondon households that took part in the UK Power Networks-led Low\\nCarbon London project between November 2011 and February 2014.\\nReadings were taken at half-hourly intervals. Some metadata about the\\nhouseholds is also available as part of the dataset. Let's look at what\\nmetadata is available as part of the dataset:\\nCACI UK segmented the UK's population into demographic types, called\\nAcorn. For each household in the data, we have the corresponding\\nAcorn classiﬁcation. The Acorn classes (Lavish Lifestyles, City\\nSophisticates, Student Life, and so on) are grouped into parent classes\\n(Aﬄuent Achievers, Rising Prosperity, Financially Stretched, and so\\non). A full list of Acorn classes can be found in Table 2.1. The complete\\ndocumentation detailing each class is available at\\nhttps://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf.\\nThe dataset contains two groups of customers – one group who was\\nsubjected to dynamic time-of-use (dToU) energy prices\\nthroughout 2013, and another group who were on ﬂat-rate tariﬀs. The\\ntariﬀ prices for the dToU were given a day ahead via the Smart Meter\\nIHD or via text message.\\nJean-Michel D also enriched the dataset with weather and UK bank\\nholidays data.\\nThe following table shows the Acorn classes:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 49, 'page_label': '50'}, page_content='Table 2.1 – ACORN classiﬁcation'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 50, 'page_label': '51'}, page_content='NOTE\\nThe Kaggle dataset also preprocesses the time series data daily\\nand combines all the separate ﬁles. Here, we will ignore those\\nﬁles and start with the raw ﬁles, which can be found in the\\nhhblock_dataset folder. Learning to work with the raw ﬁles\\nis an integral part of working with real-world datasets in the\\nindustry.\\nPreparing a data model\\nOnce we understand where the data is coming from, we can look at the\\ndata, understand the information present in the diﬀerent ﬁles, and ﬁgure\\nout a mental model of how to relate the diﬀerent ﬁles. You may call it old\\nschool, but Microsoft Excel is an excellent tool for gaining this ﬁrst-level\\nunderstanding. If the ﬁle is too big to open in Excel, we can also read it in\\nPython and save a sample of the data to an Excel ﬁle and open it.\\nHowever, keep in mind that Excel sometimes messes with the format of\\nthe data, especially dates, so we need to take care to not save the ﬁle and\\nwrite back the formatting changes Excel made. If you are allergic to\\nExcel, you can do it in Python as well, albeit with a lot more keystrokes.\\nThe purpose of this exercise is to see what the diﬀerent data ﬁles contain,\\nexplore the relationship between the diﬀerent ﬁles, and so on. We can\\nmake this more formal and explicit by drawing a data model, similar to\\nthe one shown in the following diagram:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 51, 'page_label': '52'}, page_content=\"Figure 2.1 – Data model of the London Smart Meters dataset\\nThe data model is more for us to understand the data rather than any\\ndata engineering purpose. Therefore, it only contains bare-minimum\\ninformation, such as the key columns on the left and the sample data on\\nthe right. We also have arrows connecting diﬀerent ﬁles, with keys used\\nto link the ﬁles.\\nLet's look at a few key column names and their meanings:\\nLCLid: The unique consumer ID for a household\\nstdorTou: Whether the household has dToU or standard\\nAcorn: The ACORN class\\nAcorn_grouped: The ACORN group\\nfile: The block number\\nEach LCLid has a unique time series attached to it. The time series ﬁle is\\nformatted in a slightly tricky format – each day, there will be 48\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 52, 'page_label': '53'}, page_content='observations at a half-hourly frequency in the columns of the ﬁle.\\nNOTEBOOK ALERT\\nTo follow along with the complete code, use the\\n01 - Pandas Refresher & Missing Values Treatment.ipynb\\nnotebook in the chapter01 folder.\\nBefore we start working with our dataset, there are a few concepts we\\nneed to establish. One of them is a concept in pandas DataFrames,\\nwhich is of utmost importance – the pandas datetime properties and\\nindex. Let\\'s quickly look at a few pandas concepts that will be useful.\\nNOTE\\nIf you are familiar with the datetime manipulations in pandas,\\nfeel free to skip ahead to the next section.\\npandas datetime operations, indexing, and\\nslicing – a refresher\\nInstead of using our dataset, which is slightly complex, let\\'s pick an easy,\\nwell-formatted stock exchange price dataset from the UCI Machine\\nLearning Repository and look at the functionality of pandas:\\ndf = pd.read_excel(\"https://archive.ics.uci.edu/ml/machi\\nThe DataFrame that we read looks as follows:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 53, 'page_label': '54'}, page_content='Figure 2.2 – The DataFrame with stock exchange prices\\nNow that we have read the DataFrame, let\\'s start manipulating it.\\nConverting the date columns into\\npd.Timestamp/DatetimeIndex\\nFirst, we must convert the date column (which may not always be parsed\\nas dates automatically by pandas) into pandas datetime. For that,\\npandas has a handy function called pd.to_datetime. It infers the\\ndatetime format automatically and converts the input into a\\npd.Timestamp, if the input is a string, or into a DatetimeIndex if\\nthe input is a list of strings. So, if we pass a single date as a string,\\npd.to_datetime converts it into pd.Timestamp, while if we pass a list\\nof dates, it converts it into DatetimeIndex:\\n>>> pd.to_datetime(\"13-4-1987\").strftime(\"%d, %B %Y\")\\n\\'13, April 1987\\'\\nNow, let\\'s look at a case where the automatic parsing fails. The date is\\nJanuary 4, 1987. Let\\'s see what happens when we pass the string to the\\nfunction:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 54, 'page_label': '55'}, page_content='>>> pd.to_datetime(\"4-1-1987\").strftime(\"%d, %B %Y\")\\n\\'01, April 1987\\'\\nWell, that wasn\\'t expected, right? But if you think about it, anyone can\\nmake that mistake because we are not telling the computer whether the\\nmonth or the day comes ﬁrst, and pandas assumes the month comes ﬁrst.\\nLet\\'s rectify that:\\n>>> pd.to_datetime(\"4-1-1987\", dayfirst=True).strftime(\"\\n\\'04, January 1987\\'\\nAnother case where automatic date parsing fails is when the date string\\nis in a non-standard form. In that case, we can provide a strftime\\nformatted string to help pandas parse the dates correctly:\\n>>> pd.to_datetime(\"4|1|1987\", format=\"%d|%m|%Y\").strfti\\n\\'04, January 1987\\'\\nA full list of strftime conventions can be found at https://strftime.org/.\\nPRACTITIONER\\'S TIP\\nBecause of the wide variety of data formats, pandas may infer\\nthe time incorrectly. While reading a ﬁle, pandas will try to parse\\nthe dates automatically and create an error. There are many\\nways we can control this behavior: we can use the\\nparse_dates ﬂag to turn oﬀ date parsing, the date_parser'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 55, 'page_label': '56'}, page_content=\"argument to pass in a custom date parser, and year_first\\nand day_first to easily denote two popular formats of dates.\\nFrom version 2.0, pandas supports date_format which can be\\nused to pass in the exact format of the date as a python\\ndictionary with column name as the key.\\nOut of all these options, I prefer to use date_format, if using\\npandas >=2.0. We can keep parse_dates=True and then pass\\nin the exact date format using strftime conventions. This\\nensures that the date is parsed in the way we want it to be.\\nIf working with pandas <2.0, then I prefer to keep\\nparse_dates=False in both pd.read_csv and\\npd.read_excel to make sure pandas is not parsing the data\\nautomatically. After that, you can convert the date using the\\nformat parameter, which lets you explicitly set the date format\\nof the column using strftime conventions. There are two\\nother parameters in pd.to_datetime that will also make\\ninferring dates less error-prone – yearfirst and dayfirst.\\nIf you don't provide an explicit date format, at least provide one\\nof these.\\nNow, let's convert the date column in our stock prices dataset into\\ndatetime:\\ndf['date'] = pd.to_datetime(df['date'], yearfirst=True)\\nNow, the 'date' column, dtype, should be either datetime64[ns]\\nor <M8[ns], which are both pandas/NumPy native datetime formats.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 56, 'page_label': '57'}, page_content='But why do we need to do this?\\nIt\\'s because of the wide range of additional functionalities this unlocks.\\nThe traditional min() and max() functions will start working because\\npandas knows it is a datetime column:\\n>>> df.date.min(),df.date.max()\\n(Timestamp(\\'2009-01-05 00:00:00\\'), Timestamp(\\'2011-02-22\\nLet\\'s look at a few cool features the datetime format gives us.\\nUsing the .dt accessor and datetime properties\\nSince the column is now in date format, all the semantic information that\\nis encoded in the date can be used through pandas datetime properties.\\nWe can access many datetime properties, such as month, day_of_week,\\nday_of_year, and so on, using the .dt accessor:\\n>>> print(f\"\"\"\\n     Date: {df.date.iloc[0]}\\n     Day of year: {df.date.dt.day_of_year.iloc[0]}\\n     Day of week: {df.date.dt.dayofweek.iloc[0]}\\n     Month: {df.date.dt.month.iloc[0]}\\n     Month Name: {df.date.dt.month_name().iloc[0]}\\n     Quarter: {df.date.dt.quarter.iloc[0]}\\n     Year: {df.date.dt.year.iloc[0]}\\n     ISO Week: {df.date.dt.isocalendar().week.iloc[0]}\\n     \"\"\")\\nDate: 2009-01-05 00:00:00\\nDay of year: 5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 57, 'page_label': '58'}, page_content='Day of week: 0\\nMonth: 1\\nMonth Name: January\\nQuarter: 1\\nYear: 2009\\nISO Week: 2\\nAs of pandas 1.1.0, week_of_year has been deprecated because of the\\ninconsistencies it produces at the end/start of the year. Instead, the ISO\\nCalendar standards (which are commonly used in government and\\nbusiness) have been adopted and we can access the ISO calendar to get\\nthe ISO weeks.\\nSlicing and indexing\\nThe real fun starts when we make the date column the index of the\\nDataFrame. By doing this, you can use all the fancy slicing operations\\nthat pandas supports but on the datetime axis. Let\\'s take a look at few of\\nthem:\\n# Setting the index as the datetime column\\ndf.set_index(\"date\", inplace=True)\\n# Select all data after 2010-01-04(including)\\ndf[\"2010-01-04\":]\\n# Select all data between 2010-01-04 and 2010-02-06(not \\ndf[\"2010-01-04\": \"2010-02-06\"]\\n# Select data 2010 and before\\ndf[: \"2010\"]\\n# Select data between 2010-01 and 2010-06(both including\\ndf[\"2010-01\": \"2010-06\"]'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 58, 'page_label': '59'}, page_content='In addition to the semantic information and intelligent indexing and\\nslicing, pandas also provide tools for creating and manipulating date\\nsequences.\\nCreating date sequences and managing date oﬀsets\\nIf you are familiar with range in Python and np.arange in NumPy,\\nthen you will know they help us create integer/float sequences by\\nproviding a start point and an end point. pandas has something similar\\nfor datetime – pd.date_range. The function accepts start and end\\ndates, along with a frequency (daily or monthly, and so on) and creates\\nthe sequence of dates in between. Let\\'s look at a couple of ways of\\ncreating a sequence of dates:\\n# Specifying start and end dates with frequency\\npd.date_range(start=\"2018-01-20\", end=\"2018-01-23\", freq\\n# Output: [\\'2018-01-20\\', \\'2018-01-21\\', \\'2018-01-22\\', \\'20\\n# Specifying start and number of periods to generate in \\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"D\").a\\n# Output: [\\'2018-01-20\\', \\'2018-01-21\\', \\'2018-01-22\\', \\'20\\n# Generating a date sequence with every 2 days\\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"2D\").\\n# Output: [\\'2018-01-20\\', \\'2018-01-22\\', \\'2018-01-24\\', \\'20\\n# Generating a date sequence every month. By default it \\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"M\").a\\n# Output: [\\'2018-01-31\\', \\'2018-02-28\\', \\'2018-03-31\\', \\'20\\n# Generating a date sequence every month, but month star\\npd.date_range(start=\"2018-01-20\", periods=4, freq=\"MS\").\\n# Output: [\\'2018-02-01\\', \\'2018-03-01\\', \\'2018-04-01\\', \\'20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 59, 'page_label': '60'}, page_content='We can also add or subtract days, months, and other values to/from dates\\nusing pd.TimeDelta:\\n# Add four days to the date range\\n(pd.date_range(start=\"2018-01-20\", end=\"2018-01-23\", fre\\n# Output: [\\'2018-01-24\\', \\'2018-01-25\\', \\'2018-01-26\\', \\'20\\n# Add four weeks to the date range\\n(pd.date_range(start=\"2018-01-20\", end=\"2018-01-23\", fre\\n# Output: [\\'2018-02-17\\', \\'2018-02-18\\', \\'2018-02-19\\', \\'20\\nThere are a lot of these aliases in pandas, including W, W-MON, MS, and\\nothers. The full list can be found at\\nhttps://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-\\noﬀset-aliases.\\nIn this section, we looked at a few useful features and operations we can\\nperform on datetime indices and know how to manipulate DataFrames\\nwith datetime columns. Now, let\\'s review a few techniques we can use to\\ndeal with missing data.\\nHandling missing data\\nWhile dealing with large datasets in the wild, you are bound to\\nencounter missing data. If it is not part of the time series, it may be part\\nof the additional information you collect and map. Before we jump the\\ngun and ﬁll it with a mean value or drop those rows, let\\'s think about a\\nfew aspects:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 60, 'page_label': '61'}, page_content=\"The ﬁrst consideration should be whether the missing data we are\\nworried about is missing or not. For that, we need to think about the\\nData Generating Process (DGP) (the process that is generating\\nthe time series). As an example, let's look at sales at a local\\nsupermarket. You have been given the point-of-sale (POS)\\ntransactions for the last 2 years and you are processing the data into a\\ntime series. While analyzing the data, you found that there are a few\\nproducts where there aren't any transactions for a few days. Now,\\nwhat you need to think about is whether the missing data is missing or\\nwhether there is some information that this missingness is giving you.\\nIf you don't have any transactions for a particular product for a day, it\\nwill appear as missing data while you are processing it, even though it\\nis not missing. What that tells us is that there were no sales for that\\nitem, and that you should ﬁll such missing data with zeros.\\nNow, what if you see that, every Sunday, the data is missing – that is,\\nthere is a pattern to the missingness. This becomes tricky because how\\nyou ﬁll in such gaps depends on the model that you intend to use. If\\nyou ﬁll in such gaps with zeros, a model that looks at the immediate\\npast to predict the future might be thrown oﬀ, especially for Monday.\\nHowever, if you tell the model that the previous day was Sunday, then\\nthe model still can learn to tell the diﬀerence.\\nLastly, what if you see zero sales on one of the best-selling products\\nthat always gets sold? This can happen because of something such as a\\nPOS machine malfunction, a data entry mistake, or an out-of-stock\\nsituation. These types of missing values can be imputed with a few\\ntechniques.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 61, 'page_label': '62'}, page_content=\"Let's look at an Air Quality dataset published by the ACT Government,\\nCanberra, Australia under the CC by Attribution 4.0 International License\\n(https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-\\nData/94a5-zqnn) and see how we can impute such values using pandas\\n(there are more sophisticated techniques available, all of which will be\\ncovered later in this chapter).\\nPRACTITIONER'S TIP\\nWhen reading data using a method such as read_csv, pandas\\nprovides a few handy ways to handle missing values. pandas\\ntreats many values such as #N/A, null, and so on as NaN by\\ndefault. We can control this list of allowable NaN values using\\nthe na_values and keep_default_na parameters.\\nWe have chosen region Monash and PM2.5 readings and artiﬁcially\\nintroduced some missing values, as shown in the following diagram:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 62, 'page_label': '63'}, page_content=\"Figure 2.3 – Missing values in the Air Quality dataset\\nNow, let's look at a few simple techniques we can use to ﬁll the missing\\nvalues:\\nLast Observation Carried Forward or Forward Fill: This\\nimputation technique takes the last observed value and uses that to ﬁll\\nall the missing values until it ﬁnds the next observation. It is also\\ncalled forward ﬁll. We can do this like so:\\ndf['pm2_5_1_hr'].ffill()\\nNext Observation Carried Backward of Backward Fill: This\\nimputation technique takes the next observation and backtracks to ﬁll\\nin all the missing values with this value. This is also called backward\\nﬁll. Let's see how we can do this in pandas:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 63, 'page_label': '64'}, page_content=\"df['pm2_5_1_hr'].bfill()\\nMean Value Fill: This imputation technique is also pretty simple.\\nWe calculate the mean of the entire series and wherever we ﬁnd\\nmissing values, we ﬁll it with the mean value:\\ndf['pm2_5_1_hr'].fillna(df['pm2_5_1_hr'].mean())\\nLet's plot the imputed lines we get from using these three techniques:\\nFigure 2.4 – Imputed missing values using forward, backward, and mean\\nvalue ﬁll\\nAnother family of imputation techniques covers interpolation:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 64, 'page_label': '65'}, page_content='Linear Interpolation: Linear interpolation is just like drawing a\\nline between the two observed points and ﬁlling the missing values so\\nthat they lie on this line. This is how we do it:\\ndf[\\'pm2_5_1_hr\\'].interpolate(method=\"linear\")\\nNearest Interpolation: This is intuitively like a combination of\\nthe forward and backward ﬁll. For each missing value, the closest\\nobserved value is found and is used to ﬁll in the missing value:\\ndf[\\'pm2_5_1_hr\\'].interpolate(method=\"nearest\")\\nLet\\'s plot the two interpolated lines:\\nFigure 2.5 – Imputed missing values using linear and nearest interpolation'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 65, 'page_label': '66'}, page_content='There are a few non-linear interpolation techniques as well:\\nSpline, Polynomial, and Other Interpolations: In addition\\nto linear interpolation, pandas also supports non-linear interpolation\\ntechniques that call a SciPy routine at the backend. Spline and\\npolynomial interpolations are similar. They ﬁt a spline/polynomial of a\\ngiven order to the data and use that to ﬁll in missing values. While\\nusing spline or polynomial as the method in interpolate, we\\nshould always provide order as well. The higher the order, the more\\nﬂexible the function that is used will be to ﬁt the observed points. Let\\'s\\nsee how we can use spline and polynomial interpolation:\\ndf[\\'pm2_5_1_hr\\'].interpolate(method=\"spline\", order=2)\\ndf[\\'pm2_5_1_hr\\'].interpolate(method=\"polynomial\", order=\\nLet\\'s plot these two non-linear interpolation techniques:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 66, 'page_label': '67'}, page_content=\"Figure 2.6 – Imputed missing values using spline and polynomial\\ninterpolation\\nFor a complete list of interpolation techniques supported by\\ninterpolate, go to https://pandas.pydata.org/pandas-\\ndocs/stable/reference/api/pandas.Series.interpolate.html and\\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.inte\\nrp1d.html#scipy.interpolate.interp1d.\\nNow that we are more comfortable with the way pandas manages\\ndatetime, let's go back to our dataset and convert the data into a more\\nmanageable form.\\nNOTEBOOK ALERT\\nTo follow along with the complete code for pre-processing, use\\nthe\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 67, 'page_label': '68'}, page_content=\"02 - Preprocessing London Smart Meter Dataset.ipynb\\nnotebook in the chapter01 folder.\\nConverting the half-hourly block-level data (hhblock) into\\ntime series data\\nBefore we start processing, let's understand a few general categories of\\ninformation we will ﬁnd in a time series dataset:\\nTime Series Identiﬁers: These are identiﬁers for a particular time\\nseries. It can be a name, an ID, or any other unique feature – for\\nexample, the SKU name or the ID of a retail sales dataset or the\\nconsumer ID in the energy dataset that we are working with are all\\ntime series identiﬁers.\\nMetadata or Static Features: This information does not vary with\\ntime. An example of this is the ACORN classiﬁcation of the household\\nin our dataset.\\nTime-Varying Features: This information varies with time – for\\nexample, the weather information. For each point in time, we have a\\ndiﬀerent value for weather, unlike the Acorn classiﬁcation.\\nNext. Let's discuss formatting of a dataset.\\nCompact, expanded, and wide forms of data\\nThere are many ways to format a time series dataset, especially a dataset\\nwith many related time series, like the one we have now. Apart from the\\nstandard terminology of wide data, we can also look at two non-\\nstandard ways of formatting time series data. Although there is no\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 68, 'page_label': '69'}, page_content='standard nomenclature for those, we will refer to them as compact and\\nexpanded in this book.\\nCompact form data is when any particular time series occupies only a\\nsingle row in the pandas DataFrame – that is, the time dimension is\\nmanaged as an array within a DataFrame row. The time series identiﬁers\\nand the metadata occupy the columns with scalar values and then the\\ntime series values; other time-varying features occupy the columns with\\nan array. Two additional columns are included to extrapolate time –\\nstart_datetime and frequency. If we know the start datetime and the\\nfrequency of the time series, we can easily construct the time and\\nrecover the time series from the DataFrame. This only works for\\nregularly sampled time series. The advantage is that the DataFrames take\\nup much less memory and are easy and faster to work with:\\nFigure 2.7 – Compact form data\\nThe expanded form is when the time series is expanded along the rows\\nof a DataFrame. If there are n steps in the time series, it occupies n rows\\nin the DataFrame. The time series identiﬁers and the metadata get\\nrepeated along all the rows. The time-varying features also get expanded'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 69, 'page_label': '70'}, page_content='along the rows. And instead of the start date and frequency, we have the\\ndatetime as a column:\\nFigure 2.8 – Expanded form data\\nIf the compact form had a time series identiﬁer as the key, the time series\\nidentiﬁer and the datetime column would be combined and become the\\nkey.\\nWide-format data is more common in traditional time series literature. It\\ncan be considered a legacy format, which is limiting in many ways. Do\\nyou remember the stock data we saw earlier (Figure 2.2)? We have the\\ndate as an index or as one of the columns and the diﬀerent time series as\\ndiﬀerent columns of the DataFrame. As the number of time series\\nincreases, they become wider and wider, hence the name. This data\\nformat does not allow us to include any metadata about the time series.\\nFor instance, in our data, we have information about whether a\\nparticular household is under standard or dynamic pricing. There is no\\nway for us to include such metadata in the wide format. From an\\noperational perspective, the wide format also does not play well with'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 70, 'page_label': '71'}, page_content=\"relational databases because we have to keep adding columns to the\\ntable when we get new time series. We won't be using this format in this\\nbook.\\nEnforcing regular intervals in time series\\nOne of the ﬁrst things you should check and correct is whether the\\nregularly sampled time series data that you have has equal intervals of\\ntime. In practice, even regularly sampled time series have some samples\\nmissing in between because of some data collection error or some other\\npeculiar way data is collected. So, while working with the data, we will\\nmake sure we enforce regular intervals in the time series.\\nBEST PRACTICE\\nWhile working with datasets with multiple time series, it is best\\npractice to check the end dates of all the time series. If they are\\nnot uniform, we can align them with the latest date across all the\\ntime series in the dataset.\\nIn our smart meters dataset, some LCLid columns end much earlier\\nthan the rest. Maybe the household opted out of the program, or they\\nmoved out and left the house empty; the reason could be anything.\\nHowever, we need to handle that while we enforce regular intervals.\\nWe will learn how to convert the dataset into a time series format in the\\nnext section. The code for this process can be found in the\\n02 - Preprocessing London Smart Meter Dataset.ipynb\\nnotebook.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 71, 'page_label': '72'}, page_content=\"Converting the London Smart Meters dataset into a time\\nseries format\\nFor each dataset that you come across, the steps you would have to take\\nto convert it into either a compact or expanded form would be diﬀerent.\\nIt depends on how the original data is structured. Here, we will look at\\nhow the London Smart Meters dataset can be transformed so that we can\\ntransfer those learnings to other datasets.\\nThere are two steps we need to do before we can start processing the\\ndata into either compact or expanded form:\\n1. Find the Global End Date: We must ﬁnd the maximum date\\nacross all the block ﬁles so that we know the global end date of the\\ntime series.\\n2. Basic Preprocessing: If you remember how hhblock_dataset is\\nstructured, you will remember that each row had a date and that\\nalong the columns, we have half-hourly blocks. We need to reshape\\nthat into a long form, where each row has a date and a single half-\\nhourly block. It's easier to handle that way.\\nNow, let's deﬁne separate functions for converting the data into compact\\nand expanded forms and apply that function to each of the LCLid\\ncolumns. We will do this for each LCLid separately since the start date\\nfor each LCLid is diﬀerent.\\nExpanded form\\nThe function for converting into expanded form does the following:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 72, 'page_label': '73'}, page_content='1. Finds the start date.\\n2. Create a standard DataFrame using the start date and the global end\\ndate.\\n3. Left merges the DataFrame for LCLid to the standard DataFrame,\\nleaving the missing data as np.nan.\\n4. Returns the merged DataFrame.\\nOnce we have all the LCLid DataFrames, we must perform a couple of\\nadditional steps to complete the expanded form processing:\\n1. Concatenate all the DataFrames into a single DataFrame.\\n2. Create a column called oﬀset, which is the numerical representation of\\nthe half-hour blocks; for example, hh_3 → 3.\\n3. Create a timestamp by adding a 30-minute oﬀset to the day and\\ndropping the unnecessary columns.\\nFor one block, this representation takes up ~47 MB of memory.\\nCompact form\\nThe function for converting into compact form does the following:\\n1. Finds the start date and time series identiﬁers.\\n2. Creates a standard DataFrame using the start date and the global end\\ndate.\\n3. Left merges the DataFrame for LCLid to the standard DataFrame,\\nleaving the missing data as np.nan.\\n4. Sorts the values on the date.\\n5. Returns the time series array, along with the time series identiﬁer,\\nstart date, and the length of the time series.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 73, 'page_label': '74'}, page_content='Once we have this information for each LCLid, we can compile it into a\\nDataFrame and add 30min as the frequency.\\nFor one block, this representation takes up only ~0.002 MB of memory.\\nWe are going to use the compact form because it is easy to work with and\\nmuch less resource hungry.\\nMapping additional information\\nFrom the data model that we prepared earlier, we know that there are\\nthree key ﬁles that we have to map: Household Information, Weather, and\\nBank Holidays.\\nThe informations_households.csv ﬁle contains metadata about the\\nhousehold. There are static features that are not dependent on time. For\\nthis, we just need to left merge informations_households.csv to the\\ncompact form based on LCLid, which is the time series identiﬁer.\\nBEST PRACTICE\\nWhile doing a pandas merge, one of the most common and\\nunexpected outcomes is that the number of rows before and after\\nthe operation is not the same (even if you are doing a left merge).\\nThis typically happens because there are duplicates in the keys\\non which you are merging. As a best practice, you can use the\\nvalidate parameter in the pandas merge, which takes in\\ninputs such as one_to_one and many_to_one so that this\\ncheck is done while merging and will throw an error if the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 74, 'page_label': '75'}, page_content='assumption is not met. For more information, go to\\nhttps://pandas.pydata.org/docs/reference/api/pandas.merge.html\\nBank Holidays and Weather, on the other hand, are time-varying features\\nand should be dealt with accordingly. The most important aspect to keep\\nin mind is that while we map this information, it should perfectly align\\nwith the time series that we have already stored as an array.\\nuk_bank_holidays.csv is a ﬁle that contains the dates of the holidays\\nand the kind of holiday. The holiday information is quite important here\\nbecause the energy consumption patterns would be diﬀerent on a\\nholiday when the family members are at home spending time with each\\nother or watching television, and so on. Follow these steps to process this\\nﬁle:\\n1. Convert the date column into the datetime format and set it as the\\nindex of the DataFrame.\\n2. Using the resample function we saw earlier, we must ensure that the\\nindex is resampled every 30 minutes, which is the frequency of the\\ntimes series.\\n3. Forward ﬁll the holidays within a day and ﬁll in the rest of the NaN\\nvalues with NO_HOLIDAY.\\nNow, we have converted the holiday ﬁle into a DataFrame that has a row\\nfor each 30-minute interval. On each row, we have a column that\\nspeciﬁes whether that day was a holiday or not.\\nweather_hourly_darksky.csv is a ﬁle that is, once again, at the daily\\nfrequency. We need to downsample it to a 30-minute frequency because'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 75, 'page_label': '76'}, page_content=\"the data that we need to map to this is at a half-hourly frequency. If we\\ndon't do this, the weather will only be mapped to the hourly timestamps,\\nleaving the half-hourly timestamps empty.\\nThe steps we must follow to process this ﬁle are also similar to the way\\nwe processed holidays:\\n1. Convert the date column into the datetime format and set it as the\\nindex of the DataFrame.\\n2. Using the resample function, we must ensure that the index is\\nresampled every 30 minutes, which is the frequency of the times\\nseries.\\n3. Forward ﬁll the weather features to ﬁll the missing values that were\\ncreated while resampling.\\nNow that you have made sure the alignment between the time series and\\nthe time-varying features is ensured, you can loop over each of the time\\nseries and extract the weather and bank holiday array before storing it in\\nthe corresponding row of the DataFrame.\\nSaving and loading ﬁles to disk\\nThe fully merged DataFrame in its compact form takes up only ~10 MB.\\nBut saving this ﬁle requires a little bit of engineering. If we try to save\\nthe ﬁle in CSV format, it will not work because of the way we have stored\\narrays in pandas columns (since the data is in its compact form). We can\\nsave it in pickle or parquet format, or any of the binary forms of ﬁle\\nstorage. This can work, depending on the size of the RAM available in our\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 76, 'page_label': '77'}, page_content=\"machines. Although the fully merged DataFrame is just ~10 MB, saving it\\nin pickle format will make the size explode to ~15 GB.\\nWhat we can do is save this as a text ﬁle while making a few tweaks to\\naccommodate the column names, column types, and other metadata that\\nis required to read the ﬁle back into memory. The resulting ﬁle size on\\ndisk still comes out to ~15 GB but since we are doing it as an I/O\\noperation, we are not keeping all that data in our memory. We call this\\nthe time series (.ts) format. The functions for saving a compact form in\\n.ts format, reading the .ts format, and converting the compact form\\ninto expanded form are available in this book's GitHub repository under\\nsrc/data_utils.py.\\nIf you don't need to store all of the DataFrame in a single ﬁle, you can\\nsplit it into multiple chunks and save them individually in a binary\\nformat, such as parquet. For our datasets, let's follow this route and\\nsplit the whole DataFrame into chunks of blocks and save them as\\nparquet ﬁles. This is the best route for us because of a few reasons:\\nIt leverages the compression that comes with the format\\nIt reads in parts of the whole data for quick iteration and\\nexperimentation\\nThe data types are retained between the read and write operations,\\nleading to less ambiguity\\nNOTE\\nFor very large datasets, we can use some pandas alternatives which\\nmakes it easier to process datasets which are out of memory. Polars is a\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 77, 'page_label': '78'}, page_content='great library which has lazy loading and is very fast. And for truly huge\\ndatasets, pyspark with a distributed cluster might be the right choice.\\nNow that we have processed the dataset and stored it on disk, let\\'s read it\\nback into memory and look at a few more techniques to handle missing\\ndata.\\nHandling longer periods of missing data\\nWe saw some techniques for handling missing data earlier – forward and\\nbackward ﬁlling, interpolation, and so on. Those techniques usually work\\nif there are one or two missing data points. But if a large section of data\\nis missing, then these simple techniques fall short.\\nNOTEBOOK ALERT\\nTo follow along with the complete code for missing data\\nimputation, use the\\n03 - Handling Missing Data (Long Gaps).ipynb\\nnotebook in the chapter02 folder.\\nLet\\'s read blocks 0-7 parquet from memory:\\nblock_df = pd.read_parquet(\"data/london_smart_meters/pre\\nThe data that we have saved is in compact form. We need to convert it\\ninto expanded form because it is easier to work with time series data in\\nthat form. Since we only need a subset of the time series (for faster\\ndemonstration purposes), we are just extracting one block from these'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 78, 'page_label': '79'}, page_content='seven blocks. To convert compact form into expanded form, we can use a\\nhelpful function in src/utils/data_utils.py called\\ncompact_to_expanded:\\n#Converting to expanded form\\nexp_block_df = compact_to_expanded(block_df[block_df.fil\\nstatic_cols = [\"frequency\", \"series_length\", \"stdorToU\",\\ntime_varying_cols = [\\'holidays\\', \\'visibility\\', \\'windBear\\n       \\'pressure\\', \\'apparentTemperature\\', \\'windSpeed\\', \\'\\n       \\'humidity\\', \\'summary\\'],\\nts_identifier = \"LCLid\")\\nOne of the best ways to visualize the missing data in a group of related\\ntime series is by using a very helpful package called missingno:\\n# Pivot the data to set the index as the datetime and th\\nplot_df = pd.pivot_table(exp_block_df, index=\"timestamp\"\\n# Generate Plot. Since we have a datetime index, we can \\nmsno.matrix(plot_df, freq=\"M\")\\nThe preceding code produces the following output:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 79, 'page_label': '80'}, page_content='Figure 2.9 – Visualization of the missing data in block 7\\nIMPORTANT NOTE\\nOnly attempt the missingno visualization on related time\\nseries where there are less than 25 time series. If you have a\\ndataset that contains thousands of time series (such as in our\\nfull dataset), applying this visualization will give us an illegible\\nplot and a frozen computer.\\nThis visualization tells us a lot of things at a single glance. The Y-axis\\ncontains the dates that we are plotting the visualization for, while the X-\\naxis contains the columns, which in this case are the diﬀerent\\nhouseholds. We know that all the time series are not perfectly aligned –\\nthat is, not all of them start at the same time and end at the same time.\\nThe big white gaps we can see at the beginning of many of the time series\\nshow that data collection for those consumers started later than the\\nothers. We can also see that a few time series ﬁnish earlier than the rest,\\nwhich means either they stopped being consumers or the measurement'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 80, 'page_label': '81'}, page_content='phase stopped. There are also a few smaller white lines in many time\\nseries, which are real missing values. We can also notice a sparkline to\\nthe right, which is a compact representation of the number of missing\\ncolumns for each row. If there are no missing values (all time series have\\nsome value), then the sparkline would be at the far right. Finally, if there\\nare a lot of missing values, the line will be to the left.\\nJust because there are missing values, we are not going to ﬁll/impute\\nthem because the decision of whether to impute missing data or not\\ncomes later in the workﬂow. For some models, we do not need to do the\\nimputation, while for others, we do. There are multiple ways of imputing\\nmissing data and which one to choose is another decision we cannot\\nmake beforehand.\\nSo, for now, let\\'s pick one LCLid and dig deeper. We already know that\\nthere are some missing values between 2012-09-30 and 2012-10-31. Let\\'s\\nvisualize that period:\\n# Taking a single time series from the block\\nts_df = exp_block_df[exp_block_df.LCLid==\"MAC000193\"].se\\nmsno.matrix(ts_df[\"2012-09-30\": \"2012-10-31\"], freq=\"D\")\\nThe preceding code produces the following output:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 81, 'page_label': '82'}, page_content='Figure 2.10 – Visualization of missing data of MAC000193 between 2012-\\n09-30 and 2012-10-31\\nHere, we can see that the missing data is really between 2012-10-18 and\\n2012-10-19. Normally, we would go ahead and impute the missing data in\\nthis period, but since we are looking at this with an academic lens, we\\nwill take a slightly diﬀerent route. Let\\'s introduce an artiﬁcial missing\\ndata section and see how the diﬀerent techniques we are going to look at\\nimpute the missing data:\\n# The dates between which we are nulling out the time se\\nwindow = slice(\"2012-10-07\", \"2012-10-08\")\\n# Creating a new column and artificially creating missin\\nts_df[\\'energy_consumption_missing\\'] = ts_df.energy_consu\\nts_df.loc[window, \"energy_consumption_missing\"] = np.nan\\nNow, let\\'s plot the missing area in the time series:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 82, 'page_label': '83'}, page_content='Figure 2.11 – The energy consumption of MAC000193 between 2012-10-05\\nand 2012-10-10\\nWe are missing 2 whole days of energy consumption readings, which\\nmeans there are 96 missing data points (half-hourly). If we use one of the\\ntechniques we saw earlier, such as interpolation, we will see that it will\\nmostly be a straight line because none of the methods are complex\\nenough to capture the pattern over a long time.\\nThere are a few techniques that we can use to ﬁll in such large missing\\ngaps in data. We will cover these now.\\nImputing with the previous day\\nSince this is a half-hourly time series of energy consumption, it stands to\\nreason that there might be a pattern that is repeating day after day. The\\nenergy consumption between 9:00 A.M. and 10:00 A.M. might be higher\\nas everybody gets ready to go to the oﬃce and a slump during the day'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 83, 'page_label': '84'}, page_content='when most houses may be empty. So, the simplest way to ﬁll in the\\nmissing data would be to use the last day energy readings so that the\\nenergy reading at 10:00 A.M. 2012-10-18 can be ﬁlled with the energy\\nreading at 10:00 A.M. 2012-10-17:\\n#Shifting 48 steps to get previous day\\nts_df[\"prev_day\"] = ts_df[\\'energy_consumption\\'].shift(48\\n#Using the shifted column to fill missing\\nts_df[\\'prev_day_imputed\\'] =  ts_df[\\'energy_consumption_m\\nts_df.loc[null_mask,\"prev_day_imputed\"] = ts_df.loc[null\\nmae = mean_absolute_error(ts_df.loc[window, \"prev_day_im\\nLet\\'s see what the imputation looks like:\\nFigure 2.12 – Imputing with the previous day'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 84, 'page_label': '85'}, page_content='While this looks better, this is also very brittle. When we are copying the\\nprevious day, we are also assuming that any kind of variation or\\nanomalous behavior is also repeated. We can already see that the\\npatterns for the day before and the day after are not the same.\\nHourly average proﬁle\\nA better approach would be to calculate an hourly proﬁle from the data –\\nthe mean consumption for every hour – and use the average to ﬁll the\\nmissing data:\\n#Create a column with the Hour from timestamp\\nts_df[\"hour\"] = ts_df.index.hour\\n#Calculate hourly average consumption\\nhourly_profile = ts_df.groupby([\\'hour\\'])[\\'energy_consump\\nhourly_profile.rename(columns={\"energy_consumption\": \"ho\\n#Saving the index because it gets lost in merge\\nidx = ts_df.index\\n#Merge the hourly profile dataframe to ts dataframe\\nts_df = ts_df.merge(hourly_profile, on=[\\'hour\\'], how=\\'le\\nts_df.index = idx\\n#Using the hourly profile to fill missing\\nts_df[\\'hourly_profile_imputed\\'] = ts_df[\\'energy_consumpt\\nts_df.loc[null_mask,\"hourly_profile_imputed\"] = ts_df.lo\\nmae = mean_absolute_error(ts_df.loc[window, \"hourly_prof\\nLet\\'s see if this is better:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 85, 'page_label': '86'}, page_content='Figure 2.13 – Imputing with an hourly proﬁle\\nThis is giving us a much more generalized curve that does not have the\\nspikes that we saw for the individual days. The hourly ups and downs\\nhave also been captured as per our expectations. The\\nmean absolute error (MAE) is also lower than before.\\nThe hourly average for each weekday\\nWe can further reﬁne this rule by introducing a speciﬁc proﬁle for each\\nweekday. It stands to reason that the usage pattern on a weekday is not\\ngoing to be the same on a weekend. Hence, we can calculate the average\\nhourly consumption for each weekday separately so that we have one\\nproﬁle for Monday, another for Tuesday, and so on:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 86, 'page_label': '87'}, page_content='#Create a column with the weekday from timestamp\\nts_df[\"weekday\"] = ts_df.index.weekday\\n#Calculate weekday-hourly average consumption\\nday_hourly_profile = ts_df.groupby([\\'weekday\\',\\'hour\\'])[\\'\\nday_hourly_profile.rename(columns={\"energy_consumption\":\\n#Saving the index because it gets lost in merge\\nidx = ts_df.index\\n#Merge the day-hourly profile dataframe to ts dataframe\\nts_df = ts_df.merge(day_hourly_profile, on=[\\'weekday\\', \\'\\nts_df.index = idx\\n#Using the day-hourly profile to fill missing\\nts_df[\\'day_hourly_profile_imputed\\'] = ts_df[\\'energy_cons\\nts_df.loc[null_mask,\"day_hourly_profile_imputed\"] = ts_d\\nmae = mean_absolute_error(ts_df.loc[window, \"day_hourly_\\nLet\\'s see what this looks like:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 87, 'page_label': '88'}, page_content=\"Figure 2.14 – Imputing the hourly average for each weekday\\nThis looks very similar to the other one, but this is because the day we\\nare imputing is a weekday and the weekday proﬁles are similar. The MAE\\nis also lower than the day proﬁle. The weekend proﬁle is slightly\\ndiﬀerent, which you can see in the associated Jupyter Notebook.\\nSeasonal interpolation\\nAlthough calculating seasonal proﬁles and using them to impute works\\nwell, there are instances, especially when there is a trend in the time\\nseries, where such a simple technique falls short. The simple seasonal\\nproﬁle doesn't capture the trend at all and ignores it completely. For such\\ncases, we can do the following:\\n1. Calculate the seasonal proﬁle, similar to how we calculated the\\naverages earlier.\\n2. Subtract the seasonal proﬁle and apply any of the interpolation\\ntechniques we saw earlier.\\n3. Add the seasonal proﬁle back to the interpolated series.\\nThis process has been implemented in this book's GitHub repository in\\nthe src/imputation/interpolation.py ﬁle. We can use it as follows:\\nfrom src.imputation.interpolation import SeasonalInterpo\\n# Seasonal interpolation using 48*7 as the seasonal peri\\nrecovered_matrix_seas_interp_weekday_half_hour = Seasona\\nts_df['seas_interp_weekday_half_hour_imputed'] = recover\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 88, 'page_label': '89'}, page_content='The key parameter here is seasonal_period, which tells the algorithm\\nto look for patterns that repeat every seasonal_period. If we mention\\nseasonal_period=48, it will look for patterns that repeat every 48 data\\npoints. In our case, they are after each day (because we have 48 half-hour\\ntimesteps in a day). In addition to this, we need to specify what kind of\\ninterpolation we need to perform.\\nADDITIONAL INFORMATION\\nInternally, we use something called seasonal decomposition\\n(statsmodels.tsa.seasonal.seasonal_decompose),\\nwhich will be covered in Chapter 3, Analyzing and Visualizing\\nTime Series Data, to isolate the seasonality component.\\nHere, we have done seasonal interpolation using 48 (half-hourly) and\\n48*7 (weekday to half-hourly) and plotted the resulting imputation:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 89, 'page_label': '90'}, page_content='Figure 2.15 – Imputing with seasonal interpolation\\nHere, we can see that both have captured the seasonality patterns, but\\nthe half-hourly proﬁle every weekday has captured the peaks in the ﬁrst\\nday better, so they have a lower MAE. There is no improvement in terms\\nof hourly averages, mostly because there is no strong increasing or\\ndecreasing patterns in the time series.\\nWith this, we have come to the end of this chapter. We are now oﬃcially\\ninto the nitty-gritty of juggling time series data, cleaning it, and\\nprocessing it. Congratulations on ﬁnishing this chapter!\\nSummary\\nAfter a short refresher on pandas DataFrames, especially on the datetime\\nmanipulations and simple techniques for handling missing data, we'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 90, 'page_label': '91'}, page_content='learned about the two forms of storing and working with time series data\\n– compact and expanded. With all this knowledge, we took our raw\\ndataset and built a pipeline to convert it into compact form. If you have\\nrun the accompanying notebook, you should have the preprocessed\\ndataset saved on disk. We also had an in-depth look at some techniques\\nfor handling long gaps of missing data.\\nNow that we have the processed datasets, in the next chapter, we will\\nlearn how to visualize and analyze a time series dataset.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 91, 'page_label': '92'}, page_content='3 Analyzing and Visualizing Time\\nSeries Data'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 92, 'page_label': '93'}, page_content=\"Join our book community on Discord\\nhttps://packt.link/EarlyAccess/\\nIn the previous chapter, we learned where to obtain time series\\ndatasets, as well as how to manipulate time series data using\\npandas, handle missing values, and so on. Now that we have\\nthe processed time series data, it's time to understand the\\ndataset, which data scientists call Exploratory Data Analysis\\n(EDA). It is a process by which the data scientist analyzes the\\ndata by looking at aggregate statistics, feature distributions,\\nvisualizations, and so on to try and uncover patterns in the data\\nthat they can leverage in modeling. In this chapter, we will look\\nat a couple of ways to analyze a time series dataset, a few\\nspeciﬁc techniques that are tailor-made for time series, and\\nreview some of the visualization techniques for time series\\ndata.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 93, 'page_label': '94'}, page_content='In this chapter, we will cover the following topics:\\nComponents of a time series\\nVisualizing time series data\\nDecomposing a time series\\nDetecting and treating outliers\\nTechnical requirements\\nYou will need to set up the Anaconda environment following\\nthe instructions in the Preface of the book to get a working\\nenvironment with all the libraries and datasets required for the\\ncode in this book. Any additional library will be installed while\\nrunning the notebooks.\\nYou will need to run 02 - Preprocessing London Smart Meter\\nDataset.ipynb notebook from Chapter02 folder.\\nThe code for this chapter can be found at\\nhttps://github.com/PacktPublishing/Modern-Time-Series-\\nForecasting-with-Python-/tree/main/notebooks/Chapter03.\\nComponents of a time series\\nBefore we start analyzing and visualizing time series, we need\\nto understand the structure of a time series. Any time series can'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 94, 'page_label': '95'}, page_content='contain some or all of the following components:\\n1. Trend\\n2. Seasonal\\n3. Cyclical\\n4. Irregular\\nThese components can be mixed in diﬀerent ways, but two very\\ncommonly assumed ways are additive (Y = Trend + Seasonal +\\nCyclical + Irregular) and multiplicative (Y = Trend * Seasonal *\\nCyclical * Irregular).\\nThe trend component\\nThe trend is a long-term change in the mean of a time series. It\\nis the smooth and steady movement of a time series in a\\nparticular direction. When the time series moves upward, we\\nsay there is an upward or increasing trend, while when it moves\\ndownward, we say there is a downward or decreasing trend. At\\nthe time of writing, if we think about the revenue of Tesla over\\nthe years, as shown in the following ﬁgure, we can see that it\\nhas been increasing consistently for the last few years:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 95, 'page_label': '96'}, page_content=\"Figure 3.1 – Tesla's revenue in millions of USD\\nLooking at the preceding ﬁgure, we can say that Tesla's revenue\\nis having an increasing trend. The trend doesn't need to be\\nlinear; it can also be non-linear.\\nThe seasonal component\\nWhen a time series exhibits regular, repetitive, up-and-down\\nﬂuctuations, we call that seasonality. For instance, retail sales\\ntypically shoot up during the holidays, speciﬁcally Christmas in\\nwestern countries. Similarly, electricity consumption peaks\\nduring the summer months in the tropics and the winter\\nmonths in colder countries. In all these examples, you can see a\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 96, 'page_label': '97'}, page_content='speciﬁc up-and-down pattern repeating every year. Another\\nexample is sunspots, as shown in the following ﬁgure:\\nFigure 3.2 – Number of sunspots from 1749 to 2017\\nAs you can see, sunspots peak every 11 years.\\nThe cyclical component\\nThe cyclical component is often confused with seasonality, but\\nit stands apart due to a very subtle diﬀerence. Like seasonality,\\nthe cyclical component also exhibits a similar up-and-down\\npattern around the trend line, but instead of repeating the\\npattern every period, the cyclical component is irregular. A\\ngood example of this is economic recession, which happens'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 97, 'page_label': '98'}, page_content='over a 10-year cycle. However, this doesn\\'t happen like\\nclockwork; sometimes, it can be fewer or more than every 10\\nyears.\\nThe irregular component\\nThis component is left after removing the trends, seasonality,\\nand cyclicity from a time series. Traditionally, this component is\\nconsidered unpredictable and is also called the residual or error\\nterm. In common classical statistics-based models, the point of\\nany \"model\" is to capture all the other components to the point\\nthat the only part that is not captured is the irregular\\ncomponent. In modern machine learning, we do not consider\\nthis component entirely unpredictable. We try to capture this\\ncomponent, or parts of it, by using exogenous variables. For\\ninstance, the irregular component of retail sales may be\\nexplained as the diﬀerent promotional activities they run.\\nWhen we have this additional information, the \"unpredictable\"\\ncomponent starts to become predictable again. But no matter\\nhow many additional variables you add to the model, there will\\nalways be some component, which is the true irregular\\ncomponent (or true error), that is left behind.\\nNow that we know what the diﬀerent components of a time\\nseries are, let\\'s see how we can visualize them.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 98, 'page_label': '99'}, page_content='Visualizing time series data\\nIn Chapter 2, Acquiring and Processing Time Series Data, we\\nlearned how to prepare a data model as a ﬁrst step toward\\nanalyzing a new dataset. If preparing a data model is like\\napproaching someone you like and making that ﬁrst contact,\\nthen EDA is like dating that person. At this point, you have the\\ndataset, and you are trying to get to know them, trying to ﬁgure\\nout what makes them tick, what the person likes and dislikes,\\nand so on.\\nEDA often employs visualization techniques to uncover\\npatterns, spot anomalies, form and test hypotheses, and so on.\\nSpending some time understanding your dataset will help you a\\nlot when you are trying to squeeze out every last bit of\\nperformance from the models. You may understand what sort\\nof features you must create, or what kind of modeling\\ntechniques should be applied, and so on.\\nIn this chapter, we will cover a few visualization techniques\\nthat are well suited for time series datasets.\\nNOTEBOOK ALERT\\nTo follow along with the complete code for visualizing\\ntime series, use the 01-Visualizing Time Series.ipynb'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 99, 'page_label': '100'}, page_content=\"notebook in the chapter03 folder.\\nLine charts\\nThis is the most basic and common visualization that is used for\\nunderstanding a time series. We just plot the time on the X-axis\\nand the time series value on the Y-axis. Let's see what it looks\\nlike if we plot one of the households from our dataset:\\nFigure 3.3 – Line plot of household MAC000193\\nWhen you have a long time series with high variation, as we\\nhave, the line plot can get a bit chaotic. One of the options to get\\na macroview of the time series in terms of trends and\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 100, 'page_label': '101'}, page_content=\"movement is to plot a smoothed version of the time series. Let's\\nsee what a rolling monthly average of the time series look like:\\nFigure 3.4 – Rolling monthly average energy consumption of\\nhousehold MAC000193\\nWe can see the macro patterns much more clearly now. The\\nseasonality is clear – the series peaks in winter and troughs\\nduring summer. And if you think about it critically, it makes\\nsense. This is London we are talking about, and the energy\\nconsumption would be higher during the winter because of\\nlower temperatures and subsequent heating system usage. For\\na household in the tropics, for example, the pattern may be\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 101, 'page_label': '102'}, page_content=\"reversed, with the peaks coming in summer when air\\nconditioners come into play.\\nAnother use for the line chart is to visualize two or more time\\nseries together and investigate any correlations between them.\\nIn our case, let's try plotting the temperature along with the\\nenergy consumption and see if the hypothesis we have about\\ntemperature inﬂuencing energy consumption holds good:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 102, 'page_label': '103'}, page_content='Figure 3.5 – Temperature and energy consumption (zoomed-in\\nplot at the bottom) (Color Figure)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 103, 'page_label': '104'}, page_content='Here, we can see a clear negative correlation in yearly\\nresolution between energy consumption and temperature.\\nWinters show higher energy consumption on a macro scale. We\\ncan also see the daily patterns that are loosely correlated with\\ntemperature, but maybe because of other factors such as people\\ncoming back home after work and so on.\\nThere are a few other visualizations that are more suited to\\nbringing out seasonality in a time series. Let\\'s take a look.\\nSeasonal plots\\nA seasonal plot is very similar to a line plot, but the key\\ndiﬀerence here is that the X-axis denotes the \"seasons\", the Y-\\naxis denotes the time series value, and the diﬀerent seasonal\\ncycles are represented in diﬀerent colors or line types. For\\ninstance, the yearly seasonality at a monthly resolution can be\\ndepicted with months on the X-axis and diﬀerent years in\\ndiﬀerent colors.\\nLet\\'s see what this looks like for our household in question.\\nHere, we have plotted the average monthly energy\\nconsumption across multiple years:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 104, 'page_label': '105'}, page_content='Figure 3.6 – Seasonal plot at a monthly resolution(Color Figure)\\nWe can instantly see the appeal in this visualization because it\\nlets us visualize the seasonality pattern easily. We can see that\\nthe consumption goes down in the summer months and we can\\nalso see that it happens consistently across multiple years. In\\nthe 2 years that we have data for, we can see that in October, the\\nbehavior in 2013 slightly deviated from 2012. Maybe there is\\nsomething else that can help us explain this diﬀerence – what\\nabout temperature? We can also plot the seasonal plots with\\nanother variable of interest, such as the temperature:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 105, 'page_label': '106'}, page_content='Figure 3.7 – Seasonal plot at a monthly resolution (energy\\nconsumption versus temperature) (Color Figure)\\nNotice October? In October 2013, the temperature stayed\\nwarmer for 1 month more, hence why the energy consumption\\npattern was slightly diﬀerent from last year.\\nWe can plot these kinds of plots at other resolutions as well,\\nsuch as hourly seasonality. But when there are too many\\nseasonal cycles to be plotted, it increases visual clutter. An\\nalternative to a seasonal plot is a seasonal box plot.\\nSeasonal box plots'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 106, 'page_label': '107'}, page_content='Instead of plotting the diﬀerent seasonal cycles in diﬀerent\\ncolors or line types, we can represent them as a box plot. This\\ninstantly clears up the clutter in the plot. The additional beneﬁt\\nyou get from this representation is that it lets us understand the\\nvariability across seasonal cycles:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 107, 'page_label': '108'}, page_content='Figure 3.8 – Seasonal plot (top) and seasonal box plot (bottom) at\\nan hourly resolution(Color Figure)\\nHere, we can see that the seasonal plot at this resolution is too\\ncluttered to make out the pattern and the variation across'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 108, 'page_label': '109'}, page_content='seasonal cycles. However, the seasonal box plot is much more\\ninformative. The horizontal line in the box tells us about the\\nmedian, the box is the interquartile range (IQR), and the\\npoints that are marked are the outliers. By looking at the\\nmedians, we can see that the peak consumption occurs from 9\\nA.M. onward. But the variability is also higher from 9 A.M. If\\nyou plot separate box plots for each week, for example, you will\\nsee that the patterns are slightly diﬀerent on Sundays\\n(additional visualizations are in the associated notebook).\\nHowever, there is another visualization that lets you inspect\\nthese patterns along two dimensions.\\nCalendar heatmaps\\nInstead of having separate box plots or separate line charts for\\neach week of the day, it would be useful if we could condense\\nthat information into a single plot. This is where calendar\\nheatmaps come in. A calendar heatmap uses colored cells in a\\nrectangular block to represent the information. Along the two\\nsides of the rectangle, we can ﬁnd two separate granularities of\\ntime, such as month and year. In each intersection, the cell is\\ncolored relative to the value of the time series at that\\nintersection.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 109, 'page_label': '110'}, page_content=\"Let's look at the hourly average energy consumption across the\\ndiﬀerent weekdays in a calendar heatmap:\\nFigure 3.9 – A calendar heatmap for energy consumption(Color\\nFigure)\\nFrom the color scale on the right, we know that lighter colors\\nmean higher values. We can see how Monday to Saturday have\\nsimilar peaks – that is, once in the morning and once in the\\nevening. However, Sunday has a slightly diﬀerent pattern, with\\nhigher consumption throughout the day.\\nSo far, we've reviewed a lot of visualizations that can bring out\\nseasonality. Now, let's look at a visualization for inspecting\\nautocorrelation.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 110, 'page_label': '111'}, page_content=\"Autocorrelation plot\\nIf correlation indicates the strength and direction of the linear\\nrelationship between two variables, autocorrelation is the\\ncorrelation between the values of a time series in successive\\nperiods. Most time series have a heavy dependence on the\\nvalue in the previous period, and this is a critical component in\\na lot of the forecasting models we will be seeing as well.\\nSomething such as ARIMA (which we will brieﬂy look at in\\nChapter 4, Setting a Strong Baseline Forecast) is built on\\nautocorrelation. So, it's always helpful to just visualize and\\nunderstand how strong the dependence on previous time steps\\nis.\\nThis is where autocorrelation plots come in handy. In such\\nplots, we have the diﬀerent lags (t-1, t-2, t-3, and so on) on the X-\\naxis and the correlations between t and the diﬀerent lags on the\\nY-axis. In addition to autocorrelation, we can also look at\\npartial autocorrelation, which is very similar to\\nautocorrelation but with one key diﬀerence: partial\\nautocorrelation removes any indirect correlation that may be\\npresent before presenting the correlations. Let's look at an\\nexample to understand this. If t is the current time step, let's\\nassume t-1 is highly correlated to t. So, by extending this logic, t-\\n2 will be highly correlated with t-1 and because of this\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 111, 'page_label': '112'}, page_content=\"correlation, the autocorrelation between t and t-2 would be\\nhigh. However, partial autocorrelation corrects this and extracts\\nthe correlation, which can be purely attributed to t-2 and t.\\nOne thing we need to keep in mind is that the autocorrelation\\nand partial autocorrelation analysis works best if the time\\nseries is stationary (we will talk about stationarity in detail in\\nChapter 6, Feature Engineering for Time Series Forecasting).\\nBEST PRACTICE\\nThere are many ways of making a series stationary, but\\na quick and dirty way is to use seasonal decomposition\\nand just pick the residuals. It should be devoid of\\ntrends and seasonality, which are the major drivers of\\nnon-stationarity in a time series. But as we will see\\nlater in this book, this is not a foolproof method of\\nmaking a series stationary in the truest sense.\\nNow, let's see what these plots look like for our household from\\nthe dataset (after making it stationary):\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 112, 'page_label': '113'}, page_content=\"Figure 3.10 – Autocorrelation and partial autocorrelation plots\\nHere, we can see that the ﬁrst lag (t-1) has the most inﬂuence\\nand that its inﬂuence quickly drops down to close to zero in the\\npartial autocorrelation plot. This means that the energy\\nconsumption of a day is highly correlated with the energy\\nconsumption the day before.\\nIf you've seen such charts before, you would have seen an\\nenvelope over this showing the conﬁdence intervals as a guide\\nto selecting signiﬁcant auto-correlations. While that is a good\\nthumb of rule, it's not included here because I don't want you to\\nuse it as a rule. The relevance of the conﬁdence intervals\\ndepends on some assumptions (normality and so on) which\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 113, 'page_label': '114'}, page_content=\"may not be satisﬁed all the time, especially in real world use\\ncases.\\nWith that, we've looked at the diﬀerent components of a time\\nseries and learned how to visualize a few of them. Now, let's see\\nhow we can decompose a time series into its components.\\nDecomposing a time series\\nSeasonal decomposition is the process by which we deconstruct\\na time series into its components – typically, trend, seasonality,\\nand residuals. The general approach for decomposing a time\\nseries is as follows:\\n1. Detrending: Here, we estimate the trend component (which\\nis the smooth change in the time series) and remove it from\\nthe time series, giving us a detrended time series.\\n2. Deseasonalizing: Here, we estimate the seasonality\\ncomponent from the detrended time series. After removing\\nthe seasonal component, what is left is the residual.\\nLet's discuss them in detail.\\nDetrending\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 114, 'page_label': '115'}, page_content='Detrending can be done in a few diﬀerent ways. Two popular\\nways of doing it are by using moving averages and locally\\nestimated scatterplot smoothing (LOESS) regression.\\nMoving averages\\nOne of the easiest ways of estimating trends is by using a\\nmoving average along the time series. It can be seen as a\\nwindow that is moved along the time series in steps, and at\\neach step, the average of all the values in the window is\\nrecorded. This moving average is a smoothed-out time series\\nand helps us estimate the slow change in time series, which is\\nthe trend. The downside is that the technique is quite noisy.\\nEven after smoothing out a time series using this technique, the\\nextracted trend will not be smooth; it will be noisy. The noise\\nshould ideally reside with the residuals and not the trend (see\\nthe trend line shown in Figure 3.11).\\nLOESS\\nThe LOESS algorithm, which is also called locally weighted\\npolynomial regression, was developed by Bill Cleveland through\\nthe 70s to the 90s. It is a non-parametric method that is used to\\nﬁt a smooth curve onto a noisy signal. We use an ordinal\\nvariable that moves between the time series as the independent\\nvariable and the time series signal as the dependent variable.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 115, 'page_label': '116'}, page_content=\"For each value in the ordinal variable, the algorithm uses a\\nfraction of the closest points and estimates a smoothed trend\\nusing only those points in a weighted regression. The weights in\\nthe weighted regression are the closest points to the point in\\nquestion. This is given the highest weight and it decays as we\\nmove farther away from it. This gives us a very eﬀective tool for\\nmodeling the smooth changes in the time series (trend).\\nDeseasonalizing\\nThe seasonality component can also be estimated in a few\\ndiﬀerent ways. The two most popular ways of doing this are by\\nusing period-adjusted averages or a Fourier series.\\nPeriod adjusted averages\\nThis is a pretty simple technique wherein we calculate a\\nseasonality index for each period in the expected cycle by\\ntaking the average values of all such periods over all the cycles.\\nTo make that clear, let's look at a monthly time series where we\\nexpect an annual seasonality in this time series. So, the up-and-\\ndown pattern would complete a full cycle in 12 months, or the\\nseasonality period is 12. In other words, every 12 points in the\\ntime series have similar seasonal components. So, we take the\\naverage of all January values as the period-adjusted average for\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 116, 'page_label': '117'}, page_content=\"January. In the same way, we calculate the period average for\\nall 12 months. At the end of the exercise, we have 12 period\\naverages, and we can also calculate an average period average.\\nNow, we can make these period averages into an index by\\neither subtracting the average of all period averages from each\\nof the period averages (for additive) or dividing the average of\\nall period averages from each of the period averages\\n(multiplicative).\\nFourier series\\nIn the late 1700s, Joseph Fourier, a mathematician and physicist,\\nwhile studying heat ﬂow, realized something profound – any\\nperiodic function can be broken down into a simple series of\\nsine and cosine waves. Let's dwell on that for a minute. Any\\nperiodic function, no matter the shape, curve, or absence of it,\\nor how wildly it oscillates around the axis, can be broken down\\ninto a series of sine and cosine waves.\\nADDITIONAL INFORMATION\\nFor the more mathematically inclined, the original\\ntheory proposes to decompose any periodic function\\ninto an integral of exponentials. Using Euler's identity,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 117, 'page_label': '118'}, page_content=', we can consider them as a summation of sine and\\ncosine waves. The Further reading section contains a\\nfew resources if you want to delve deeper and explore\\nrelated concepts, such as the Fourier transform.\\nIt is this property that we use to extract seasonality from a time\\nseries because seasonality is a periodic function, and any\\nperiodic function can be approximated by a combination of\\nsine and cosine waves. The sine-cosine form of a Fourier series\\nis as follows:\\nHere,\\nis the N-term approximation of the signal, S. Theoretically,\\nwhen N is inﬁnite, the resulting approximation is equal to the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 118, 'page_label': '119'}, page_content=\"original signal. P is the maximum length of the cycle. We can\\nuse this Fourier series, or a few terms from the Fourier series,\\nto model our seasonality. In our application, P is the maximum\\nlength of the cycle we are trying to model. For instance, for a\\nyearly seasonality for monthly data, the maximum length of the\\ncycle (P) is 12. x would be an ordinal variable that increases\\nfrom 1 to P. In this example, x would be 1, 2, 3, … 12. Now, with\\nthese terms, all that is left to do is ﬁnd\\nand\\n, which we can do by regressing on the signal.\\nWe've seen that with the right combination of Fourier terms, we\\ncan replicate any signal. But the question is, should we? What\\nwe want to learn from data is a generalized seasonality proﬁle\\nthat does well with unseen data as well. So, we use N as a\\nhyperparameter to extract as complex a signal as we want from\\nthe data.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 119, 'page_label': '120'}, page_content='This is a good time to brush up on your trigonometry and\\nremember what sine and cosine waves look like. The ﬁrst\\nFourier term (n=1) is your age-old sine and cosine waves, which\\ncomplete one full cycle in the maximum cycle length (P). As we\\nincrease n, we get sine and cosine waves that have multiple\\ncycles in the maximum cycle length (P). This can be seen in the\\nfollowing ﬁgure:\\nFigure 3.11 – Cosine Fourier terms (n=1, 2, 3)\\nThe sine and cosine waves are complementary to each other, as\\nshown in the following ﬁgure:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 120, 'page_label': '121'}, page_content=\"Figure 3.12 – Sine and cosine Fourier terms (n=1)\\nNow, let's see how we can use this in practice.\\nImplementations\\nNOTEBOOK ALERT\\nTo follow along with the complete code for\\ndecomposing time series, use the 02-Decomposing\\nTime Series.ipynb notebook in the chapter03 folder.\\nThere are four implementations that we will cover here in the\\nfollowing subsections.\\nseasonal_decompose from statsmodel\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 121, 'page_label': '122'}, page_content='statsmodels.tsa.seasonal has a function called\\nseasonal_decompose. This is an implementation that uses\\nmoving averages for the trend component and period-adjusted\\naverages for the seasonal component. It supports both additive\\nand multiplicative modes of decomposition. However, it doesn\\'t\\ntolerate missing values. Let\\'s see how we can use it:\\n#Does not support missing values, so using imputed ts\\ninstead\\nres = seasonal_decompose(ts, period=7*48, model=\"\\nA few key parameters to keep in mind are as follows:\\n1. period is the seasonal period you expect the pattern to\\nrepeat.\\n2. model takes additive or multiplicative as arguments to\\ndetermine the type of decomposition.\\n3. ﬁlt takes in an array that is used as the weights in the moving\\naverage (convolution, to be speciﬁc). It can also be used to\\ndeﬁne the window over which we need our moving average.\\nWe can increase it to smooth out the trend component to\\nsome extent.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 122, 'page_label': '123'}, page_content=\"4. extrapolate_trend is a parameter that we can use to extend\\nthe trend component to both sides to avoid the missing\\nvalues that are generated when applying the moving average\\nﬁlter.\\n5. two_sided is a parameter that lets us deﬁne how the moving\\naverages are calculated. If True, which it is by default, the\\nmoving average is calculated using the past as well as future\\nvalues because the window for the moving average is\\ncentered. If False, it only uses past values to calculate the\\nmoving average.\\nLet's see how well we have been able to decompose one of the\\ntime series in our datasets. We used period=7*48 to capture a\\nweekday-hourly proﬁle and ﬁlt=np.repeat(1/(30*48), 30*48) to\\nmake the moving average over 30 days with uniform weights:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 123, 'page_label': '124'}, page_content=\"Figure 3.13 – Seasonal decomposition using statsmodels\\nWe can't see the seasonal pattern because it's too small in the\\ngrand scale of the plot. The associated notebook has zoomed-in\\nplots to help you understand the seasonal pattern. Even with a\\nlarge window (for example, 20 days) of smoothing, the trend\\nstill has some noise in it. We may be able to reduce this a bit\\nmore by increasing the window, but there is a better\\nalternative, as we will see now.\\nSeasonality and trend decomposition using LOESS (STL)\\nAs we saw earlier, LOESS is much more suited for trend\\nestimation. Seasonality and trend decomposition using\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 124, 'page_label': '125'}, page_content='LOESS (STL) is an implementation that uses LOESS for trend\\nestimation and period averages for seasonality. Although\\nstatsmodels has an implementation, we have reimplemented it\\nfor better performance and ﬂexibility. This implementation can\\nbe found in this book\\'s GitHub repository under\\nsrc.decomposition.seasonal.py. It expects a pandas\\nDataFrame or series with a datetime index as an input. Let\\'s see\\nhow we can use this:\\nstl = STL(seasonality_period=7*48, model = \"addit\\nres_new = stl.fit(ts_df.energy_consumption)\\nThe key parameters here are as follows:\\n1. seasonality_period is the seasonal period you expect the\\npattern to repeat.\\n2. model takes additive or multiplicative as arguments to\\ndetermine the type of decomposition.\\n3. lo_frac is the fraction of the data that will be used to ﬁt the\\nLOESS regression.\\n4. lo_delta is the fractional distance within which we use a\\nlinear interpolation instead of weighted regression. Using a\\nnon-zero lo_delta signiﬁcantly decreases computation time.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 125, 'page_label': '126'}, page_content=\"Let's see what this decomposition looks like. Here, we used\\nseasonality_period=7*48 to capture a weekday-hourly proﬁle:\\nFigure 3.14 – STL decomposition\\nLet's also look at the decomposition for just 1 month to see the\\nextracted seasonality patterns clearer:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 126, 'page_label': '127'}, page_content='Figure 3.15 – STL decomposition (zoomed-in for a month)\\nThe trend is smooth enough now and seasonality has also been\\ncaptured. Here, we can clearly see the hourly peaks and valleys\\nand the higher peaks on weekends. But since we are relying on\\naverages to derive the seasonality, it is also highly inﬂuenced by\\noutliers. A few very high or very low values in the time series\\nwill skew your seasonality proﬁle that\\'s been derived from\\nperiod averages. Another disadvantage of this technique is that\\nthe \"goodness\" of the seasonality that\\'s been extracted suﬀers\\nwhen the diﬀerence between the resolution of the data and the\\nexpected seasonality cycle is greater. For instance, when\\nextracting a yearly seasonality on daily or sub-daily data, this'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 127, 'page_label': '128'}, page_content='would make the extracted seasonality very noisy. This\\ntechnique will also not work if you have less than two cycles of\\nthe expected seasonality – for instance, if we want to extract a\\nyearly seasonality, but we have less than 2 years of data.\\nFourier decomposition\\nWe can ﬁnd the Python implementation for decomposing a time\\nseries using Fourier terms in src.decomposition.seasonal.py.\\nIt uses LOESS for trend detection and Fourier terms for\\nseasonality extraction. There are two ways we can use it. First,\\nwe can specify seasonality_period as one of the pandas\\ndatetime properties (such as hour, week_of_day, and so on):\\nstl = FourierDecomposition(seasonality_period=\"ho\\nres_new = stl.fit(pd.Series(ts.squeeze(), index=t\\nAlternatively, we can create any custom seasonality array that\\'s\\nthe same length as the time series that has an ordinal\\nrepresentation of the seasonality. If it is an annual seasonality\\nof daily data, the array would have a minimum value of 1 and a\\nmaximum value of 365 as it increases by one every day of the\\nyear:\\n#Making a custom seasonality term'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 128, 'page_label': '129'}, page_content='ts_df[\"dayofweek\"] = ts_df.index.dayofweek\\nts_df[\"hour\"] = ts_df.index.hour\\n#Creating a sorted unique combination df\\nmap_df = ts_df[[\"dayofweek\",\"hour\"]].drop_duplica\\n# Assigning an ordinal variable to capture the order\\nmap_df[\"map\"] = np.arange(1, len(map_df)+1)\\n# mapping the ordinal mapping back to the original df and\\ngetting the seasonality array\\nseasonality = ts_df.merge(map_df, on=[\"dayofweek\"\\nstl = FourierDecomposition(model = \"additive\", n_\\nres_new = stl.fit(pd.Series(ts, index=ts_df.index\\nThe key parameters that are involved in this process are as\\nfollows:\\n1. seasonality_period is the seasonality to be extracted from\\nthe datetime index. pandas datetime properties such as'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 129, 'page_label': '130'}, page_content=\"week_of_day, month, and so on can be used to specify the\\nmost prominent seasonality. If left set to None, you need to\\nprovide the seasonality array while calling ﬁt.\\n2. model takes additive or multiplicative as arguments to\\ndetermine the type of decomposition.\\n3. n_fourier_terms determines the number of Fourier terms to\\nbe used to extract the seasonality. The more we increase this\\nparameter, the more complex the seasonality that is extracted\\nfrom the data.\\n4. lo_frac is the fraction of the data that will be used to ﬁt the\\nLOESS regression.\\n5. lo_delta is the fractional distance within which we use linear\\ninterpolation instead of weighted regression. Using a non-\\nzero lo_delta signiﬁcantly decreases computation time.\\nLet's see the zoomed-in plot for the decomposition using\\nFourierDecomposition:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 130, 'page_label': '131'}, page_content='Figure 3.16 – Decomposition using Fourier terms (zoomed-in for a\\nmonth)\\nThe trend is going to be the same as the STL one because we are\\nusing LOESS here as well. The seasonality proﬁle may be\\nslightly diﬀerent and robust to outliers because we are doing\\nregularized regression using the Fourier terms on the signal.\\nAnother advantage is that we have decoupled the resolution of\\nthe data and the expected seasonality. Now, extracting a yearly\\nseasonality on sub-daily data is not as challenging as with\\nperiod averages.\\nSo far, we have only seen techniques that extract one\\nseasonality per series; mostly, we extract the major seasonality.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 131, 'page_label': '132'}, page_content=\"So, what do we do when we have multiple seasonal patterns?\\nMultiple seasonality decomposition using LOESS (MSTL)\\nTime series with high-frequency data (such as daily, hourly, or\\nminutely data) are prone to exhibit multiple seasonal patterns.\\nFor instance, there may be an hourly seasonality pattern, a\\nweekly seasonality pattern, and a yearly seasonality pattern.\\nBut if we extract only the dominant pattern and leave the rest\\nto residuals, we are not doing justice to the decomposition.\\nKasun Bandara et al. proposed an extension of STL\\ndecomposition for multiple seasonality, known as multiple\\nseasonal-trend decomposition using LOESS (MSTL), and a\\ncorresponding implementation is present in the R ecosystem. A\\nvery similar implementation in Python can be found in\\nsrc.decomposition.seasonal.py. In addition to MSTL, the\\nimplementation extracts multiple seasonality using Fourier\\nterms.\\nREFERENCE CHECK\\nThe research paper by Kasun Bandara et al. is cited in\\nthe References section as reference 1.\\nLet's look at an example of how we can use this:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 132, 'page_label': '133'}, page_content='stl = MultiSeasonalDecomposition(seasonal_model=\"\\nres_new = stl.fit(pd.Series(ts, index=ts_df.index\\nThe key parameters here are as follows:\\n1. seasonality_periods is the list of expected seasonalities. For\\nSTL, it is a list of seasonal periods, while for\\nFourierDecomposition, it is a list of strings that denotes\\npandas datetime properties.\\n2. seasonality_model takes fourier or averages as arguments\\nto determine the type of seasonality decomposition.\\n3. model takes additive or multiplicative as arguments to\\ndetermine the type of decomposition.\\n4. n_fourier_terms determines the number of Fourier terms to\\nbe used to extract the seasonality. As we increase this\\nparameter, the more complex the seasonality that is extracted\\nfrom the data.\\n5. lo_frac is the fraction of the data that will be used to ﬁt the\\nLOESS regression.\\n6. lo_delta is the fractional distance within which we use linear\\ninterpolation instead of weighted regression. Using a non-\\nzero lo_delta signiﬁcantly decreases computation time.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 133, 'page_label': '134'}, page_content=\"Let's see what the decomposition looks like when using Fourier\\ndecomposition:\\nFigure 3.17 – Multiple seasonality decomposition using Fourier\\nterms\\nHere, we can see that the day_of_week seasonality has been\\nextracted. To see the day_of_week and hour seasonal\\ncomponents, we need to zoom in a bit:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 134, 'page_label': '135'}, page_content='Figure 3.18 – Multiple seasonality decomposition using Fourier\\nterms (zoomed-in for a month)\\nHere, we can observe that the hour seasonality has been\\nextracted well and that it has also isolated the day_of_week\\nseasonal component, which peaks on weekends. The discrete\\nstep nature of the day_of_week seasonal component is because\\nthe frequency of the data is half-hourly, and for 48 data points,\\nday_of_week will be the same.\\nNOTE\\nMSTL has also been implemented in statsmodels and\\nthe accompanying notebook has the code to use that.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 135, 'page_label': '136'}, page_content=\"The key diﬀerence between statsmodels and the\\nimplementation bundled within the code for the book is\\nthat the one in the book also has the option for using\\nFourier series based decomposition.\\nWe have summarized the four techniques we've covered in the\\nfollowing table:\\nTable 3.1 – Diﬀerent seasonal decomposition techniques\\nNow, let's understand and analyze a time series dataset.\\nDetecting and treating outliers\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 136, 'page_label': '137'}, page_content=\"An outlier, as its name suggests, is an observation that lies at an\\nabnormal distance from the rest of the observations. If we are\\nlooking at a data generating process (DGP) as a stochastic\\nprocess that generates the time series, the outliers are the\\npoints that have the least probability of being generated from\\nthe DGP. This can be for many reasons, including faulty\\nmeasurement equipment, incorrect data entry, and black-swan\\nevents, to name a few. Being able to detect such outliers and\\ntreat them may help your forecasting model understand the\\ndata better.\\nOutlier/anomaly detection is a specialized ﬁeld itself in time\\nseries, but in this book, we are going to restrict ourselves to\\nsimpler techniques of identifying and treating outliers. This is\\nbecause our main aim is not to detect outliers, but to clean the\\ndata for our forecasting models to perform better. If you want\\nto learn more about anomaly detection, head over to the\\nFurther reading section for a few resources to get started.\\nNow, let's look at a few techniques for identifying outliers.\\nNOTEBOOK ALERT\\nTo follow along with the complete code for detecting\\noutliers, use the 03-Outlier Detection.ipynb notebook\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 137, 'page_label': '138'}, page_content='in the chapter03 folder.\\nStandard deviation\\nThis is a rule of thumb that almost everyone who has worked\\nwith data for some time would have heard of – if\\nis the mean of the time series and\\nis the standard deviation, then anything that falls beyond\\nis an outlier. The underlying theory is deeply rooted in\\nstatistics. If we assume that the values of the time series follow\\na normal distribution (which is a symmetrical distribution with\\nvery desirable properties), using probability theory, we can\\nderive that 68% of the area under the normal distribution lies\\nwithin one standard deviation on either side of the mean, about\\n95% of the area within two standard deviations, and about 99%\\nof the area within three standard deviations. So, when we make'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 138, 'page_label': '139'}, page_content=\"the bounds as three standard deviations (by using the rule of\\nthumb), what we are saying is that if any observation whose\\nprobability of belonging to the probability distribution is less\\nthan 1%, then they are an outlier. Moving slightly to more\\npractical issues, this cutoﬀ of three standard deviations is in no\\nway sacrosanct. We need to try out diﬀerent values of this\\nmultiple and determine the right multiple by subjectively\\nevaluating the results we get. The higher the multiple is, the\\nfewer outliers there will be.\\nFor highly seasonal data, the naïve way of applying the rule to\\nthe raw time series will not work well. In such cases, we must\\ndeseasonalize the data using any of the techniques we\\ndiscussed earlier and then apply the outliers to the residuals. If\\nwe don't do that, we may ﬂag a seasonal peak as an outlier,\\nwhich is not what we want.\\nAnother key assumption here is the normal distribution.\\nHowever, in reality, a lot of the time series we come across may\\nnot be normal and hence the rule will lose its theoretical\\nguarantees fast.\\nInterquartile range (IQR)\\nAnother very similar technique is using the IQR instead of the\\nstandard deviation to deﬁne the bounds beyond which we\\nmark the observations as outliers. IQR is the diﬀerence between\\nthe 3rd quartile (or the 75th percentile or 0.75 quantile) and the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 139, 'page_label': '140'}, page_content='1st quartile (or the 25th percentile or 0.25 quantile). The upper\\nand lower bounds are deﬁned as follows:\\nUpper bound = Q3 + n x IQR\\nLower bound = Q1 - n x IQR\\nHere, IQR = Q3-Q2, and n is the multiple of IQRs that determines\\nthe width of the acceptable area.\\nFor datasets where we observe high occurrences of outliers and\\nwild variations, this is slightly more robust than the standard\\ndeviation. This is because the standard deviation and the mean\\nare highly inﬂuenced by individual points in the dataset. If 2\\nwas the rule of thumb in the earlier method, here, it is 1.5 times\\nthe IQR. This also ties back to the same normal distribution\\nassumption, and 1.5 times the IQR is equivalent to ~3\\n(2.7'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 140, 'page_label': '141'}, page_content='to be exact). The point about deseasonalizing before applying\\nthe rule applies here as well. It applies to all the techniques we\\nwill see here.\\nIsolation Forest\\nIsolation Forest is an unsupervised anomaly detection\\nalgorithm based on decision trees. A typical anomaly detection\\nalgorithm models the normal points and proﬁles outliers as any\\npoints that do not ﬁt the normal. But Isolation Forest takes a\\ndiﬀerent path and models the outliers directly. It does this by\\ncreating a forest of decision trees by randomly splitting the\\nfeature space. This technique works on the assumption that the\\noutlier points fall in the outer periphery and are easier to fall\\ninto a leaf node of a tree. Therefore, you can ﬁnd the outliers in\\nshort branches, whereas normal points, which are closer\\ntogether, will require longer branches. The \"anomaly score\" of\\nany point is determined by the depth of the tree to be traversed\\nbefore reaching that particular point. scikit-learn has an\\nimplementation of the algorithm under\\nsklearn.ensemble.IsolationForest. Apart from the standard\\nparameters for decision trees, the key parameter here is\\ncontamination. It is set to auto by default but can be set to any\\nvalue between 0 and 0.5. This parameter speciﬁes what\\npercentage of the dataset you expect to be anomalous. But one'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 141, 'page_label': '142'}, page_content=\"thing we have to keep in mind is that IsolationForest does not\\nconsider time at all and just highlights values that fall outside\\nthe norm.\\nExtreme studentized deviate (ESD) and seasonal ESD\\n(S-ESD)\\nThis statistics-based technique is more sophisticated than the\\nbasic\\ntechnique, but still uses the same assumption of normality. It is\\nbased on another statistical test, called Grubbs's Test, which is\\nused to ﬁnd a single outlier in a normally distributed dataset.\\nESD iteratively uses Grubbs's test by identifying and removing\\nan outlier at each step. It also adjusts the critical value based on\\nthe number of points left. For a more detailed understanding of\\nthe test, go to the Further reading section, where we have\\nprovided a couple of resources about ESD and S-ESD. In 2017,\\nHochenbaum et al. from Twitter Research proposed to use the\\ngeneralized ESD with deseasonalization as a method of\\ndetecting outliers for time series.\\nWe have adapted an existing implementation of the algorithm\\nfor our use case, and it is available in this book's GitHub\\nrepository. While all the other methods leave it to the user to\\ndetermine the right level of outliers by tweaking a few\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 142, 'page_label': '143'}, page_content=\"parameters, S-ESD only takes in an upper bound on the number\\nof expected outliers and then identiﬁes the outliers\\nindependently. For instance, we set the upper bound to 800 and\\nthe algorithm identiﬁed ~400 outliers in the data we are\\nworking with.\\nREFERENCE CHECK\\nThe research paper by Hochenbaum et al. is cited in the\\nReferences section as reference 2.\\nLet's see how the outliers were detected using all the techniques\\nwe have reviewed:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 143, 'page_label': '144'}, page_content='Figure 3.19 – Outliers detected using diﬀerent techniques\\nNow that we\\'ve learned how to detect outliers, let\\'s talk about\\nhow we can treat them and clean the dataset.\\nTreating outliers\\nThe ﬁrst question that we must answer is whether or not we\\nshould correct the outliers we have identiﬁed. The statistical\\ntests that identify outliers automatically should go through\\nanother level of human veriﬁcation. If we blindly \"treat\"\\noutliers, we might be chopping oﬀ a valuable pattern that will'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 144, 'page_label': '145'}, page_content=\"help us forecast the time series. If you are only forecasting a\\nhandful of time series, then it still makes sense to look at the\\noutliers and anchor them to reality by looking at the causes for\\nsuch outliers.\\nBut when you have thousands of time series, a human can't\\ninspect all the outliers, so we will have to resort to automated\\ntechniques. A common practice is to replace an outlier with a\\nheuristic such as the maximum, minimum, 75th percentile, and\\nso on. A better method is to consider the outliers as missing\\ndata and use any of the techniques we discussed earlier to\\nimpute the outliers.\\nOne thing we must keep in mind is that outlier correction is not\\na necessary step in forecasting, especially when using modern\\nmethods such as machine learning or deep learning. Whether\\nwe do outlier correction or not is something we have to\\nexperiment with and ﬁgure out.\\nWell done! This was a pretty busy chapter, with a lot of concepts\\nand code, so congratulations on ﬁnishing it. Feel free to head\\nback and revise a few topics as needed.\\nSummary\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 145, 'page_label': '146'}, page_content='In this chapter, we learned about the key components of a time\\nseries and familiarized ourselves with terms such as trend,\\nseasonality, and so on. We also reviewed a few time series-\\nspeciﬁc visualization techniques that will come in handy during\\nEDA. Then, we learned about techniques that let you\\ndecompose a time series into its components and saw\\ntechniques for detecting outliers in the data. Finally, we learned\\nhow to treat the identiﬁed outliers. Now, you are all set to start\\nforecasting the time series, which we will start in the next\\nchapter.\\nReferences\\nThe following are the references for this chapter:\\n1. Kasun Bandara and Rob J Hyndman and Christoph Bergmeir.\\n(2021). MSTL: A Seasonal-Trend Decomposition Algorithm for\\nTime Series with Multiple Seasonal Patterns. arXiv:2107.13462\\n[stat.AP]. https://arxiv.org/abs/2107.13462.\\n2. Hochenbaum, J., Vallis, O., & Kejariwal, A. (2017). Automatic\\nAnomaly Detection in the Cloud Via Statistical Learning.\\nArXiv, abs/1704.07706. https://arxiv.org/abs/1704.07706.\\nFurther reading'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 146, 'page_label': '147'}, page_content='To learn more about the topics that were covered in this\\nchapter, take a look at the following resources:\\nFourier Series: https://www.setzeus.com/public-blog-post/the-\\nfourier-series.\\nFourier Series from Khan Academy:\\nhttps://www.youtube.com/watch?v=UKHBWzoOKsY.\\nFourier Transform - https://betterexplained.com/articles/an-\\ninteractive-guide-to-the-fourier-transform/.\\nAne Blázquez-García, Angel Conde, Usue Mori, and Jose A.\\nLozano. (2021). A Review on Outlier/Anomaly Detection in\\nTime Series Data. arXiv:2002.04236.\\nhttps://arxiv.org/abs/2002.04236.\\nBraei, M., & Wagner, S. (2020). Anomaly Detection in\\nUnivariate Time-series: A Survey on the State-of-the-Art. ArXiv,\\nabs/2004.00433. https://arxiv.org/abs/2004.00433.\\nGeneralized ESD Test for Outliers:\\nhttps://www.itl.nist.gov/div898/handbook/eda/section3/eda35\\nh3.htm.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 147, 'page_label': '148'}, page_content='4 Setting a Strong Baseline Forecast'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 148, 'page_label': '149'}, page_content=\"Join our book community on Discord\\nhttps://packt.link/EarlyAccess/\\nIn the previous chapter, we saw some techniques we can use to\\nunderstand time series data, do some Exploratory Data\\nAnalysis (EDA), and so on. But now, let's get to the crux of the\\nmatter – time series forecasting. The point of understanding\\nthe dataset and looking at patterns, seasonality, and so on was to\\nmake the job of forecasting that series easier. And with any\\nmachine learning exercise, one of the ﬁrst things we need to\\nestablish before going further is a baseline.\\nA baseline is a simple model that provides reasonable results\\nwithout requiring a lot of time to come up with them. Many\\npeople think of baselines as something that is derived from\\ncommon sense, such as an average or some rule of thumb. But\\nas a best practice, a baseline can be as sophisticated as we want\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 149, 'page_label': '150'}, page_content=\"it to be, so long as it is quickly and easily implemented. Any\\nfurther progress we want to make will be in terms of the\\nperformance of this baseline.\\nIn this chapter, we will look at a few classical techniques that\\ncan be used as baselines, and a strong baseline at that. Some\\nmay feel that the forecasting techniques we will be discussing in\\nthis chapter shouldn't be baselines, but we are keeping them in\\nhere because these techniques have stood the test of time – and\\nfor good reason. They are also very mature and can be applied\\nwith very little eﬀort, thanks to the awesome open source\\nlibraries that implement them. There can be many types of\\nproblems/datasets where it is diﬃcult to beat the baseline\\ntechniques we will discuss in this chapter, and in those cases,\\nthere is no shame in just sticking to one of these baseline\\ntechniques.\\nIn this chapter, we will cover the following topics:\\n1. Setting up a test harness\\n2. Generating strong baseline forecasts\\n3. Assessing the forecastability of a time series\\nTechnical requirements\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 150, 'page_label': '151'}, page_content='You will need to set up the Anaconda environment following the\\ninstructions in the Preface of the book to get a working\\nenvironment with all the libraries and datasets required for the\\ncode in this book. Any additional library will be installed while\\nrunning the notebooks\\nYou will need to run the following notebooks before using the\\ncode in this chapter:\\n02-Preprocessing London Smart Meter Dataset.ipynb\\npreprocessing notebook from Chapter02.\\nThe code for this chapter can be found at\\nhttps://github.com/PacktPublishing/Modern-Time-Series-\\nForecasting-with-Python-2E/tree/main/notebooks/Chapter04.\\nSetting up a test harness\\nBefore we start forecasting and setting up baselines, we need to\\nset up a test harness. In software testing, a test harness is a\\ncollection of code and the inputs that have been conﬁgured to\\ntest a program under various situations. In terms of machine\\nlearning, a test harness is a set of code and data that can be used\\nto evaluate algorithms. It is important to set up a test harness so'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 151, 'page_label': '152'}, page_content=\"that we can evaluate all future algorithms in a standard and\\nquick way.\\nThe ﬁrst thing we need is holdout (test) and validation\\ndatasets.\\nCreating holdout (test) and validation datasets\\nAs a standard practice, in machine learning, we set aside two\\nparts of the dataset, name them validation data and test data,\\nand don't use them at all to train the model. The validation data\\nis used in the modelling process to assess the quality of the\\nmodel. To select between diﬀerent model classes, tune the\\nhyperparameters, perform feature selection, and so on, we need\\na dataset. Test data is like the ﬁnal test of your chosen model. It\\ntells you how well your model is doing in unseen data. If\\nvalidation data is like the mid-term exams, the test data is your\\nﬁnal exam.\\nIn regular regression or classiﬁcation, we usually sample a few\\nrecords at random and set them aside. But while dealing with\\ntime series, we need to respect the temporal aspect of the\\ndataset. Therefore, a best practice is to set aside the latest part of\\nthe dataset as the test data. Another rule of thumb is to set\\nequal-sized validation and test datasets so that the key\\nmodelling decisions we make based on the validation data are\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 152, 'page_label': '153'}, page_content=\"as close as possible to the test data. The dataset that we\\nintroduced in Chapter 2, Acquiring and Processing Time Series\\nData (the London Smart Energy dataset), contains the energy\\nconsumption readings of households in London from November\\n2011 to February 2014. So, we are going to put aside January\\n2014 as the validation data and February 2014 as the test data.\\nLet's open 01-Setting up Experiment Harness.ipynb from the\\nchapter04 folder and run it. In the notebook, we must create the\\ntrain-test split both before and after ﬁlling the missing values\\nwith SeasonalInterpolation and save them accordingly. Once\\nthe notebook ﬁnishes running, you will have created the\\nfollowing ﬁles in the pre-processed folder with the 2014 data\\nsaved separately:\\nselected_blocks_train.parquet\\nselected_blocks_val.parquet\\nselected_blocks_test.parquet\\nselected_blocks_train_missing_imputed.parquet\\nselected_blocks_val_missing_imputed.parquet\\nselected_blocks_test_missing_imputed.parquet\\nNow that we have a ﬁxed dataset that can be used to fairly\\nevaluate multiple algorithms, we need a way to evaluate the\\ndiﬀerent forecasts.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 153, 'page_label': '154'}, page_content='Choosing an evaluation metric\\nIn machine learning, we have a handful of metrics that can be\\nused to measure continuous outputs, mainly Mean Absolute\\nError (MAE) and Mean Squared Error (MSE). But in the time\\nseries forecasting realm, there are scores of metrics with no real\\nconsensus on which ones to use. One of the reasons for this\\noverwhelming number of metrics is that no one metric\\nmeasures every characteristic of a forecast. Therefore, we have\\na whole chapter devoted to this topic (Chapter 18, Evaluating\\nForecasts – Forecast Metrics). For now, we will just review a few\\nmetrics, all of which we are going to use to measure the\\nforecasts. We are just going to consider them at face value:\\nMean Absolute Error (MAE): MAE is a very simple metric. It\\nis the average of the unsigned error between the forecast at\\ntimestep\\nand the observed value at time'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 154, 'page_label': '155'}, page_content='. The formula is as follows:\\nHere, N is the number of time series, L is the length of time\\nseries (in this case, the length of the test period), and f and y are\\nthe forecast and observed values, respectively.\\nMean Squared Error (MSE): MSE is the average of the\\nsquared error between the forecast\\nand observed\\nvalues:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 155, 'page_label': '156'}, page_content='Mean Absolute Scaled Error (MASE): MASE is slightly more\\ncomplicated than MSE or MAE but gives us a slightly better\\nmeasure to overcome the scale-dependent nature of the\\nprevious two measures. If we have multiple time series with\\ndiﬀerent average values, MAE and MSE will show higher\\nerrors for the high-value time series as opposed to the low-\\nvalued time series. MASE overcomes this by scaling the errors\\nbased on the in-sample MAE from the naïve forecasting\\nmethod (which is one of the most basic forecasts possible; we\\nwill review it later in this chapter). Intuitively, MASE gives us\\nthe measure of how much better our forecast is as compared\\nto the naïve forecast:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 156, 'page_label': '157'}, page_content=\"Forecast Bias (FB): This is a metric with slightly diﬀerent\\naspects from the other metrics we've seen. While the other\\nmetrics help assess the correctness of the forecast,\\nirrespective of the direction of the error, forecast bias lets us\\nunderstand the overall bias in the model. Forecast bias is a\\nmetric that helps us understand whether the forecast is\\ncontinuously over- or under-forecasting. We calculate\\nforecast bias as the diﬀerence between the sum of the\\nforecast and the sum of the observed values, expressed as a\\npercentage over the sum of all actuals:\\nNow, our test harness is ready. We also know how to evaluate\\nand compare forecasts that have been generated from diﬀerent\\nmodels on a single, ﬁxed holdout dataset with a set of\\npredetermined metrics. Now, it's time to start forecasting.\\nGenerating strong baseline forecasts\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 157, 'page_label': '158'}, page_content=\"Time series forecasting has been around since the early 1920s,\\nand through the years, many brilliant people have come up with\\ndiﬀerent models, some statistical and some heuristic-based. I\\nrefer to them collectively as classical statistical models or\\neconometrics models, although they are not strictly\\nstatistical/econometric.\\nIn this section, we are going to review a few such models that\\ncan form really strong baselines when we want to try modern\\ntechniques in forecasting. As an exercise, we are going to use an\\nexcellent open source library for time series forecasting –\\nNIXTLA (https://github.com/Nixtla). The 02-Baseline Forecasts\\nusing NIXTLA.ipynb notebook contains the code for this section\\nso that you can follow along.\\nBefore we start looking at forecasting techniques, let's quickly\\nunderstand how to use the NIXTLA library to generate the\\nforecasts. We are going to pick one consumer from the dataset\\nand try out all the baseline techniques on the validation dataset\\none by one.\\nThe ﬁrst thing we need to do is select the consumer we want\\nusing the unique ID for each customer, the LCLid column (from\\nthe expanded form of data), and set the timestamp as the index\\nof the DataFrame:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 158, 'page_label': '159'}, page_content='ts_train = train_df.loc[train_df.LCLid==\"MAC00019\\nts_val = val_df.loc[val_df.LCLid==\"MAC000193\", [\\'\\nts_test = test_df.loc[test_df.LCLid==\"MAC000193\",\\nNIXTLA has the ﬂexibility to work directly with either pandas\\nor Polars dataframes. By default, NIXTLA looks for 3 columns:\\nid_col: By default, it expects a column \\'unique_id\\'. This\\ncolumn uniquely identiﬁes the time series. If you only have 1\\ntime series, add a dummy column with the same unique\\nidentiﬁer.\\ntime_col: By default, it expects a column \\'ds\\'. This is the\\ncolumn of your timestamp.\\ntarget_col: By default, it expects a column \\'y\\'. This column is\\nwhat you want NIXTLA to forecast.\\nThis is very convenient as there is no need for further\\nmanipulation to go from data to modelling. NIXTLA follows the\\nsci-kit learn style with .ﬁt() and .predict(), and also adopts a\\n.forecast() method. Forecast is a memory eﬃcient method that\\navoids storing the partial model outputs, whereas the Scikit-\\nlearn interface stores the ﬁtted models.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 159, 'page_label': '160'}, page_content=\"sf = StatsForecast(\\n    models=[model],\\n    freq=freq,\\n    n_jobs=-1,\\n    fallback_model=Naive()\\n)\\nsf.fit( df = _ts_train,   \\n     id_col = 'LCLid',\\n     time_col = 'timestamp',\\n     target_col = 'energy_consumption',\\n)\\nbaseline_test_pred_df = sf.predict(len(ts_test) )\\nOr\\n# Efficiently fit and predict without storing mem\\ny_pred = sf.forecast(\\n    h=len(ts_test),\\n    df=ts_train,\\n    id_col = 'LCLid',\\n    time_col = 'timestamp',\\n    target_col = 'energy_consumption',\\n)\\nWhen we call .predict or .forecast, we have to tell the model\\nhow long into the future we have to predict. This is called the\\nhorizon of the forecast. In our case, we need to predict our test\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 160, 'page_label': '161'}, page_content=\"period, which we can easily do by just taking the length of the\\nts_test array.\\nWe can also calculate the metrics we discussed earlier in the test\\nharness easily using NIXTLA's classes. For added ﬂexibility, we\\ncan loop through a list of metrics to get multiple measurements\\nfor each forecast.\\n# Calculate metrics\\nmetrics = [mase, mae, mse, rmse, smape, forecast_\\nfor metric in metrics:\\n    metric_name = metric.__name__\\n    if metric_name == 'mase':\\n        evaluation[metric_name] =   \\nmetric(results[target_col].values,            res\\nts_train[target_col].values, seasonality=48)\\n    else:\\n        evaluation[metric_name] =\\nmetric(results[target_col].values,\\nresults[model_name].values)\\nNotice for MASE, the training set is also include\\nFor ease of experimentation, we have encapsulated all of this\\ninto a handy function, evaluate_performance, in the notebook.\\nThis returns the predictions and the calculated metrics in a\\ndataframe.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 161, 'page_label': '162'}, page_content=\"Now, let's start looking at a few very simple methods of\\nforecasting.\\nNaïve forecast\\nA naïve forecast is as simple as you can get. The forecast is just\\nthe last/most recent observation in a time series. If the latest\\nobservation in a time series is 10, then the forecast for all future\\ntimesteps is 10. This can be implemented as follows using the\\nNaive class in NIXTLA:\\nfrom statsforecast.models import Naive\\nmodels = Naive()\\nOnce we have initialized the model, we can call our helpful\\nevaluate_performance function in the notebook to run and\\nrecord the forecast and metrics.\\nLet's visualize the forecast we just generated:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 162, 'page_label': '163'}, page_content=\"Figure 4.1 – Naïve forecast\\nHere, we can see that the forecast is a straight line and\\ncompletely ignores any pattern in the series. This is by far the\\nsimplest way to forecast, hence why it is naïve. Now, let's look at\\nanother simple method.\\nMoving average forecast\\nWhile a naïve forecast memorizes the most recent past, it also\\nmemorizes the noise at any timestep. A moving average\\nforecast is another simple method that tries to overcome the\\npure memorization of the naïve method. Instead of taking the\\nlatest observation, it takes the mean of the latest n steps as the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 163, 'page_label': '164'}, page_content=\"forecast. Moving average is not one of the models present in\\nNIXTLA, but we have implemented a NIXTLA-compatible model\\nin this book's GitHub repository in the chapter04 folder:\\nfrom src.forecasting.baselines import NaiveMoving\\n#Taking a moving average over 48 timesteps, i.e, \\nnaive_model = NaiveMovingAverage(window=48)\\nLet's look at the forecast we generated:\\nFigure 4.2 – Moving average forecast\\nThis forecast is also almost a straight line. Now, let's look at\\nanother simple method, but one that considers seasonality as\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 164, 'page_label': '165'}, page_content=\"well.\\nSeasonal naive forecast\\nA seasonal naive forecast is a twist on the simple naive\\nmethod. Whereas in the naive method, we took the last\\nobservation (\\n), in seasonal naïve, we take the\\nobservation. So, we look back k steps for each forecast. This\\nenables the algorithm to mimic the last seasonality cycle. For\\ninstance, if we set k=48*7, we will be able to mimic the latest\\nseasonal weekly cycle. This method is implemented in NIXTLA\\nand we can use it like so:\\nfrom statsforecast.models import SeasonalNaive\\nseasonal_naive = SeasonalNaive(season_length=48*7\\nLet's see what this forecast looks like:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 165, 'page_label': '166'}, page_content=\"Figure 4.3 – Seasonal naïve forecast\\nHere, we can see that the forecast is trying to mimic the\\nseasonality pattern. However, it's not very accurate because it is\\nblindly following the last seasonal cycle.\\nNow that we've looked at a few simple methods, let's look at a\\nfew statistical models.\\nExponential smoothing (ETS)\\nExponential smoothing (ETS) is one of the most popular\\nmethods for generating forecasts. It has been around since the\\nlate 1950s and has proved its mettle and stood the test of time.\\nThere are a few diﬀerent variants of ETS – single exponential\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 166, 'page_label': '167'}, page_content=\"smoothing, double exponential smoothing, Holt-Winters'\\nseasonal smoothing, and so on. But all of them have one key\\nidea that has been used in diﬀerent ways. In the naïve method,\\nwe were just using the latest observation, which is like saying\\nonly the most recent data point in history matters and no data\\npoint before that matters. On the other hand, the moving\\naverage method considers the last n observations to be equally\\nimportant and takes the mean of them.\\nETS combines both these intuitions and says that all the history\\nis important, but the recent history is more important.\\nTherefore, the forecast is generated using a weighted average\\nwhere the weights decrease exponentially as we move farther\\ninto the history:\\nHere,\\nis the smoothing parameter that lets us decide how fast or slow\\nthe weights should decay,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 167, 'page_label': '168'}, page_content='is the actuals at timestep t, and\\nis the forecast at timestep t.\\nSimple exponential smoothing (SES) is when you simply apply\\nthis smoothing procedure to the history. This is more suited for\\ntime series that have no trends or seasonality, and the forecast is\\ngoing to be a ﬂat line. The forecast is generated using the\\nfollowing formula:\\nDouble exponential smoothing (DES) extends the smoothing\\nidea to model trends as well. It has two smoothing equations –\\none for the level and the other for the trend. Once you have the\\nestimate of the level and trend, you can combine them. This\\nforecast is not necessarily ﬂat because the estimated trend is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 168, 'page_label': '169'}, page_content='used to extrapolate it into the future. The forecast is generated\\naccording to the following formula:\\nFirst, we estimate the level (\\n) using the Level Equation with the available observations. Then,\\nwe estimate the trend using the Trend Equation. Finally, to get\\nthe forecast, we combine\\nand'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 169, 'page_label': '170'}, page_content=\"using the Forecast Equation. Researchers have found empirical\\nevidence that this kind of constant extrapolation can result in\\nover-forecasts over the long-term forecast. This is because, in\\nthe real world, time series data doesn't increase at a constant\\nrate forever. Motivated by this, an addition to this has also been\\nintroduced that dampens the trend by a factor of\\n, such that when\\n, there is no damping, and it is identical to double exponential\\nsmoothing. Triple exponential smoothing or Holt -Winters\\n(HW) takes this one step forward by including another\\nsmoothing term to model the seasonality. This has three\\nparameters (\\n) for the smoothing and uses a seasonality period (m) as input\\nparameters. You can also choose between additive or\\nmultiplicative seasonality. The forecast equations for the\\nadditive model are as follows:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 170, 'page_label': '171'}, page_content='These formulae are also used like in the double exponential\\ncase. Instead of estimating level and trend, we estimate level,\\ntrend, and seasonality separately.\\nThe family of exponential smoothing methods is not limited to\\nthe three that we just discussed. A way to think about the\\ndiﬀerent models is in terms of the trend and seasonal\\ncomponents of these models. The trend can either be no trend,\\nadditive, or additive damped. The seasonality can be no\\nseasonality, additive, or multiplicative. Every combination of\\nthese parameters is a diﬀerent technique in the family, as shown\\nin the following table:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 171, 'page_label': '172'}, page_content=\"Table 4.1 – Exponential smoothing family\\nNIXTLA has an entire family of exponential smoothing methods.\\nLet's see how we can initialize the ETS model in NIXTLA:\\nfrom statsforecast.models import (SimpleExponenti\\nexp_smooth = HoltWinters(error_type = 'A', season\\nHere the error_type = 'A' refers to additive error. The user\\nhas the option for either additive error or multiplicative error,\\nwhich could be called using error_type = 'M'. NIXTLA\\nmodels has an option to use AutoETS(). This model will\\nautomatically choose which exponential smoothing model is the\\nbest option. Simple exponential smoothing, double exponential\\nsmoothing (Holt's Method) or triple exponential smoothing\\n(Holt-Winters Method). It will also choose which parameters and\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 172, 'page_label': '173'}, page_content=\"error types are best for each individual time series. Refer to the\\nGitHub notebooks for examples on how to use AutoETS().\\nLet's see what the forecast using ETS looks like in Figure 4.4:\\nFigure 4.4 – Exponential smoothing forecast\\nThe forecast has captured the seasonality but has failed to\\ncapture the peaks. But we can see the improvement in MAE\\nalready.\\nNow, let's look at one of the most popular forecasting methods\\nout there.\\nARIMA\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 173, 'page_label': '174'}, page_content='Autoregressive Integrated Moving Average (ARIMA) models\\nare the other class of methods that, like ETS, have stood the test\\nof time and are one of the most popular classical methods of\\nforecasting. The ETS family of methods is modelled around\\ntrend and seasonality, while ARIMA relies on autocorrelation\\n(the correlation of\\nwith\\n,\\n, and so on).\\nThe simplest in the family are the AR(p) models, which use\\nlinear regression with p previous timesteps or, in other words,\\np lags. Mathematically, it can be written as follows:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 174, 'page_label': '175'}, page_content='Here, c is the intercept and\\nis the noise or error at timestep t.\\nThe next in the family are MA(q) models, in which instead of\\npast observed values, we use the past q errors in the forecast\\n(which is assumed to be pure white noise) to come up with a\\nforecast:\\nHere,\\nis white noise and c is the intercept. This is not typically used on\\nits own but in conjunction with AR(p) models, which makes the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 175, 'page_label': '176'}, page_content='next one on our list ARMA(p,q) models. ARMA models are\\ndeﬁned as\\n.\\nIn all the ARIMA models, there is one underlying assumption –\\nthe time series is stationary (we talked about stationarity in\\nChapter 1, Introducing Time Series, and will elaborate on this in\\nChapter 6, Feature Engineering for Time Series Forecasting).\\nThere are many ways to make the series stationary but taking\\nthe diﬀerence of successive values is one such technique. This is\\nknown as diﬀerencing. Sometimes, we need to do diﬀerencing\\nonce, while other times, we have to perform successive\\ndiﬀerencing before the time series becomes stationary. The\\nnumber of times we do the diﬀerencing operation is called the\\norder of diﬀerencing. The I in ARIMA, and the ﬁnal piece of the\\npuzzle, stands for Integrated. It deﬁnes the order of diﬀerencing\\nwe need to do before the series becomes stationary and is\\ndenoted by d.\\nSo, the complete ARIMA(p,d,q) model says that we do dth order\\nof diﬀerencing and then consider the last p terms in an'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 176, 'page_label': '177'}, page_content='autoregressive manner, and then include the last q moving\\naverage terms to come up with the forecast.\\nThe ARIMA models we have discussed so far only handle non-\\nseasonal time series. But using the same concepts we discussed,\\nbut on a seasonal cycle, we get Seasonal ARIMA. p, d, and q are\\nslightly tweaked so that they work on the seasonal period, m. To\\ndiﬀerentiate them from the normal p, d, andq, we call the\\nseasonal values P, D, and Q. For instance, if p meant taking the\\nlast p lags, P means taking the last P seasonal lags. If\\nis\\n,\\nwould be'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 177, 'page_label': '178'}, page_content=\". Similarly, D means the order of seasonal diﬀerencing.\\nPicking the right p, d, and q and P, D, and Q values is not very\\nintuitive, and we will have to resort to statistical tests to ﬁnd\\nthem. However, this becomes a bit impractical when you are\\nforecasting many time series. An automatic way of iterating\\nthrough the diﬀerent parameters and ﬁnding the best p, d, and\\nq, and P, D, and Q values for the data is called Auto ARIMA. In\\nPython, NIXTLA has implemented this method AutoARIMA().\\nNIXTLA also has a normal ARIMA implementation as well,\\nwhich is much faster but requires p,d,q to be entered manually.\\nPRACTICAL CONSIDERATIONS\\nAlthough ARIMA and Auto ARIMA can give you good-\\nperforming models in many cases, they can be quite slow\\nwhen you have long seasonal periods and a long time series.\\nIn our case, where we have almost 27k observations in the\\nhistory, ARIMA becomes very slow and a memory hog. Even\\nwhen subsetting the data, a single AutoARIMA ﬁt takes\\naround 60 minutes. Letting go of the seasonal parameters\\nbrings down the runtime drastically, but for a seasonal time\\nseries such as energy consumption, it doesn't make sense.\\nAuto ARIMA includes many such ﬁts to identify the best\\nparameters and therefore becomes impractical for long time\\nseries datasets. Almost all the implementations in the Python\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 178, 'page_label': '179'}, page_content=\"ecosystem suﬀer from this drawback. NIXLTA claims to have\\nthe fastest and most accurate version of AutoARIMA, faster\\nthan the original R method as well.\\nLet's see how we can apply ARIMA and AutoARIMA using\\nNIXTLA:\\nfrom statsforecast.models import (ARIMA, AutoARIM\\n#ARIMA model by specifying parameters\\narima_model = ARIMA(order = (2,1,1), seasonal_ord\\n#AutoARIMA model by specifying max limits for par\\nauto_arima_model = AutoARIMA( max_p = 2, max_d=1,\\nFor the entire list of parameters for AutoARIMA, head over to\\nthe NIXTLA documentation at\\nhttps://nixtlaverse.nixtla.io/statsforecast/docs/models/autoarima.\\nhtml.\\nLet's see what the ETS and ARIMA forecasts look like for the\\nhouseholds we were experimenting with:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 179, 'page_label': '180'}, page_content=\"Figure 4.5 – ETS and ARIMA forecasts (Color Figure)\\nWith NIXTLA, both ETS and ARIMA have done a good job at\\ncapturing both the seasonality and the peaks. The resulting MAE\\nscores are also very similar with 0.191 vs. 0.203 respectively.\\nNow, let's look at another method – the Theta Forecast.\\nTheta Forecast\\nThe Theta Forecast was the top-performing submission in the\\nM3 forecasting competition that was held in 2002. The method\\nrelies on a parameter,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 180, 'page_label': '181'}, page_content=', that ampliﬁes or smooths the local curvature of a time series,\\ndepending on the value chosen. Using\\n, we smooth or amplify the original time series. These smoothed\\nlines are called theta lines. V. Assimakopoulos and K.\\nNikolopoulos proposed this method as a decomposition\\napproach to forecasting. Although in theory any number of\\ntheta lines can be used, the originally proposed method used\\ntwo theta lines,\\nand\\n, and took an average of the forecast of the two theta lines as the\\nﬁnal forecast.\\nSIDE NOTE\\nThe M-competitions are forecasting competitions\\norganized by Spyros Makridakis, a leading forecasting\\nresearcher. They typically curate a dataset of time'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 181, 'page_label': '182'}, page_content='series, lay down the metrics with which the forecasts\\nwill be evaluated, and open these competitions to\\nresearchers all around the world to get the best forecast\\npossible. These competitions are considered to be some\\nof the biggest and most popular time series forecasting\\ncompetitions in the world. At the time of writing, six\\nsuch competitions have already been completed. To\\nlearn more about the latest competition, visit this\\nwebsite: https://mofc.unic.ac.cy/the-m6-competition/.\\nIn 2002, Rob Hyndman et al. simpliﬁed the Theta method and\\nshowed that we can use ETS with a drift term to get equivalent\\nresults to the original Theta method, which is what is adapted\\ninto most of the implementations of the method that exist today.\\nThe major steps that are involved in the Theta Forecast (which is\\nimplemented in NIXTLA) are as follows:\\n1. Deseasonalization: Apply a classical multiplicative\\ndecomposition to remove the seasonal component from the\\ntime series (if exists). This focuses the analysis on the\\nunderlying trend and cyclical components. Deseasonalization\\nis done using\\nstatsmodels.tsa.seasonal.seasonal_decompose. This step\\ncreates a new deseasonalized time series, .'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 182, 'page_label': '183'}, page_content='Theta Coeﬃcients Application: Decompose the deseasonalized\\nseries into 2 \"Theta\" lines using coeﬃcients and . These\\ncoeﬃcients modify the second diﬀerence of the time series to\\neither dampen ( local ﬂuctuations.\\n1. Extrapolation of Theta Lines: Treat each Theta line as a\\nseparate series and forecast them into the future. This is done\\nusing linear regression for the Theta line where producing a\\nstraight line, and simple exponential smoothing for the Theta\\nline where\\n2. Recomposition: Combine the forecasts from the two theta\\nlines. The original method uses equal weighting for both\\nlines, which integrates the long-term trend and short-term\\nmovements eﬀectively.\\n3. Reseasonalize: If the data was deseasonalized in the\\nbeginning.\\nNIXTLA has a very diﬀerent variations of the THETA method.\\nMore information on the speciﬁcs of the NIXTLA\\nimplementation can be found here:\\nhttps://nixtlaverse.nixtla.io/statsforecast/docs/models/autotheta.\\nhtml\\nLet\\'s see how we can use it practically:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 183, 'page_label': '184'}, page_content=\"theta_model = Theta(season_length =48, decomposit\\nThe key parameters here are as follows:\\nseason_length & decomposition_type: These parameters are\\nused for the initial seasonal decomposition. If left empty, the\\nimplementation automatically tests for seasonality and\\ndeseasonalizes the time series automatically using\\nmultiplicative decomposition. It is recommended to set these\\nparameters with our domain knowledge if we know them.\\nDecompositive type can be 'multiplicative' (default) or\\n'additive'.\\n1. :\\nLet's visualize the forecast we just generated using the Theta\\nForecast:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 184, 'page_label': '185'}, page_content='Figure 4.6 – The Theta Forecast\\nREFERENCE CHECK\\nThe research paper in which V. Assimakopoulos and K.\\nNikolopoulos proposed the Theta method is cited as\\nreference 1 in the References section, while subsequent\\nsimpliﬁcation by Rob Hyndman is cited as reference 2.\\nTBATS\\nSometimes a time series has more than 1 seasonality pattern or\\na non-integer seasonal period, commonly referred to as complex\\nseasonality. An example would be an hourly forecast could have\\na daily seasonality for time of day, a weekly seasonality for day'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 185, 'page_label': '186'}, page_content='of week, and a yearly seasonality for day in year. Additionally,\\nmost time series models are designed for smaller integer\\nseasonal periods, such as monthly (12) or quarterly (4) data, but\\nyearly seasonality can pose a problem since a year is 364.25\\ndays. TBATS was meant to combat these many challenges which\\npose problems for many forecasting models. However, with any\\nautomated approach, at times it is susceptible to poor forecasts.\\nTBATS stands for\\nTrigonometric seasonality\\nBox-Cox transformation\\nARMA errors\\nTrend\\nSeasonal components\\nThis model was ﬁrst introduced by Rob J Hyndman, Alysha M De\\nLivera, and Ralph D Snyder in 2011. There is also another\\nvariant of TBATS, referred to as BATS, which is without the\\ntrigonometric seasonality component. TBATS is from the state\\nspace model family. In state space forecasting models, the\\nobserved time series is assumed to be a combination of the\\nunderlying state variables and a measurement equation that\\nrelates the state variables to the observed data. The state\\nvariables capture the underlying patterns, trends, and\\nrelationships in the data.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 186, 'page_label': '187'}, page_content='BATS has parameters indicating the Box-Cox parameter,\\ndamping parameter, ARMA parameters and the seasonal periods\\nDue to its ﬂexibility, BATS model can be considered a family of\\nmodels encompassing many other models we have seen earlier.\\nFor instance:\\n= Holt Winters Additive Seasonality\\n= Holt Winters Additive Double Seasonality\\nBATS has the ﬂexibility for multiple seasonality; however, it has\\na limitation to only integer based seasonal periods, and with\\nmultiple seasonalities it can have a large number of states\\nresulting in increasing model complexity. This is what TBATS\\nwas meant to address.\\nFor reference, the TBATS parameter space is\\nThe main advantages of TBATS are as follows:\\n1. Works with single, complex, and non-integer seasonality\\n(Trigonometric Seasonality)\\n2. Handles nonlinear patterns common in real world time series\\n(Box-Cox Transformation)\\n3. Handles autocorrelation in the residuals (ARMA errors)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 187, 'page_label': '188'}, page_content='To better understand the inner workings of TBATS, lets break\\ndown each step.\\nThe order in which operations are done using TBATS is:\\n1. Box-Cox Transformation\\n2. Exponentially smoothed Trend\\n3. Seasonal Decomposition using Fourier series.\\n4. Autoregressive moving average (ARMA)\\n5. Parameter estimation through a likelihood based approach.\\nBox Cox Transformation\\nBox-Cox is a transformation in the family of power\\ntransformations.\\nIn time series, making data stationary is an important step\\nbefore forecasting (as discussed in chapter 1). Stationarity\\nensure that our data does not statistically change over time, and\\nthus more accurately resemble a probability distribution. There\\nare several possible transformations that could be applied. More\\ndetails on various target transformations, including Box-Cox can\\nbe found in chapter 7.\\nAs a preview, here is a sample output from a Box-Cox\\ntransformation. After the transformation, our data more closely'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 188, 'page_label': '189'}, page_content='resembles that of a normal distribution. Box-Cox\\ntransformations can only be used with positive data, but in\\npractice this is often the case. Figure 4.7 shows an example of\\nhow a time series might look before and after a Box-Cox\\ntransformation.\\nFigure 4.7 – Box-Cox Transformation\\nExponentially Smoothed Trend\\nUsing Locally Estimated Scatterplot Smoothing (LOESS), a\\nsmoothed trend is extracted from the time series.\\nLOESS works by applying a locally weighted, low-degree\\npolynomial regression over the data points to create a smooth,\\nﬂowing line through them. This technique is highly eﬀective in'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 189, 'page_label': '190'}, page_content='capturing local trend variations without assuming a global form\\nfor the data, which makes it particularly useful for data with\\nvarying trends or seasonal variations.\\nTrigonometric Seasonality\\nThe remaining residuals are then modelled using Fourier terms\\n(discussed in chapter 3) to decompose the seasonality\\ncomponent.\\nThe main advantage of using Fourier to model seasonality is its\\nability to model multiple seasonalities, as well as non-integer\\nseasonality, such as yearly seasonality with daily data, since\\nthere is 364.25 days in a year. Most other decomposition\\nmethods cannot handle the non-integer period, and has to resort\\nto rounding to 365 which can fail to identify the true seasonality.\\nAn example of what a decomposed time series would like using\\nFourier is below. The observed time series in this example is\\nhourly data. Therefore, our seasonal periods are:\\ndaily = 24\\nweekly = 24 * 7 = 168.\\nHere you can clearly see the deﬁned seasonal patterns, the\\ntrend, and the remaining residuals. Figure 4.8 shows the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 190, 'page_label': '191'}, page_content='decomposition of the trend and seasonality, which then the\\nresiduals are modelled using an ARMA process.\\nFigure 4.8 – Decomposed Time Series\\nAutoregressive Moving Average (ARMA)\\nARMA was discussed earlier as a subset from the ARIMA family.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 191, 'page_label': '192'}, page_content='The ARMA model in TBATS is used to model the remaining\\nresiduals to capture any autocorrelations of the lagged\\nvariables. The Autoregressive (AR) component captures to\\ncorrelation between an observation and several lagged\\nobservations. This deals with the momentum or continuation of\\nthe series. The moving average (MA) component models the\\nerror terms as a linear combination of errors at previous time\\nperiods, capturing information not explained by the AR part\\nalone.\\nParameter Optimization\\nTo select the optimal parameter space, TBATS will ﬁt several\\nmodels and automatically select the best parameters. A few of\\nthe models TBATS ﬁts internally are:\\nWith and without Box-Cox Transformation\\nWith and without Trend\\nWith and without Trend Damping\\nSeason and non-seasonal model\\nARMA(p,q) parameters\\nThe ﬁnal model is chosen by which combination of parameters\\nminimizes the Akaike Information Criterion (AIC), and auto-\\nARIMA is used to determine the ARMA parameters.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 192, 'page_label': '193'}, page_content=\"As with all forecasting methods, there are beneﬁts and trade-\\noﬀs to diﬀerent models. While TBATS oﬀers some enhancements\\non many other models' shortcomings, the trade-oﬀ is the need to\\nbuild many models which result in longer computation time.\\nThis can pose a problem if having to model multiple time series.\\nAdditionally, TBATS does not allow for the inclusion of\\nexogenous variables.\\nPractitioner's Note:\\nTBATS cannot handle exogenous regression since it is\\nrelated to ETS models per Hyndman himself, suggests it\\nis unlikely to include covariates (Hyndman, 2014). If\\nexternal regressors are to be used, other methods such\\nas ARIMAX or SARIMAX should be used. If the time\\nseries has complex seasonality, you can add Fourier\\nfeatures as covariates to your ARIMAX or SARIMAX\\nmodel to help capture the seasonal patterns.\\nThis is implemented in NIXLA, and we can use the\\nimplementation shown here:\\nTBATS_model = TBATS(seasonal_periods  = 48, use_t\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 193, 'page_label': '194'}, page_content=\"In NIXTLA, you can also use AutoTBATS to let the system\\noptimize how to handle the various parameters.\\nLet's see what the TBATS forecast looks like:\\nFigure 4.9 – TBATS forecast\\nAgain, the seasonality pattern has been replicated, and is\\ncapturing most of the peaks in the forecast.\\nMSTL\\nThe MSTL method in NIXTLA applies the LOESS technique to\\ndecompose a time series into its various seasonal components.\\nFollowing this decomposition, it employs a specialized non-\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 194, 'page_label': '195'}, page_content=\"seasonal model to forecast the trend, and a SeasonalNaive\\nmodel to predict each of the seasonal components. This\\napproach allows for the detailed analysis and forecasting of\\ntime series with complex seasonal patterns.\\nMSTL_model = MSTL(season_length  = 48)\\nMore information on the inner workings of MSTL was discussed\\nin Chapter 3.\\nLet's see what the MSTL forecast looks like:\\nFigure 4.10 – MSTL forecast\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 195, 'page_label': '196'}, page_content=\"Let's also take a look at how the diﬀerent metrics that we chose\\ndid for each of these forecasts for the household we were\\nexperimenting with (from the notebook):\\nFigure 4.11 – Summary of all the baseline algorithms\\nOut of all the baseline algorithms we tried, AutoETS is\\nperforming the best on both MAE as well as MSE. ARIMA was\\nthe second-best model followed by TBATS. However, if you look\\nat the Time Elapsed column, TBATS stands out taking just 7.4\\nseconds vs 19 seconds for ARIMA. Since they had similar\\nperformance, we will choose TBATS over ARIMA, along with\\nAutoETS as our baseline, and run them on all 399 households in\\nthe dataset (both validation and test) we've chosen (the code for\\nthis is available in the 02-Baseline Forecasts using\\nNIXTLA.ipynb notebook).\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 196, 'page_label': '197'}, page_content='Evaluating the baseline forecasts\\nSince we have the baseline forecasts generated from ETS as well\\nas TBATS, we should also evaluate these forecasts. The aggregate\\nmetrics for all the selected households for both these methods\\nare as follows:\\nValidation\\nFigure 4.12 – The aggregate metrics of all the selected\\nhouseholds (both validation and test)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 197, 'page_label': '198'}, page_content=\"It looks like AutoETS is performing much better in all three\\nmetrics. We also have these metrics calculated at a household\\nlevel. Let's look at the distribution of these metrics in the\\nvalidation dataset for all the selected households:\\nFigure 4.13 – The distribution of MASE and forecast bias of the\\nbaseline forecast in the validation dataset\\nThe MASE histogram of ETS seems to have a smaller spread than\\nTBATS. ETS also has a lower median MASE than TBATS. We can\\nsee a similar pattern for forecast bias as well, with the forecast\\nbias of ETS centered around zero and much less spread.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 198, 'page_label': '199'}, page_content='Back in Chapter 1, Introducing Time Series, we saw why every\\ntime series is not equally predictable and saw three factors to\\nhelp us think about the issue – understanding the Data\\nGenerating Process (DGP), the amount of data, and adequately\\nrepeating the pattern. In most cases, the ﬁrst two are pretty easy\\nto evaluate, but the third one requires some analysis. Although\\nthe performance of baseline methods gives us some idea about\\nhow predictable any time series is, they still are model-\\ndependent. So, instead of measuring how well a time series is\\nforecastable, we might be better measuring how well the chosen\\nmodel can approximate the time series. This is where a few\\ntechniques that are more fundamental (relying on the statistical\\nproperties of a time series) come in.\\nAssessing the forecastability of a time\\nseries\\nAlthough there are many statistical measures that we can use to\\nassess the predictability of a time series, we will just look at a\\nfew that are easier to understand and practical when dealing\\nwith large time series datasets. The associated notebook (02-\\nForecastability.ipynb) contains the code to follow along.\\nCoeﬃcient of Variation (CoV)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 199, 'page_label': '200'}, page_content=\"The Coeﬃcient of Variation (CoV) relies on the intuition that\\nthe more variability that you ﬁnd in a time series, the harder it\\nis to predict it. And how do we measure variability in a random\\nvariable? Standard deviation.\\nIn many real-world time series, the variation we see in the time\\nseries is dependent on the scale of the time series. Let's imagine\\nthat there are two retail products, A and B. A has a mean\\nmonthly sale of 15, while B has 50. If we look at a few real-world\\nexamples like this, we will see that if A and B have the same\\nstandard deviation, B, which has a higher mean, is much more\\nforecastable than A. To accommodate this phenomenon and to\\nmake sure we bring all the time series in a dataset to a common\\nscale, we can use the CoV:\\nHere,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 200, 'page_label': '201'}, page_content=\"is the standard deviation and\\nis the mean of the time series, n.\\nThe CoV is the relative dispersion of data points around the\\nmean, which is much better than looking at the pure standard\\ndeviation.\\nThe larger the value for the CoV, the worse the predictability of\\nthe time series. There is no hard cutoﬀ, but a value of 0.49 is\\nconsidered a rule of thumb to separate time series that are\\nrelatively easier to forecast from the hard ones. But depending\\non the general hardness of the dataset, we can tweak this cutoﬀ.\\nSomething I have found useful is to plot a histogram of CoV\\nvalues in a dataset and derive cutoﬀs based on that.\\nEven though the CoV is widely used in the industry, it suﬀers\\nfrom a few key issues:\\n1. It doesn't consider seasonality. A sine or cosine wave will\\nhave a higher CoV than a horizontal line, but we know both\\nare equally predictable.\\n2. It doesn't consider the trend. A linear trend will make a series\\nhave a higher CoV, but we know it is equally predictable like a\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 201, 'page_label': '202'}, page_content=\"horizontal line is.\\n3. It doesn't handle negative values in the time series. If you\\nhave negative values, it makes the mean smaller, thereby\\ninﬂating the CoV.\\nTo overcome these shortcomings, we propose another derived\\nmeasure.\\nResidual variability (RV)\\nThe thought behind residual variability (RV) is to try and\\nmeasure the same kind of variability that we were trying to\\ncapture with the CoV but without the shortcomings. I was\\nbrainstorming on ways to avoid the problems of using the CoV,\\ntypically the seasonality issue, and was applying the CoV to the\\nresiduals after seasonal decomposition. It was then I realized\\nthat the residuals would have a few negative values and that the\\nCoV wouldn't work well. Stefan de Kok, who is a thought leader\\nin demand forecasting and probabilistic forecasting, suggested\\nusing the mean of the original actuals, which worked.\\nTo calculate RV, you must perform the following steps:\\n1. Perform seasonal decomposition.\\n2. Calculate the standard deviation of the residuals or the\\nirregular component.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 202, 'page_label': '203'}, page_content='3. Divide the standard deviation by the mean of the original\\nobserved values (before decomposition).\\nThe key assumption here is that seasonality and trend are\\ncomponents that can be predicted. Therefore, our assessment of\\nthe predictability of a time series should only look at the\\nvariability of the residuals. But we cannot use CoV on the\\nresiduals because the residuals can have negative and positive\\nvalues, so the mean of the residuals loses the interpretation of\\nthe level of the series and tends to zero. When residuals tend to\\nzero, the CoV measure tends to inﬁnity because of the division\\nby mean. Therefore, we use the mean of the original series as\\nthe scaling factor.\\nLet\\'s see how we can calculate RV for all the time series in our\\ndataset (which are in a compact form):\\nblock_df[\"rv\"] = block_df.progress_apply(lambda x\\nIn this section, we looked at two measures that are based on the\\nstandard deviation of the time series. Now, let\\'s look at assessing\\nthe forecastability of a time series.\\nEntropy-based measures'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 203, 'page_label': '204'}, page_content='Entropy is a ubiquitous term in science. We see it popping up in\\nPhysics, quantum mechanics, social sciences, and information\\ntheory. And everywhere, it is used to talk about a measure of\\nchaos or lack of predictability in a system. The entropy we are\\nmost interested in now is the one from Information Theory.\\nInformation Theory involves quantifying, storing, and\\ncommunicating digital information.\\nClaude E. Shannon presented the qualitative and quantitative\\nmodel of communication as a statistical process in his seminal\\npaper A Mathematical Theory of Communication. While the\\npaper introduced a lot of ideas, some of the concepts that are\\nrelevant to us are Information Entropy and the concept of a bit –\\na fundamental unit of measurement of information.\\nREFERENCE CHECK\\nA Mathematical Theory of Communication by Claude E.\\nShannon is cited as reference 3 in the References\\nsection.\\nThe theory in itself is quite a lot to cover, but to summarize the\\nkey bits of information, take a look at the following short\\nglossary:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 204, 'page_label': '205'}, page_content=\"Information is nothing but a sequence of symbols, which can\\nbe transmitted from the receiver to the sender through a\\nmedium, which is called a channel. For instance, when we are\\ntexting somebody, the sequence of symbols are the\\nletters/words of the language in which we are texting; the\\nchannel is the electronic medium.\\nEntropy can be thought of as the amount of uncertainty or\\nsurprise in a sequence of symbols given some distribution of\\nthe symbols.\\nA bit, as we mentioned earlier, is a unit of information and is\\na binary digit. It can either be 0 or 1.\\nNow, if we were to transfer 1 bit of information, it would reduce\\nthe uncertainty of the receiver by 2. To understand this better,\\nlet's consider a coin toss. We toss the coin in the air, and as it is\\nspinning through the air, we don't know whether it is going to be\\nheads or tails. But we know it is going to be one of these two.\\nWhen the coin hits the ground and ﬁnally comes to rest, we ﬁnd\\nthat it is heads. We can represent whether the coin toss is heads\\nor tails with 1 bit of information (0 for heads and 1 for tails). So,\\nthe information that was passed to us when the coin fell\\nreduced the possible outcomes from two to one (heads). This\\ntransfer was possible with 1 bit of information.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 205, 'page_label': '206'}, page_content=\"In Information Theory, the entropy of a discrete random\\nvariable is the average level of information, surprise, or\\nuncertainty inherent in the variable's possible outcomes. In\\nmore technical parlance, it is the expected number of bits\\nrequired for the best possible encoding scheme of the\\ninformation present in the random variable.\\nADDITIONAL READING\\nIf you want to intuitively understand entropy, cross-\\nentropy, Kullback-Leibler divergence, and so on, head\\nover to the Further reading section. There are a couple\\nof links to blogs (one of which is my own) where we try\\nto lay down the intuition behind these metrics).\\nEntropy is formally deﬁned as follows:\\nHere, X is the discrete random variable with possible outcomes,\\nand\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 206, 'page_label': '207'}, page_content=\". Each of those outcomes has a probability of occurring, which is\\ndenoted by and\\n.\\nTo develop some intuition around this, we can think that the\\nmore spread out a probability distribution is, the more chaos is\\nin the distribution, and thus more entropy. Let's quickly check\\nthis in code:\\n# Creating an array with a well balanced probabil\\nflat = np.array([0.1,0.2, 0.3,0.2, 0.2])\\n# Calculating Entropy\\nprint((-np.log2(flat)* flat).sum())\\n>> 2.2464393446710154\\n# Creating an array with a peak in probability\\nsharp = np.array([0.1,0.6, 0.1,0.1, 0.1])\\n# Calculating Entropy\\nprint((-np.log2(sharp)* sharp).sum())\\n>> 1.7709505944546688\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 207, 'page_label': '208'}, page_content='Here, we can see that the probability distribution that spread its\\nmass has higher entropy.\\nIn the context of a time series, n is the total number of time\\nseries observations, and\\nis the probability for each symbol of the time series alphabet. A\\nsharp distribution means that the time series values are\\nconcentrated on a small area and should be easier to predict. On\\nthe other hand, a wide or ﬂat distribution means that the time\\nseries value can be equally likely across a wider range of values\\nand hence is diﬃcult to predict.\\nIf we have two time series – one containing the result of a coin\\ntoss and the other containing the result of a dice throw – the\\ndice throw would have any output between one and six,\\nwhereas the coin toss would be either zero or one. The coin toss\\ntime series would have lower entropy and be easier to predict\\nthan the dice throw time series.\\nBut since time series is typically continuous, and entropy\\nrequires a discrete random variable, we can resort to a few\\nstrategies to convert the continuous time series into a discrete\\none. Many strategies, such as quantization or binning, can be'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 208, 'page_label': '209'}, page_content=\"applied, which leads to a myriad of complexity measures. Let's\\nreview one such measure that is useful and practical.\\nSpectral entropy\\nTo calculate the entropy of a time series, we need to discretize\\nthe time series. One way to do that is by using FFT and power\\nspectral density (PSD). This discretization of the continuous\\ntime series is used to calculate spectral entropy.\\nWe learned what Fourier Transform is earlier in this chapter\\nand used it to generate a baseline forecast. But using FFT, we\\ncan also estimate a quantity called power spectral density. This\\nanswers the question, How much of the signal is at a particular\\nfrequency? There are many ways of estimating power spectral\\ndensity from a time series, but one of the easiest ways is by\\nusing the Welch method, which is a non-parametric method\\nbased on Discrete Fourier Transform. This is also implemented\\nas a handy function with the periodogram(x) signature in\\nscipy.\\nThe returned PSD will have a length equal to the number of\\nfrequencies estimated, but these are densities and not well-\\ndeﬁned probabilities. So, we need to normalize PSD to be\\nbetween zero and one:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 209, 'page_label': '210'}, page_content='Here, F is the number of frequencies that are part of the\\nreturned power spectrum density.\\nNow that we have the probabilities, we can just plug this into\\nthe entropy formula and arrive at the spectral entropy:\\nWhen we introduced entropy-based measures, we saw that the\\nmore spread out the probability mass of a distribution is, the\\nhigher the entropy is. In this context, the more frequencies\\nacross which the spectral density is spread, the higher the\\nspectral entropy. So, a higher spectral entropy means the time\\nseries is more complex and therefore more diﬃcult to forecast.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 210, 'page_label': '211'}, page_content='Since FFT has an assumption of stationarity, it is recommended\\nthat we make the series stationary before using spectral entropy\\nas a metric. We can even apply this metric to a detrended and\\ndeseasonalized time series, which we can refer to as residual\\nspectral entropy. This book\\'s GitHub repository contains an\\nimplementation of spectral entropy under\\nsrc.forecastability.entropy.spectral_entropy. This\\nimplementation also has a parameter, transform_stationary,\\nwhich, if set to True, will detrend the series before we apply\\nspectral entropy. Let\\'s see how we can calculate spectral entropy\\nfor our dataset:\\nfrom src.forecastability.entropy import spectral_\\nblock_df[\"spectral_entropy\"] = block_df.energy_co\\nblock_df[\"residual_spectral_entropy\"] = block_df.\\nThere are other entropy-based measures such as approximate\\nentropy and sample entropy, but we will not cover those in this\\nbook. They are more computationally intensive and don\\'t tend\\nto work for time series that contain fewer than 200 values. If\\nyou are interested in learning more about these measures, head\\nover to the Further reading section.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 211, 'page_label': '212'}, page_content=\"Another metric that takes a slightly diﬀerent path is the\\nKaboudan metric.\\nKaboudan metric\\nIn 1999, Kaboudan deﬁned a metric for time series\\npredictability, calling it the\\n-metric. The idea behind it is very simple. If we block -shuﬄe a\\ntime series, we are essentially destroying the information in the\\ntime series. Block shuﬄing is the process of dividing the time\\nseries into blocks and then shuﬄing those blocks. So, if we\\ncalculate the sum of squared errors (SSE) of a forecast that's\\nbeen trained on a time series and then contrast it with the SSE of\\na forecast trained on a shuﬄed time series, we can infer the\\npredictability of the time series. The formula to calculate this is\\nas follows:\\nHere,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 212, 'page_label': '213'}, page_content='is the SSE of the forecast that was generated from the original\\ntime series, while\\nis the SSE of the forecast that was generated from the block-\\nshuﬄed series. If the time series contains some predictable\\nsignals,\\nwould be lower than\\nand'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 213, 'page_label': '214'}, page_content='would approach one. This is because there was some\\ninformation or patterns that were broken due to the block\\nshuﬄing. On the other hand, if a series is just white noise\\n(which is unpredictable by deﬁnition) there would be hardly\\nany diﬀerence between\\nand\\n, and\\nwould approach zero. In 2002, Duan investigated this metric and\\nsuggested some modiﬁcations in his thesis. One of the problems\\nhe identiﬁed, especially in long time series, is that the\\nvalues are found in a narrow band around 1 and suggested a\\nslight modiﬁcation to the formula. We call this the modiﬁed'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 214, 'page_label': '215'}, page_content='Kaboudan metric. The measure on the lower side is also\\nclipped to zero. Sometimes, the metric can go below zero\\nbecause\\nis lower than\\n, which is because the series is unpredictable and, by pure\\nchance, block shuﬄing made the SSE lower:\\nREFERENCE CHECK\\nThe research paper that proposed the Kaboudan metric\\nis cited as reference 4 in the References section. The'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 215, 'page_label': '216'}, page_content='subsequent modiﬁcation that Duan suggested is cited as\\nreference 5.\\nThis modiﬁed version, as well as the original, has been\\nimplemented in this book\\'s GitHub repository.\\nThere is no restriction on the forecasting model you use to\\ngenerate the forecast, which makes it a bit more ﬂexible. Ideally,\\nwe can choose one of the classical statistical methods that is fast\\nenough to be applied to the whole dataset. But this also makes\\nthe Kaboudan metric dependent on the model, and the\\nlimitations of the model are inherent in the metric. The metric\\nmeasures a combination of how diﬃcult a series is to forecast\\nand how diﬃcult it is for the model to forecast the series.\\nAgain, both metrics are implemented in this book\\'s GitHub\\nrepository. Let\\'s see how we can use them:\\nfrom src.forecastability.kaboudan import kaboudan\\nmodel = Theta(theta=3, seasonality_period=48*7, s\\nblock_df[\"kaboudan_metric\"] = [kaboudan_metric(r[\\nblock_df[\"modified_kaboudan_metric\"] = [modified_\\nAlthough there are many more metrics we can use for this\\npurpose, the metrics we just reviewed for assessing'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 216, 'page_label': '217'}, page_content='forecastability cover a lot of the popular use cases and should be\\nmore than enough to gauge any time series dataset in regards to\\nthe diﬃculty of forecasting it. We can use these metrics to\\ncompare onetime series with another time series or to proﬁle a\\nwhole set of related time series in a dataset with another dataset\\nfor benchmarking purposes.\\nADDITIONAL READING\\nIf you want to delve a little deeper and analyze the\\nbehavior of these metrics, how similar they are to each\\nother, and how eﬀective they are in measuring\\nforecastability, go to the end of the 03-\\nForecastability.ipynb notebook. We compute rank\\ncorrelations among these metrics to understand how\\nsimilar these metrics are. We can also ﬁnd rank\\ncorrelations with the computed metrics from the best\\nperforming baseline method to understand how well\\nthese metrics did in estimating the forecastability of a\\ntime series. I strongly encourage you to play around\\nwith the notebook and understand the diﬀerences\\nbetween the diﬀerent metrics. Pick a few time series and\\ncheck how the diﬀerent metrics give you slightly\\ndiﬀerent interpretations.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 217, 'page_label': '218'}, page_content=\"Congratulations on generating your baseline forecasts – the ﬁrst\\nset of forecasts we have generated using this book! Feel free to\\nhead over to the notebooks and play around with the\\nparameters of the methods and see how forecasts change. It'll\\nhelp you develop an intuition around what the baseline\\nmethods are doing. If you are interested in learning more about\\nhow to make these baseline methods better, head over to the\\nFurther reading section, where we have provided a link to the\\npaper The Wisdom of the Data: Getting the Most Out of\\nUnivariate Time Series Forecasting, by F. Petropoulos and E.\\nSpiliotis.\\nSummary\\nAnd with this, we have come to the end of Section 1, Getting\\nFamiliar with Time Series. We have come a long way from just\\nunderstanding what a time series is to generating competitive\\nbaseline forecasts. Along the way, we learned how to handle\\nmissing values and outliers and how to manipulate time series\\ndata using pandas. We used all those skills on a real-world\\ndataset regarding energy consumption. We also looked at ways\\nto visualize and decompose time series. In this chapter, we set\\nup a test harness, learned how to use the NIXTLA library to\\ngenerate a baseline forecast, and looked at a few metrics that\\ncan be used to understand the forecastability of a time series.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 218, 'page_label': '219'}, page_content='For some of you, this may be a refresher, and we hope this\\nchapter added some value in terms of some subtleties and\\npractical considerations. For the rest of you, we hope you are in\\na good place, foundationally, to start venturing into modern\\ntechniques using machine learning in the next section of the\\nbook.\\nIn the next chapter, we will discuss the basics of machine\\nlearning and delve into time series forecasting.\\nReferences\\nThe following references were provided in this chapter:\\nAssimakopoulos, Vassilis and Nikolopoulos, K.. (2000). The\\ntheta model: A decomposition approach to forecasting.\\nInternational Journal of Forecasting. 16. 521-530.\\nhttps://www.researchgate.net/publication/223049702_The_thet\\na_model_A_decomposition_approach_to_forecasting.\\nRob J. Hyndman, Baki Billah. (2003). Unmasking the Theta\\nmethod. International Journal of Forecasting. 19. 287-290.\\nhttps://robjhyndman.com/papers/Theta.pdf.\\nShannon, C.E. (1948), A Mathematical Theory of\\nCommunication. Bell System Technical Journal, 27: 379-423.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 219, 'page_label': '220'}, page_content='https://people.math.harvard.edu/~ctm/home/text/others/shan\\nnon/entropy/entropy.pdf.\\nKaboudan, M. (1999). A measure of time series\\' predictability\\nusing genetic programming applied to stock returns. Journal of\\nForecasting, 18, 345-357:\\nhttp://www.aiecon.org/conference/efmaci2004/pdf/GP_Basics_\\npaper.pdf.\\nDuan, M. (2002). TIME SERIES PREDICTABILITY:\\nhttps://citeseerx.ist.psu.edu/viewdoc/download?\\ndoi=10.1.1.68.1898&rep=rep1&type=pdf.\\nDe Livera, A. M., & Hyndman, R. J. (2009). Forecasting time\\nseries with complex seasonal patterns using exponential\\nsmoothing ( Department of Econometrics and Business\\nStatistics Working Paper Series 15/09)\\nHyndman, Rob. \"Rob J Hyndman - TBATS with Regressors.\"\\nRob J Hyndman, 6 Oct. 2014,\\nhttp://robjhyndman.com/hyndsight/tbats-with-regressors\\nFurther reading\\nTo learn more about the topics that were covered in this chapter,\\ntake a look at the following resources:\\nInformation Theory and Entropy, by Manu Joseph:\\nhttps://deep-and-shallow.com/2020/01/09/deep-learning-and-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 220, 'page_label': '221'}, page_content='information-theory/.\\nVisual Information, by Chris Olah:\\nhttps://colah.github.io/posts/2015-09-Visual-Information.\\nFourier Transform: https://betterexplained.com/articles/an-\\ninteractive-guide-to-the-fourier-transform/.\\nFourier Transform by 3blue1brown – a visual introduction:\\nhttps://www.youtube.com/watch?v=spUNpyF58BY&vl=en.\\nUnderstanding Fourier Transform by Example, by Richie Vink:\\nhttps://www.ritchievink.com/blog/2017/04/23/understanding-\\nthe-fourier-transform-by-example/.\\nDelgado-Bonal A, Marshak A. Approximate Entropy and\\nSample Entropy: A Comprehensive Tutorial. Entropy. 2019;\\n21(6):541: https://www.mdpi.com/1099-4300/21/6/541.\\nYentes, J.M., Hunt, N., Schmid, K.K. et al. The Appropriate Use\\nof Approximate Entropy and Sample Entropy with Short Data\\nSets. Ann Biomed Eng 41, 349–365 (2013):\\nhttps://doi.org/10.1007/s10439-012-0668-3\\nPonce-Flores M, Frausto-Solís J, Santamaría-Bonﬁl G, Pérez-\\nOrtega J, González-Barbosa JJ. Time Series Complexities and\\nTheir Relationship to Forecasting Performance. Entropy. 2020;\\n22(1):89. https://www.mdpi.com/1099-4300/22/1/89\\nPetropoulos F, Spiliotis E. The Wisdom of the Data: Getting the\\nMost Out of Univariate Time Series Forecasting. Forecasting.\\n2021; 3(3):478-497. https://doi.org/10.3390/forecast3030029'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 221, 'page_label': '222'}, page_content='5 Time Series Forecasting as\\nRegression'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 222, 'page_label': '223'}, page_content=\"Join our book community on Discord\\nhttps://packt.link/EarlyAccess/\\nIn the previous part of the book, we have developed a\\nfundamental understanding of time series and equipped\\nourselves with tools and techniques to analyze and visualize\\ntime series and even generate our ﬁrst baseline forecasts. We\\nhave mainly covered classical and statistical techniques in this\\nbook so far. Let's now dip our toes into modern machine\\nlearning and learn how we can leverage this comparatively\\nnewer ﬁeld for time series forecasting. Machine learning is a\\nﬁeld that has grown in leaps and bounds in recent times, and\\nbeing able to leverage these newer techniques for time series\\nforecasting is a skill that will be invaluable in today's world.\\nIn this chapter, we will be covering these main topics:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 223, 'page_label': '224'}, page_content='Understanding the basics of machine learning\\nTime series forecasting as regression\\nLocal versus global models\\nUnderstanding the basics of machine\\nlearning\\nBefore we get started with using machine learning for time\\nseries, let\\'s spend some time establishing what machine\\nlearning is and setting up a framework to demonstrate what it\\ndoes (if you are already very comfortable with machine\\nlearning, feel free to skip ahead, or just stay with us and refresh\\nthe concepts). In 1959, Arthur Samuel deﬁned machine learning\\nas a \"ﬁeld of study that gives computers the ability to learn\\nwithout being explicitly programmed.\" Traditionally,\\nprogramming has been a paradigm under which we know a set\\nof rules/logic to perform an action, and that action is performed\\non the given data to get the output that we want. But, machine\\nlearning ﬂipped this on its head. In machine learning, we start\\nwith data and the output, and we ask the computer to tell us\\nabout the rules with which the desired output can be achieved\\nfrom the data:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 224, 'page_label': '225'}, page_content=\"Figure 5.1 – Traditional programming versus machine learning\\nThere are many kinds of problem settings in machine learning,\\nsuch as supervised learning, unsupervised learning, self-\\nsupervised learning, and so on, but we will stick to supervised\\nlearning, which is the most popular one and the most\\napplicable one to the contents of this book.\\nLet's start our discussion small and slowly build up to the whole\\nschematic, which encapsulates most of the key components of a\\nsupervised machine learning problem:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 225, 'page_label': '226'}, page_content='Figure 5.2 – Supervised machine learning schematic, part 1 – the\\nideal function\\nAs we already discussed, what we want from machine learning\\nis to learn from the data and come up with a set of rules/logic.\\nThe closest analogy in mathematics for logic/rules is a function,\\nwhich takes in an input (here, data) and provides an output.\\nMathematically, it can be written as follows:\\nwhere X is the set of features and g is the ideal target function\\n(denoted by 1 in Figure 5.2) that maps the X input (denoted by 2\\nin the schematic) to the target (ideal) output, y (denoted by 3 in\\nthe schematic). The ideal target function is largely an unknown\\nfunction, similar to the data generating process (DGP) we saw\\nin Chapter 1, Introducing Time Series, which is not in our\\ncontrol.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 226, 'page_label': '227'}, page_content='Figure 5.3 – Supervised machine learning schematic, part 2 – the\\nlearned approximation\\nBut, we want the computer to learn this ideal target function.\\nThis approximation of the ideal target function is denoted by\\nanother function, h (4 in the schematic), which takes in the\\nsame set of features, X, and outputs a predicted target, (5 in the\\nschematic). are the parameters of the h function (or model\\nparameters):'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 227, 'page_label': '228'}, page_content='Figure 5.4 – Supervised machine learning schematic, part 3 –\\nputting it all together\\nNow, how do we ﬁnd this approximation h function and its\\nparameters, ? With the dataset of examples (6 in the schematic).\\nThe supervised machine learning problem works on the\\npremise that we are able to collect a set of examples that shows\\nthe features, X, and the corresponding target, y, which is also\\nreferred to as labels in literature. It is from this set of examples\\n(the dataset) that the computer learns the approximation\\nfunction, h, and the optimal model parameters, . In the\\npreceding diagram, the only real unknown entity is the ideal\\ntarget function, g. So, we can use the training dataset, D, to get\\npredicted targets for every sample in the dataset. We already\\nknow the ideal target for all the examples. We need a way to\\ncompare the ideal targets and predicted targets, and this is\\nwhere the loss function (7 in the schematic) comes in. This loss\\nfunction tells us how far away from the real truth we are with\\nthe approximated function, h. Although h can be any function,\\nit is typically chosen from a set of a well-known class of\\nfunctions, is the ﬁnite set of functions that can be ﬁt to the data.\\nThis class of functions is what we colloquially call models. For\\ninstance, h can be chosen from all the linear functions or all the\\ntree-based functions, and so on. Choosing an h from is done by\\na combination of hyperparameters (which the modeler\\nspeciﬁes) and the model parameter, which is learned from data.\\nNow, all that is left is to run through the diﬀerent functions so\\nthat we ﬁnd the best approximation function, h, which gives us\\nthe lowest loss. This is an optimization process that we call\\ntraining.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 228, 'page_label': '229'}, page_content=\"Let's also take a look at a few key concepts, which will be\\nimportant in all our discussions ahead.\\nSupervised machine learning tasks\\nMachine learning can be used to solve a wide variety of tasks\\nsuch as regression, classiﬁcation, and recommendation. But,\\nsince classiﬁcation and regression are the most popular classes\\nof problems, we will spend just a little bit of time reviewing\\nwhat they are.\\nThe diﬀerence between classiﬁcation and regression tasks is\\nvery simple. In the machine learning schematic (Figure 5.2), we\\ntalked about y, the target. This target can be either a real-valued\\nnumber or a class of items. For instance, we could be predicting\\nthe stock price for next week or we could just predict whether\\nthe stock was going to go up or down. In the ﬁrst case, we are\\npredicting a real-valued number, which is called regression. In\\nthe other case, we are predicting one out of two classes (up or\\ndown), and this is called classiﬁcation.\\nOverﬁtting and underﬁtting\\nThe biggest challenge in machine learning systems is that the\\nmodel we trained must perform well on a new and unseen\\ndataset. The ability of a machine learning model to do that is\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 229, 'page_label': '230'}, page_content=\"called the generalization capability of the model. The training\\nprocess in a machine learning setup is akin to mathematical\\noptimization, with one subtle diﬀerence. The aim of\\nmathematical optimization is to arrive at the global maxima in\\nthe provided dataset. But in machine learning, the aim is to\\nachieve low test error by using the training error as a proxy.\\nHow well a machine learning model is doing on training error\\nand testing error is closely related to the concepts of overﬁtting\\nand underﬁtting. Let's use an example to understand these\\nterms.\\nThe learning process of a machine learning model has many\\nparallels to how humans learn. Suppose three students, A, B,\\nand C, are studying for an examination. A is a slacker and went\\nclubbing the night before. B decided to double down and\\nmemorize the textbook end to end. And, C paid attention in\\nclass and understood the topics up for the examination.\\nAs expected, A ﬂunked the examination, C got the highest score,\\nand B did OK.\\nA ﬂunked the examination because they didn't learn enough.\\nThis happens to machine learning models as well when they\\ndon't learn enough patterns, and this is called underﬁtting.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 230, 'page_label': '231'}, page_content=\"This is characterized by high training errors and high test\\nerrors.\\nB didn't score as highly as expected; after all, they did memorize\\nthe whole text, word for word. But many questions in the\\nexamination weren't directly from the textbook and B wasn't\\nable to answer them correctly. In other words, the questions in\\nthe examination were new and unseen. And because B\\nmemorized everything but didn't make an eﬀort to understand\\nthe underlying concepts, B wasn't able to generalize the\\nknowledge they had to new questions. This situation, in\\nmachine learning, is called overﬁtting. This is typically\\ncharacterized by a big delta in training and test errors.\\nTypically, we will see very low training errors and high test\\nerrors.\\nThe third student, C, learned the right way and understood the\\nunderlying concepts and because of that, was able to generalize\\nto new and unseen questions. This is the ideal state for a\\nmachine learning model, as well. This is characterized by\\nreasonably low test errors and a small delta between training\\nand test errors.\\nWe just saw the two greatest challenges in machine learning.\\nNow, let's also look at a few ways we have that can be used to\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 231, 'page_label': '232'}, page_content=\"tackle these challenges.\\nThere is a close relationship between the capacity of a model\\nand underﬁtting or overﬁtting. A model's capacity is its ability\\nto be ﬂexible enough to ﬁt a wide variety of functions. Models\\nwith low capacity may struggle to ﬁt the training data, leading\\nto underﬁtting. Models with high capacity may overﬁt by\\nmemorizing the training data too much. Just to develop an\\nintuition around this concept of capacity, let's look at an\\nexample. When we move from linear regression to polynomial\\nregression, we are adding more capacity to the model. Instead\\nof ﬁtting just straight lines, we are letting the model ﬁt curved\\nlines as well. Machine learning models generally do well when\\ntheir capacity is appropriate for the learning problem at hand.\\nFigure 5.5 – Underﬁtting versus overﬁtting\\nFigure 5.5 shows a very popular case to illustrate overﬁtting and\\nunderﬁtting. We create a few random points using a known\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 232, 'page_label': '233'}, page_content='function and try to learn that by using those data samples. We\\ncan see that the linear regression, which is one of the simplest\\nmodels, has underﬁtted the data by drawing a straight line\\nthrough those points. Polynomial regression is linear\\nregression, but with some higher-order features. For now, you\\ncan consider the move from linear regression to polynomial\\nregression with higher degrees as increasing the capacity of the\\nmodel. So, when we use a degree of 4, we see that the learned\\nfunction ﬁts the data well and matches our ideal function. But if\\nwe keep increasing the capacity of the model and reach degree\\n= 15, we see that the learned function is still passing through\\nthe training samples, but has learned a very diﬀerent function,\\noverﬁtting to the training data. Finding the optimal capacity to\\nlearn a generalizable function is one of the core challenges of\\nmachine learning.\\nWhile capacity is one aspect of the model, another aspect is\\nregularization. Even with the same capacity, there are multiple\\nfunctions a model can choose from the hypothesis space of all\\nfunctions. With regularization, we try to give preference to a set\\nof functions in the hypothesis space over the others. While all\\nthese functions are valid functions that can be chosen, we\\nnudge the optimization process in such a way that we end up\\nwith a kind of function toward which we have a preference.\\nAlthough regularization is a general term used to refer to any'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 233, 'page_label': '234'}, page_content=\"kind of constraint we place on the learning process to reduce\\nthe complexity of the learned function, more commonly, it is\\nused in the form of a weight decay. Let's take an example of\\nlinear regression, which is when we ﬁt a straight line to the\\ninput features by learning a weight associated with each\\nfeature.\\nA linear regression model can be written mathematically as\\nfollows:\\nHere, N is the number of features, c is the intercept, is the ith\\nfeature, and is the weight associated with the ith feature. We\\nestimate the right weight (L) by considering this as an\\noptimization problem that minimizes the error between\\nand y (real output).\\nNow, with regularization, we add an additional term to L,\\nwhich forces the weights to become smaller. Commonly, this is\\ndone using an l1 or l2 regularizer. An l1 regularizer is when you\\nadd the sum of squared weights to L:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 234, 'page_label': '235'}, page_content=\"where\\nis the regularization coeﬃcient that determines how strongly\\nwe penalize the weights. An l2 regularizer is when you add the\\nsum of absolute weights to L:\\nIn both cases, we are enforcing a preference for smaller\\nweights over larger weights because it keeps the function from\\nrelying too much on any one feature from the ones used in the\\nmachine learning model. Regularization is an entire topic unto\\nitself and if you want to learn more, head over to the Further\\nreading section for a few resources on regularization.\\nAnother really eﬀective way to reduce overﬁtting is to simply\\ntrain the model with more data. With a larger dataset, the\\nchances of the model overﬁtting become less because of the\\nsheer variety that can be captured in a large dataset.\\nNow, how do we tune the knobs to strike a balance between\\nunderﬁtting and overﬁtting? Let's look at it in the following\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 235, 'page_label': '236'}, page_content=\"section.\\nHyperparameters and validation sets\\nAlmost all machine learning models have a few\\nhyperparameters associated with them. Hyperparameters are\\nparameters of the model that are not learned from data but\\nrather are set before the start of training. For instance, the\\nweight of the regularization is a hyperparameter. Most\\nhyperparameters either help us control the capacity of the\\nmodel or apply regularization to the model. By controlling\\neither capacity or regularization or both, we can travel the\\nfrontier between underﬁtting and overﬁtting models and arrive\\nat a model that is just right.\\nBut since these hyperparameters have to be set outside of the\\nalgorithm, how do we estimate the best hyperparameters?\\nAlthough it is not part of the core learning process, we learn the\\nhyperparameters also from the data. But if we just use the\\ntraining data to learn the hyperparameters, it will just choose\\nthe maximum possible model capacity, which results in\\noverﬁtting. This is where we need a validation set, a part of the\\ndata that the training process does not have access to. But when\\nthe dataset is small (not hundreds of thousands of samples), the\\nperformance on a single validation set doesn't guarantee a fair\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 236, 'page_label': '237'}, page_content='evaluation. In such cases, we rely on cross-validation. The\\ngeneral trick is to repeat the training and evaluation procedure\\non diﬀerent subsets of the original dataset. A common way of\\ndoing this is called k-fold cross validation, when the original\\ndataset is divided into k equal, non-overlapping, and random\\nsubsets, and each subset is evaluated after training on all the\\nother subsets. We have provided a link in the Further reading\\nsection if you want to read up about cross-validation\\ntechniques. Later in the book, we will also be covering this\\ntopic, but from the time series perspective, which has a few\\ndiﬀerences from the standard way of doing cross-validation.\\nSUGGESTED READING\\nAlthough we have touched the surface of machine\\nlearning in the book, there is a lot more, and to truly\\nappreciate the rest of the book better, we suggest\\ngaining more understanding of machine learning. We\\nsuggest starting with Machine Learning by Stanford\\n(Andrew Ng) –\\nhttps://www.coursera.org/learn/machine-learning. If\\nyou are in a hurry, the Machine Learning Crash Course\\nby Google is also a good starting point –\\nhttps://developers.google.com/machine-learning/crash-\\ncourse/ml-intro.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 237, 'page_label': '238'}, page_content=\"Now that we have a basic understanding of machine learning,\\nlet's start looking at how we can use it to do time series\\nforecasting.\\nTime series forecasting as regression\\nA time series, as we saw in Chapter 1, Introducing Time Series, is\\na set of observations taken sequentially in time. And typically,\\ntime series forecasting is about trying to predict what these\\nobservations will be in the future. Given a sequence of\\nobservations of arbitrary length of history, we predict the\\nfuture to an arbitrary horizon.\\nWe saw that regression, or machine learning to predict a\\ncontinuous variable, works on a dataset of examples, and each\\nexample is a set of input features and targets. We can see that\\nregression, which is tasked with predicting a single output\\nprovided with a set of inputs, is fundamentally incompatible\\nwith forecasting, where we are given a set of historical values\\nand asked to predict the future values. This fundamental\\nincompatibility between the time series and machine learning\\nregression paradigms is why we cannot use regression for time\\nseries forecasting directly.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 238, 'page_label': '239'}, page_content=\"Moreover, time series forecasting, by deﬁnition, is an\\nextrapolation problem, whereas regression, most of the time, is\\nan interpolation one. Extrapolation is typically harder to solve\\nusing data-driven methods. Another key assumption in\\nregression problems is that the samples used for training are\\nindependent and identically distributed (IID). But time series\\nbreak that assumption as well because subsequent observations\\nin a time series display considerable dependence.\\nHowever, to use the wide variety of techniques from machine\\nlearning, we need to cast time series forecasting as a regression.\\nThankfully, there are ways to convert a time series into a\\nregression and get over the IID assumption by introducing\\nsome memory to the machine learning model through some\\nfeatures. Let's see how it can be done.\\nTime delay embedding\\nWe talked about the ARIMA model in Chapter 4, Setting a Strong\\nBaseline Forecast, and saw how it is an autoregressive model.\\nWe can use the same concept to convert a time series problem\\ninto a regression one. Let's use the following diagram to make\\nthe concept clear:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 239, 'page_label': '240'}, page_content=\"Figure 5.6 – Time series to regression conversion using a sliding\\nwindow\\nLet's assume we have a time series with time steps, . Consider\\nwe are at time t, and we have a time series of the length of\\nhistory as L. So, our time series will look something like in the\\ndiagram with\\nas the latest observation in the time series, and and so on as we\\nmove backward in time. In an ideal world, each observation in\\nshould be conditioned on all the previous observations when\\nwe forecast. But, it is not practical because L can be arbitrarily\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 240, 'page_label': '241'}, page_content='long. We often restrict the forecasting function to use only the\\nmost recent M observations of the series, where M < L. These\\nare called ﬁnite memory models or Markov models and M is\\ncalled the order of autoregression, memory size, or the\\nreceptive ﬁeld.\\nTherefore, in time delay embedding, we assume a window of\\narbitrary length M < L and extract ﬁxed-length subsequences\\nfrom the time series by sliding the window over the length of\\nthe time series.\\nIn the diagram, we have taken a sliding window with a memory\\nsize of 3. So, the ﬁrst subsequence we can extract (if we are\\nstarting from the most recent and working backward) is . And\\nis the observation that comes right after the subsequence. This\\nbecomes our ﬁrst example in the dataset (row 1 in the table in\\nthe diagram). Now, we slide the window one time step to the left\\n(backward in time) and extract the new subsequence,\\n. The corresponding target would become . We repeat this\\nprocess as we move back to the beginning of the time series,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 241, 'page_label': '242'}, page_content=\"and at each step of the sliding window, we add one more\\nexample to the dataset.\\nAt the end of it, we have an aligned dataset with a ﬁxed vector\\nsize of features (which will be equal to the window size) and a\\nsingle target, which is what a typical machine learning dataset\\nlooks like.\\nNow that we have a table with three features, let's also assign\\nsemantic meaning to the three features. If we look at the right-\\nmost column in the table in the diagram, we can see that the\\ntime step present in the column is always one time step behind\\nthe target. We call it Lag 1. The second column from the right is\\nalways two time steps behind the target, and this is called Lag 2.\\nGeneralizing this, the feature that has observations that are n\\ntime steps behind the target, we call Lag n.\\nThis transformation from time series to regression using time-\\ndelay embedding encodes the autoregressive structure of a\\ntime series in a way that can be utilized by standard regression\\nframeworks. Another way we can think about using regression\\nfor time series forecasting is to perform regression on time.\\nTemporal embedding\\nIf we rely on previous observations in autoregressive models,\\nwe rely on the concept of time for temporal embedding models.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 242, 'page_label': '243'}, page_content=\"The core idea is that we forget the autoregressive nature of the\\ntime series and assume that any value in the time series is only\\ndependent on time. We derive features that capture time, the\\npassage of time, periodicity of time, and so on, from the\\ntimestamps associated with the time series, and then we use\\nthese features to predict the target using a regression model.\\nThere are many ways to do this, from simply aligning a\\nmonotonically and uniformly increasing numerical column that\\ncaptures the passage of time to sophisticated Fourier terms to\\ncapture the periodic components in time. We will talk about\\nthose techniques in detail in Chapter 6, Feature Engineering for\\nTime Series Forecasting.\\nBefore we wind up the chapter, let's also talk about a key\\nconcept that is gaining ground steadily in the time series\\nforecasting space. A large part of this book embraces this new\\nparadigm of forecasting.\\nGlobal forecasting models – a paradigm\\nshift\\nTraditionally, each time series was treated in isolation. Because\\nof that, traditional forecasting has always looked at the history\\nof a single time series alone in ﬁtting a forecasting function. But\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 243, 'page_label': '244'}, page_content=\"recently, because of the ease of collecting data in today's digital-\\nﬁrst world, many companies have started collecting large\\namounts of time series from similar sources, or related time\\nseries.\\nFor example, retailers such as Walmart collect data on sales of\\nmillions of products across thousands of stores. Companies\\nsuch as Uber or Lyft collect the demand for rides from all the\\nzones in a city. In the energy sector, energy consumption data is\\ncollected across all consumers. All these sets of time series have\\nshared behavior and are hence called related time series.\\nWe can consider that all the time series in a related time series\\ncome from separate data generating processes (DGPs), and\\nthereby model them all separately. We call these the local\\nmodels of forecasting. An alternative to this approach is to\\nassume that all the time series are coming from a single DGP.\\nInstead of ﬁtting a separate forecast function for each time\\nseries individually, we ﬁt a single forecast function to all the\\nrelated time series. This approach has been called global or\\ncross-learning in literature.\\nREFERENCE CHECK\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 244, 'page_label': '245'}, page_content='The terminology global was introduced by David\\nSalinas et al. in the DeepAR paper (reference number 1)\\nand Cross-learning by Slawek Smyl (reference number\\n2).\\nWe saw earlier that having more data will lead to lower\\nchances of overﬁtting and, therefore, lower generalization\\nerror (the diﬀerence between training and testing errors). This\\nis exactly one of the shortcomings of the local approach.\\nTraditionally, time series are not very long, and in many cases,\\nit is diﬃcult and time-consuming to collect more data as well.\\nFitting a machine learning model (with all its expressiveness)\\non small data is prone to overﬁtting. This is why time series\\nmodels that enforce strong priors were used to forecast such\\ntime series, traditionally. But these strong priors, which restrict\\nthe ﬁtting of traditional time series models, can also lead to a\\nform of underﬁtting and limit accuracy.\\nStrong and expressive data-driven models, as in machine\\nlearning, require a larger amount of data to have a model that\\ngeneralizes to new and unseen data. A time series, by deﬁnition,\\nis tied to time, and sometimes, collecting more data means\\nwaiting for months or years and that is not desirable. So, if we\\ncannot increase the length of the time-series dataset, we can\\nincrease the width of the time series dataset. If we add multiple'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 245, 'page_label': '246'}, page_content='time series to the dataset, we increase the width of the dataset,\\nand there by increase the amount of data the model is getting\\ntrained with. Figure 5.7 shows the concept of increasing the\\nwidth of a time series dataset visually:\\nFigure 5.7 – The length and width of a time series dataset'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 246, 'page_label': '247'}, page_content='This works in favor of machine learning models because with\\nhigher ﬂexibility in ﬁtting a forecast function and the addition\\nof more data to work with, the machine learning model can\\nlearn a more complex forecast function than traditional time\\nseries models, which are typically shared between the related\\ntime series, in a completely data-driven way.\\nAnother shortcoming of the local approach revolves around\\nscalability. In the case of Walmart we mentioned earlier, there\\nare millions of time series that need to be forecasted and it is\\nnot possible to have human oversight on all these models. If we\\nthink about this from an engineering perspective, training and\\nmaintaining millions of models in a production system would\\ngive any engineer a nightmare. But under the global approach,\\nwe only train a single model for all these time series, which\\ndrastically reduces the number of models we need to maintain\\nand yet can generate all the required forecasts.\\nThis new paradigm of forecasting has gained traction and has\\nconsistently been shown to improve the local approaches in\\nmultiple time series competitions, mostly in datasets of related\\ntime series. In Kaggle competitions, such as Rossman Store Sales\\n(2015), Wikipedia WebTraﬃc Time Series Forecasting (2017),\\nCorporación Favorita Grocery Sales Forecasting (2018), and M5\\nCompetition (2020), the winning entries were all global models'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 247, 'page_label': '248'}, page_content='—either machine learning or deep learning or a combination of\\nboth. The Intermarché Forecasting Competition (2021) also had\\nglobal models as the winning submissions. Links to these\\ncompetitions are provided in the Further reading section.\\nAlthough we have many empirical ﬁndings where the global\\nmodels have outperformed local models for related time series,\\nglobal models are still a relatively new area of research.\\nMontero-Manson and Hyndman (2020) showed a few very\\ninteresting results and showed that any local method can be\\napproximated by a global model with required complexity, and\\nthe most interesting ﬁnding they put forward is that the global\\nmodel will perform better, even with unrelated time series. We\\nwill talk more about global models and strategies for global\\nmodels in Chapter 10, Global Forecasting Models.\\nREFERENCE CHECK\\nThe Montero-Manson and Hyndman (2020) research\\npaper is cited in References under reference number 3.\\nSummary\\nWe have started our journey beyond baseline forecasting\\nmethods and dipped our toes into the world of machine'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 248, 'page_label': '249'}, page_content='learning. After a brief refresher on machine learning, where we\\nlooked at key concepts such as overﬁtting, underﬁtting,\\nregularization, and so on, we saw how we can convert a time\\nseries forecasting problem into a regression problem from the\\nmachine learning world. We also developed a conceptual\\nunderstanding of diﬀerent embeddings, such as time delay\\nembedding and temporal embedding, which can be used to\\nconvert a time series problem into a regression problem. To\\nwrap things up, we also learned about a new paradigm in time\\nseries forecasting – global models – and contrasted them with\\nlocal models on a conceptual level. In the next few chapters, we\\nwill start putting these concepts into practice, and see\\ntechniques for feature engineering, and strategies for global\\nmodels.\\nReferences\\nFollowing are the references that we used in this chapter:\\n1. David Salinas, Valentin Flunkert, Jan Gasthaus, Tim\\nJanuschowski (2020). DeepAR: Probabilistic forecasting with\\nautoregressive recurrent networks. International Journal of\\nForecasting. 36-3. 1181-1191:\\nhttps://doi.org/10.1016/j.ijforecast.2019.07.001'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 249, 'page_label': '250'}, page_content='2. Slawek Smyl (2020). A hybrid method of exponential\\nsmoothing and recurrent neural networks for time series\\nforecasting. International Journal of Forecasting. 36-1: 75-85\\nhttps://doi.org/10.1016/j.ijforecast.2019.03.017\\n3. Montero-Manso, P., Hyndman, R.J.. (2020), Principles and\\nalgorithms for forecasting groups of time series: Locality and\\nglobality. arXiv:2008.00444[cs.LG]:\\nhttps://arxiv.org/abs/2008.00444\\nFurther reading\\nYou can check out the following resources for further reading:\\nRegularization for Sparsity from Google Machine Learning\\nCrash Course: https://developers.google.com/machine-\\nlearning/crash-course/regularization-for-sparsity/l1-\\nregularization\\nL1 and L2 Regularization from Foundations of Machine\\nLearning, Bloomberg ML EDU:\\nhttps://www.youtube.com/watch?v=d6XDOS4btck\\nCross-validation: evaluating estimator performance from\\nscikit-learn: https://scikit-\\nlearn.org/stable/modules/cross_validation.html\\nRossmann Store Sales: https://www.kaggle.com/c/rossmann-\\nstore-sales'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 250, 'page_label': '251'}, page_content='Web Traﬃc Time Series Forecasting –\\nhttps://www.kaggle.com/c/web-traﬃc-time-series-forecasting\\nCorporación Favorita Grocery Sales Forecasting –\\nhttps://www.kaggle.com/c/favorita-grocery-sales-forecasting\\nM5 Forecasting – Accuracy – https://www.kaggle.com/c/m5-\\nforecasting-accuracy'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 251, 'page_label': '252'}, page_content='6 Feature Engineering for Time Series\\nForecasting'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 252, 'page_label': '253'}, page_content=\"Join our book community on Discord\\nhttps://packt.link/EarlyAccess/\\nIn the previous chapter, we started looking at machine learning (ML) as a\\ntool to solve the problem of time series forecasting. We also talked about a\\nfew techniques such as time delay embedding and temporal embedding,\\nwhich cast time series forecasting problems as classical regression\\nproblems from the ML paradigm. In this chapter, we'll look at those\\ntechniques in detail and go through them in a practical sense using the\\ndataset we have been working with throughout this book.\\nIn this chapter, we will cover the following topics:\\nFeature engineering\\nAvoiding data leakage\\nSetting a forecast horizon\\nTime delay embedding\\nTemporal embedding\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 253, 'page_label': '254'}, page_content='Technical requirements\\nYou will need to set up the Anaconda environment following the\\ninstructions in the Preface of the book to get a working environment with\\nall the libraries and datasets required for the code in this book. Any\\nadditional library will be installed while running the notebooks\\nYou will need to run the following notebooks before using the code in this\\nchapter:\\n02-Preprocessing London Smart Meter Dataset.ipynb from Chapter02\\n01-Setting up Experiment Harness.ipynb from Chapter04\\nThe code for this chapter can be found at\\nhttps://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-\\nPython-/tree/main/notebooks/Chapter06.\\nFeature engineering\\nFeature engineering, as the name suggests, is the process of engineering\\nfeatures from the data, mostly using domain knowledge, to make the\\nlearning process smoother and more eﬃcient. In a typical ML setting,\\nengineering good features is essential to get good performance from any\\nML model. Feature engineering is a highly subjective part of ML where\\neach problem at hand has a diﬀerent path – one that is hand-crafted to that\\nproblem.\\nWhen we are casting a time series problem as a regression problem, there\\nare a few standard techniques that we can apply. This is a key step in the\\nprocess because how well an ML model acquires an understanding of time'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 254, 'page_label': '255'}, page_content='is dependent on how well we engineer features to capture time. The\\nbaseline methods we covered in Chapter 4, Setting a Strong Baseline\\nForecast, are the methods that are created for the speciﬁc use case of time\\nseries forecasting and because of that, the temporal aspect of the problem is\\nbuilt into those models. For instance, ARIMA doesn\\'t need any feature\\nengineering to understand time because it is built into the model. But a\\nstandard regression model has no explicit understanding of time, so we\\nneed to create good features to embed the temporal aspect of the problem.\\nIn the previous chapter (Chapter 5, Time Series Forecasting as Regression),\\nwe talked about two main ideas to encode time into the regression\\nframework: time delay embedding and temporal embedding. Although\\nwe touched on these concepts at a high level, it is time to dig deeper and see\\nthem in action.\\nNOTEBOOK ALERT\\nTo follow along with the complete code, use the 01-Feature\\nEngineering.ipynb notebook in the chapter06 folder.\\nWe have already split the dataset that we were working on into train,\\nvalidation, and test datasets. But since we are generating features that are\\nbased on previous observations, operationally, it is better when we have the\\ntrain, validation, and test datasets combined. It will be clearer why shortly,\\nbut for now, let\\'s take it on faith and move ahead. Now, let\\'s go ahead and\\ncombine the two datasets:\\n# Reading the missing value imputed and train test split d\\ntrain_df = pd.read_parquet(preprocessed / \"selected_blocks\\nval_df = pd.read_parquet(preprocessed / \"selected_blocks_v'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 255, 'page_label': '256'}, page_content='test_df = pd.read_parquet(preprocessed / \"selected_blocks_\\n#Adding train, validation and test tags to distinguish the\\ntrain_df[\\'type\\'] = \"train\"\\nval_df[\\'type\\'] = \"val\"\\ntest_df[\\'type\\'] = \"test\"\\nfull_df = pd.concat([train_df, val_df, test_df]).sort_valu\\ndel train_df, test_df, val_df\\nNow, we have a full_df that combines the train, validation, and test\\ndatasets. Some of you may already have alarm bells ringing in your head at\\ncombining the train and test sets. What about data leakage? Let\\'s check it\\nout.\\nAvoiding data leakage\\nData leakage occurs when the model is trained with some information that\\nwould not be available at the time of prediction. Typically, this leads to high\\nperformance in the training set, but very poor performance in unseen data.\\nThere are two types of data leakage:\\nTarget leakage is when the information about the target (that we are\\ntrying to predict) leaks into some of the features in the model, leading to\\nan overreliance of the model on those features, ultimately leading to\\npoor generalization. This includes features that use the target in any way.\\nTrain-test contamination is when there is some information leaking\\nbetween the train and test datasets. This can happen because of careless\\nhandling and splitting of data. But it can also happen in more subtle\\nways, such as scaling the dataset before splitting the train and test sets.\\nWhen we are working with time series forecasting problems, the biggest\\nand most common mistake that we can make is target leakage. We will have'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 256, 'page_label': '257'}, page_content=\"to think hard about each of the features to ensure we are not using any data\\nthat will not be available during prediction. The following diagram will\\nhelp us remember and internalize this concept:\\nFigure 6.1 – Usable and not-usable information to avoid data leakage\\nTo make this concept clearer and more relevant to the time series\\nforecasting context, let's look at an example. Let's say we are forecasting\\nsales for shampoo, and we are using sales for conditioner as a feature. We\\ndeveloped the model, trained it on the training data, and tested it on the\\nvalidation data. The model is doing very well. The moment we start\\npredicting for the future, we will see the problem. We don't know what the\\nsales for conditioner are in the future either. While this example is pretty\\nstraightforward, there will be times when this becomes not so obvious. And\\nthat is why we need to exercise a fair amount of caution while creating\\nfeatures and always evaluate the features through the lens of, will this\\nfeature be available at the time of prediction?\\nBEST PRACTICES\\nThere are many ways of identifying target leakage, apart from\\nthinking hard about the features:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 257, 'page_label': '258'}, page_content=\"• If the model you've built is too good to be true, you most likely\\nhave a leakage problem\\n• If any single feature has too much weightage in the feature\\nimportance of the model, that feature may have a problem with\\nleakage\\n• Double-check the features that are highly correlated with the\\ntarget\\nNow, let's learn about forecast horizons.\\nSetting a forecast horizon\\nAlthough we generated forecasts earlier in this book, we never explicitly\\ndiscussed forecast horizons. A forecast horizon is the number of time steps\\ninto the future we want to forecast at any point in time. For instance, if we\\nwant to forecast the next 24 hours for the electricity consumption dataset\\nthat we have been working with, the forecast horizon becomes 48 (because\\nthe data is half-hourly). In Chapter 5, Time Series Forecasting as Regression,\\nwhere we generated baselines, we just predicted the entire test data at\\nonce. In such cases, the forecast horizon becomes equal to the length of the\\ntest data.\\nWe never had to worry about this until now because, in the classical\\nstatistical methods of forecasting, this decision is decoupled from modeling.\\nIf we train a model, we can use that model to predict any future point\\nwithout retraining. But with time series forecasting as regression, we have a\\nconstraint on the forecast horizon and it has its roots in data leakage. For\\nnow, let's only look at single-step-ahead forecasting. In the context of the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 258, 'page_label': '259'}, page_content=\"dataset we are working with, this means that we will be answering the\\nquestion, what is the energy consumption in the next half an hour? We will\\ntalk about multi-step forecasting and other mechanics of forecasting in\\nSection 4 – The Mechanics of Forecasting.\\nNow that we have set some ground rules, let's start looking at the diﬀerent\\nfeature engineering techniques. To follow along with the Jupyter notebook,\\nhead over to the chapter06 folder and use the 01-Feature\\nEngineering.ipynb ﬁle.\\nTime delay embedding\\nThe basic idea behind time delay embedding is to embed time in terms of\\nrecent observations. If you want to head back to Chapter 5, Time Series\\nForecasting as Regression, and review this concept (Figure 5.1), please go\\nahead and do so now.\\nIn Figure 5.1, we talked about including previous observations of a time\\nseries as lags. However, there are a few more ways to capture recent and\\nseasonal information using this concept. Let's take a look.\\nLags or backshift\\nLet's assume we have a time series with time steps, . Consider that we are at\\ntime t and that we have a time series where the length of history is L. So,\\nour time series will have as the latest observation in the time series, and\\nthen\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 259, 'page_label': '260'}, page_content='and so on as we move back in time. So, lags, as explained in Chapter 5, Time\\nSeries Forecasting as Regression, are features that include the previous\\nobservations in the time series, as shown in the following diagram:\\nFigure 6.2 – Lag features (Color ﬁgure)\\nWe can create multiple lags by including observations that are a timesteps\\nbefore ( ); we will call this Lag a. In the preceding diagram, we have shown\\nLag 1, Lag 2, and Lag 3. However, we can add any number of lags we like.\\nNow, let\\'s learn how to do that in code:\\ndf[\"lag_1\"]=df[\"column\"].shift(1)\\nRemember when we combined the train and test datasets and I asked you\\nto take it in good faith? It\\'s time to put a reason to that faith. If we consider'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 260, 'page_label': '261'}, page_content='the lag operation (or any autoregressive feature), it relies on a continuous\\nrepresentation along the time axis. If we consider the test dataset, for the\\nﬁrst few rows (or earliest dates), the lags would be missing because it is\\npart of the training dataset. So, by combining the two, we create a\\ncontinuous representation along the time axis where standard functions in\\npandas such as shift can be utilized to create these features easily and\\neﬃciently.\\nIt is as simple as that, but we need to perform the lag operation for each\\nLCLid separately. We have included a helpful method in\\nsrc.feature_engineering.autoregressive_features called add_lags that\\nadds all the lags you want for each LCLid in a fast and eﬃcient manner.\\nLet\\'s see how we can use that.\\nWe are going to import the method and use a few of its parameters to\\nconﬁgure the lag operation the way we want:\\nfrom src.feature_engineering.autoregressive_features impor\\n# Creating first 5 lags and then same 5 lags but from prev\\nlags = (\\n    (np.arange(5) + 1).tolist()\\n    + (np.arange(5) + 46).tolist()\\n    + (np.arange(5) + (48 * 7) - 2).tolist()\\n)\\nfull_df, added_features = add_lags(\\n    full_df, lags=lags, column=\"energy_consumption\", ts_id\\n)\\nNow, let\\'s look at the parameters that we used in the previous code snippet:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 261, 'page_label': '262'}, page_content=\"lags: This parameter takes in a list of integers denoting all the lags we\\nneed to create as features.\\ncolumn: The name of the column to be lagged. In our case, this is\\nenergy_consumption.\\nts_id: The name of the column that contains the unique ID of a time\\nseries. If None, it assumes the DataFrame only contains a single time\\nseries. In our case, LCLid is the name of that column.\\nuse_32_bit: This parameter doesn't do anything functionally but makes\\nthe DataFrames much smaller in memory, sacriﬁcing the precision of the\\nﬂoating-point numbers.\\nThis method returns the DataFrame with the lags added and a list with\\ncolumn names of the newly added features.\\nRolling window aggregations\\nWith lags, we were connecting the present points to single points in the\\npast, but with rolling window features, we are connecting the present with\\nan aggregate statistic of a window from the past. Instead of looking at the\\nobservation from previous time steps, we would be looking at an average of\\nthe observations from the last three timesteps. Take a look at the following\\ndiagram to understand this better:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 262, 'page_label': '263'}, page_content='Figure 6.3 – Rolling window aggregation features(Color ﬁgure)\\nWe can calculate rolling statistics with diﬀerent widows, and each of them\\nwould capture slightly diﬀerent aspects of the history. In the preceding\\ndiagram, we can see an example of a window of three and a window of\\nfour. When we are at timestep t, a rolling window of three would have\\nas the vector of past observations. Once we have these, we can apply any\\naggregation functions, such as the mean, standard deviation, min, max, and\\nso on. Once we have a scalar value after the aggregation function, we can\\ninclude that as a feature for timestep t.\\nNOTE\\nWe are not including'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 263, 'page_label': '264'}, page_content='in the vector of past observations because that leads to data\\nleakage.\\nLet\\'s see how we can do this with pandas:\\n# We shift by one to make sure there is no data leakage\\ndf[\"rolling_3_mean\"] = df[\"column\"].shift(1).rolling(3).me\\nSimilar to the lags, we need to do this operation for each LCLid column\\nseparately. We have included a helpful method in\\nsrc.feature_engineering.autoregressive_features called\\nadd_rolling_features that adds all the rolling features you want for each\\nLCLid in a fast and eﬃcient manner. Let\\'s see how we can use that.\\nWe are going to import this method and use a few of its parameters to\\nconﬁgure the rolling operation the way we want:\\nfrom src.feature_engineering.autoregressive_features impor\\nfull_df, added_features = add_rolling_features(\\n    full_df,\\n    rolls=[3, 6, 12, 48],\\n    column=\"energy_consumption\",\\n    agg_funcs=[\"mean\", \"std\"],\\n    ts_id=\"LCLid\",\\n    use_32_bit=True,\\n)\\nNow, let\\'s look at the parameters that we used in the previous code snippet:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 264, 'page_label': '265'}, page_content=\"rolls: This parameter takes in a list of integers denoting all the windows\\nover which we need to calculate the aggregate statistics.\\ncolumn: The name of the column to be lagged. In our case, this is\\nenergy_consumption.\\nagg_funcs: This is a list of aggregations that we want to do for each\\nwindow we declared in rolls. Allowable aggregation functions include\\n{mean, std, max, min}.\\nn_shift: This is the number of timesteps we need to shift before doing the\\nrolling operation. This parameter avoids data leakage. Although we are\\nshifting by one now, there are cases where we need to shift by more than\\none as well. This is typically used in multi-step forecasting, which we will\\ncover in Section 3 – Mechanics of Forecasting.\\nts_id: The name of the column name that contains the unique ID of a\\ntime series. If None, it assumes that the DataFrame only has a single time\\nseries. In our case, LCLid is the name of that column.\\nuse_32_bit: This parameter doesn't do anything functionally but makes\\nthe DataFrames much smaller in memory, sacriﬁcing the precision of the\\nﬂoating-point numbers.\\nThis method returns the DataFrame with the rolling features added and a\\nlist with column names of the newly added features.\\nSeasonal rolling window aggregations\\nSeasonal rolling window aggregations are very similar to rolling window\\naggregations, but instead of taking past n consecutive observations in the\\nwindow, this takes a seasonal window, skipping a constant number of\\ntimesteps between each item in a window. The following diagram will make\\nthis clearer:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 265, 'page_label': '266'}, page_content='Figure 6.4 – Seasonal rolling window aggregations(Color ﬁgure)\\nThe key parameter here is the seasonality period, which is commonly\\nreferred to as m. This is the number of timesteps after which we expect the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 266, 'page_label': '267'}, page_content=\"seasonality pattern to repeat. When we are at timestep t, a rolling window\\nof three would have\\nas the vector of past observations. But the seasonal rolling window would\\nskip m timesteps between each item in the window. This means that the\\nobservations that are there in the seasonal rolling window would be . And\\nas usual, once we have the window vector, we just need to apply the\\naggregation function to get a scalar value and include that as a feature.\\nIMPORTANT NOTE\\nWe are not including\\nas an element in the seasonal rolling window vector to avoid data\\nleakage.\\nThis is not an operation that you can do easily and eﬃciently with pandas.\\nSome fancy NumPy indexing and Python loops should do the trick. We are\\nusing an implementation from github.com/jmoralez/window_ops/ that uses\\nNumPy and Numba to make the operation fast and eﬃcient.\\nJust like the features we saw earlier, we need to do this operation for each\\nLCLid separately. We have included a helpful method in\\nsrc.feature_engineering.autoregressive_features called\\nadd_seasonal_rolling_features that adds all the seasonal rolling features\\nyou want for each LCLid in a fast and eﬃcient manner. Let's see how we\\ncan use that.\\nWe are going to import the method and use a few parameters of the method\\nto conﬁgure the seasonal rolling operation the way we want:\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 267, 'page_label': '268'}, page_content='from src.feature_engineering.autoregressive_features impor\\nfull_df, added_features = add_seasonal_rolling_features(\\n    full_df,\\n    rolls=[3],\\n    seasonal_periods=[48, 48 * 7],\\n    column=\"energy_consumption\",\\n    agg_funcs=[\"mean\", \"std\"],\\n    ts_id=\"LCLid\",\\n    use_32_bit=True,\\n)\\nNow, let\\'s look at the parameters that we used in the previous code snippet:\\nseasonal_periods: This is a list of seasonal periods that should be used in\\nthe seasonal rolling windows. In the case of multiple seasonalities, we\\ncan include the seasonal rolling features of all the seasonalities.\\nrolls: This parameter takes in a list of integers denoting all the windows\\nover which we need to calculate aggregate statistics.\\ncolumn: The name of the column to be lagged. In our case, this is\\nenergy_consumption.\\nagg_funcs: This is a list of aggregations that we want to do for each\\nwindow we declared in rolls. The allowable aggregation functions are\\n{mean, std, max, min}.\\nn_shift: This is the number of seasonal timesteps we need to shift before\\ndoing the rolling operation. This parameter avoids data leakage.\\nts_id: The name of the column name that contains the unique ID of a\\ntime series. If None, it assumes the DataFrame only contains a single\\ntime series. In our case, LCLid is the name of that column.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 268, 'page_label': '269'}, page_content=\"Use_32_bit: This parameter doesn't do anything functionally but makes\\nthe DataFrames much smaller in memory, sacriﬁcing the precision of the\\nﬂoating-point numbers.\\nAs always, the method returns the DataFrame with seasonal rolling\\nfeatures and a list containing the column names of the newly added\\nfeatures.\\nExponentially weighted moving averages (EWMA)\\nWith the rolling window mean operation, we were calculating the average\\nof the window, and it works synonymously with the moving average.\\nEWMA is the slightly smarter cousin of the moving average. While the\\nmoving average considers a rolling window and considers each item in the\\nwindow equally on the computed average, EWMA tries to do a weighted\\naverage on the window, and the weights decay at an exponential rate.\\nThere is a parameter,\\n, that determines how fast the weights decay. And because of this, we can\\nconsider all the history available as a window and let the parameter decide\\nhow much recency is included in EWMA. This can be written simply and\\nrecursively, as follows:\\nHere, we can see that the larger the value of , the more the average is\\nskewed toward recent values (see Figure 6.6 to get a visual intuition of how\\nthe weights would be). If we expand the recursion, the weights of each term\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 269, 'page_label': '270'}, page_content='work out to be: where k is the number of timesteps behind t. If we plot the\\nweights, we can see them in an exponential decay; determines how fast the\\ndecay happens. Another way to think about is in terms of span. Span is the\\nnumber of periods at which the decayed weights approach zero (not in a\\nstrictly mathematical way, but intuitively). and span are related through\\nthis equation: This will become clearer in the following diagram, where we\\nhave plotted how the weights decay for diﬀerent values of :\\nFigure 6.5 – Exponential weight decay for diﬀerent values of\\nHere, we can see that the weight becomes small by the time we reach the\\nspan.\\nIntuitively, we can think of EWMA as an average of the entire history of the\\ntime series, but with parameters such as and span, we can make diﬀerent\\nperiods of history more representative of the average. If we deﬁne a 60-\\nperiod span, we can think that the last 60 time periods are what majorly\\ndrive the average. So, making EWMAs with diﬀerent spans or s gives us\\nrepresentative features that capture diﬀerent periods of history.\\nThe overall process is depicted in the following diagram:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 270, 'page_label': '271'}, page_content='Figure 6.6 – EWMA features(Color ﬁgure)\\nNow, let\\'s see how we can do this in pandas:\\ndf[\"ewma\"]=df[\\'column\\'].shift(1).ewm(alpha=0.5).mean()\\nLike the other features we discussed earlier, EWMA also needs to be done\\nfor each LCLid separately. We have included a helpful method in\\nsrc.feature_engineering.autoregressive_features called add_ewma that\\nadds all the EWMA features you want for each LCLid in a fast and eﬃcient\\nmanner. Let\\'s see how we can use that.\\nWe are going to import the method and use a few parameters of the method\\nto conﬁgure EWMA the way we want to:'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 271, 'page_label': '272'}, page_content='from src.feature_engineering.autoregressive_features impor\\nfull_df, added_features = add_ewma(\\n    full_df,\\n    spans=[48 * 60, 48 * 7, 48],\\n    column=\"energy_consumption\",\\n    ts_id=\"LCLid\",\\n    use_32_bit=True,\\n)\\nNow, let\\'s look at the parameters that we used in the previous code snippet:\\nalphas: This is a list of all\\ns we need to calculate the EWMA features for.\\nspans: Alternatively, we can use this to list all the spans we need to\\ncalculate the EWMA features for. If you use this feature, alphas will be\\nignored.\\ncolumn: The name of the column to be lagged. In our case, this is\\nenergy_consumption.\\nn_shift: This is the number of seasonal timesteps we need to shift before\\ndoing the rolling operation. This parameter avoids data leakage.\\nts_id: The name of the column name that has a unique ID for a time\\nseries. If None, it assumes the DataFrame only contains a single time\\nseries. In our case, LCLid is the name of that column.\\nuse_32_bit: This parameter doesn\\'t do anything functionally but makes\\nthe DataFrames much smaller in memory, sacriﬁcing the precision of the\\nﬂoating-point numbers.\\nAs always, the method returns the DataFrame containing EWMA features\\nand a list with the column names of the newly added features.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 272, 'page_label': '273'}, page_content=\"These are a few standard ways of including time delay embedding in your\\nML model, but you are not restricted to just these. As always, feature\\nengineering is a space that is not bound by rules and we can get as creative\\nas we want and inject domain knowledge into the model. Apart from the\\nfeatures we have seen, we can include the diﬀerence in lag as custom lags\\nthat inject domain knowledge, and so on.\\nNow, let's look at the other class of features we can add via temporal\\nembedding.\\nTemporal embedding\\nIn Chapter 5, Time Series Forecasting as Regression, we brieﬂy talked about\\ntemporal embedding as a process where we try to embed time into features\\nthat the ML model can leverage. If we think about time for a second, we will\\nrealize that there are two aspects of time that are important to us in the\\ncontext of time series forecasting – passage of time and periodicity of time.\\nLet's look at a few features that can help us capture these aspects in an ML\\nmodel.\\nCalendar features\\nThe ﬁrst set of features that we can extract are features based on calendars.\\nAlthough the strict deﬁnition of time series is a set of observations taken\\nsequentially in time, more often than not, we will have the timestamps of\\nthese collected observations alongside the time series. We can utilize these\\ntimestamps and extract calendar features such as the month, quarter, day\\nof the year, hour, minutes, and so on. These features capture the periodicity\\nof time and help the ML model capture seasonality well. Only the calendar\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 273, 'page_label': '274'}, page_content='features that are temporally higher than the frequency of the time series\\nmake sense. For instance, an hour feature in a time series with a weekly\\nfrequency doesn\\'t make sense, but a month feature and week feature make\\nsense. We can utilize inbuilt datetime functionalities in pandas to create\\nthese features and treat these as categorical features in the model.\\nTime elapsed\\nThis is another feature that captures the passage of time in an ML model.\\nThis feature increases monotonically as time increases, giving the ML\\nmodel a sense of the passage of time. There are many ways to create this\\nfeature, but one of the easiest and most eﬃcient ways is to use the integer\\nrepresentation of dates in NumPy:\\ndf[\\'time_elapsed\\'] = df[\\'timestamp\\'].values.astype(np.int6\\nWe have included a helpful method in\\nsrc.feature_engineering.temporal_features called\\nadd_temporal_features that adds all relevant temporal features in an\\nautomated way. Let\\'s see how we can use it.\\nWe are going to import the method and use a few parameters of this\\nmethod to conﬁgure and create the temporal features:\\nfull_df, added_features = add_temporal_features(\\n    full_df,\\n    field_name=\"timestamp\",\\n    frequency=\"30min\",\\n    add_elapsed=True,\\n    drop=False,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 274, 'page_label': '275'}, page_content=\"use_32_bit=True,\\n)\\nNow, let's look at the parameters that we used in the previous code snippet:\\nﬁeld_name: This is the column name that contains the datetime that\\nshould be used to create features.\\nfrequency: We should provide the frequency of the time series as input\\nso that the method automatically extracts the relevant features. These\\nare standard pandas frequency strings.\\nadd_elapsed: This ﬂag turns the creation of the time elapsed feature on\\nor oﬀ.\\nuse_32_bit: This parameter doesn't do anything functionally but makes\\nthe DataFrames much smaller in memory, sacriﬁcing the precision of the\\nﬂoating-point numbers.\\nJust like the previous methods we discussed, this also returns the new\\nDataFrame with the temporal features added and a list containing the\\ncolumn names of the newly added features.\\nFourier terms\\nPreviously, we extracted a few calendar features such as the month, year,\\nand so on and talked about using them as categorical variables in the ML\\nmodel. Another way we can represent the same information, but on a\\ncontinuous scale, is by using Fourier terms. We discussed Fourier series in\\nChapter 3, Analyzing and Visualizing Time Series Data. Just to recall, the\\nsine-cosine form of the Fourier series is as follows:\\nHere, is the N-term approximation of the signal, S. Theoretically, when N is\\ninﬁnite, the resulting approximation is equal to the original signal. P is the\\nmaximum length of the cycle. We can create these cosine and sine functions\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 275, 'page_label': '276'}, page_content='as features to represent the seasonal cycle. If we are encoding the month,\\nwe know that the month goes from 1 to 12 and then repeats itself. So, P, in\\nthis case, will be 12 and x will be 1, 2, …12. Therefore, for each x, we can\\ncalculate the cosine and sine terms and add them as features to the ML\\nmodel. The following diagram shows the diﬀerence in representations\\nbetween the month on an ordinal scale and as a Fourier series:\\nFigure 6.7 – Month as an ordinal step function (top) versus Fourier terms\\n(bottom)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 276, 'page_label': '277'}, page_content='The preceding diagram shows just a single Fourier term; we can add\\nmultiple Fourier terms to help capture complex seasonality.\\nWe cannot say that continuous representation of seasonality is better than\\ncategorical because it depends on the type of model you are using and the\\ndataset. This is something we will have to ﬁnd out empirically.\\nTo make the process of adding Fourier features easy, we have made some\\neasy-to-use methods available in\\nsrc.feature_engineering.temporal_features in a ﬁle called\\nbulk_add_fourier_features that adds Fourier features for all the calendar\\nfeatures we want in an automated way. Let\\'s see how we can use that.\\nWe are going to import the method and use a few parameters of the method\\nto conﬁgure and create the Fourier series-based features:\\nfull_df, added_features = bulk_add_fourier_features(\\n    full_df,\\n    [\"timestamp_Month\", \"timestamp_Hour\", \"timestamp_Minut\\n    max_values=[12, 24, 60],\\n    n_fourier_terms=5,\\n    use_32_bit=True,\\n)\\nNow, let\\'s look at the parameters that we used in the previous code snippet:\\ncolumns_to_encode: This is the list of calendar features we need to\\nencode using Fourier terms.\\nmax_values: This is a list of max values for the seasonal cycle for the\\ncalendar features in the same order as they are given in'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 277, 'page_label': '278'}, page_content=\"columns_to_encode. For instance, for month as a column to encode, we\\ngive 12 as the corresponding max_value. If not given, max_value will be\\ninferred. This is only recommended if the data you have contains at least\\na single complete seasonal cycle.\\nn_fourier_terms: The number of Fourier terms to be added. This is\\nsynonymous to n in the equation for the Fourier series mentioned\\npreviously.\\nuse_32_bit: This parameter doesn't do anything functionally but makes\\nthe DataFrames much smaller in memory, sacriﬁcing the precision of the\\nﬂoating-point numbers.\\nJust like the previous methods we've discussed, this also returns a new\\nDataFrame with the Fourier features added and a list with column names\\nof the newly added features.\\nAfter executing the 01-Feature Engineering.ipynb notebook in chapter06,\\nwe will have the following feature engineered ﬁles written to disk:\\nselected_blocks_train_missing_imputed_feature_engg.parquet\\nselected_blocks_val_missing_imputed_feature_engg.parquet\\nselected_blocks_test_missing_imputed_feature_engg.parquet\\nIn this section, we looked at a few popular and eﬀective ways of generating\\nfeatures for time series. But there are many more and depending on your\\nproblem and the domain, many of them will be relevant.\\nADDITIONAL INFORMATION\\nThe world of feature engineering is vast and there are a few open-\\nsource libraries that make exploring that space easier. A few of\\nthem include https://github.com/Nixtla/tsfeatures,\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2024-10-05T18:39:20+00:00', 'author': 'Unknown', 'keywords': 'deep learning time series; arima; stochastic; conformal prediction; demand forecasting; microeconomics; forecasting principles and practice', 'moddate': '2024-10-05T18:39:20+00:00', 'title': 'Modern Time Series Forecasting with Python - B22389_06 (for True Epub)', 'source': 'data/Modern Time Series Forecasting with Python (2024, Packt Publishing) - libgen.li.pdf', 'total_pages': 279, 'page': 278, 'page_label': '279'}, page_content=\"https://tsfresh.readthedocs.io/en/latest/, and\\nhttps://github.com/DynamicsAndNeuralSystems/catch22. A preprint\\nby Ben D. Fulcher titled Feature-based time-series analysis at\\nhttps://arxiv.org/abs/1709.08055 also gives a nice summary of the\\nspace.\\nA newer library called functime(https://github.com/functime-\\norg/functime) also provides fast feature engineering routines\\nwritten in Polars and is worth checking out. A lot of the feature\\nengineering discussed in the book can be made even faster using\\nfunctime and Polars.\\nSummary\\nAfter a brief overview of the ML for time series forecasting paradigm in the\\nprevious chapter, in this chapter, we looked at this practically and saw how\\nwe can prepare the dataset with the required features to start using these\\nmodels. We reviewed a few time series-speciﬁc feature engineering\\ntechniques such as lags, rolling, and seasonal features. All the techniques\\nwe learned in this chapter are tools with which we can quickly iterate\\nthrough experiments to ﬁnd out what works for our dataset. However, we\\nonly talked about feature engineering, which aﬀects one side of the\\nstandard regression equation ( ). The other side, which is the target ( ) we\\nare predicting, is also equally important. In the next chapter, we'll look at a\\nfew concepts such as stationarity and some transformations that aﬀect the\\ntarget.\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='Preprint\\nCOMMA: A C OMMUNICATIVE MULTIMODAL MULTI -\\nAGENT BENCHMARK\\nTimothy Ossowski1,4, ∗ Jixuan Chen1,5, ∗ Danyal Maqbool1,4, Zefan Cai1,4,\\nTyler Bradshaw3,4, Junjie Hu2,4\\n1Department of Computer Science, 2Department of Biostatistics and Medical Informatics\\n3Department of Radiology\\n4University of Wisconsin, Madison, 5Nanjing University\\nossowski@wisc.edu\\nABSTRACT\\nThe rapid advances of multi-modal agents built on large foundation models have\\nlargely overlooked their potential for language-based communication between\\nagents in collaborative tasks. This oversight presents a critical gap in under-\\nstanding their effectiveness in real-world deployments, particularly when com-\\nmunicating with humans. Existing agentic benchmarks fail to address key aspects\\nof inter-agent communication and collaboration, particularly in scenarios where\\nagents have unequal access to information and must work together to achieve\\ntasks beyond the scope of individual capabilities. To fill this gap, we introduce\\na novel benchmark designed to evaluate the collaborative performance of mul-\\ntimodal multi-agent systems through language communication. Our benchmark\\nfeatures a variety of scenarios, providing a comprehensive evaluation across four\\nkey categories of agentic capability in a communicative collaboration setting. By\\ntesting both agent-agent and agent-human collaborations using open-source and\\nclosed-source models, our findings reveal surprising weaknesses in state-of-the-\\nart models, including proprietary models like GPT-4o. These models struggle to\\noutperform even a simple random agent baseline in agent-agent collaboration and\\nonly surpass the random baseline when a human is involved.1\\n1 I NTRODUCTION\\nThe field of multimodal agents is experiencing rapid growth (Xu et al., 2024; Xie et al., 2024; Cao\\net al., 2024), with research efforts expanding at an unprecedented pace. However, amidst this growth,\\na critical gap in research has emerged: the lack of focus on collaborative work (Gurcan, 2024; Park\\net al., 2023; Hong et al., 2024; Liu et al., 2024) among multiple multimodal agents. Synergistic\\noperation of such agents is a highly promising but largely unexplored domain. Language agents can\\ncollaboratively finish complex tasks such as software development by assuming functional roles such\\nas system designer, function generator, etc (Qian et al., 2023; Du et al., 2024)). Current research on\\nmultimodal agents (Xu et al., 2024; Xie et al., 2024; Cao et al., 2024) has predominantly focused on\\nindividual agent capabilities, neglecting the potential for inter-agent collaboration. This limitation is\\nfurther compounded by existing benchmarks such as VisualWebArena (Koh et al., 2024) and MME-\\nRealWorld (Zhang et al., 2024), which fail to adequately assess collaborative performance between\\nagents. As a result, our ability to evaluate and improve multi-agent systems remains constrained,\\nhindering progress in this crucial area.\\nSeveral critical questions emerge in the context of multimodal agent collaboration. How can dif-\\nferent agents effectively communicate multimodal information through language when they have\\nvarying levels of access to information? In scenarios where different agents possess diverse task-\\nspecific capabilities, how can they collaborate to accomplish objectives that are beyond the scope\\nof any individual agent? These research settings remain largely uncharted and present significant\\nchallenges. Furthermore, the ability of agents to handle incomplete information is of paramount\\n∗Co-first author, equal contribution.\\n1We will release our benchmark and evaluation code upon acceptance.\\n1\\narXiv:2410.07553v1  [cs.AI]  10 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='Preprint\\nAgent Dialogue\\nPrompt:\\xa0As the expert, you\\nhave access to the solution\\nmanuals ...\\nListen carefully to the solver\\'s\\ndescription and provide clear\\nand accurate instructions ...\\nExpert Agent View\\nWirePuzzle: The\\nmodule can have 3-6\\nwires on it. Only\\ncut one wire based\\non these rules...\\nKeyPadPuzzle: Press\\nthe four buttons in\\nthe order based on\\n...\\nLEDPuzzle: 2-5 LEDs\\nare installed at the\\ntop of the module...\\nEvaluation\\nSuccess\\nRate\\nSolver Agent View\\nSolver:\\xa0Expert, I see four symbols on\\nthe keypad puzzle: Top left: A symbol\\nresembling an ornate \"H\". 2. Top Right:\\nA backwards \"C\" with a ...\\nCan you identify the column that has\\nthese four symbols and let me know the\\norder to press them?\\nExpert:\\xa0To solve the puzzle, you need to\\nfind the column that includes all four\\nsymbols shown in the keypad. press the\\nbuttons in the following order...\\nSolver:\\xa0 \\xa0press_top_left_button\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 press_top_right_button\\n        \\xa0 \\xa0 \\xa0 \\xa0 \\xa0press_bottom_right_button\\n                   press_bottom_left_button\\nPrompt: As the solver, you arepresented with an image of thepuzzle along with possibleactions you may take.\\nYou should only attempt someactions if you are certain of thesolution...\\nFigure 1: Overview of the interaction between the Solver and Expert agents in our benchmark. The\\ngame manager presents the Solver agent with a puzzle, where the Solver can choose to interact by\\nclicking or requesting advice from the Expert agent. The Solver is shown an image of the puzzle\\n(a KeyPad puzzle in this instance) and makes decisions based on the possible actions. The Expert,\\ninformed by instruction manuals, provides guidance based on the Solver’s descriptions, such as\\nadvising which buttons to press. The interaction between the Solver and Expert is captured in the\\ndialogue, reflecting the cooperation necessary to complete the task. Both agents use self-reflection\\non their choices by being prompted with the conversation history as it progresses.\\nimportance, particularly when working with sensitive data (Li et al., 2024) (i.e. Agent application\\nin healthcare where privacy concerns are critical (Tang et al., 2023)). The exploration of these\\nquestions is crucial for advancing the field of multimodal agent collaboration. By addressing these\\nchallenges, we can expand the applicability of multimodal agents in real-world scenarios (Zhang\\net al., 2024), particularly those involving sensitive or restricted information.\\nMotivated by these aforementioned issues, we propose a novel benchmark for evaluating collabo-\\nrative multi-modal multi-agent frameworks to address critical gaps in current approaches (see Fig-\\nure 1). Our evaluation setting simulates a scenario where an in-house agent with direct access to\\nsensitive data (i.e., the AI solver) collaborates with external expert agents (i.e., the AI expert) to\\nanalyze information without compromising privacy. This evaluation setting could revolutionize how\\nwe handle and extract insights from sensitive datasets across various domains.\\nWe assess multi-modal multi-agent systems using a series of carefully designed collaborative puzzle\\ngames. These scenarios typically involve two-player setups where agents have access to different,\\ncomplementary information. (i.e., in a bomb defusal game, one agent possesses details about the\\nbomb, while the other has access to a disarming manual). By employing such diverse and interactive\\nscenarios, we aim to provide a thorough assessment of multi-modal multi-agent performance.\\nOur benchmark includes 10 distinct, easily customizable puzzles with thousands of unique solutions.\\nWe tested two different settings (AI-AI and AI-Human) and evaluated several popular multimodal\\nmodels, including closed-source models (GPT-4V , GPT-4O, and GPT-4o1) and open-source models\\n(Qwen-VL (Bai et al., 2023), and InternVL(Chen et al., 2024)). Surprisingly, the GPT series does\\nnot outperform even a simple random baseline in the AI-AI setting, highlighting a potential growth\\narea for future model development. Our contributions are as follows:\\n• We propose an evaluation framework called COMMA, a multimodal agent benchmark focusing\\non language communication between multiple agents (Section 3).\\n• Using COMMA, we carefully record conversations and performance metrics between state-of-\\nthe-art multimodal models such as QWenVL, InternVL, GPT-4o, GPT-4o1, etc (Section 4).\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='Preprint\\n• We categorize the agent capabilities tested in our model and common failure modes, providing\\ninsight into future research directions for improving inter-agent communication (Section 5).\\n2 R ELATED WORK\\nMulti-agent Frameworks: There are many emergent agent collaboration works (Gurcan, 2024;\\nPark et al., 2023; Hong et al., 2024; Liu et al., 2024; Ghafarollahi & Buehler, 2024; Li et al., 2023;\\nWu et al., 2023) among multiple language agents. Multi-agent systems arise mainly in two different\\nscenarios: (1) role-playing different task executors(e.g., software development requiring different\\nroles of agents, such as program manager, software architect, programmer Du et al. (2024); Qian\\net al. (2023); Hong et al. (2024), scientific discovery simulation Wu et al. (2023), and social simu-\\nlation Park et al. (2023); Gurcan (2024)); (2) communicating between agents with different pieces\\nof information Wu et al. (2023); Li et al. (2023) (e.g., consulting experts without sharing some\\nsensitive or confidential data. In our case, the AI solver has some private multimodal data, and the\\nAI expert has domain-specific knowledge or instructions).\\nInstruction-based Agent Benchmarks: Instruction-based agent benchmarks evaluate an agent’s\\ncapability of following a human instructions to finish a task (e.g., navigating on a website, interacting\\nwith an operating system Xu et al. (2024); Xie et al. (2024); Cao et al. (2024)). However, our\\nbenchmark focuses more on a communication-based evaluation where two clients engage in multi-\\nturn conversations to solve a task collaboratively.\\n3 B ENCHMARK\\n3.1 D ESIGN PRINCIPLES OF THE BENCHMARK\\nOur benchmark is inspired by the cooperative gameplay scenario in Keep Talking and Nobody Ex-\\nplodes Games (2015). In this game, two players work together to defuse a bomb under time pressure.\\nOne player, the defuser, can see the bomb but lacks the instructions to disarm it. The other player,\\nthe expert, has access to the bomb’s manual but cannot see the bomb itself. The players must rely\\non effective communication to exchange information, navigate challenges, and defuse the bomb.\\nWe generalize this dynamic for our benchmark, shifting the focus to solving vision-language puz-\\nzles in a communication-based agent framework. As multimodal agent systems gain momentum,\\nour goal is to create a benchmark that rigorously evaluates their reasoning, communication, and\\ncollaborative abilities. The design of our benchmark revolves around the following core principles:\\nLanguage communication: A critical aspect of our benchmark is evaluating natural language\\ncommunication between agents. Similar to how players in the original game exchange information\\nverbally, agents in our framework must use language to share observations, clarify ambiguities, and\\nreason about tasks. In order for the agents to succeed, they must display clarity, efficiency, and depth\\nof communication, making it an essential factor in task completion.\\nMulti-agent collaboration: In our benchmark, agents must work together, much like the two-\\nplayer dynamic of Keep Talking and Nobody Explodes. The collaboration element ensures that\\ntasks require mutual dependency, where each agent contributes unique information or capabilities\\nthat are critical to solving the puzzle. This principle highlights how well agents can cooperate and\\nleverage each other’s strengths.\\nMultimodality: Our benchmark emphasizes the integration of multiple sensory inputs and out-\\nputs, such as vision, language, and audio. The puzzles involve visual elements that agents must\\nperceive, describe, and interpret, alongside linguistic interactions. This principle assesses an agent’s\\nability to handle and synthesize multimodal information, a skill crucial to real-world applications.\\n3.2 C ATEGORIES OF AGENT CAPABILITY\\nWe benchmark agents working under different roles to solve various tasks in multiple settings, each\\nrequiring different capabilities. Specifically, the Solver agent must demonstrate strong instruction-\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Preprint\\nfollowing and multimodal reasoning, while the Expert agent is expected to excel in long text summa-\\nrization and information retrieval. Both agents must possess visual comprehension and descriptive\\nskills to succeed. Below, we outline the core capabilities tested in our benchmark.\\nMemory Recall (MR) In many puzzles, agents must remember their previous actions to progress.\\nThis ability is also implicitly tested when agents make mistakes. A competent agent should recall\\ninstances where past actions led to errors and adapt to avoid repeating them. The capacity to learn\\nfrom mistakes and leverage memory is crucial for effective problem-solving in real-world situations.\\nMultimodal Grounding (MG) Since the solver agent can only communicate with the expert with\\ntext, it must be able to ground relevant spans of the expert’s instructions to the image it currently sees.\\nThis grounding of language in visual context is essential for interpreting and following guidance\\nfrom the expert agent effectively.\\nMulti-Step Reasoning (MSR) Certain puzzles require agents to follow a sequence of actions based\\non step-by-step reasoning. Much like real-world tasks, such as following a recipe or placing an\\nonline order, each action must be deliberate and contribute toward the overall goal. Our benchmark\\nenables fine-grained evaluation of progress within these multi-step reasoning tasks, allowing for a\\nprecise assessment of models’ reasoning capabilities.\\nReal-time Reaction (RT) Some puzzles challenge agents to process information rapidly and act\\nwith precise timing. This is a critical skill for embodied agents operating in dynamic, real-world\\nenvironments where precise timing and quick reactions are vital.\\n3.3 T ASKS\\nWe create 10 puzzles across 4 different categories briefly summarized below. A more comprehensive\\ndescription along with example images and instruction manuals can be found in Appendix A.\\n• ButtonPuzzle (RT ): The solver must hold a colored button for a specific number of seconds\\nbased on the button’s color, the strip’s color when pressed, and the time remaining on a timer.\\n• ColorPuzzle (MR , MSR ): The solver presses squares of the least common color in a 4x4 grid,\\nthen follows a sequence based on a table, aiming to turn all squares white.\\n• KeypadPuzzle (MG , MSR ): The solver must describe the symbol of each button in a 2x2\\ngrid. The expert must then identify a column in the manual containing these four unique symbols\\nand tell the solver to press the symbols in the correct order from top to bottom.\\n• LedPuzzle (MR , MSR ): The solver presses a button if the value of its letter, when multiplied\\nby a stage’s LED color multiplier and taken modulo 26, matches the value of the letter diagonally\\nopposite it. At each stage, the letters on the buttons change.\\n• MazePuzzle (MG , MSR ): The solver navigates a mouse through a maze to a colored sphere,\\npressing the correct button to disarm the module based on the maze’s layout.\\n• MemoryPuzzle (MR , MSR ): The solver presses buttons according to specific positional and\\nlabel-based rules over five stages, with incorrect presses resetting progress.\\n• PasswordPuzzle (MG , MSR ): The solver cycles through letters to form a valid word from a\\npredefined list, submitting the correct word to complete the puzzle.\\n• DogPuzzle (RT ): The solver is presented with an image containing 0-4 dogs. Based on the\\nnumber of dogs in the image, the solver must press the submit button when the last digit of the\\ntimer matches the number of dogs in the image.\\n• WhoPuzzle (MG ): The solver must tell the value on a display to the expert, who will identify\\na button label. The solver must then tell this label to the expert, and then press the correct button\\nbased on a detailed list of instructions.\\n• WirePuzzle (MG ): The solver must cut one of the wires on the display. There are 3 to 6 colored\\nwires, and the correct wire to cut changes depending on the number and order of colors.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Preprint\\n4 E VALUATION\\n4.1 E XPERIMENTAL SETUP\\nIn this section, we describe the experimental settings of our multi-agent interaction environment\\nwhere two distinct agents, namely the Solver agent and the Expert agent, engage in iterative dialogue\\nsessions. The primary aim of this setup is to assess the collaborative problem-solving capabilities\\nbetween different agents or multimodal large language models (MLLMs). During our experiments,\\nwe limit the number of conversation turns to 20, allowing for a unified and systematic assessment of\\ninteractions. We use greedy decoding when available to maintain consistent agent output across runs\\nand run inference on a single NVIDIA A100 GPU with 80GB RAM. We parse the solver’s chosen\\nactions at each conversation turn using exact string matching, and use PyAutoGUI (Sweigart, 2023)\\nto directly perform the action on the interface if the solver outputs a valid action. Our exact prompts\\nfor both the solver and expert agent can be found in Appendix D.\\n4.2 E VALUATION METRICS\\nWe meticulously recorded several key performance metrics through multiple iterations of the exper-\\niments described below:\\n• Success Rate (SR):The solver agent is assigned a 0 or 1 value for each puzzle depending on if\\nit completed it. These values are averaged across all puzzles to obtain the success rate.\\n• Partial Success Rate (PSR):Because our benchmark includes puzzles with multi-step reason-\\ning, some puzzles can have a more precise success rate evaluation. For these multi-step puzzles,\\nwe assign the solver a number between 0 and 100 to indicate its progress towards the solution,\\nand average this number across puzzles to obtain partial success rate. For single-step puzzles,\\npartial success rate is identical to success rate.\\n• Average Mistakes (AM):After an action is chosen by the solver, the environment checks if\\nthe action was a mistake. We tally up the mistakes made during each puzzle and take a global\\naverage across puzzles to obtain average mistakes.\\n• Average Conversation Length (ACL):We count the number of conversation turns the Solver\\ntook to arrive at the solution, or default to the maximum of 20 if the solver failed. This count is\\naveraged across all puzzles to get Average Conversation Length.\\n4.3 M ODELS\\nOpen-Source Models\\n• Human: We conduct several experiments in which a human plays as the solver or expert to pro-\\nvide a strong baseline. As hiring participants was prohibitively expensive and time consuming,\\nwe role played as agents ourselves across 30 sampled puzzles as a preliminary study, and leave\\nfurther human participation to future work.\\n• InternVL (Chen et al., 2024): A vision-language model by Shanghai AI Lab, designed for\\ncross-modal tasks like visual question answering and image-text retrieval. We evaluate both the\\n26b and 8b variants of the model.\\n• QwenVL (Bai et al., 2023): We use version 2 of QWenVL (QWen-VL2), offering enhanced\\npretraining for improved performance on vision-language tasks. We use the 2b and 7b variants.\\nClosed-Source Models\\n• GPT-4V: A version of OpenAI’s GPT-4, GPT-4V incorporates visual processing, enabling it to\\ninterpret both text and images.\\n• GPT-4o: An optimized, faster, and more cost-effective variant of GPT-4, used for applications\\nrequiring speed and efficiency.\\n• GPT-4o1: The most recent version of OpenAI’s GPT series models, which claims to have im-\\nproved reasoning capability via internal chain of thought.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='Preprint\\n5 R ESULTS AND ANALYSIS\\nSolver Expert Average Partial Success Rate % (↑)\\nButton Dog Wire Who LED Memory Keypad Password Color Maze Overall\\nGPT4V\\nHuman\\n100 100 100 100 60 80 100 33 19 31 74\\nGPT4o 67 100 100 100 93 73 100 100 35 0 77\\nInternVL8b 100 100 100 100 67 47 100 67 4 0 69\\nHuman\\nGPT4o1 100 100 100 100 100 100 0 0 44 100 74\\nGPT4V 67 100 67 100 0 100 67 100 19 67 71\\nGPT4o 100 100 100 100 33 67 0 0 19 100 62\\nInternVL8b 100 100 50 100 50 10 62 50 0 50 55\\nRandom InternVL 100 100 100 100 85 25 33 0 1 15 56\\nGPT4V GPT4V 80 60 100 90 68 24 72 0 14 21 53\\nGPT4o GPT4o 100 100 100 90 26 34 40 0 14 0 50\\nInternVL GPT4o 100 100 100 50 30 56 7 0 16 0 46\\nInternVL26b InternVL26b 90 100 80 20 30 20 10 0 0 0 35\\nInternVL8b InternVL8b 100 100 30 40 4 12 5 0 11 0 30\\nInternVL8b QwenVL7b 100 100 20 20 56 20 15 0 7 25 36\\nQwenVL2b QwenVL2b 100 100 30 20 100 0 17 0 9 2 38\\nQwenVL7b GPT4o 90 90 55 30 16 26 35 0 7 9 36\\nQwenVL7b InternVL8b 100 100 40 30 0 34 5 0 6 13 33\\nQwenVL7b QwenVL7b 90 90 30 10 52 0 33 0 4 12 32\\nTable 1: Average Partial Success Rate of multimodal agents on each puzzle. The solver is assigned a\\nvalue between 0-100 indicating how far the solver progressed through the puzzle. The partial success\\nrate is calculated by averaging this value over 10, 3, and 100 independent runs of each puzzle for the\\nAI-AI, AI-Human, and random settings. The overall column is an average across all the puzzles.\\n5.1 O VERALL PERFORMANCE\\nTable 1 illustrates the performances of all combinations of solver and expert pairs we evaluated.\\nWe evaluate some combinations of different open-source models because they are free, and leave\\npairings of separate closed-source models for future work. Intrerestingly, increasing model size\\ndoes not always improve performance, as QWenVL 2b (38% PSR) outperforms QWenVL 7b (32%\\nPSR). We observe that the random agent performs well on simple puzzles which only require one\\ncorrect action (e.g. 100% on WirePuzzle, 100% on WhoPuzzle) and even outperforms all of the\\nAI-AI setting performances. However, a random agent struggles with puzzles which require several\\ncorrect actions in a row, as evidenced by its 0 percent success rate and low partial success rate in\\nMemory, Maze, Password, Keypad, and Color puzzles.\\nOur results show that GPT-4o1 is the most powerful agent to use, although we only run a single\\niteration of each puzzle with a Human-AI setting due to rate limit issues. It makes the most progress\\nin the color puzzle, achieving a partial success rate of 44%, followed by GPT-4o at 35%. In this\\npuzzle, we observed that GPT-4o1’s “reasoning process” made correct connections with different\\nparts of the puzzle’s manual, though sometimes it made incorrect conclusions at the end of the\\nprocess. GPT-4V is the second best agent, with an average partial score of 53% in the AI-AI setting,\\nalthough the performance is still far worse than human performance. For example, switching from\\na gpt-4o solver to a human solver increases overall partial success rate from 53% to 74%.\\n5.2 Q UALITATIVE ANALYSIS ON MODEL FAILURES\\nIn this section, we highlight key takeaways and common failure modes displayed by the agents\\nduring their conversations. We manually classify errors across 50 conversations into the following\\ncategories, and report the distribution of these errors across the benchmark in Figure 3 and Figure 4.\\n• Roleplay: The expert thinks it is the solver or vice versa. Figure 2 illustrates how the expert can\\nmisunderstand its role assignment, leading to miscommunication and failure to solve the puzzle.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content=\"Preprint\\nExpert: What is the display and\\nwhat buttons do you see?\\nSolver: The display shows the\\nnumber 3\\xa0in green. There arefour buttons labeled 1, 2, 3, and4 below the display. On the rightside, there is a vertical bar withdifferent levels of shading. Thetime left is 76:39, and the serialnumber is 9854440.Action: press_3\\nExpert: Can you please wait for\\nme to tell you what to do beforeyou do the action\\nSolver: press_2\\nSolver Agent View Agent Dialogue (Human Solver)\\nSolver Agent View\\n Miscommunication Error Roleplay Error\\nSolver:\\xa0I need you to tell mewhat to do about these letters.\\nExpert:\\xa0SOLVER: I'm sorry, Imeant to say that I need you totell me what to do about theletters SHJOG.\\nSolver:\\xa0I'm not sure why you arepretending to be the solver. Youshould have been provided amanual with instructions on howto disarm this puzzle\\nExpert:\\xa0SOLVER: I'm sorry, Imeant to say that I'm not surewhy you are pretending to be thesolver. You should have beenprovided a manual withinstructions on how to disarmthis puzzle.\\nSolver: cut_wire_1\\nExpert: I apologize for the\\nmistake. Please follow theserevised instructions:1. If there is more than one redwire and the last digit of theserial number is odd, cut the lastred wire.\\n2. If the last wire is yellow ...\\nSolver: cut_wire_1\\nExpert:\\xa0I apologize for themistake. Please follow theserevised instructions ...\\nSolver: cut_wire_1\\nSolver Agent View Agent Dialogue (Human Solver)Solver Agent View Repetition Loop Error Misinterpretation Error\\nSolver:\\xa0The image shows a scenewith four dogs near a body ofwater. There are three dogs onthe left side of the water, andone dog on the right side. Thebackground features greeneryand a fence. The image has a grayborder with a SUBMIT\\xa0button atthe bottom...\\xa0\\nExpert:\\xa0Thank you for thedetailed description. The imagecontains four dogs. Please waitfor the time to reach the lastdigit matching the number ofdogs, which is 4. When the timeleft is 4, press the SUBMITbutton.\\nFigure 2: Case study examples of with the InternVL 8b model playing the role of AI solver and\\nAI expert, resulting in failure to complete the task. Top Left: An AI solver Miscommunication\\nerror causes it to ignore instructions from the human expert, causing it to try out actions without\\nunderstanding the solution. Top Right: The AI expert misunderstands its role with aRoleplay error\\nand pretends it is the Solver. Bottom Left: The solver repeats the same bad action, resulting in a\\nRepetition Looperror. Bottom Right: The solver misinterprets the number of dogs in the image,\\nleading to a Misinterpretation error.\\nFigure 3: Distribution of error category across\\nall puzzles in AI-AI (GPT-4o) setting.\\nFigure 4: Distribution of error category across all\\npuzzles in the AI-AI (InternVL26B) setting.\\n• Misinterpretation: The solver misunderstands the current puzzle state/signal, resulting in fail-\\nure. For instance, Figure 2 showcases the solver misinterpreting the number of dogs in the image,\\nleading to incorrect instructions from the expert.\\n• Repetition Loop:The solver sometimes repeats its past incorrect actions, even if is in a situation\\nit has encountered before. We classify any repeated incorrect state, action pair into this category.\\n• Miscommunication: As shown in Figure 2, the agent occasionally does not listen to the expert’s\\ninstructions, attempting to solve the puzzle on its own as if it were the expert. We also observed\\nsome open source models such as LLaV A don’t have instruction following capability for this task\\n7\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Preprint\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\\nConversation Length\\n0\\n20\\n40\\n60\\n80\\n100Success Rate (%)\\nOverall Success Rate\\nGPT4V\\nGPT4o\\nHuman + GPT4o\\nHuman + InternVL8b\\nInternVL8b\\nRandom\\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\\nConversation Length\\n0\\n2\\n4\\n6\\n8\\n10Average Mistakes\\nOverall Mistakes Rate\\nGPT4V\\nGPT4o\\nHuman + GPT4o\\nHuman + InternVL8b\\nInternVL8b\\nRandom\\nFigure 5: We plot the overall success and mistake rate on our benchmark as a function of the number\\nof allowed conversation turns. We obtain the overall success rate and mistake rate by averaging over\\n100 sampled instances across all puzzles for the AI-AI setting, and 30 sampled instances for the\\nAI-Human setting. Diamond marker indicates a human is the solver agent. Square marker indicates\\nthe solver and expert are played by the same AI model. Random is a baseline where the solver agent\\nchooses actions uniformly at random at each time step.\\nwithout further finetuning. Additionally, the solver sometimes describes the puzzle incorrectly\\nto the expert which results in failure.\\nRepeated Actions are a Common Failure for AgentsBoth open-source and closed-source mod-\\nels often fail due to repeating bad actions. As shown in Figure 4, InternVL is more inclined to get\\nstuck in a repetition loop compared to GPT-4o (40% vs 18.5%). This suggests a potential improve-\\nment direction by including multi-step repetitive conversations during training, in which the model\\nshould learn to break out of the loop.\\nAgents Make More Miscommunication Errors than MisinterpretationWe observe that misin-\\nterpretation accounts for a much smaller proportion of total errors compared to miscommunication\\nrelated errors (9.3% vs 44.5%) for GPT-4o and (12.0% vs 30.0%) for InternVL. We hypothesize this\\noccurs because the training data mixture for these models primarily includes high quality single-\\nagent data from academic benchmarks such as Visual Question Answering (Antol et al., 2015),\\nImage Captioning, etc. Including tasks requiring communication may help address this issue.\\n5.3 F INE -GRAINED ANALYSIS\\nMultimodal Agents Struggle to Learn from Past MistakesAn important skill for humans is to\\nlearn from past mistakes to adapt to new situations. Here we analyze if agents can display a similar\\ncapability and recover when exploring a bad trajectory when solving a puzzle. Figure 5 plots the\\nnumber of allowed conversation turns to solve a puzzle, along with the overall success and mistakes\\nrate of several multimodal agents. We note the following observations. First, incorporating a human\\nin the pipline in the form of a human solver significantly improves overall success rate, being the\\nonly agent to achieve over random baseline performance at the 20-turn conversation mark. This\\nis also supported by the mistakes plot, in which the human solver setting generally displays lower\\nmistakes as the conversation progresses compared to the full AI setting. In fact, the human solver,\\ngpt-4o expert setting shows zero mistakes over the course of most conversations, with the main\\nreason for failure being the conversation limit. Second, humans appear to have greater ability to\\nrecover, as indicated by the faster increase in their success rate as conversation length increases, as\\nwell as the fact that they make less mistakes over time in the mistakes chart.\\nPerformance Based on CapabilityHere we group the model performance based on the category\\ntested: Memory ( MR), Grounding (MG), Reasoning (MSR), and Reaction (RT).\\nMultimodal Agents Excel at Simple Realtime TasksFigure 6 gives a more nuanced look at how\\nwell multimodal agents are equipped to deal with puzzles of different nature. The agents performed\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Preprint\\nKeypad Maze\\nPassword\\nWho Wire\\n0\\n20\\n40\\n60\\n80\\n100Partial Success Rate (%)\\nMultimodal Grounding\\nHuman Agent\\nNo Human Agent\\nColor Keypad\\nLed Maze\\nMemoryPassword\\nMulti-Step Reasoning\\nButton\\nDog\\nReal-time Reaction\\nColor Led\\nMemory\\nMemory Recall\\nKeypad Maze\\nPassword\\nWho Wire\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16Average Number of Mistakes\\nMultimodal Grounding\\nHuman Agent\\nNo Human Agent\\nColor Keypad\\nLed Maze\\nMemoryPassword\\nMulti-Step Reasoning\\nButton\\nDog\\nReal-time Reaction\\nColor Led\\nMemory\\nMemory Recall\\nFigure 6: Average partial success rate ( Top) and mistake rate ( Bottom) for puzzles based on cate-\\ngories with and without human involvement. Each bar is an average performance across all model\\ncombinations. Note, some puzzles fall into multiple categorie and appear in multiple subplots.\\nwell in the RT category, with the Button puzzle having the highest average partial score at 95%\\nPSR, and Dog puzzle follows closely at 94% PSR. This suggests that agents are best at real-time\\nreaction tasks which may involve quick decision-making based on immediate visual input and not\\nmuch further communication.\\nGrounding is a Challenge for Multimodal AgentsMultimodal grounding presents a significant\\nchallenge to agents in the AI-AI setting, as seen in the varied performance. This ability requires\\nagents to interpret and connect visual stimuli with textual instructions. The stronger performance\\non puzzles like Wire puzzle (88% PSR) and Who puzzle (100% PSR) indicates that agents manage\\nbetter when tasks are more structured or involve simpler visual-text connections. In contrast, puzzles\\nlike the Password puzzle (1% PSR) and Maze puzzle (8% PSR), which are more abstract or less\\nstructured, present greater difficulties.\\nMultimodal Agents Struggle with Memory and Multi-Step ReasoningMemory-based puzzles\\npresent a challenge for agents. While the LED Puzzle (38% PSR) shows moderate performance,\\nthe Color puzzle highlights a significant difficulty (9% PSR). This suggests that agents may struggle\\nwith tasks requiring them to remember previous states and actions to progress or solve sequential\\nproblems efficiently like the Memory puzzle (23% PSR). The complexity of multi-stage memory\\ntasks could explain the poor performance. In the same vein, multi-step logical reasoning puzzles\\nrequire agents to think ahead and execute a series of steps to achieve the final goal. The low perfor-\\nmance on the Color puzzle (9% PSR) and KeyPad puzzle (24% PSR) suggests that complex reason-\\ning tasks, especially those involving multiple stages, remain a significant challenge for agents.\\n6 C ONCLUSION\\nIn this paper, we address a critical gap in the field of multi-modal agents by introducing a novel\\nbenchmark specifically designed to evaluate communication in a multi-modal, multi-agent system.\\nWhile substantial progress has been made in developing individual multi-modal agents, collaborative\\nframeworks remain under-explored, particularly in scenarios requiring secure communication and\\nthe handling of sensitive data. Our benchmark aims to bridge this gap by simulating real-world\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Preprint\\nconditions where agents possess complementary information and must work together to achieve\\ncomplex goals. We comprehensively evaluate metrics such as partial success rate, mistake rate,\\nand document common failure modes for both AI-AI and AI-Human interactions. Our findings\\nshow that multimodal agents struggle to communicate with each other, often falling short of even\\na simple random baseline due to poor communication and frequently repeated bad actions. These\\nfindings emphasize the need for deeper investigation into enhancing inter-agent collaboration. We\\nhope the insights from our benchmark lay the foundation for future research on multi-modal agent\\ncollaboration and inspires the community to explore innovative approaches to improve multimodal\\nagent capabilities this emerging field of communicative multimodal systems.\\n7 L IMITATIONS\\nWhile we aim to construct a holistic framework for multimodal agent communication, our experi-\\nments may not represent all possible scenarios in our puzzles. We conduct a preliminary study by\\nsampling puzzle configurations and conversations between agents, and we leave more comprehen-\\nsive evaluation and expansion of puzzle categories to future work. Additionally, there will inevitably\\nbe a simulation-to-reality gap from our benchmark to real-world situations, thus a high score on our\\nbenchmark may not perfectly generalize to real-world communication scenarios. Lastly, we ac-\\nknowledge that there is inherent risk to using multimodal agents when handling private data. Given\\nthat LLMs have been shown to be prone to jailbreaking, it is critical to take additional safety mea-\\nsures before deploying an agent in practice, even if it achieves a high score on our benchmark.\\n8 A CKNOWLEDGEMENT\\nResearch reported in this publication was partially supported by the National Institute Of Biomed-\\nical Imaging And Bioengineering of the National Institutes of Health under Award Number\\nR01EB033782. The content is solely the responsibility of the authors and does not necessarily\\nrepresent the official views of the National Institutes of Health.\\nREFERENCES\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-\\nnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international\\nconference on computer vision, pp. 2425–2433, 2015.\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\\nZhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities.\\narXiv preprint arXiv:2308.12966, 2023.\\nRuisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang\\nXiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, Tianbao Xie, Hongshen Xu, Danyang\\nZhang, Sida Wang, Ruoxi Sun, Pengcheng Yin, Caiming Xiong, Ansong Ni, Qian Liu, Victor\\nZhong, Lu Chen, Kai Yu, and Tao Yu. Spider2-v: How far are multimodal agents from au-\\ntomating data science and engineering workflows?, 2024. URL https://arxiv.org/abs/\\n2407.10956.\\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\\nZhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\\nfor generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 24185–24198, 2024.\\nZhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng\\nYang. Multi-agent software development through cross-team collaboration, 2024. URL https:\\n//arxiv.org/abs/2406.08979.\\nSteel Crate Games. Keep talking and nobody explodes. Video Game, 2015. URL https://\\nkeeptalkinggame.com/.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Preprint\\nAlireza Ghafarollahi and Markus J. Buehler. Atomagents: Alloy design and discovery through\\nphysics-aware multi-modal multi-agent artificial intelligence, 2024. URL https://arxiv.\\norg/abs/2407.10022.\\nOnder Gurcan. Llm-augmented agent-based modelling for social simulations: Challenges and op-\\nportunities, 2024. URL https://arxiv.org/abs/2405.06700.\\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,\\nChenglin Wu, and J¨urgen Schmidhuber. MetaGPT: Meta programming for a multi-agent collab-\\norative framework. In The Twelfth International Conference on Learning Representations, 2024.\\nURL https://openreview.net/forum?id=VtmBAGCN7o.\\nJing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham\\nNeubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating\\nmultimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for ”mind” exploration of large language model society, 2023.\\nURL https://arxiv.org/abs/2303.17760.\\nQinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang,\\nDan Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, and Dawn Song. Llm-pbe: Assess-\\ning data privacy in large language models, 2024. URL https://arxiv.org/abs/2408.\\n12787.\\nYang Liu, Peng Sun, and Hang Li. Large language models as agents in two-player games, 2024.\\nURL https://arxiv.org/abs/2402.08078.\\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023. URL\\nhttps://arxiv.org/abs/2304.03442.\\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize\\nChen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chat-\\ndev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.\\nURL https://arxiv.org/abs/2307.07924.\\nAl Sweigart. Pyautogui. https://pyautogui.readthedocs.io/, 2023. Accessed: 2024-\\n09-30.\\nXiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark\\nGerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning.\\narXiv preprint arXiv:2311.10537, 2023.\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and\\nChi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.\\nURL https://arxiv.org/abs/2308.08155.\\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing\\nHua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Sil-\\nvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal\\nagents for open-ended tasks in real computer environments, 2024. URL https://arxiv.\\norg/abs/2404.07972.\\nTianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie,\\nYongchao Chen, Shilong Liu, Bochen Qian, Philip Torr, Bernard Ghanem, and Guohao Li. Crab:\\nCross-environment agent benchmark for multimodal language model agents, 2024. URLhttps:\\n//arxiv.org/abs/2407.01511.\\nYi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng\\nLi, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal\\nllm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint\\narXiv:2408.13257, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='Preprint\\nA M ANUALS\\nBUTTON PUZZLE\\nIf the button is yellow, hold the button and refer to the next set of instructions of when to release it.\\nIf you start holding the button down, a colored strip will light up on the right side of the module.\\nBased on its color, you must release the button at a specific point in time:\\n• Blue strip: release when the countdown timer has a 4 in any position.\\n• White strip: release when the countdown timer has a 1 in any position.\\n• Yellow strip: release when the countdown timer has a 5 in any position.\\n• Any other color strip: release when the countdown timer has a 1 in any position.\\nCOLOR PUZZLE\\nPress all squares in the correct group to progress the module. Pressing a square will cause it to light\\nup white. Make all squares white to disarm the module.\\nTo begin, press the color group containing the fewest squares. If there is a tie, you should choose\\nthe first color that appears in the list:\\n• Red\\n• Blue\\n• Green\\n• Yellow\\n• Magenta\\nThen use the table to determine the next group to press in each stage. ”Group” refers to all squares\\nof a particular color, or all non-white squares in the topmost row or leftmost column containing\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Preprint\\nnon-white squares. Pressing an incorrect square will result in a strike and reset the module. White\\nsquares will remain white for the duration of the module, but non-white squares may change color\\nin each stage.\\nThe table below helps to choose the next subgroup to press. The numbered keys correspond to the\\nnumber of currently white squares, and the ”previously pressed color” key gives you values that\\nindicate what color to press next based on the corresponding number of white squares.\\nPreviously Pressed Color: {Red, Blue, Green, Yellow, Magenta, Row, Column}\\n1 :{Blue, Column, Red, Y ellow, Row, Green, Magenta}\\n2 :{Row, Green, Blue, Magenta, Red, Column, Y ellow}\\n3 :{Y ellow, Magenta, Green, Row, Blue, Red, Column}\\n4 :{Blue, Green, Y ellow, Column, Red, Row, Magenta}\\n5 :{Y ellow, Row, Blue, Magenta, Column, Red, Green}\\n6 :{Magenta, Red, Y ellow, Green, Column, Blue, Row}\\n7 :{Green, Row, Column, Blue, Magenta, Y ellow, Red}\\n8 :{Magenta, Red, Green, Blue, Y ellow, Column, Row}\\n9 :{Column, Y ellow, Red, Green, Row, Magenta, Blue}\\n10 :{Green, Column, Row, Red, Magenta, Blue, Y ellow}\\n11 :{Red, Y ellow, Row, Column, Green, Magenta, Blue}\\n12 :{Column, Row, Column, Row, Row, Column, Row}\\n13 :{Row, Column, Row, Column, Row, Column, Column}\\n14 :{Column, Column, Row, Row, Column, Row, Column}\\n15 :{Row, Row, Column, Row, Column, Column, Row}\\nKEYPAD PUZZLE\\nOnly one column has all four symbols from the keypad. Press the four buttons in the order their\\nsymbols appear from top to bottom within that column.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='Preprint\\nLED P UZZLE\\nTwo to five LEDs are installed at the top of the module, representing stages. To disarm the module,\\nthese stages must be solved in order. Four buttons with four different letters are shown. The letters\\nchange at each stage. The current stage is indicated by a number in the top left of the module. The\\ncurrent stage’s multiplier is indicated by that stage’s LED according to the following mapping:\\n• Red: 2\\n• Green: 3\\n• Blue: 4\\n• Yellow: 5\\n• Purple: 6\\n• Orange: 7\\nAssign each letter of the alphabet to the numbers 0-25 (A = 0, B = 1, C = 2, etc.). A button is correct\\nif its letter value, multiplied by the current stage’s multiplier, modulo 26, is equal to the regular value\\nof the letter on its diagonally opposite button. At each stage, press a correct button. There may be\\nmore than one possible answer.\\nMAZE PUZZLE\\nThe mouse is the grey sphere. It can only move into other white squares. Dark squares are walls\\nand it cannot move into those. The mouse can move forward or backward or turn left or right. To\\ndisarm the module, navigate the mouse to the accepting position and press the circular button with\\nthe labyrinth. Pressing the button at any other location causes a strike. The accepting position is\\nmarked with one of four colored spheres. Which one depends on the color of the torus in the middle\\nof the maze, according to the table below.\\n• Torus Colors: Green, Blue, Red, Yellow\\n• Sphere Colors: Blue, Red, Green, Yellow\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Preprint\\nMEMORY PUZZLE\\nPress the correct button to progress the module to the next stage. Complete all stages to disarm the\\nmodule. Pressing an incorrect button will reset the module back to stage 1. Button positions are\\nordered from left to right.\\nSTAGE 1\\n• If the display is 1, press the button in the second position.\\n• If the display is 2, press the button in the second position.\\n• If the display is 3, press the button in the third position.\\n• If the display is 4, press the button in the fourth position.\\nSTAGE 2\\n• If the display is 1, press the button labeled ”4”.\\n• If the display is 2, press the button in the same position as you pressed in stage 1.\\n• If the display is 3, press the button in the first position.\\n• If the display is 4, press the button in the same position as you pressed in stage 1.\\nSTAGE 3\\n• If the display is 1, press the button with the same label you pressed in stage 2.\\n• If the display is 2, press the button with the same label you pressed in stage 1.\\n• If the display is 3, press the button in the third position.\\n• If the display is 4, press the button labeled ”4”.\\nSTAGE 4\\n• If the display is 1, press the button in the same position as you pressed in stage 1.\\n• If the display is 2, press the button in the first position.\\n• If the display is 3, press the button in the same position as you pressed in stage 2.\\n• If the display is 4, press the button in the same position as you pressed in stage 2.\\nSTAGE 5\\n• If the display is 1, press the button with the same label you pressed in stage 1.\\n• If the display is 2, press the button with the same label you pressed in stage 2.\\n• If the display is 3, press the button with the same label you pressed in stage 4.\\n• If the display is 4, press the button with the same label you pressed in stage 3.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='Preprint\\nPASSWORD PUZZLE\\nThe buttons above and below each letter will cycle through the possibilities for that position. Each\\ncycle will have 3 consecutive letters. Only one combination of the available letters will match a\\npassword from the list below. Press the submit button once the correct word has been set.\\nLIST OF POSSIBLE WORDS :\\n• about, after, again, below, could, every, first, found, great, house, large, learn, never, other,\\nplace, plant, point, right, small, sound, spell, still, study, their, there, these, thing, think,\\nthree, water, where, which, world, would, write.\\nWHO PUZZLE\\n1. Read the display and use step 1 to determine which button label to read. 2. Using this button\\nlabel, use step 2 to determine which button to push.\\nSTEP 1:\\nBased on the display, ask the SOLVER to read the label of a particular button and proceed to step 2:\\n• ”YES”: Middle Left\\n• ”FIRST”: Top Right\\n• ”DISPLAY”: Bottom Right\\n• ”OKAY”: Top Right\\n• ”SAYS”: Bottom Right\\n• ”NOTHING”: Middle Left\\n• ”(No Text)”: Bottom Left\\n• ”BLANK”: Middle Right\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='Preprint\\n• ”NO”: Bottom Right\\n• ”LED”: Middle Left\\n• ”LEAD”: Bottom Right\\n• ”READ”: Middle Right\\n• ”RED”: Middle Right\\n• ”REED”: Bottom Left\\n• ”LEED”: Bottom Left\\n• ”HOLD ON”: Bottom Right\\n• ”YOU”: Middle Right\\n• ”YOU ARE”: Bottom Right\\n• ”YOUR”: Middle Right\\n• ”YOU’RE”: Middle Right\\n• ”UR”: Top Left\\n• ”THERE”: Bottom Right\\n• ”THEY’RE”: Bottom Left\\n• ”THEIR”: Middle Right\\n• ”THEY ARE”: Middle Left\\n• ”SEE”: Bottom Right\\n• ”C”: Top Right\\n• ”CEE”: Bottom Right\\nSTEP 2:\\nUsing the label from step 1, push the first button that appears in its corresponding list:\\n• ”READY”: YES, OKAY , WHAT, MIDDLE, LEFT, PRESS, RIGHT, BLANK, READY ,\\nNO, FIRST, UHHH, NOTHING, W AIT\\n• ”FIRST”: LEFT, OKAY , YES, MIDDLE, NO, RIGHT, NOTHING, UHHH, W AIT,\\nREADY , BLANK, WHAT, PRESS, FIRST\\n• ”NO”: BLANK, UHHH, W AIT, FIRST, WHAT, READY , RIGHT, YES, NOTHING,\\nLEFT, PRESS, OKAY , NO, MIDDLE\\n• ”BLANK”: W AIT, RIGHT, OKAY , MIDDLE, BLANK, PRESS, READY , NOTHING,\\nNO, WHAT, LEFT, UHHH, YES, FIRST\\n• ”NOTHING”: UHHH, RIGHT, OKAY , MIDDLE, YES, BLANK, NO, PRESS, LEFT,\\nWHAT, W AIT, FIRST, NOTHING, READY\\n• ”YES”: OKAY , RIGHT, UHHH, MIDDLE, FIRST, WHAT, PRESS, READY , NOTHING,\\nYES, LEFT, BLANK, NO, W AIT\\n• ”WHAT”: UHHH, WHAT, LEFT, NOTHING, READY , BLANK, MIDDLE, NO, OKAY ,\\nFIRST, W AIT, YES, PRESS, RIGHT\\n• ”UHHH”: READY , NOTHING, LEFT, WHAT, OKAY , YES, RIGHT, NO, PRESS,\\nBLANK, UHHH, MIDDLE, W AIT, FIRST\\n• ”LEFT”: RIGHT, LEFT, FIRST, NO, MIDDLE, YES, BLANK, WHAT, UHHH, W AIT,\\nPRESS, READY , OKAY , NOTHING\\n• ”RIGHT”: YES, NOTHING, READY , PRESS, NO, W AIT, WHAT, RIGHT, MIDDLE,\\nLEFT, UHHH, BLANK, OKAY , FIRST\\n• ”MIDDLE”: BLANK, READY , OKAY , WHAT, NOTHING, PRESS, NO, W AIT, LEFT,\\nMIDDLE, RIGHT, FIRST, UHHH, YES\\n• ”OKAY”: MIDDLE, NO, FIRST, YES, UHHH, NOTHING, W AIT, OKAY , LEFT,\\nREADY , BLANK, PRESS, WHAT, RIGHT\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='Preprint\\n• ”W AIT”: UHHH, NO, BLANK, OKAY , YES, LEFT, FIRST, PRESS, WHAT, W AIT,\\nNOTHING, READY , RIGHT, MIDDLE\\n• ”PRESS”: RIGHT, MIDDLE, YES, READY , PRESS, OKAY , NOTHING, UHHH,\\nBLANK, LEFT, FIRST, WHAT, NO, W AIT\\n• ”YOU”: SURE, YOU ARE, YOUR, YOU’RE, NEXT, UH HUH, UR, HOLD, WHAT?,\\nYOU, UH UH, LIKE, DONE, U\\n• ”YOU ARE”: YOUR, NEXT, LIKE, UH HUH, WHAT?, DONE, UH UH, HOLD, YOU,\\nU, YOU’RE, SURE, UR, YOU ARE\\n• ”YOUR”: UH UH, YOU ARE, UH HUH, YOUR, NEXT, UR, SURE, U, YOU’RE, YOU,\\nWHAT?, HOLD, LIKE, DONE\\n• ”YOU’RE”: YOU, YOU’RE, UR, NEXT, UH UH, YOU ARE, U, YOUR, WHAT?, UH\\nHUH, SURE, DONE, LIKE, HOLD\\n• ”UR”: DONE, U, UR, UH HUH, WHAT?, SURE, YOUR, HOLD, YOU’RE, LIKE,\\nNEXT, UH UH, YOU ARE, YOU\\n• ”U”: UH HUH, SURE, NEXT, WHAT?, YOU’RE, UR, UH UH, DONE, U, YOU, LIKE,\\nHOLD, YOU ARE, YOUR\\n• ”UH HUH”: UH HUH, YOUR, YOU ARE, YOU, DONE, HOLD, UH UH, NEXT, SURE,\\nLIKE, YOU’RE, UR, U, WHAT?\\n• ”UH UH”: UR, U, YOU ARE, YOU’RE, NEXT, UH UH, DONE, YOU, UH HUH, LIKE,\\nYOUR, SURE, HOLD, WHAT?\\n• ”WHAT?”: YOU, HOLD, YOU’RE, YOUR, U, DONE, UH UH, LIKE, YOU ARE, UH\\nHUH, UR, NEXT, WHAT?, SURE\\n• ”DONE”: SURE, UH HUH, NEXT, WHAT?, YOUR, UR, YOU’RE, HOLD, LIKE, YOU,\\nU, YOU ARE, UH UH, DONE\\n• ”NEXT”: WHAT?, UH HUH, UH UH, YOUR, HOLD, SURE, NEXT, LIKE, DONE,\\nYOU ARE, UR, YOU’RE, U, YOU\\n• ”HOLD”: YOU ARE, U, DONE, UH UH, YOU, UR, SURE, WHAT?, YOU’RE, NEXT,\\nHOLD, UH HUH, YOUR, LIKE\\n• ”SURE”: YOU ARE, DONE, LIKE, YOU’RE, YOU, HOLD, UH HUH, UR, SURE, U,\\nWHAT?, NEXT, YOUR, UH UH\\n• ”LIKE”: YOU’RE, NEXT, U, UR, HOLD, DONE, UH UH, WHAT?, UH HUH, YOU,\\nLIKE, SURE, YOU ARE, YOUR\\nWIRE PUZZLE\\nHere is the manual: The WirePuzzle module can have 3-6 wires on it. Only the one correct wire\\nneeds to be cut to disarm the module. Wire ordering begins with the first on the top.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='Preprint\\n3 WIRES :\\n• If there are no red wires, cut the second wire.\\n• Otherwise, if the last wire is white, cut the last wire.\\n• Otherwise, if there is more than one blue wire, cut the last blue wire.\\n• Otherwise, cut the last wire.\\n4 WIRES :\\n• If there is more than one red wire and the last digit of the serial number is odd, cut the last\\nred wire.\\n• Otherwise, if the last wire is yellow and there are no red wires, cut the first wire.\\n• Otherwise, if there is exactly one blue wire, cut the first wire.\\n• Otherwise, if there is more than one yellow wire, cut the last wire.\\n• Otherwise, cut the second wire.\\n5 WIRES :\\n• If the last wire is black and the last digit of the serial number is odd, cut the fourth wire.\\n• Otherwise, if there is exactly one red wire and there is more than one yellow wire, cut the\\nfirst wire.\\n• Otherwise, if there are no black wires, cut the second wire.\\n• Otherwise, cut the first wire.\\n6 WIRES :\\n• If there are no yellow wires and the last digit of the serial number is odd, cut the third wire.\\n• Otherwise, if there is exactly one yellow wire and there is more than one white wire, cut\\nthe fourth wire.\\n• Otherwise, if there are no red wires, cut the last wire.\\n• Otherwise, cut the fourth wire.\\nDOG PUZZLE\\nA picture containing 0-5 dogs will be shown on the display. Based on the number of dogs in the\\nimage, press the submit button when the last digit of the time left matches the number of dogs in the\\nimage.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Preprint\\nB P UZZLE LISTS\\n• ButtonPuzzle: The solver is presented with an empty strip, a colored button and a timer\\ncounting down from 10 minutes. When the button is pressed and held, the strip turns a\\ncertain color. Based off of a combination of the color of the button, the color of the strip\\nand the time on the clock, the solver has to keep the button pressed for a certain number of\\nseconds.\\n• ColorPuzzle: The solver is presented with a 4 ×4 grid of colored tiles. The solver must\\nfirst identify the color group with the fewest squares on a 4x4 grid and press all the squares\\nof that color to start the module. The solver then needs to refer to a table to determine\\nthe next group to press based on the current configuration. Pressing any incorrect square\\nresults in a strike and resets the module. Non-white squares may change color after each\\nstage. The goal is to make all squares on the grid white by following the correct sequence\\nof groups.\\n• KeypadPuzzle: The solver has to examine a 2x2 grid of unique symbols and identify\\nwhich of the four columns below the grid contains all four symbols from the grid. Once\\nthe correct column is found, the solver must press the buttons in that column in the order\\nthe symbols appear from top to bottom.\\n• LedPuzzle: The solver progresses through 2 to 5 stages, each indicated by an LED color\\nthat specifies a multiplier (Red: 2, Green: 3, Blue: 4, Yellow: 5, Purple: 6, Orange: 7).\\nFour buttons with changing letters are shown at each stage. The solver must assign values\\nto letters (A = 0, B = 1, etc.) and press a button if its letter value, when multiplied by\\nthe stage’s multiplier and taken modulo 26, equals the value of the letter on its diagonally\\nopposite button. Each stage requires pressing a correct button, and there may be multiple\\nvalid choices.\\n• MazePuzzle: In ”MazePuzzle,” the solver must navigate a mouse through a maze by mov-\\ning it forward, backward, or turning left or right to reach the accepting position, which is\\nmarked by a colored sphere. The color of the accepting sphere depends on the color of the\\ntorus in the middle of the maze, with the mapping being Green → Blue, Blue → Red, Red\\n→ Green, and Yellow → Yellow. To disarm the module, the solver must press the circular\\nbutton with the labyrinth; pressing any other button results in a strike.\\n• MemoryPuzzle: The solver must press the correct button based on the display number to\\nadvance through five stages. Incorrect presses reset the module to stage 1. Each stage has\\nspecific rules: Stage 1 requires pressing buttons in specific positions based on the display;\\nStage 2 involves pressing a button labeled ”4” or positions from Stage 1; Stage 3 requires\\npressing buttons with labels matching previous stages or specific positions; Stage 4 uses\\npositions from earlier stages; and Stage 5 involves pressing buttons with labels matching\\nearlier stages’ labels.\\n• PasswordPuzzle: The solver cycles through letters above and below each position to form\\na word. Each cycle displays three consecutive letters, and only one combination will match\\na predefined list of possible words. Once the correct word is set, the solver must press\\nthe submit button to complete the puzzle. The list of possible words includes terms like\\n”about,” ”after,” ”great,” and ”write.”\\n• SoundPuzzle: The solver listens to a sound clip and matches it to one of the options\\nprovided. Each sound clip is associated with a code made up of four symbols ($, *, &,\\n#). After identifying the correct option from the list (e.g., Taxi Dispatch, Cow, Extractor\\nFan, Train Station), the solver enters the corresponding code using a four-button keypad to\\nproceed.\\n• WhoPuzzle The solver reads a display to determine which button label to reference and\\nthen uses that label to find which button to press based on a predefined list. The process\\ninvolves two steps: first, the display directs you to a specific button label according to a\\ndetailed list of instructions. Second, using that label, you select the appropriate button from\\na secondary list of options. Successfully following these steps in sequence will advance the\\nmodule.\\n• WirePuzzle: The solver is presented with between 3 and 6 wires of different colors. Based\\noff of the ordering and number of colors of each type, the solver has to cut the wires in\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Preprint\\na specific order. The manual lists out the different branches that can be possible for each\\nsetting.\\nC A DDITIONAL STATISTICS\\nSolver Expert Average Number of Mistakes (↓)\\nButton Dog Wire Who LED Memory Keypad Password Color Maze Overall\\nHuman\\nGPT4V 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\\nGPT4o 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 7.00 0.00 0.64\\nInternVL8b 2.50 6.00 0.75 5.00 7.50 12.00 9.00 2.50 0.00 0.50 4.14\\nGPT4V\\nHuman\\n3.00 2.33 2.33 0.33 1.67 1.33 0.00 0.00 0.00 3.00 1.45\\nGPT4o 1.67 3.67 0.00 0.33 2.00 3.67 1.33 0.00 4.00 1.67 1.83\\nInternVL8b 3.00 1.50 2.67 1.00 6.33 6.33 3.33 5.67 0.00 0.00 3.15\\nGPT4V GPT4V 2.30 2.40 0.10 0.60 5.50 1.30 5.40 1.40 0.00 4.70 2.37\\nQwenVL7b InternVL8b 5.50 4.20 5.00 13.40 0.00 6.90 10.80 0.00 0.00 0.00 4.58\\nInternVL GPT4o 3.70 3.80 0.00 7.60 8.40 6.70 16.00 1.60 0.00 0.00 4.78\\nGPT4o GPT4o 4.10 5.10 0.20 2.50 6.80 3.70 12.00 15.20 0.00 0.00 4.96\\nQwenVL7b GPT4o 3.00 3.30 6.55 12.10 9.60 7.00 7.90 0.00 0.00 1.40 5.10\\nInternVL26b InternVL26b 4.10 2.80 2.90 11.50 11.30 8.00 14.00 0.00 0.00 2.40 5.70\\nRandom InternVL 4.17 3.21 2.80 3.48 9.92 14.08 14.68 2.04 0.00 3.96 5.86\\nInternVL8b QwenVL7b 3.30 2.40 14.80 13.50 10.30 8.80 16.30 0.00 0.00 0.00 6.94\\nQwenVL7b QwenVL7b 3.10 4.30 11.80 17.20 11.60 9.10 12.60 0.00 0.00 0.00 6.97\\nQwenVL2b QwenVL2b 2.50 10.60 13.30 15.20 1.90 16.00 16.20 0.00 0.00 1.00 7.67\\nInternVL8b InternVL8b 4.50 3.80 12.60 11.00 13.10 17.20 12.00 12.50 0.00 0.00 8.67\\nTable 2: Average number of mistakes the solver made for various puzzles. We average the mistakes\\nover 10, 3, and 100 independent runs of each puzzle for the AI-AI, AI-Human, and random settings.\\nThe overall column is an average across all the puzzles.\\nSolver Expert Average Conversation Length (↓)\\nButton Dog Wire Who LED Memory Keypad Password Color Maze Overall\\nHuman\\nGPT4V 2.00 2.00 2.67 3.00 3.67 7.67 6.67 6.00 20.00 2.67 5.30\\nGPT4o 2.67 2.00 3.00 6.00 1.33 8.33 10.50 3.00 20.00 2.67 5.79\\nInternVL8b 4.00 7.00 3.00 9.50 18.50 20.00 19.50 13.00 20.00 16.00 12.38\\nGPT4o\\nHuman\\n3.67 10.00 2.33 2.33 17.00 13.33 6.67 11.33 20.00 20.00 10.67\\nGPT4V 8.33 5.67 6.67 2.33 15.67 15.33 2.00 18.00 15.50 20.00 10.79\\nInternVL8b 5.50 4.00 5.67 3.33 14.00 17.00 7.67 11.00 20.00 20.00 10.93\\nGPT4V GPT4V 8.20 5.40 2.20 3.40 15.10 15.80 12.90 15.60 20.00 18.00 11.66\\nRandom InternVL 5.17 4.21 3.80 4.48 12.96 20.00 19.88 20.00 20.00 19.79 12.98\\nGPT4o GPT4o 9.70 8.90 2.20 6.90 20.00 19.00 14.70 20.00 20.00 20.00 14.14\\nInternVL GPT4o 5.20 6.90 2.60 12.30 20.00 17.90 20.00 20.00 20.00 20.00 14.49\\nQwenVL2b QwenVL2b 3.50 11.60 14.30 16.20 4.60 20.00 18.20 20.00 20.00 20.00 14.84\\nInternVL26b InternVL26b 8.10 5.90 5.80 16.40 15.90 18.60 18.70 20.00 20.00 20.00 14.94\\nInternVL8b QwenVL7b 4.30 4.70 16.40 17.00 15.60 20.00 18.40 20.00 20.00 20.00 15.64\\nInternVL8b InternVL8b 5.50 6.30 14.70 13.20 20.00 18.80 20.00 20.00 20.00 19.90 15.84\\nQwenVL7b GPT4o 9.80 9.00 10.45 15.90 20.00 18.10 16.80 20.00 20.00 20.00 15.95\\nQwenVL7b InternVL8b 7.80 6.00 12.50 14.40 20.00 19.00 20.00 20.00 20.00 20.00 15.97\\nQwenVL7b QwenVL7b 8.90 12.10 14.70 18.20 17.30 20.00 18.60 20.00 20.00 20.00 16.98\\nTable 3: Average conversation length for various puzzles. We average the length of agent dialogue\\nover 10, 3, and 100 independent runs of each puzzle for the AI-AI, AI-Human, and random settings.\\nThe overall column is an average dialogue length across all the puzzles.\\nWe report additional metrics recorded during evaluation such as Average Mistakes (Table 2) and\\nConversation Length (Table 3)\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-11T00:24:07+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-11T00:24:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/research_paper.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='Preprint\\nD A GENT PROMPTS\\nSolver Prompt:\\nYou are the solver in a cooperative game involving\\nsolving puzzles. As the solver, you are presented\\nwith an image of the puzzle, along with possible\\nactions you may take. You should only attempt\\nsome actions if you are certain of the solution.\\nOtherwise, you should describe the image and ask\\nthe expert. When asking the expert, keep in mind\\nthe expert cannot see the image. Your description\\nshould be concise but also detailed enough to convey\\nthe details to the expert through text only. Once\\nyou are certain of the solution, respond with just\\nthe name of the action you chose. If in a puzzle\\nyou can take multiple steps to solve it, you could\\noutput a list of action names, separated by the line\\nbreak \\\\n and in the sequential order to be executed.\\nONLY FINISH THE SOLVER’S DIALOGUE.\\nExpert Prompt:\\nYou are the expert in a cooperative game involving\\nsolving puzzles. As the expert, you hold the puzzle\\nsolution manual, containing vital information on\\nvarious modules and their corresponding solution\\nprocedures. Your task is to listen carefully to the\\nsolver’s descriptions of the puzzles and provide\\nclear and accurate instructions to guide them\\nthrough the solution. Be as concise and precise in\\nyour instructions as possible. If the solver does\\nnot provide you with enough information, ask for\\nclarification if needed. ONLY FINISH THE EXPERT’S\\nDIALOGUE.\\n22'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 0, 'page_label': '1'}, page_content='OpenAI, Inc.\\nCompany type Private\\nIndustry Artificial intelligence\\nFounded December 11, 2015\\nHeadquarters San Francisco,\\nCalifornia, U.S.[1]\\nKey people Bret Taylor (chairman)\\nSam Altman (CEO)\\nGreg Brockman\\n(president)\\nSarah Friar (CFO)[2]\\nProducts OpenAI Five\\nGPT-1 · 2 · 3 · 4 · 4o\\nDALL·E\\nOpenAI Codex\\nChatGPT\\nSearchGPT\\nSora\\no1\\nRevenue\\n  US$28 million[3] (2022)\\nNet income\\n  US$−540\\nmillion[3] (2022)\\nNumber of\\nemployees\\nc. 1,700 (2024)[2][4]\\nWebsite openai.com (https://open\\nai.com/)\\nOpenAI\\nOpenAI is an American artificial intelligence (AI)\\nresearch organization founded in December 2015 and\\nheadquartered in San Francisco, California. Its mission\\nis to develop \"safe and beneficial\" artificial general\\nintelligence (AGI), which it defines as \"highly\\nautonomous systems that outperform humans at most\\neconomically valuable work\".[5] As a leading\\norganization in the ongoing AI boom,[6] OpenAI is\\nknown for the GPT family of large language models,\\nthe DALL-E series of text-to-image models, and a text-\\nto-video model named Sora.[7][8] Its release of\\nChatGPT in November 2022 has been credited with\\ncatalyzing widespread interest in generative AI.\\nThe organization consists of the non-profit OpenAI,\\nInc.,[9] registered in Delaware, and its for-profit\\nsubsidiary introduced in 2019, OpenAI Global,\\nLLC.[10] Microsoft owns roughly 49% of OpenAI\\'s\\nequity, having invested US$13 billion.[11] It also\\nprovides computing resources to OpenAI through its\\nMicrosoft Azure cloud platform.[12]\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for\\nalleged copyright infringement against authors and\\nmedia companies whose work was used to train some\\nof OpenAI\\'s products. In November 2023, OpenAI\\'s\\nboard removed Sam Altman as CEO, citing a lack of\\nconfidence in him, and then reinstated him five days\\nlater after negotiations resulting in a reconstructed\\nboard. Many AGI safety researchers left OpenAI in\\n2024.[13]\\nIn December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman,\\nTrevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and\\nWojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. $1 billion in total was pledged by\\nSam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web\\nHistory\\n2015–2018: Non-profit beginnings'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 1, 'page_label': '2'}, page_content='Former headquarters at the Pioneer\\nBuilding in San Francisco\\nServices (AWS), Infosys, and YC Research.[14][15] The actual\\ncollected total amount of contributions was only $130 million\\nuntil 2019.[10] According to an investigation led by TechCrunch,\\nMusk was its largest donor while YC Research did not contribute\\nanything at all.[16] The organization stated it would \"freely\\ncollaborate\" with other institutions and researchers by making its\\npatents and research open to the public.[17][18] OpenAI was\\ninitially run from Brockman\\'s living room.[19] It was later\\nheadquartered at the Pioneer Building in the Mission District, San\\nFrancisco.[20][21]\\nAccording to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning,\\nand drew up a list of the \"best researchers in the field\".[22] Brockman was able to hire nine of them as the\\nfirst employees in December 2015.[22] In 2016, OpenAI paid corporate-level (rather than nonprofit-level)\\nsalaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.[22]\\nMicrosoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback\\nprospect.[22] OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said\\nhe was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a\\nvery large extent, because of its mission.\"[22] Brockman stated that \"the best thing that I could imagine\\ndoing was moving humanity closer to building real AI in a safe way.\"[22] OpenAI co-founder Wojciech\\nZaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to\\njoin OpenAI instead.[22]\\nIn April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning\\nresearch.[23] Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger\\nand more complex AI models with the capability of reducing processing time from six days to two\\nhours.[24][25] In December 2016, OpenAI released \"Universe\", a software platform for measuring and\\ntraining an AI\\'s general intelligence across the world\\'s supply of games, websites, and other\\napplications.[26][27][28][29]\\nIn 2017, OpenAI spent $7.9 million, or a quarter of its functional expenses, on cloud computing alone.[30]\\nIn comparison, DeepMind\\'s total expenses in 2017 were $442 million. In the summer of 2018, simply\\ntraining OpenAI\\'s Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple\\nweeks.\\nIn 2018, Musk resigned from his Board of Directors seat, citing \"a potential future conflict [of interest]\"\\nwith his role as CEO of Tesla due to Tesla\\'s AI development for self-driving cars.[31] Sam Altman claims\\nthat Musk believed that OpenAI had fallen behind other players like Google and Musk proposed instead\\nto take over OpenAI himself, which the board rejected. Musk subsequently left OpenAI.\\nIn February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like\\ntext.[32]\\n2019: Transition from non-profit'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 2, 'page_label': '3'}, page_content='In 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100\\ntimes any investment.[33] According to OpenAI, the capped-profit model allows OpenAI Global, LLC to\\nlegally attract investment from venture funds and, in addition, to grant employees stakes in the\\ncompany.[34] Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock\\noptions that a nonprofit would be unable to.[35] Before the transition, public disclosure of the\\ncompensation of top employees at OpenAI was legally required.[36]\\nThe company then distributed equity to its employees and partnered with Microsoft,[37] announcing an\\ninvestment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-\\nbased supercomputing platform from Microsoft.[38][39][40]\\nOpenAI Global, LLC then announced its intention to commercially license its technologies.[41] It planned\\nto spend the $1 billion \"within five years, and possibly much faster.\"[42] Altman has stated that even a\\nbillion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any\\nnon-profit has ever raised\" to achieve artificial general intelligence.[43]\\nThe transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni\\nof the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult,\\nbut stated \"I disagree with the notion that a nonprofit can\\'t compete\" and pointed to successful low-budget\\nprojects by OpenAI and others. \"If bigger and better funded was always better, then IBM would still be\\nnumber one.\"\\nThe nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite\\nbeing a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.\\'s nonprofit charter. A\\nmajority of OpenAI, Inc.\\'s board is barred from having financial stakes in OpenAI Global, LLC.[34] In\\naddition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to\\nconflict of interest.[35] Some researchers have argued that OpenAI Global, LLC\\'s switch to for-profit\\nstatus is inconsistent with OpenAI\\'s claims to be \"democratizing\" AI.[44]\\nIn 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed\\nat natural language answering questions, but it can also translate between languages and coherently\\ngenerate improvised text. It also announced that an associated API, named simply \"the API\", would form\\nthe heart of its first commercial product.[45]\\nEleven employees left OpenAI, mostly between December 2020 and January 2021, in order to establish\\nAnthropic.[46]\\nIn 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex\\ndigital images from textual descriptions, utilizing a variant of the GPT-3 architecture.[47]\\nIn December 2022, OpenAI received widespread media coverage after launching a free preview of\\nChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a\\nmillion signups within the first five days.[48] According to anonymous sources cited by Reuters in\\nDecember 2022, OpenAI Global, LLC was projecting $200 million of revenue in 2023 and $1 billion in\\nrevenue in 2024.[49]\\n2020–2023: ChatGPT, DALL-E, partnership with Microsoft'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 3, 'page_label': '4'}, page_content='In January 2023, OpenAI Global, LLC was in talks for funding that would value the company at $29\\nbillion, double its 2021 value.[50] On January 23, 2023, Microsoft announced a new US$10 billion\\ninvestment in OpenAI Global, LLC over multiple years, partially needed to use Microsoft\\'s cloud-\\ncomputing service Azure.[51][52] Rumors of this deal suggested that Microsoft may receive 75% of\\nOpenAI\\'s profits until it secures its investment return and a 49% stake in the company.[53] The investment\\nis believed to be a part of Microsoft\\'s efforts to integrate OpenAI\\'s ChatGPT into the Bing search engine.\\nGoogle announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT\\ncould threaten Google\\'s place as a go-to source for information.[54][55]\\nOn February 7, 2023, Microsoft announced that it was building AI technology based on the same\\nfoundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.[56]\\nOn March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of\\ninterest with his investments in AI companies via Greylock Partners, and his co-founding of the AI\\nstartup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.[57]\\nOn March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of\\nChatGPT Plus.[58]\\nOn May 22, 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the\\ngovernance of superintelligence.[59] They consider that superintelligence could happen within the next 10\\nyears, allowing a \"dramatically more prosperous future\" and that \"given the possibility of existential risk,\\nwe can\\'t just be reactive\". They propose creating an international watchdog organization similar to IAEA\\nto oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on\\nthe other side should not be overly regulated. They also call for more technical safety research for\\nsuperintelligences, and ask for more coordination, for example through governments launching a joint\\nproject which \"many current efforts become part of\".[59][60]\\nIn July 2023, OpenAI launched the superalignment project, aiming to find within 4 years how to align\\nfuture superintelligences by automating alignment research using AI.[61]\\nIn August 2023, it was announced that OpenAI had acquired the New York-based start-up Global\\nIllumination, a company that deploys AI to develop digital infrastructure and creative tools.[62]\\nOn September 21, 2023, Microsoft had begun rebranding all variants of its Copilot to Microsoft Copilot,\\nincluding the former Bing Chat and the Microsoft 365 Copilot.[63] This strategy was followed in\\nDecember 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well\\nas a standalone Microsoft Copilot app released for Android[64] and one released for iOS thereafter.[65]\\nIn September 2024, OpenAI\\'s global affairs chief, Anna Makanju, expressed support for the UK\\'s\\napproach to AI regulation during her testimony to the House of Lords Communications and Digital\\nCommittee, stating that the company favors \"smart regulation\" and sees the UK\\'s AI white paper as a\\npositive step towards responsible AI development.[66]\\nIn October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI\\nwould let G42 deploy Open AI technology.[67]'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 4, 'page_label': '5'}, page_content='On November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of\\nChatGPT for specific purposes, further expanding the possibilities of AI applications across various\\nindustries.[68] On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for\\nChatGPT Plus due to high demand.[69] Access for newer subscribers re-opened a month later on\\nDecember 13.[70]\\nOn January 16, 2024, in response to intense scrutiny from regulators around the world, OpenAI\\nannounced the formation of a new Collective Alignment team that would aim to implement ideas from\\nthe public for ensuring its models would \"align to the values of humanity.\" The move was from its public\\nprogram launched in May 2023. The company explained that the program would be separate from its\\ncommercial endeavors.[71] On January 18, 2024, OpenAI announced a partnership with Arizona State\\nUniversity that would give it complete access to ChatGPT Enterprise. ASU plans to incorporate the\\ntechnology into various aspects of its operations, including courses, tutoring and research. It is OpenAI\\'s\\nfirst partnership with an educational institution.[72]\\nIn February 2024, the U.S. Securities and Exchange Commission was reportedly investigating OpenAI\\nover whether internal company communications made by Altman were used to mislead investors; and an\\ninvestigation of Altman\\'s statements, opened by the Southern New York U.S. Attorney\\'s Office the\\nprevious November, was ongoing.[73][74]\\nOn February 15, 2024, OpenAI announced a text-to-video model named Sora, which it plans to release to\\nthe public at an unspecified date.[75] It is currently available for red teams for managing critical harms\\nand risks.[76]\\nOn February 29, 2024, OpenAI and CEO Sam Altman were sued by Elon Musk, who accused them of\\nprioritizing profits over public good, contrary to OpenAI\\'s original mission[10] of developing AI for\\nhumanity\\'s benefit.[77] The lawsuit cited OpenAI\\'s policy shift after partnering with Microsoft,\\nquestioning its open-source commitment and stirring the AI ethics-vs.-profit debate.[78] In a blog post,\\nOpenAI stated that \"Elon understood the mission did not imply open-sourcing AGI.\"[79] In a staff memo,\\nthey also denied being a de facto Microsoft subsidiary.[80]\\nOn March 11, 2024, court filing, OpenAI said it was \"doing just fine without Elon Musk\" after he left the\\ncompany in 2018. They also responded to Musk\\'s lawsuit, calling the billionaire\\'s claims \"incoherent\",\\n\"frivolous\", \"extraordinary\" and \"a fiction\".[81] On June 11, 2024, Musk unexpectedly withdrew the\\nlawsuit.[82] On August 5, 2024, Musk reopened the lawsuit against Altman and others, alleging that\\nAltman claimed that OpenAI was going to be founded as a non-profit organization.[83][84]\\nOn May 15, 2024, Ilya Sutskever resigned from OpenAI and was replaced with Jakub Pachocki to be the\\nChief Scientist.[85] Hours later, Jan Leike, the other co-leader of the superalignment team, announced his\\ndeparture, citing an erosion of safety and trust in OpenAI\\'s leadership.[86] Their departures along with\\nseveral researchers leaving the group, led OpenAI to absorb the team\\'s work into other research areas,\\nand officially shut down the superalignment group.[87] According to sources interviewed by Fortune,\\nOpenAI\\'s promise of allocating 20% of its computing capabilities to the superalignment project had not\\nbeen fulfilled.[88]\\n2024–present: Public/non-profit efforts, Sora, partnership with Apple'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 5, 'page_label': '6'}, page_content='On May 19, 2024, Reddit and OpenAI announced a partnership to integrate Reddit\\'s content into OpenAI\\nproducts, including ChatGPT. This collaboration allows OpenAI to access Reddit\\'s Data API, providing\\nreal-time, structured content to enhance AI tools and user engagement with Reddit communities.\\nAdditionally, Reddit plans to develop new AI-powered features for users and moderators using OpenAI\\'s\\nplatform. The partnership aligns with Reddit\\'s commitment to privacy, adhering to its Public Content\\nPolicy and existing Data API Terms, which restrict commercial use without approval. OpenAI will also\\nserve as a Reddit advertising partner.[89]\\nOn May 22, 2024, OpenAI entered into an agreement with News Corp to integrate news content from The\\nWall Street Journal, the New York Post, The Times, and The Sunday Times into its AI platform.\\nMeanwhile, other publications like The New York Times chose to sue OpenAI and Microsoft for copyright\\ninfringement over the use of their content to train AI models.[90]\\nOn May 29, 2024, Axios reported that OpenAI had signed deals with Vox Media and The Atlantic to share\\ncontent to enhance the accuracy of AI models like ChatGPT by incorporating reliable news sources,\\naddressing concerns about AI misinformation.[91] Concerns were expressed about the decision by\\njournalists, including those working for the publications, as well as the publications\\' unions. The Vox\\nUnion stated, \"As both journalists and workers, we have serious concerns about this partnership, which\\nwe believe could adversely impact members of our union, not to mention the well-documented ethical\\nand environmental concerns surrounding the use of generative AI.\"[92]\\nA group of nine current and former OpenAI employees has accused the company of prioritizing profits\\nover safety, using restrictive agreements to silence concerns, and moving too quickly with inadequate risk\\nmanagement. They call for greater transparency, whistleblower protections, and legislative regulation of\\nAI development.[93]\\nOn June 10, 2024, it was announced at WWDC 2024 that OpenAI had partnered with Apple Inc. to bring\\nChatGPT features to Apple Intelligence and iPhone.[94]\\nOn June 13, 2024, OpenAI announced that Paul Nakasone, the former head of the NSA was joining the\\ncompany\\'s board of directors. Nakasone also joined the company\\'s security subcommittee.[95]\\nOn June 24, 2024, OpenAI acquired Multi, a startup running a collaboration platform based on Zoom.[96]\\nIn July 2024, Reuters reported that OpenAI was working on a project code-named \"Strawberry\"\\n(previously known as Q*) aiming to enhance AI reasoning capabilities. The project reportedly seeks to\\nenable AI to plan ahead, navigate the internet autonomously, and conduct \"deep research\".[97][98] The\\nproject was officially released on September 12 and named o1.[99]\\nOn August 5, TechCrunch reported that OpenAI\\'s cofounder John Schulman has left the company to join\\nrival AI startup Anthropic. Schulman cited a desire to focus more deeply on AI alignment research as his\\nreason for the move. Additionally, OpenAI\\'s president and co-founder, Greg Brockman, is taking an\\nextended leave until the end of the year.[100]\\nOn September 25, OpenAI\\'s Chief Technology Officer (CTO) Mira Murati announced her departure from\\nthe company to \"create the time and space to do my own exploration\".[101] It had previously been\\nreported Murati was among those who expressed concerns to the Board about Altman.[102]'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 6, 'page_label': '7'}, page_content=\"Co-founder and CEO of\\nOpenAI, Sam Altman\\nCo-founder and President of\\nOpenAI, Greg Brockman\\nIn October 2024, OpenAI raised $6.6 billion from investors, potentially valuing the company at $157\\nbillion and solidifying its status as one of the most valuable private firms globally. The funding attracted\\nreturning venture capital firms like Thrive Capital and Khosla Ventures, along with major backer\\nMicrosoft and new investors Nvidia and Softbank.[103] OpenAI's CFO, Sarah Friar, informed employees\\nthat a tender offer for share buybacks would follow the funding, although specifics were yet to be\\ndetermined. Thrive Capital invested around $1.2 billion, with the option for an additional $1 billion if\\nrevenue goals were met. Apple, despite initial interest, did not participate in this funding round.[104]\\nCEO and co-founder: Sam Altman,\\nformer president of the startup\\naccelerator Y Combinator\\nPresident and co-founder: Greg\\nBrockman, former CTO, 3rd employee\\nof Stripe[105]\\nChief Scientist Officer: Jakub Pachocki,\\nformer Director of Research at\\nOpenAI[85]\\nChief Operating Officer: Brad Lightcap,\\npreviously at Y Combinator and\\nJPMorgan Chase[106]\\nChief Financial Officer: Sarah Friar, former Nextdoor CEO and former CFO at Block, Inc.[107]\\nChief Product Officer: Kevin Weil, previously at Twitter, Inc. and Meta Platforms[107]\\nBret Taylor (chairman)\\nSam Altman\\nLawrence Summers\\nAdam D'Angelo\\nSue Desmond-Hellmann\\nNicole Seligman\\nFidji Simo\\nPaul Nakasone,[108] Former Director of the National Security Agency (2018-2024)\\nZico Kolter[109]\\nSources:[10][110]\\nReid Hoffman, LinkedIn co-founder[111]\\nPeter Thiel, PayPal co-founder[111]\\nParticipants\\nKey employees\\nBoard of directors of the OpenAI nonprofit\\nPrincipal individual investors[105]\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 7, 'page_label': '8'}, page_content='Jessica Livingston, a founding partner of Y Combinator\\nElon Musk, co-founder\\nMicrosoft[112]\\nKhosla Ventures[113]\\nInfosys[114]\\nThrive Capital[115]\\nY Combinator[116]\\nNvidia[117]\\nSoftbank[117]\\nMGX (United Arab Emirates)[117]\\nSome scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced\\nAI gains the ability to redesign itself at an ever-increasing rate, an unstoppable \"intelligence explosion\"\\ncould lead to human extinction. Co-founder Musk characterizes AI as humanity\\'s \"biggest existential\\nthreat\".[118]\\nMusk and Altman have stated they are partly motivated by concerns about AI safety and the existential\\nrisk from artificial general intelligence.[119][120] OpenAI states that \"it\\'s hard to fathom how much\\nhuman-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could\\ndamage society if built or used incorrectly\".[18] Research on safety cannot safely be postponed: \"because\\nof AI\\'s surprising history, it\\'s hard to predict when human-level AI might come within reach.\"[121]\\nOpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as\\nbroadly and evenly distributed as possible.\"[18] Co-chair Sam Altman expects the decades-long project to\\nsurpass human intelligence.[122]\\nVishal Sikka, former CEO of Infosys, stated that an \"openness\", where the endeavor would \"produce\\nresults generally in the greater interest of humanity\", was a fundamental requirement for his support; and\\nthat OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful\\nwork\".[123] Cade Metz of Wired suggested that corporations such as Amazon might be motivated by a\\ndesire to use open-source software and data to level the playing field against corporations such as Google\\nand Facebook, which own enormous supplies of proprietary data. Altman stated that Y Combinator\\ncompanies would share their data with OpenAI.[122]\\nIn the early years before his 2018 departure, Musk posed the question: \"What is the best thing we can do\\nto ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or\\nwe could participate with the right structure with people who care deeply about developing AI in a way\\nthat is safe and is beneficial to humanity.\" He acknowledged that \"there is always some risk that in\\nactually trying to advance (friendly) AI we may create the thing we are concerned about\"; but\\nCorporate investors\\nMotives\\nStrategy'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 8, 'page_label': '9'}, page_content='nonetheless, that the best defense was \"to empower as many people as possible to have AI. If everyone\\nhas AI powers, then there\\'s not any one person or a small set of individuals who can have AI\\nsuperpower.\"[105]\\nMusk and Altman\\'s counterintuitive strategy—that of trying to reduce of harm from AI by giving\\neveryone access to it—is controversial among those concerned with existential risk from AI. Philosopher\\nNick Bostrom said, \"If you have a button that could do bad things to the world, you don\\'t want to give it\\nto everyone.\"[120] During a 2016 conversation about technological singularity, Altman said, \"We don\\'t\\nplan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect\\nrepresentatives to a new governance board\". Greg Brockman stated, \"Our goal right now... is to do the\\nbest thing there is to do. It\\'s a little vague.\"[124]\\nConversely, OpenAI\\'s initial decision to withhold GPT-2 around 2019, due to a wish to \"err on the side of\\ncaution\" in the presence of potential misuse, was criticized by advocates of openness. Delip Rao, an\\nexpert in text generation, stated, \"I don\\'t think [OpenAI] spent enough time proving [GPT-2] was actually\\ndangerous.\" Other critics argued that open publication was necessary to replicate the research and to\\ncreate countermeasures.[125]\\nMore recently, in 2022, OpenAI published its approach to the alignment problem, anticipating that\\naligning AGI to human values would likely be harder than aligning current AI systems: \"Unaligned AGI\\ncould pose substantial risks to humanity[,] and solving the AGI alignment problem could be so difficult\\nthat it will require all of humanity to work together\". They stated that they intended to explore how to\\nbetter use human feedback to train AI systems, and how to safely use AI to incrementally automate\\nalignment research.[126] Some observers believe the company\\'s November 2023 reorganization—\\nincluding Altman\\'s return as CEO, and the changes to its board of directors—indicated a probable shift\\ntowards a business focus and reduced influence of \"cautious people\" at OpenAI.[127]\\nAt its beginning, OpenAI\\'s research included many projects focused on reinforcement learning (RL).[128]\\nOpenAI has been viewed as an important competitor to DeepMind.[129]\\nAnnounced in 2016, Gym is an open-source Python library designed to facilitate the development of\\nreinforcement learning algorithms. It aimed to standardize how environments are defined in AI research,\\nmaking published research more easily reproducible[23][130] while providing users with a simple interface\\nfor interacting with these environments. In 2022, new developments of Gym have been moved to the\\nlibrary Gymnasium.[131][132]\\nProducts and applications\\nReinforcement learning\\nGym\\nGym Retro'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 9, 'page_label': '10'}, page_content='Released in 2018, Gym Retro is a platform for reinforcement learning (RL) research on video games,[133]\\nusing RL algorithms and study generalization. Prior RL research focused mainly on optimizing agents to\\nsolve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but\\ndifferent appearances.\\nReleased in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack\\nknowledge of how to even walk, but are given the goals of learning to move and to push the opposing\\nagent out of the ring.[134] Through this adversarial learning process, the agents learn how to adapt to\\nchanging conditions. When an agent is then removed from this virtual environment and placed in a new\\nvirtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to\\nbalance in a generalized way.[134][135] OpenAI\\'s Igor Mordatch argued that competition between agents\\ncould create an intelligence \"arms race\" that could increase an agent\\'s ability to function even outside the\\ncontext of the competition.[134]\\nOpenAI Five is a team of five OpenAI-curated bots used in the competitive five-on-five video game Dota\\n2, that learn to play against human players at a high skill level entirely through trial-and-error algorithms.\\nBefore becoming a team of five, the first public demonstration occurred at The International 2017, the\\nannual premiere championship tournament for the game, where Dendi, a professional Ukrainian player,\\nlost against a bot in a live one-on-one matchup.[136][137] After the match, CTO Greg Brockman explained\\nthat the bot had learned by playing against itself for two weeks of real time, and that the learning software\\nwas a step in the direction of creating software that can handle complex tasks like a surgeon.[138][139] The\\nsystem uses a form of reinforcement learning, as the bots learn over time by playing against themselves\\nhundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking\\nmap objectives.[140][141][142]\\nBy June 2018, the ability of the bots expanded to play together as a full team of five, and they were able\\nto defeat teams of amateur and semi-professional players.[143][140][144][145] At The International 2018,\\nOpenAI Five played in two exhibition matches against professional players, but ended up losing both\\ngames.[146][147][148] In April 2019, OpenAI Five defeated OG, the reigning world champions of the game\\nat the time, 2:0 in a live exhibition match in San Francisco.[149][150] The bots\\' final public appearance\\ncame later that month, where they played in 42,729 total games in a four-day open online competition,\\nwinning 99.4% of those games.[151]\\nOpenAI Five\\'s mechanisms in Dota 2\\'s bot player shows the challenges of AI systems in multiplayer\\nonline battle arena (MOBA) games and how OpenAI Five has demonstrated the use of deep\\nreinforcement learning (DRL) agents to achieve superhuman competence in Dota 2 matches.[152]\\nDeveloped in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to\\nmanipulate physical objects.[153] It learns entirely in simulation using the same RL algorithms and\\ntraining code as OpenAI Five. OpenAI tackled the object orientation problem by using domain\\nrandomization, a simulation approach which exposes the learner to a variety of experiences rather than\\nRoboSumo\\nOpenAI Five\\nDactyl'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 10, 'page_label': '11'}, page_content='The original GPT model\\nAn instance of GPT-2 writing a\\nparagraph based on a prompt from\\nits own Wikipedia article in February\\n2021\\ntrying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB\\ncameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that\\nthe system was able to manipulate a cube and an octagonal prism.[154]\\nIn 2019, OpenAI demonstrated that Dactyl could solve a Rubik\\'s Cube. The robot was able to solve the\\npuzzle 60% of the time. Objects like the Rubik\\'s Cube introduce complex physics that is harder to model.\\nOpenAI did this by improving the robustness of Dactyl to perturbations by using Automatic Domain\\nRandomization (ADR), a simulation approach of generating progressively more difficult environments.\\nADR differs from manual domain randomization by not needing a human to specify randomization\\nranges.[155]\\nIn June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models\\ndeveloped by OpenAI\" to let developers call on it for \"any English language AI task\".[156][157]\\nThe company has popularized generative pretrained transformers (GPT).[158]\\nThe original paper on generative pre-training of a transformer-\\nbased language model was written by Alec Radford and his\\ncolleagues, and published in preprint on OpenAI\\'s website on June\\n11, 2018.[159] It showed how a generative model of language\\ncould acquire world knowledge and process long-range\\ndependencies by pre-training on a diverse corpus with long\\nstretches of contiguous text.\\nGenerative Pre-trained Transformer 2 (\"GPT-2\") is an\\nunsupervised transformer language model and the successor to\\nOpenAI\\'s original GPT model (\"GPT-1\"). GPT-2 was announced\\nin February 2019, with only limited demonstrative versions\\ninitially released to the public. The full version of GPT-2 was not\\nimmediately released due to concern about potential misuse,\\nincluding applications for writing fake news.[160] Some experts\\nexpressed skepticism that GPT-2 posed a significant threat.\\nIn response to GPT-2, the Allen Institute for Artificial Intelligence\\nresponded with a tool to detect \"neural fake news\".[161] Other\\nresearchers, such as Jeremy Howard, warned of \"the technology to\\ntotally fill Twitter, email, and the web up with reasonable-\\nsounding, context-appropriate prose, which would drown out all\\nother speech and be impossible to filter\".[162] In November 2019,\\nAPI\\nText generation\\nOpenAI\\'s original GPT model (\"GPT-1\")\\nGPT-2'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 11, 'page_label': '12'}, page_content='OpenAI released the complete version of the GPT-2 language model.[163] Several websites host\\ninteractive demonstrations of different instances of GPT-2 and other transformer models.[164][165][166]\\nGPT-2\\'s authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2\\nachieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further\\ntrained on any task-specific input-output examples).\\nThe corpus it was trained on, called WebText, contains slightly 40 gigabytes of text from URLs shared in\\nReddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens\\nby using byte pair encoding. This permits representing any string of characters by encoding both\\nindividual characters and multiple-character tokens.[167]\\nFirst described in May 2020, Generative Pre-trained[a] Transformer 3 (GPT-3) is an unsupervised\\ntransformer language model and the successor to GPT-2.[168][169][170] OpenAI stated that the full version\\nof GPT-3 contained 175 billion parameters,[170] two orders of magnitude larger than the 1.5 billion[171] in\\nthe full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also\\ntrained).[172]\\nOpenAI stated that GPT-3 succeeded at certain \"meta-learning\" tasks and could generalize the purpose of\\na single input-output pair. The GPT-3 release paper gave examples of translation and cross-linguistic\\ntransfer learning between English and Romanian, and between English and German.[170]\\nGPT-3 dramatically improved benchmark results over GPT-2. OpenAI cautioned that such scaling-up of\\nlanguage models could be approaching or encountering the fundamental capability limitations of\\npredictive language models.[173] Pre-training GPT-3 required several thousand petaflop/s-days[b] of\\ncompute, compared to tens of petaflop/s-days for the full GPT-2 model.[170] Like its predecessor,[160] the\\nGPT-3 trained model was not immediately released to the public for concerns of possible abuse, although\\nOpenAI planned to allow access through a paid cloud API after a two-month free private beta that began\\nin June 2020.[156][175]\\nOn September 23, 2020, GPT-3 was licensed exclusively to Microsoft.[176][177]\\nAnnounced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from\\n54 million GitHub repositories,[178][179] and is the AI powering the code autocompletion tool GitHub\\nCopilot.[179] In August 2021, an API was released in private beta.[180] According to OpenAI, the model\\ncan create working code in over a dozen programming languages, most effectively in Python.[178]\\nSeveral issues with glitches, design flaws and security vulnerabilities were cited.[181][182]\\nGitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.[183]\\nOpenAI announced that they would discontinue support for Codex API on March 23, 2023.[184]\\nGPT-3\\nCodex\\nGPT-4'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 12, 'page_label': '13'}, page_content='On March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4),\\ncapable of accepting text or image inputs.[185] They announced that the updated technology passed a\\nsimulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5\\nscored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000\\nwords of text, and write code in all major programming languages.[186]\\nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-\\n3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions.[187]\\nGPT-4 is also capable of taking images as input on ChatGPT.[188] OpenAI has declined to reveal various\\ntechnical details and statistics about GPT-4, such as the precise size of the model.[189]\\nOn May 13, 2024, OpenAI announced and released GPT-4o, which can process and generate text, images\\nand audio.[190] GPT-4o achieved state-of-the-art results in voice, multilingual, and vision benchmarks,\\nsetting new records in audio speech recognition and translation.[191][192] It scored 88.7% on the Massive\\nMultitask Language Understanding (MMLU) benchmark compared to 86.5% by GPT-4.[193]\\nOn July 18, 2024, OpenAI released GPT-4o mini, a smaller version of GPT-4o replacing GPT-3.5 Turbo\\non the ChatGPT interface. Its API costs $0.15 per million input tokens and $0.60 per million output\\ntokens, compared to $5 and $15 respectively for GPT-4o. OpenAI expects it to be particularly useful for\\nenterprises, startups and developers seeking to automate services with AI agents.[194]\\nOn September 12, 2024, OpenAI released the o1-preview and o1-mini models, which have been designed\\nto take more time to think about their responses, leading to higher accuracy. These models are particularly\\neffective in science, coding, and reasoning tasks, and were made available to ChatGPT Plus and Team\\nmembers.[195][196]\\nRevealed in 2021, CLIP (Contrastive Language–Image Pre-training) is a model that is trained to analyze\\nthe semantic similarity between text and images. It can notably be used for image classification.[197]\\nRevealed in 2021, DALL-E is a Transformer model that creates images from textual descriptions.[198]\\nDALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a\\ngreen leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate\\nGPT-4o\\no1\\nImage classification\\nCLIP\\nText-to-image\\nDALL-E'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 13, 'page_label': '14'}, page_content='Images produced by DALL-E when given the text\\nprompt \"a professional high-quality illustration of a\\ngiraffe dragon chimera. a giraffe imitating a\\ndragon. a giraffe made of dragon.\"\\ncorresponding images. It can create images of realistic\\nobjects (\"a stained-glass window with an image of a\\nblue strawberry\") as well as objects that do not exist\\nin reality (\"a cube with the texture of a porcupine\").\\nAs of March 2021, no API or code is available.\\nIn April 2022, OpenAI announced DALL-E 2, an\\nupdated version of the model with more realistic\\nresults.[199] In December 2022, OpenAI published on\\nGitHub software for Point-E, a new rudimentary\\nsystem for converting a text description into a 3-\\ndimensional model.[200]\\nIn September 2023, OpenAI announced DALL-E 3, a\\nmore powerful model better able to generate images\\nfrom complex descriptions without manual prompt\\nengineering and render complex details like hands\\nand text.[201] It was released to the public as a\\nChatGPT Plus feature in October.[202]\\nSora is a text-to-video model that can generate videos based on short descriptive prompts[203] as well as\\nextend existing videos forwards or backwards in time.[204] It can generate videos with resolution up to\\n1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\\nSora\\'s development team named it after the Japanese word for \"sky\", to signify its \"limitless creative\\npotential\".[203] Sora\\'s technology is an adaptation of the technology behind the DALL·E 3 text-to-image\\nmodel.[205] OpenAI trained the system using publicly-available videos as well as copyrighted videos\\nlicensed for that purpose, but did not reveal the number or the exact sources of the videos.[203]\\nOpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024,\\nstating that it could generate videos up to one minute long. It also shared a technical report highlighting\\nthe methods used to train the model, and the model\\'s capabilities.[205] It acknowledged some of its\\nshortcomings, including struggles simulating complex physics.[206] Will Douglas Heaven of the MIT\\nTechnology Review called the demonstration videos \"impressive\", but noted that they must have been\\ncherry-picked and might not represent Sora\\'s typical output.[205]\\nDespite skepticism from some academic leaders following Sora\\'s public demo, notable entertainment-\\nindustry figures have shown significant interest in the technology\\'s potential. In an interview,\\nactor/filmmaker Tyler Perry expressed his astonishment at the technology\\'s ability to generate realistic\\nDALL-E 2\\nDALL-E 3\\nText-to-video\\nSora'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 14, 'page_label': '15'}, page_content='video from text descriptions, citing its potential to revolutionize storytelling and content creation. He said\\nthat his excitement about Sora\\'s possibilities was so strong that he had decided to pause plans for\\nexpanding his Atlanta-based movie studio.[207]\\nReleased in 2022, Whisper is a general-purpose speech recognition model.[208] It is trained on a large\\ndataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition\\nas well as speech translation and language identification.[209]\\nReleased in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI\\nmusic files. It can generate songs with 10 instruments in 15 styles. According to The Verge, a song\\ngenerated by MuseNet tends to start reasonably but then fall into chaos the longer it plays.[210][211] In pop\\nculture, initial applications of this tool were used as early as 2020 for the internet psychological thriller\\nBen Drowned to create music for the titular character.[212][213]\\nReleased in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on\\n1.2 million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples.\\nOpenAI stated the songs \"show local musical coherence [and] follow traditional chord patterns\" but\\nacknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and\\nthat \"there is a significant gap\" between Jukebox and human-generated music. The Verge stated \"It\\'s\\ntechnologically impressive, even if the results sound like mushy versions of songs that might feel\\nfamiliar\", while Business Insider stated \"surprisingly, some of the resulting songs are catchy and sound\\nlegitimate\".[214][215][216]\\nIn 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a\\nhuman judge. The purpose is to research whether such an approach may assist in auditing AI decisions\\nand in developing explainable AI.[217][218]\\nReleased in 2020, Microscope[219] is a collection of visualizations of every significant layer and neuron\\nof eight neural network models which are often studied in interpretability.[220] Microscope was created to\\nanalyze the features that form inside these neural networks easily. The models included are AlexNet,\\nSpeech-to-text\\nWhisper\\nMusic generation\\nMuseNet\\nJukebox\\nUser interfaces\\nDebate Game\\nMicroscope'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 15, 'page_label': '16'}, page_content='VGG-19, different versions of Inception, and different versions of CLIP Resnet.[221]\\nLaunched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that\\nprovides a conversational interface that allows users to ask questions in natural language. The system\\nthen responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its\\nlaunch.[222][223]\\nAs of 2023, ChatGPT Plus is a GPT-4 backed version of ChatGPT[224] available for a US$20 per month\\nsubscription fee[225] (the original version is backed by GPT-3.5).[226] OpenAI also makes GPT-4\\navailable to a select group of applicants through their GPT-4 API waitlist;[227] after being accepted, an\\nadditional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and\\nUS$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of\\nthe model with an 8192-token context window; for the 32768-token context window, the prices are\\ndoubled.[228]\\nIn May 2023, OpenAI launched a user interface for ChatGPT for the App Store on iOS and later in July\\n2023 for the Play Store on Android.[229] The app supports chat history syncing and voice input (using\\nWhisper, OpenAI\\'s speech recognition model).[230][229][231] In September 2023, OpenAI announced that\\nChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users\\ncan talk to the chatbot.[232][233]\\nIn October 2023, OpenAI\\'s latest image generation model, DALL-E 3, was integrated into ChatGPT Plus\\nand ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by\\nconversation with users.[234][235]\\nOpenAI\\'s GPT Store, initially slated for a 2023 launch, is now deferred to an undisclosed date in early\\n2024, attributed likely to the leadership changes in November following the initial announcement.[236]\\nConcerns about the energy consumption of generative AI, including ChatGPT, are rising. In September\\n2024, Microsoft entered a deal with Constellation Energy to reopen the Three Mile Island nuclear plant to\\nsupply power to its AI-driven data centers.[237]\\nSearchGPT, a prototype search engine developed by OpenAI, was unveiled on July 25, 2024, with an\\ninitial limited release to 10,000 test users. It combines traditional search engine features with generative\\nAI capabilities.[238][239]\\nStargate is a potential artificial intelligence supercomputer in development by Microsoft and OpenAI.[240]\\nStargate is designed as part of a greater data center project, which could represent an investment of as\\nmuch as $100 billion by Microsoft.[241]\\nStargate is reported to be part of a series of AI-related construction projects planned in the next few years\\nby the companies Microsoft and OpenAI.[241] The supercomputers will be constructed in five phases.[240]\\nThe fourth phase should consist in a smaller OpenAI supercomputer, planned to launch around 2026.[240]\\nChatGPT\\nSearchGPT\\nStargate and other supercomputers'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 16, 'page_label': '17'}, page_content='Stargate is the fifth and final phase of the program, and will take five and six years to complete and is\\nslated to launch around 2028.[241]\\nThe artificial intelligence of Stargate is slated to be contained on millions of special server chips.[241] The\\nsupercomputer\\'s data center will be built in the US across 700 acres of land.[241] It has a planned power\\nconsumption of 5 gigawatts, for which it could rely on nuclear energy.[241] The name \"Stargate\" is a\\nhomage to the 1994 sci-fi film Stargate.[241]\\nOn November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of\\nHelen Toner, Ilya Sutskever, Adam D\\'Angelo and Tasha McCauley) cited a lack of confidence in him.\\nChief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of\\nOpenAI, was also removed as chairman of the board[242][243] and resigned from the company\\'s\\npresidency shortly thereafter.[244] Three senior OpenAI researchers subsequently resigned: director of\\nresearch and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher Szymon\\nSidor.[245][246]\\nOn November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed\\nupon the board by investors such as Microsoft and Thrive Capital, who objected to Altman\\'s\\ndeparture.[247] Although Altman himself spoke in favor of returning to OpenAI, he has since stated that\\nhe considered starting a new company and bringing former OpenAI employees with him if talks to\\nreinstate him didn\\'t work out.[248] The board members agreed \"in principle\" to resign if Altman\\nreturned.[249] On November 19, 2023, negotiations with Altman to return failed and Murati was replaced\\nby Emmett Shear as interim CEO.[250] The board initially contacted Anthropic CEO Dario Amodei (a\\nformer OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but\\nboth offers were declined.[251]\\nOn November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be\\njoining Microsoft to lead a new advanced AI research team, but added that they were still committed to\\nOpenAI despite recent events.[252] Before the partnership with Microsoft was finalized, Altman gave the\\nboard another opportunity to negotiate with him.[253] About 738 of OpenAI\\'s 770 employees, including\\nMurati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the\\nboard did not rehire Altman and then resign.[254][255] This prompted OpenAI investors to consider legal\\naction against the board as well.[256] In response, OpenAI management sent an internal memo to\\nemployees stating that negotiations with Altman and the board had resumed and would take some\\ntime.[257]\\nOn November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in\\ntheir prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and\\nLawrence Summers, with D\\'Angelo remaining.[258] On November 22, 2023, emerging reports suggested\\nthat Sam Altman\\'s dismissal from OpenAI may have been linked to his alleged mishandling of a\\nsignificant breakthrough in the organization\\'s secretive project codenamed Q*. According to sources\\nwithin OpenAI, Q* is aimed at developing AI capabilities in logical and mathematical reasoning, and\\nControversies\\nFiring of Altman'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 17, 'page_label': '18'}, page_content='reportedly involves performing math on the level of grade-school students.[259][260][261] Concerns about\\nAltman\\'s response to this development, specifically regarding the discovery\\'s potential safety\\nimplications, were reportedly raised with the company\\'s board shortly before Altman\\'s firing.[262] On\\nNovember 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as\\na non-voting member to observe the company\\'s operations;[263] Microsoft resigned from the board in July\\n2024.[264]\\nIn January 2023, OpenAI has been criticized for outsourcing the annotation of data sets to Sama, a\\ncompany based in San Francisco that employed workers in Kenya. These annotations were used to train\\nan AI model to detect toxicity, which could then be used to moderate toxic content, notably from\\nChatGPT\\'s training data and outputs. However, these pieces of text usually contained detailed\\ndescriptions of various types of violence, including sexual violence. The investigation uncovered that\\nOpenAI began sending snippets of data to Sama as early as November 2021. The four Sama employees\\ninterviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of\\nwork, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its\\nannotators. Sama\\'s spokesperson said that the $12.50 was also covering other implicit costs, among which\\nwere infrastructure expenses, quality assurance and management.[265]\\nIn March 2023, the company was also criticized for disclosing particularly few technical details about\\nproducts like GPT-4, contradicting its initial commitment to openness and making it harder for\\nindependent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and\\nsafety concerns to justify this strategic turn. OpenAI\\'s former chief scientist Ilya Sutskever argued in\\n2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons\\nfor not open-sourcing the most potent AI models would become \"obvious\" in a few years.[266]\\nOn May 17, 2024, a Vox article reported that OpenAI was asking departing employees to sign a lifelong\\nnon-disparagement agreement forbidding them from criticizing OpenAI or acknowledging the existence\\nof the agreement. Daniel Kokotajlo, a former employee, publicly stated that he forfeited his vested equity\\nin OpenAI in order to leave without signing the agreement.[267][268] Sam Altman stated that he was\\nunaware of the equity cancellation provision, and that OpenAI never enforced it to cancel any employee\\'s\\nvested equity.[269] Vox published leaked documents and emails challenging this claim.[270] On May 23,\\n2024, OpenAI sent a memo releasing former employees from the agreement.[271]\\nOpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul\\nTremblay and Mona Awad in July 2023.[272][273][274] In September 2023, 17 authors, including George\\nR. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class\\naction lawsuit against OpenAI, alleging that the company\\'s technology was illegally using their\\ncopyrighted work.[275][276] The New York Times also sued the company in late December 2023.[273][277]\\nContent moderation contract with Sama\\nLack of technological transparency\\nNon-disparagement agreement\\nCopyright infringement in training data'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 18, 'page_label': '19'}, page_content='In May 2024 it was revealed that OpenAI had destroyed its Books1 and Books2 training datasets, which\\nwere used in the training of GPT-3, and which the Authors Guild believed to have contained over 100,000\\ncopyrighted books.[278]\\nIn 2021, OpenAI developed a speech recognition tool called Whisper. OpenAI used it to transcribe more\\nthan one million hours of YouTube videos into text for training GPT-4. The automated transcription of\\nYouTube videos raised concerns within OpenAI employees regarding potential violations of YouTube\\'s\\nterms of service, which prohibit the use of videos for applications independent of the platform, as well as\\nany type of automated access to its videos. Despite these concerns, the project proceeded with notable\\ninvolvement from OpenAI\\'s president, Greg Brockman. The resulting dataset proved instrumental in\\ntraining GPT-4.[279]\\nIn February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against\\nOpenAI on copyright litigation ground.[280][281] The lawsuit is said to have charted a new legal strategy\\nfor digital-only publishers to sue OpenAI.[282]\\nOn April 30, 2024, eight newspapers filed a lawsuit in the Southern District of New York against OpenAI\\nand Microsoft, claiming illegal harvesting of their copyrighted articles. The suing publications included\\nThe Mercury News, The Denver Post, The Orange County Register, St. Paul Pioneer Press, Chicago\\nTribune, Orlando Sentinel, Sun Sentinel, and New York Daily News.[283]\\nIn April 2023, the EU\\'s European Data Protection Board (EDPB) formed a dedicated task force on\\nChatGPT \"to foster cooperation and to exchange information on possible enforcement actions conducted\\nby data protection authorities\" based on the \"enforcement action undertaken by the Italian data protection\\nauthority against Open AI about the Chat GPT service\".[284]\\nIn late April 2024 NOYB filed a complaint with the Austrian Datenschutzbehörde against OpenAI for\\nviolating the European General Data Protection Regulation. A text created with ChatGPT gave a false\\ndate of birth for a living person without giving the individual the option to see the personal data used in\\nthe process. A request to correct the mistake was denied. Additionally, neither the recipients of ChatGPT\\'s\\nwork nor the sources used, could be made available, OpenAI claimed.[285]\\nOpenAI quietly deleted its ban on using ChatGPT for \"military and warfare\". Up until January 10, 2024,\\nits \"usage policies\" included a ban on \"activity that has high risk of physical harm, including,\"\\nspecifically, \"weapons development\" and \"military and warfare.\" Its new policies prohibit \"[using] our\\nservice to harm yourself or others\" and to \"develop or use weapons\".[286][287] As one of the industry\\ncollaborators, OpenAI provides LLM to the Artificial Intelligence Cyber Challenge (AIxCC) sponsored\\nby Defense Advanced Research Projects Agency (DARPA) and Advanced Research Projects Agency for\\nHealth to protect software critical to Americans.[288]\\nGDPR compliance\\nRemoval of military and warfare clause\\nUse in state-backed influence operations'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 19, 'page_label': '20'}, page_content='In May 2024, OpenAI removed accounts used by state-backed influence operations such as China\\'s\\nSpamouflage and Russia\\'s Doppelganger.[289][290] OpenAI banned access to its services from China in\\nJuly 2024.[291]\\nIn June 2023, a lawsuit claimed that OpenAI scraped 300 billion words online without consent and\\nwithout registering as a data broker. It was filed in San Francisco, California, by sixteen anonymous\\nplaintiffs. They also claimed that OpenAI and its partner as well as customer Microsoft continued to\\nunlawfully collect and use personal data from millions of consumers worldwide to train artificial\\nintelligence models.[292]\\nAnthropic – Artificial intelligence research company\\nCenter for AI Safety – US-based AI safety research center\\nFuture of Humanity Institute – Defunct Oxford interdisciplinary research centre\\nFuture of Life Institute – International nonprofit research institute\\nGoogle DeepMind – Artificial intelligence research laboratory\\nMachine Intelligence Research Institute – Nonprofit researching AI safety\\na. The term \"pre-training\" refers to general language training as distinct from fine-tuning for\\nspecific tasks.\\nb. One petaflop/s-day is approximately equal to 1020 neural net operations.[174]\\n1. \"I Tried To Visit OpenAI\\'s Office. Hilarity Ensued\" (https://sfstandard.com/technology/i-tried-t\\no-visit-openais-office-hilarity-ensued/). The San Francisco Standard. December 20, 2022.\\nArchived (https://web.archive.org/web/20230603194312/https://sfstandard.com/technology/i-\\ntried-to-visit-openais-office-hilarity-ensued/) from the original on June 3, 2023. Retrieved\\nJune 3, 2023.\\n2. Metz, Cade; Isaak, Mike (September 3, 2024). \"OpenAI, Still Haunted by Its Chaotic Past, Is\\nTrying to Grow Up\" (https://www.nytimes.com/2024/09/03/technology/openai-chatgpt-revenu\\ne.html). New York Times. Archived (https://web.archive.org/web/20240907201621/https://ww\\nw.nytimes.com/2024/09/03/technology/openai-chatgpt-revenue.html) from the original on\\nSeptember 7, 2024. Retrieved September 3, 2024.\\n3. Woo, Erin; Efrati, Amir (May 4, 2023). \"OpenAI\\'s Losses Doubled to $540 Million as It\\nDeveloped ChatGPT\" (https://www.theinformation.com/articles/openais-losses-doubled-to-5\\n40-million-as-it-developed-chatgpt). The Information. Archived (https://web.archive.org/web/\\n20230619191257/https://www.theinformation.com/articles/openais-losses-doubled-to-540-mi\\nllion-as-it-developed-chatgpt) from the original on June 19, 2023. Retrieved June 19, 2023.\\n\"In 2022, by comparison, revenue was just $28 million, mainly from selling access to its AI\\nsoftware... OpenAI\\'s losses roughly doubled to around $540 million last year as it developed\\nChatGPT...\"\\nData scraping\\nSee also\\nNotes\\nReferences'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 20, 'page_label': '21'}, page_content='4. Efrati, Amir; Holmes, Aaron (July 24, 2024). \"Why OpenAI Could Lose $5 Billion This Year\"\\n(https://www.theinformation.com/articles/why-openai-could-lose-5-billion-this-year). The\\nInformation. Archived (https://archive.today/20240801185619/https://www.theinformation.co\\nm/articles/why-openai-could-lose-5-billion-this-year) from the original on August 1, 2024.\\n5. \"OpenAI Charter\" (https://openai.com/charter). openai.com. April 9, 2018. Archived (https://w\\neb.archive.org/web/20230714043611/https://openai.com/charter) from the original on July\\n14, 2023. Retrieved July 11, 2023.\\n6. \"Artificial: The OpenAI Story\" (https://www.wsj.com/tech/ai/artificial-the-openai-story-21587c\\nbd). WSJ. December 10, 2023. Archived (https://web.archive.org/web/20231212151657/http\\ns://www.wsj.com/tech/ai/artificial-the-openai-story-21587cbd) from the original on December\\n12, 2023. Retrieved December 12, 2023.\\n7. \"Models - OpenAI API\" (https://platform.openai.com/docs/models/overview). OpenAI.\\nArchived (https://archive.today/20231119121512/https://platform.openai.com/docs/models)\\nfrom the original on November 19, 2023. Retrieved November 19, 2023.\\n8. Jindal, Siddharth (February 16, 2024). \"OpenAI Steals the Spotlight with Sora\" (https://analy\\nticsindiamag.com/openai-steals-the-spotlight-with-sora-%E2%9C%A8/). Analytics India\\nMagazine. Archived (https://web.archive.org/web/20240420120154/https://analyticsindiama\\ng.com/openai-steals-the-spotlight-with-sora-%E2%9C%A8/) from the original on April 20,\\n2024. Retrieved July 10, 2024.\\n9. \"OPENAI, INC\" (https://opencorporates.com/companies/us_de/5902936). OpenCorporates.\\nDecember 8, 2015. Archived (https://web.archive.org/web/20230828053254/https://opencor\\nporates.com/companies/us_de/5902936) from the original on August 28, 2023. Retrieved\\nAugust 2, 2023.\\n10. \"Our structure\" (https://openai.com/our-structure). OpenAI. June 28, 2023. Archived (https://\\nweb.archive.org/web/20230729203855/https://openai.com/our-structure) from the original on\\nJuly 29, 2023.\\n11. \"Sam Altman Joins Microsoft After OpenAI Ousting\" (https://time.com/6337503/sam-altman-j\\noins-microsoft-ai/). Time. November 20, 2023. Archived (https://web.archive.org/web/20240\\n625064602/https://time.com/6337503/sam-altman-joins-microsoft-ai/) from the original on\\nJune 25, 2024. Retrieved June 25, 2024.\\n12. Roth, Emma (March 13, 2023). \"Microsoft spent hundreds of millions of dollars on a\\nChatGPT supercomputer\" (https://www.theverge.com/2023/3/13/23637675/microsoft-chatgp\\nt-bing-millions-dollars-supercomputer-openai). The Verge. Archived (https://web.archive.org/\\nweb/20230330071711/https://www.theverge.com/2023/3/13/23637675/microsoft-chatgpt-bin\\ng-millions-dollars-supercomputer-openai) from the original on March 30, 2023. Retrieved\\nJune 25, 2024.\\n13. Goldman, Sharon. \"OpenAI\\'s AGI safety team has been gutted, says ex-researcher\" (https://\\nfortune.com/2024/08/26/openai-agi-safety-researchers-exodus/). Fortune. Retrieved\\nAugust 29, 2024.\\n14. \"Introducing OpenAI\" (https://openai.com/blog/introducing-openai/). OpenAI. December 12,\\n2015. Archived (https://web.archive.org/web/20170808104802/https://openai.com/blog/intro\\nducing-openai/) from the original on August 8, 2017. Retrieved December 23, 2022.\\n15. \"Sam Altman on His Plan to Keep A.I. Out of the Hands of the \"Bad Guys\"\" (https://www.van\\nityfair.com/news/2015/12/sam-altman-elon-musk-openai). Vanity Fair. 2015. Archived (http\\ns://web.archive.org/web/20230203201103/https://www.vanityfair.com/news/2015/12/sam-alt\\nman-elon-musk-openai) from the original on February 3, 2023. Retrieved January 23, 2023.\\n16. Harris, Mark (May 17, 2023). \"Elon Musk used to say he put $100M in OpenAI, but now it\\'s\\n$50M: Here are the receipts\" (https://archive.today/20230518211335/https://techcrunch.co\\nm/2023/05/17/elon-musk-used-to-say-he-put-100m-in-openai-but-now-its-50m-here-are-the-\\nreceipts/). TechCrunch. Archived from the original (https://techcrunch.com/2023/05/17/elon-\\nmusk-used-to-say-he-put-100m-in-openai-but-now-its-50m-here-are-the-receipts/) on May\\n18, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 21, 'page_label': '22'}, page_content='17. \"Introducing OpenAI\" (https://blog.openai.com/introducing-openai/). OpenAI Blog. December\\n12, 2015. Archived (https://web.archive.org/web/20190224031626/https://blog.openai.com/in\\ntroducing-openai/) from the original on February 24, 2019. Retrieved September 29, 2018.\\n18. \"Tech giants pledge $1bn for \\'altruistic AI\\' venture, OpenAI\" (https://www.bbc.com/news/tech\\nnology-35082344). BBC News. December 12, 2015. Archived (https://web.archive.org/web/2\\n0180314021831/http://www.bbc.com/news/technology-35082344) from the original on\\nMarch 14, 2018. Retrieved December 19, 2015.\\n19. Seetharaman, Deepa (September 27, 2024). \"Turning OpenAI Into a Real Business Is\\nTearing It Apart\" (https://www.wsj.com/tech/ai/open-ai-division-for-profit-da26c24b). The Wall\\nStreet Journal. Archived (https://archive.today/20240928001622/https://www.wsj.com/tech/a\\ni/open-ai-division-for-profit-da26c24b) from the original on September 28, 2024. Retrieved\\nSeptember 28, 2024.\\n20. Conger, Kate. \"Elon Musk\\'s Neuralink Sought to Open an Animal Testing Facility in San\\nFrancisco\" (https://gizmodo.com/elon-musks-neuralink-sought-to-open-an-animal-testing-f-1\\n823167674). Gizmodo. Archived (https://web.archive.org/web/20180924145028/https://gizm\\nodo.com/elon-musks-neuralink-sought-to-open-an-animal-testing-f-1823167674) from the\\noriginal on September 24, 2018. Retrieved October 11, 2018.\\n21. Hao, Karen (February 17, 2020). \"The messy, secretive reality behind OpenAI\\'s bid to save\\nthe world\" (https://www.technologyreview.com/s/615181/ai-openai-moonshot-elon-musk-sa\\nm-altman-greg-brockman-messy-secretive-reality/). MIT Technology Review. Archived (http\\ns://web.archive.org/web/20200403023123/https://www.technologyreview.com/s/615181/ai-o\\npenai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/) from the\\noriginal on April 3, 2020. Retrieved March 9, 2020.\\n22. Cade Metz (April 27, 2016). \"Inside OpenAI, Elon Musk\\'s Wild Plan to Set Artificial\\nIntelligence Free\" (https://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-se\\nt-artificial-intelligence-free/). Wired. Archived (https://web.archive.org/web/20160427162700/\\nhttp://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-set-artificial-intelligenc\\ne-free/) from the original on April 27, 2016. Retrieved April 28, 2016.\\n23. Dave Gershgorn (April 27, 2016). \"Elon Musk\\'s Artificial Intelligence Group Opens A \\'Gym\\'\\nTo Train A.I.\" (http://www.popsci.com/elon-musks-artificial-intelligence-group-opens-gym-to-t\\nrain-ai) Popular Science. Archived (https://web.archive.org/web/20160430045139/http://ww\\nw.popsci.com/elon-musks-artificial-intelligence-group-opens-gym-to-train-ai) from the\\noriginal on April 30, 2016. Retrieved April 29, 2016.\\n24. Carr, Austin; King, Ian (June 15, 2023). \"How Nvidia Became ChatGPT\\'s Brain and Joined\\nthe $1 Trillion Club\" (https://www.bloomberg.com/news/features/2023-06-15/nvidia-s-ai-chip\\ns-power-chatgpt-and-multibillion-dollar-surge). Bloomberg News. Archived (https://archive.to\\nday/20230618072913/https://www.bloomberg.com/news/features/2023-06-15/nvidia-s-ai-chi\\nps-power-chatgpt-and-multibillion-dollar-surge) from the original on June 18, 2023.\\n25. Vanian, Jonathan (August 15, 2016). \"Elon Musk\\'s Artificial Intelligence Project Just Got a\\nFree Supercomputer\" (https://fortune.com/2016/08/15/elon-musk-artificial-intelligence-opena\\ni-nvidia-supercomputer/). Fortune. Archived (https://archive.today/20230607233501/https://f\\nortune.com/2016/08/15/elon-musk-artificial-intelligence-openai-nvidia-supercomputer/) from\\nthe original on June 7, 2023.\\n26. Metz, Cade. \"Elon Musk\\'s Lab Wants to Teach Computers to Use Apps Just Like Humans\\nDo\" (https://www.wired.com/2016/12/openais-universe-computers-learn-use-apps-like-huma\\nns/). WIRED. Archived (https://web.archive.org/web/20190113173007/https://www.wired.co\\nm/2016/12/openais-universe-computers-learn-use-apps-like-humans/) from the original on\\nJanuary 13, 2019. Retrieved December 31, 2016.\\n27. Mannes, John. \"OpenAI\\'s Universe is the fun parent every artificial intelligence deserves\" (ht\\ntps://techcrunch.com/2016/12/05/openais-universe-is-the-fun-parent-every-artificial-intellige\\nnce-deserves/). TechCrunch. Archived (https://web.archive.org/web/20190219080426/http\\ns://techcrunch.com/2016/12/05/openais-universe-is-the-fun-parent-every-artificial-intelligenc\\ne-deserves/) from the original on February 19, 2019. Retrieved December 31, 2016.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 22, 'page_label': '23'}, page_content='28. \"OpenAI – Universe\" (https://web.archive.org/web/20170101001551/https://universe.openai.\\ncom/). Archived from the original (https://universe.openai.com/) on January 1, 2017.\\nRetrieved December 31, 2016.\\n29. Claburn, Thomas. \"Elon Musk-backed OpenAI reveals Universe – a universal training\\nground for computers\" (https://www.theregister.co.uk/2016/12/05/openai_universe_reinforce\\nment_learning/). The Register. Archived (https://web.archive.org/web/20170101002022/htt\\np://www.theregister.co.uk/2016/12/05/openai_universe_reinforcement_learning/) from the\\noriginal on January 1, 2017. Retrieved December 31, 2016.\\n30. \"Microsoft to invest $1 billion in OpenAI\" (https://www.reuters.com/article/us-microsoft-opena\\ni/microsoft-to-invest-1-billion-in-openai-idUSKCN1UH1H9). Reuters. July 22, 2019. Archived\\n(https://web.archive.org/web/20200525132055/https://www.reuters.com/article/us-microsoft-\\nopenai/microsoft-to-invest-1-billion-in-openai-idUSKCN1UH1H9) from the original on May\\n25, 2020. Retrieved March 6, 2020.\\n31. Vincent, James (February 21, 2018). \"Elon Musk leaves board of AI safety group to avoid\\nconflict of interest with Tesla\" (https://www.theverge.com/2018/2/21/17036214/elon-musk-op\\nenai-ai-safety-leaves-board). The Verge. Archived (https://web.archive.org/web/2020110901\\n5711/https://www.theverge.com/2018/2/21/17036214/elon-musk-openai-ai-safety-leaves-bo\\nard) from the original on November 9, 2020. Retrieved February 22, 2018.\\n32. Hern, Alex (February 14, 2019). \"New AI fake text generator may be too dangerous to\\nrelease, say creators\" (https://www.theguardian.com/technology/2019/feb/14/elon-musk-bac\\nked-ai-writes-convincing-news-fiction). The Guardian. Archived (https://web.archive.org/web/\\n20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-\\nai-writes-convincing-news-fiction) from the original on February 14, 2019. Retrieved\\nDecember 19, 2020.\\n33. \"OpenAI shifts from nonprofit to \\'capped-profit\\' to attract capital\" (https://techcrunch.com/201\\n9/03/11/openai-shifts-from-nonprofit-to-capped-profit-to-attract-capital/). March 11, 2019.\\nArchived (https://web.archive.org/web/20230104154138/https://techcrunch.com/2019/03/11/\\nopenai-shifts-from-nonprofit-to-capped-profit-to-attract-capital/) from the original on January\\n4, 2023. Retrieved January 4, 2023.\\n34. \"To Compete With Google, OpenAI Seeks Investors–and Profits\" (https://www.wired.com/sto\\nry/compete-google-openai-seeks-investorsand-profits/). Wired. December 3, 2019. Archived\\n(https://web.archive.org/web/20200314180028/https://www.wired.com/story/compete-google\\n-openai-seeks-investorsand-profits/) from the original on March 14, 2020. Retrieved\\nMarch 6, 2020.\\n35. Kahn, Jeremy (March 11, 2019). \"AI Research Group Co-Founded by Elon Musk Starts For-\\nProfit Arm\" (https://www.bloomberg.com/news/articles/2019-03-11/ai-research-group-co-fou\\nnded-by-musk-starts-for-profit-arm). Bloomberg News. Archived (https://web.archive.org/we\\nb/20191207080100/https://www.bloomberg.com/news/articles/2019-03-11/ai-research-group\\n-co-founded-by-musk-starts-for-profit-arm) from the original on December 7, 2019.\\nRetrieved March 6, 2020.\\n36. Metz, Cade (April 19, 2018). \"A.I. Researchers Are Making More Than $1 Million, Even at a\\nNonprofit\" (https://www.nytimes.com/2018/04/19/technology/artificial-intelligence-salaries-op\\nenai.html). The New York Times. ISSN 0362-4331 (https://search.worldcat.org/issn/0362-43\\n31). Archived (https://web.archive.org/web/20180808200610/https://www.nytimes.com/2018/\\n04/19/technology/artificial-intelligence-salaries-openai.html) from the original on August 8,\\n2018. Retrieved January 28, 2023.\\n37. \"Microsoft invests in and partners with OpenAI\" (https://openai.com/blog/microsoft-invests-in\\n-and-partners-with-openai). July 22, 2019. Archived (https://web.archive.org/web/202302281\\n25554/https://openai.com/blog/microsoft-invests-in-and-partners-with-openai) from the\\noriginal on February 28, 2023. Retrieved March 17, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 23, 'page_label': '24'}, page_content='38. Langston, Jennifer (January 11, 2023). \"Microsoft announces new supercomputer, lays out\\nvision for future AI work\" (https://news.microsoft.com/source/features/ai/openai-azure-super\\ncomputer/). Source. Archived (https://web.archive.org/web/20230210180449/https://news.mi\\ncrosoft.com/source/features/ai/openai-azure-supercomputer/) from the original on February\\n10, 2023. Retrieved February 10, 2023. \"Built in collaboration with and exclusively for\\nOpenAI\"\\n39. Foley, Mary Jo (May 19, 2020). \"Microsoft builds a supercomputer for OpenAI for training\\nmassive AI models\" (https://www.zdnet.com/article/microsoft-builds-a-supercomputer-for-op\\nenai-for-training-massive-ai-models/). ZDNET. Archived (https://web.archive.org/web/20230\\n210180846/https://www.zdnet.com/article/microsoft-builds-a-supercomputer-for-openai-for-tr\\naining-massive-ai-models/) from the original on February 10, 2023. Retrieved February 10,\\n2023.\\n40. \"Microsoft\\'s OpenAI supercomputer has 285,000 CPU cores, 10,000 GPUs\" (https://www.en\\ngadget.com/microsoft-openai-supercomputer-azure-150001119.html). Engadget. May 19,\\n2020. Archived (https://web.archive.org/web/20230210180859/https://www.engadget.com/mi\\ncrosoft-openai-supercomputer-azure-150001119.html) from the original on February 10,\\n2023. Retrieved February 10, 2023. \"Microsoft\\'s OpenAI supercomputer has 285,000 CPU\\ncores, 10,000 GPUs. It\\'s one of the five fastest systems in the world.\"\\n41. \"Microsoft Invests in and Partners with OpenAI to Support Us Building Beneficial AGI\" (http\\ns://openai.com/blog/microsoft/). OpenAI. July 22, 2019. Archived (https://web.archive.org/we\\nb/20201107230518/https://openai.com/blog/microsoft/) from the original on November 7,\\n2020. Retrieved February 21, 2020.\\n42. Murgia, Madhumita (August 7, 2019). \"DeepMind runs up higher losses and debts in race\\nfor AI\" (https://www.ft.com/content/d4280856-b92d-11e9-8a88-aa6628ac896c). Financial\\nTimes. Archived (https://web.archive.org/web/20191226185727/https://www.ft.com/content/d\\n4280856-b92d-11e9-8a88-aa6628ac896c) from the original on December 26, 2019.\\nRetrieved March 6, 2020.\\n43. \"OpenAI Will Need More Capital Than Any Non-Profit Has Ever Raised\" (https://fortune.com/\\n2019/10/03/openai-will-need-more-capital-than-any-non-profit-has-ever-raised/). Fortune.\\nArchived (https://web.archive.org/web/20191208040513/https://fortune.com/2019/10/03/ope\\nnai-will-need-more-capital-than-any-non-profit-has-ever-raised/) from the original on\\nDecember 8, 2019. Retrieved March 6, 2020.\\n44. Vincent, James (July 22, 2019). \"Microsoft invests $1 billion in OpenAI to pursue holy grail\\nof artificial intelligence\" (https://www.theverge.com/2019/7/22/20703578/microsoft-openai-in\\nvestment-partnership-1-billion-azure-artificial-general-intelligence-agi). The Verge. Archived\\n(https://web.archive.org/web/20190723011910/https://www.theverge.com/2019/7/22/207035\\n78/microsoft-openai-investment-partnership-1-billion-azure-artificial-general-intelligence-agi)\\nfrom the original on July 23, 2019. Retrieved March 6, 2020.\\n45. Vance, Ashlee (June 11, 2020). \"Trillions of Words Analyzed, OpenAI Sets Loose AI\\nLanguage Colossus\" (https://www.bloomberg.com/news/articles/2020-06-11/trillions-of-word\\ns-analyzed-openai-sets-loose-ai-language-colossus). Bloomberg News. Archived (https://we\\nb.archive.org/web/20201013163856/https://www.bloomberg.com/news/articles/2020-06-11/t\\nrillions-of-words-analyzed-openai-sets-loose-ai-language-colossus) from the original on\\nOctober 13, 2020. Retrieved June 12, 2020.\\n46. Moss, Sebastian (June 2, 2021). \"Eleven OpenAI Employees Break Off to Establish\\nAnthropic, Raise $124 Million\" (https://aibusiness.com/verticals/eleven-openai-employees-br\\neak-off-to-establish-anthropic-raise-124m). AI Business. Archived (https://web.archive.org/w\\neb/20240606002642/https://aibusiness.com/verticals/eleven-openai-employees-break-off-to-\\nestablish-anthropic-raise-124m) from the original on June 6, 2024. Retrieved June 24, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 24, 'page_label': '25'}, page_content='47. \"OpenAI debuts DALL-E for generating images from text\" (https://web.archive.org/web/2021\\n0105221534/https://venturebeat.com/2021/01/05/openai-debuts-dall-e-for-generating-image\\ns-from-text/). VentureBeat. January 5, 2021. Archived from the original (https://venturebeat.c\\nom/2021/01/05/openai-debuts-dall-e-for-generating-images-from-text/) on January 5, 2021.\\nRetrieved January 5, 2021.\\n48. Roose, Kevin (December 5, 2022). \"The Brilliance and Weirdness of ChatGPT\" (https://ww\\nw.nytimes.com/2022/12/05/technology/chatgpt-ai-twitter.html). The New York Times.\\nArchived (https://web.archive.org/web/20230118134332/https://www.nytimes.com/2022/12/0\\n5/technology/chatgpt-ai-twitter.html) from the original on January 18, 2023. Retrieved\\nJanuary 5, 2023.\\n49. Dastin, Jeffrey; Hu, Krystal; Dave, Paresh; Dave, Paresh (December 15, 2022). \"Exclusive:\\nChatGPT owner OpenAI projects $1 billion in revenue by 2024\" (https://www.reuters.com/bu\\nsiness/chatgpt-owner-openai-projects-1-billion-revenue-by-2024-sources-2022-12-15/).\\nReuters. Archived (https://web.archive.org/web/20230203201121/https://www.reuters.com/b\\nusiness/chatgpt-owner-openai-projects-1-billion-revenue-by-2024-sources-2022-12-15/)\\nfrom the original on February 3, 2023. Retrieved January 5, 2023.\\n50. Kruppa, Berber Jin and Miles (January 5, 2023). \"WSJ News Exclusive | ChatGPT Creator\\nin Investor Talks at $29 Billion Valuation\" (https://www.wsj.com/articles/chatgpt-creator-open\\nai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-11672949279). Wall Street\\nJournal. Archived (https://web.archive.org/web/20230203201104/https://www.wsj.com/article\\ns/chatgpt-creator-openai-is-in-talks-for-tender-offer-that-would-value-it-at-29-billion-1167294\\n9279) from the original on February 3, 2023. Retrieved January 6, 2023.\\n51. \"Microsoft Adds $10 Billion to Investment in ChatGPT Maker OpenAI\" (https://www.bloomber\\ng.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-investment-in-openai).\\nBloomberg.com. January 23, 2023. Archived (https://web.archive.org/web/20230123180447/\\nhttps://www.bloomberg.com/news/articles/2023-01-23/microsoft-makes-multibillion-dollar-inv\\nestment-in-openai) from the original on January 23, 2023. Retrieved January 23, 2023.\\n52. Capoot, Ashley (January 23, 2023). \"Microsoft announces multibillion-dollar investment in\\nChatGPT-maker OpenAI\" (https://www.cnbc.com/2023/01/23/microsoft-announces-multibillio\\nn-dollar-investment-in-chatgpt-maker-openai.html). CNBC. Archived (https://web.archive.or\\ng/web/20230123154020/https://www.cnbc.com/2023/01/23/microsoft-announces-multibillion\\n-dollar-investment-in-chatgpt-maker-openai.html) from the original on January 23, 2023.\\nRetrieved January 23, 2023.\\n53. Warren, Tom (January 23, 2023). \"Microsoft extends OpenAI partnership in a \"multibillion\\ndollar investment\"\" (https://www.theverge.com/2023/1/23/23567448/microsoft-openai-partne\\nrship-extension-ai). The Verge. Archived (https://web.archive.org/web/20230429020214/http\\ns://www.theverge.com/2023/1/23/23567448/microsoft-openai-partnership-extension-ai) from\\nthe original on April 29, 2023. Retrieved April 29, 2023.\\n54. \"Bard: Google launches ChatGPT rival\" (https://www.bbc.com/news/technology-64546299).\\nBBC News. February 6, 2023. Archived (https://web.archive.org/web/20230207111531/http\\ns://www.bbc.com/news/technology-64546299) from the original on February 7, 2023.\\nRetrieved February 7, 2023.\\n55. Vincent, James (February 8, 2023). \"Google\\'s AI chatbot Bard makes factual error in first\\ndemo\" (https://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-\\nexoplanet-demo). The Verge. Archived (https://web.archive.org/web/20230212094317/http\\ns://www.theverge.com/2023/2/8/23590864/google-ai-chatbot-bard-mistake-error-exoplanet-d\\nemo) from the original on February 12, 2023. Retrieved February 12, 2023.\\n56. Dotan, Tom (February 7, 2023). \"Microsoft Adds ChatGPT AI Technology to Bing Search\\nEngine\" (https://www.wsj.com/articles/microsoft-adds-chatgpt-ai-technology-to-bing-search-\\nengine-11675793525). Wall Street Journal. Archived (https://web.archive.org/web/20230207\\n185720/https://www.wsj.com/articles/microsoft-adds-chatgpt-ai-technology-to-bing-search-e\\nngine-11675793525) from the original on February 7, 2023. Retrieved February 7, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 25, 'page_label': '26'}, page_content='57. Dastin, Jeffrey (March 3, 2023). \"OpenAI\\'s long-time backer Reid Hoffman leaves board\" (htt\\nps://www.reuters.com/technology/openais-long-time-backer-reid-hoffman-leaves-board-2023\\n-03-03/). Reuters. Archived (https://web.archive.org/web/20230513191651/https://www.reute\\nrs.com/technology/openais-long-time-backer-reid-hoffman-leaves-board-2023-03-03/) from\\nthe original on May 13, 2023. Retrieved March 17, 2023.\\n58. \"GPT-4\" (https://openai.com/product/gpt-4). openai.com. Archived (https://web.archive.org/w\\neb/20230314165712/https://openai.com/product/gpt-4) from the original on March 14, 2023.\\nRetrieved March 16, 2023.\\n59. \"Governance of superintelligence\" (https://openai.com/blog/governance-of-superintelligenc\\ne). openai.com. Archived (https://web.archive.org/web/20230527061619/https://openai.com/\\nblog/governance-of-superintelligence) from the original on May 27, 2023. Retrieved May 30,\\n2023.\\n60. Wodecki, Ben; Yao, Deborah (May 23, 2023). \"OpenAI Founders Warn AI \\'Superintelligence\\'\\nis Like Nuclear Power\" (https://aibusiness.com/responsible-ai/openai-leaders-want-the-publi\\nc-to-decide-ai-rules). Archived (https://web.archive.org/web/20230530042308/https://aibusin\\ness.com/responsible-ai/openai-leaders-want-the-public-to-decide-ai-rules) from the original\\non May 30, 2023. Retrieved May 30, 2023.\\n61. \"Introducing Superalignment\" (https://openai.com/index/introducing-superalignment/).\\nOpenAI. July 5, 2023. Archived (https://web.archive.org/web/20240525041645/https://opena\\ni.com/index/introducing-superalignment/) from the original on May 25, 2024. Retrieved\\nMay 25, 2024.\\n62. \"OpenAI acquires start-up Global Illumination to work on core products, ChatGPT\" (https://w\\nww.reuters.com/markets/deals/openai-acquires-start-up-global-illumination-work-core-produ\\ncts-chatgpt-2023-08-16/). Reuters. August 16, 2023. Archived (https://web.archive.org/web/2\\n0230817111738/https://www.reuters.com/markets/deals/openai-acquires-start-up-global-illu\\nmination-work-core-products-chatgpt-2023-08-16/) from the original on August 17, 2023.\\nRetrieved August 17, 2023.\\n63. Edwards, Nathan (September 21, 2023). \"Microsoft\\'s unified Copilot is coming to Windows,\\nEdge, and everywhere else\" (https://www.theverge.com/2023/9/21/23883798/microsoft-copil\\not-unified-windows-11-apps-launch-date). The Verge. Archived (https://web.archive.org/web/\\n20231207200142/https://www.theverge.com/2023/9/21/23883798/microsoft-copilot-unified-\\nwindows-11-apps-launch-date) from the original on December 7, 2023. Retrieved\\nFebruary 2, 2024.\\n64. Warren, Tom (December 26, 2023). \"Microsoft Copilot is now available as a ChatGPT-like\\napp on Android\" (https://www.theverge.com/2023/12/26/24015198/microsoft-copilot-mobile-\\napp-android-launch). The Verge. Archived (https://web.archive.org/web/20240131115238/htt\\nps://www.theverge.com/2023/12/26/24015198/microsoft-copilot-mobile-app-android-launch)\\nfrom the original on January 31, 2024. Retrieved February 2, 2024.\\n65. \"Microsoft\\'s Copilot app is now available on iOS\" (https://www.theverge.com/2023/12/29/240\\n19288/microsoft-copilot-app-available-iphone-ipad-ai). The Verge. Vox Media. December\\n29, 2023. Archived (https://web.archive.org/web/20240130064608/https://www.theverge.co\\nm/2023/12/29/24019288/microsoft-copilot-app-available-iphone-ipad-ai) from the original on\\nJanuary 30, 2024. Retrieved February 2, 2024.\\n66. \"OpenAI \\'in favour\\' of UK AI legislation, policy chief says\" (https://www.uktech.news/ai/opena\\ni-in-favour-of-uk-ai-legislation-policy-chief-says-20240923). UKTN. September 23, 2024.\\nRetrieved September 24, 2024.\\n67. Northrop, Katrina (December 4, 2023). \"G42\\'s Ties To China Run Deep\" (https://www.thewir\\nechina.com/2023/12/03/g42s-ties-to-china-run-deep-g42-peng-xiao/). The Wire China.\\nArchived (https://web.archive.org/web/20240125203013/https://www.thewirechina.com/202\\n3/12/03/g42s-ties-to-china-run-deep-g42-peng-xiao/) from the original on January 25, 2024.\\nRetrieved February 17, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 26, 'page_label': '27'}, page_content='68. \"OpenAI Launches Custom ChatGPT Versions\" (https://www.nytimes.com/2023/11/06/techn\\nology/openai-custom-chatgpt.html). The New York Times. November 6, 2023. Archived (http\\ns://web.archive.org/web/20231112122229/https://www.nytimes.com/2023/11/06/technology/\\nopenai-custom-chatgpt.html) from the original on November 12, 2023. Retrieved\\nNovember 12, 2023.\\n69. Elstrom, Peter (November 15, 2023). \"OpenAI Pauses New Signups to Manage\\nOverwhelming Demand\" (https://www.bloomberg.com/news/articles/2023-11-15/openai-pau\\nses-new-signups-to-manage-overwhelming-demand). Bloomberg. Archived (https://web.arc\\nhive.org/web/20231115130109/https://www.bloomberg.com/news/articles/2023-11-15/opena\\ni-pauses-new-signups-to-manage-overwhelming-demand) from the original on November\\n15, 2023. Retrieved November 15, 2023.\\n70. Idris, Abubakar (December 13, 2023). \"OpenAI Reopens ChatGPT Plus Subscriptions\" (http\\ns://web.archive.org/web/20231214020417/https://themessenger.com/tech/openai-re-opens-\\nchatgpt-plus-subscriptions). The Messenger. Archived from the original (https://themesseng\\ner.com/tech/openai-re-opens-chatgpt-plus-subscriptions) on December 14, 2023. Retrieved\\nDecember 14, 2023.\\n71. Wiggers, Kyle (January 16, 2024). \"OpenAI announces team to build \\'crowdsourced\\'\\ngovernance ideas into its models\" (https://techcrunch.com/2024/01/16/openai-announces-te\\nam-to-build-crowdsourced-governance-ideas-into-its-models/). TechCrunch. Archived (http\\ns://web.archive.org/web/20240116164619/https://techcrunch.com/2024/01/16/openai-annou\\nnces-team-to-build-crowdsourced-governance-ideas-into-its-models/) from the original on\\nJanuary 16, 2024. Retrieved January 16, 2024.\\n72. Field, Hayden (January 18, 2024). \"OpenAI announces first partnership with a university\" (ht\\ntps://www.cnbc.com/2024/01/18/openai-announces-first-partnership-with-a-university.html).\\nCNBC. Archived (https://web.archive.org/web/20240124135634/https://www.cnbc.com/2024/\\n01/18/openai-announces-first-partnership-with-a-university.html) from the original on\\nJanuary 24, 2024. Retrieved January 18, 2024.\\n73. Seetharaman, Deepa (February 28, 2024). \"SEC Investigating Whether OpenAI Investors\\nWere Misled\" (https://www.wsj.com/tech/sec-investigating-whether-openai-investors-were-m\\nisled-9d90b411). The Wall Street Journal. News Corp. Archived (https://web.archive.org/we\\nb/20240229044238/https://www.wsj.com/tech/sec-investigating-whether-openai-investors-w\\nere-misled-9d90b411) from the original on February 29, 2024. Retrieved February 29, 2024.\\n74. Hagey, Keach; Seetharaman, Deepa; Jin, Berber (November 22, 2023). \"Behind the Scenes\\nof Sam Altman\\'s Showdown at OpenAI\" (https://www.wsj.com/tech/ai/altman-firing-openai-5\\n20a3a8c). The Wall Street Journal. News Corp. Archived (https://web.archive.org/web/2023\\n1122002607/https://www.wsj.com/tech/ai/altman-firing-openai-520a3a8c) from the original\\non November 22, 2023. Retrieved February 29, 2024.\\n75. Metz, Cade (February 15, 2024). \"OpenAI Unveils A.I. That Instantly Generates Eye-\\nPopping Videos\" (https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.htm\\nl). The New York Times. Archived (https://web.archive.org/web/20240215220626/https://ww\\nw.nytimes.com/2024/02/15/technology/openai-sora-videos.html) from the original on\\nFebruary 15, 2024. Retrieved February 16, 2024.\\n76. \"Sora: Creating video from text\" (https://openai.com/sora). openai.com. Archived (https://we\\nb.archive.org/web/20240217015001/https://openai.com/sora) from the original on February\\n17, 2024. Retrieved February 17, 2024.\\n77. Satariano, Adam; Metz, Cade; Mickle, Tripp (March 1, 2024). \"Elon Musk Sues OpenAI and\\nSam Altman for Violating the Company\\'s Principles\" (https://www.nytimes.com/2024/03/01/te\\nchnology/elon-musk-openai-sam-altman-lawsuit.html). The New York Times. ISSN 0362-\\n4331 (https://search.worldcat.org/issn/0362-4331). Archived (https://web.archive.org/web/20\\n240306201120/https://www.nytimes.com/2024/03/01/technology/elon-musk-openai-sam-alt\\nman-lawsuit.html) from the original on March 6, 2024. Retrieved March 2, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 27, 'page_label': '28'}, page_content='78. \"Elon Musk v. Samuel Altman et al :: Superior Court of California, County of San Francisco ::\\nState Civil Lawsuit No. CGC 24 612746\" (https://www.plainsite.org/dockets/5ctu7zkg5/superi\\nor-court-of-california-county-of-san-francisco/elon-musk-v-samuel-altman-et-al/).\\nwww.plainsite.org. Archived (https://web.archive.org/web/20240309022633/https://www.plain\\nsite.org/dockets/5ctu7zkg5/superior-court-of-california-county-of-san-francisco/elon-musk-v-\\nsamuel-altman-et-al/) from the original on March 9, 2024. Retrieved March 2, 2024.\\n79. Lopatto, Elizabeth (March 6, 2024). \"OpenAI says Elon Musk wanted \\'absolute control\\' of\\nthe company\" (https://www.theverge.com/2024/3/5/24091773/openai-response-elon-musk-b\\nreach-of-contract-lawsuit). The Verge. Archived (https://web.archive.org/web/202403100504\\n48/https://www.theverge.com/2024/3/5/24091773/openai-response-elon-musk-breach-of-co\\nntract-lawsuit) from the original on March 10, 2024. Retrieved March 6, 2024.\\n80. Fried, Ina (March 1, 2024). \"Memos: OpenAI execs reject Elon Musk\\'s claims\" (https://www.\\naxios.com/2024/03/01/openai-rejects-elon-musk-lawsuit-claims). AXIOS. Archived (https://w\\neb.archive.org/web/20240304175454/https://www.axios.com/2024/03/01/openai-rejects-elon\\n-musk-lawsuit-claims) from the original on March 4, 2024. Retrieved March 5, 2024.\\n81. \"OpenAI ridicules Elon Musk\\'s \\'incoherent\\' lawsuit\" (https://edition.cnn.com/2024/03/11/tech/\\nopenai-elon-musk-lawsuit-response/index.html). CNN. March 11, 2024. Archived (https://we\\nb.archive.org/web/20240312022927/https://edition.cnn.com/2024/03/11/tech/openai-elon-m\\nusk-lawsuit-response/index.html) from the original on March 12, 2024. Retrieved March 12,\\n2024.\\n82. Field, Hayden (June 11, 2024). \"Elon Musk drops suit against OpenAI and Sam Altman\" (htt\\nps://www.cnbc.com/2024/06/11/elon-musk-drops-suit-against-openai-and-sam-altman.html).\\nCNBC. Retrieved June 12, 2024.\\n83. Kharpal, Arjun (August 5, 2024). \"Elon Musk revives lawsuit against OpenAI, Sam Altman in\\nfederal court\" (https://www.cnbc.com/2024/08/05/elon-musk-revives-lawsuit-against-openai-\\nsam-altman-in-federal-court.html). CNBC. Archived (https://web.archive.org/web/202408051\\n25804/https://www.cnbc.com/2024/08/05/elon-musk-revives-lawsuit-against-openai-sam-alt\\nman-in-federal-court.html) from the original on August 5, 2024. Retrieved August 5, 2024.\\n84. De Avila, Joseph (August 5, 2024). \"Elon Musk Revives Lawsuit Against OpenAI and Sam\\nAltman\" (https://www.wsj.com/tech/ai/elon-musk-revives-lawsuit-against-openai-and-sam-alt\\nman-d7e5a87c). The Wall Street Journal. News Corp. Archived (https://web.archive.org/we\\nb/20240805154833/https://www.wsj.com/tech/ai/elon-musk-revives-lawsuit-against-openai-a\\nnd-sam-altman-d7e5a87c) from the original on August 5, 2024. Retrieved August 5, 2024.\\n85. Hollister, Sean (May 14, 2024). \"OpenAI chief scientist Ilya Sutskever is officially leaving\" (ht\\ntps://www.theverge.com/2024/5/14/24156920/openai-chief-scientist-ilya-sutskever-leaves).\\nThe Verge. Archived (https://web.archive.org/web/20240514234107/https://www.theverge.co\\nm/2024/5/14/24156920/openai-chief-scientist-ilya-sutskever-leaves) from the original on\\nMay 14, 2024. Retrieved May 14, 2024.\\n86. Samuel, Sigal (May 17, 2024). \"\"I lost trust\": Why the OpenAI team in charge of\\nsafeguarding humanity imploded\" (https://www.vox.com/future-perfect/2024/5/17/24158403/\\nopenai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence). Vox. Archived (ht\\ntps://web.archive.org/web/20240518205458/https://www.vox.com/future-perfect/2024/5/17/2\\n4158403/openai-resignations-ai-safety-ilya-sutskever-jan-leike-artificial-intelligence) from\\nthe original on May 18, 2024. Retrieved May 18, 2024.\\n87. Knight, Will (May 17, 2024). \"OpenAI\\'s Long-Term AI Risk Team Has Disbanded\" (https://ww\\nw.wired.com/story/openai-superalignment-team-disbanded/). Wired. Archived (https://web.ar\\nchive.org/web/20240519042239/https://www.wired.com/story/openai-superalignment-team-d\\nisbanded/) from the original on May 19, 2024. Retrieved May 19, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 28, 'page_label': '29'}, page_content='88. \"OpenAI promised 20% of its computing power to combat the most dangerous kind of AI—\\nbut never delivered, sources say\" (https://fortune.com/2024/05/21/openai-superalignment-20\\n-compute-commitment-never-fulfilled-sutskever-leike-altman-brockman-murati/). Fortune.\\nArchived (https://web.archive.org/web/20240523232014/https://fortune.com/2024/05/21/ope\\nnai-superalignment-20-compute-commitment-never-fulfilled-sutskever-leike-altman-brockma\\nn-murati/) from the original on May 23, 2024. Retrieved May 25, 2024.\\n89. \"Reddit and OpenAI Build Partnership - Upvoted\" (https://www.redditinc.com/blog/reddit-and\\n-oai-partner). www.redditinc.com. May 16, 2024. Archived (https://web.archive.org/web/2024\\n0519012615/https://www.redditinc.com/blog/reddit-and-oai-partner) from the original on May\\n19, 2024. Retrieved May 19, 2024.\\n90. \"OpenAI and Wall Street Journal owner News Corp sign content deal\" (https://web.archive.o\\nrg/web/20240523030813/https://www.theguardian.com/technology/article/2024/may/22/open\\nai-chatgpt-news-corp-deal). May 22, 2024. Archived from the original (https://www.theguardi\\nan.com/technology/article/2024/may/22/openai-chatgpt-news-corp-deal) on May 23, 2024.\\nRetrieved May 23, 2024.\\n91. Fischer, Sara (May 29, 2024). \"Exclusive: The Atlantic, Vox Media ink licensing, product\\ndeals with OpenAI\" (https://www.axios.com/2024/05/29/atlantic-vox-media-openai-licensing-\\ndeal). Axios. Archived (https://web.archive.org/web/20240531134558/https://www.axios.co\\nm/2024/05/29/atlantic-vox-media-openai-licensing-deal) from the original on May 31, 2024.\\nRetrieved June 3, 2024.\\n92. Edwards, Benj; Belanger, Ashley (June 1, 2024). \"Journalists \"deeply troubled\" by OpenAI\\'s\\ncontent deals with Vox, The Atlantic\" (https://arstechnica.com/information-technology/2024/0\\n5/openai-content-deals-with-vox-and-the-atlantic-spark-criticism-from-journalists/). Ars\\nTechnica. Archived (https://web.archive.org/web/20240531224456/https://arstechnica.com/in\\nformation-technology/2024/05/openai-content-deals-with-vox-and-the-atlantic-spark-criticism\\n-from-journalists/) from the original on May 31, 2024. Retrieved June 3, 2024.\\n93. Roose, Kevin (June 4, 2024). \"OpenAI Insiders Warn of a \\'Reckless\\' Race for Dominance\"\\n(https://web.archive.org/web/20240605151211/https://www.nytimes.com/2024/06/04/technol\\nogy/openai-culture-whistleblowers.html). The New York Times. Archived from the original (ht\\ntps://www.nytimes.com/2024/06/04/technology/openai-culture-whistleblowers.html) on June\\n5, 2024. Retrieved June 6, 2024.\\n94. Wiggers, Kyle (June 10, 2024). \"Apple brings ChatGPT to its apps, including Siri\" (https://tec\\nhcrunch.com/2024/06/10/apple-brings-chatgpt-to-its-apps-including-siri/). TechCrunch.\\nArchived (https://web.archive.org/web/20240610190918/https://techcrunch.com/2024/06/10/\\napple-brings-chatgpt-to-its-apps-including-siri/) from the original on June 10, 2024.\\nRetrieved June 10, 2024.\\n95. Coldewey, Devin (June 13, 2024). \"Former NSA head joins OpenAI board and safety\\ncommittee\" (https://techcrunch.com/2024/06/13/former-nsa-head-joins-openai-board-and-sa\\nfety-committee/). TechCrunch. Archived (https://web.archive.org/web/20240614000140/http\\ns://techcrunch.com/2024/06/13/former-nsa-head-joins-openai-board-and-safety-committee/)\\nfrom the original on June 14, 2024. Retrieved June 15, 2024.\\n96. Wiggers, Kyle (June 24, 2024). \"OpenAI buys a remote collaboration platform\" (https://techc\\nrunch.com/2024/06/24/openai-buys-a-remote-collaboration-platform/). TechCrunch.\\nArchived (https://web.archive.org/web/20240629064556/https://techcrunch.com/2024/06/24/\\nopenai-buys-a-remote-collaboration-platform/) from the original on June 29, 2024. Retrieved\\nJuly 5, 2024.\\n97. Tong, Anna; Paul, Katie (July 15, 2024). \"Exclusive: OpenAI working on new reasoning\\ntechnology under code name \\'Strawberry\\'\" (https://www.reuters.com/technology/artificial-int\\nelligence/openai-working-new-reasoning-technology-under-code-name-strawberry-2024-07-\\n12/). Reuters. Archived (https://web.archive.org/web/20240713080951/https://www.reuters.c\\nom/technology/artificial-intelligence/openai-working-new-reasoning-technology-under-code-\\nname-strawberry-2024-07-12/) from the original on July 13, 2024. Retrieved July 22, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 29, 'page_label': '30'}, page_content='98. Sager, Monica (July 16, 2024). \"What we know about OpenAI\\'s secretive \\'Project\\nStrawberry\\'\" (https://www.newsweek.com/openai-strawberry-chat-gpt-ai-sam-altman-19259\\n60). Newsweek. Archived (https://web.archive.org/web/20240722214700/https://www.newsw\\neek.com/openai-strawberry-chat-gpt-ai-sam-altman-1925960) from the original on July 22,\\n2024. Retrieved July 22, 2024.\\n99. Robison, Kylie (September 12, 2024). \"OpenAI releases o1, its first model with \\'reasoning\\'\\nabilities\" (https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-straw\\nberry-chatgpt). The Verge. Archived (https://web.archive.org/web/20240913134303/https://w\\nww.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt)\\nfrom the original on September 13, 2024. Retrieved September 13, 2024.\\n100. Wiggers, Kyle (August 6, 2024). \"OpenAI co-founder Schulman leaves for Anthropic,\\nBrockman takes extended leave\" (https://techcrunch.com/2024/08/05/openai-co-founder-lea\\nves-for-anthropic/). TechCrunch. Archived (https://web.archive.org/web/20240807070712/htt\\nps://techcrunch.com/2024/08/05/openai-co-founder-leaves-for-anthropic/) from the original\\non August 7, 2024. Retrieved August 7, 2024.\\n101. Field, Hayden (September 25, 2024). \"OpenAI CTO Mira Murati announces she\\'s leaving\\nthe company\" (https://www.cnbc.com/2024/09/25/openai-cto-mira-murati-announces-shes-le\\naving-the-company.html). CNBC. Retrieved September 25, 2024.\\n102. Wiggers, Maxwell Zeff, Kyle (September 25, 2024). \"OpenAI CTO Mira Murati says she\\'s\\nleaving the company\" (https://techcrunch.com/2024/09/25/openai-cto-mira-murati-says-shes\\n-leaving-the-company/). TechCrunch. Retrieved September 27, 2024.\\n103. Cade Metz: OpenAI Completes Deal That Values Company at $157 Billion. (https://www.nyti\\nmes.com/2024/10/02/technology/openai-valuation-150-billion.html) In: New York Times,\\nOctober 2, 2024. Retrieved October 3, 2024\\n104. Hu, Krystal (October 2, 2024). \"OpenAI closes $6.6 billion funding haul with investment from\\nMicrosoft and Nvidia\" (https://www.reuters.com/technology/artificial-intelligence/openai-close\\ns-66-billion-funding-haul-valuation-157-billion-with-investment-2024-10-02). Reuters.\\nRetrieved October 3, 2024.\\n105. \"Silicon Valley investors to bankroll artificial-intelligence center\" (http://www.seattletimes.co\\nm/business/technology/silicon-valley-investors-to-bankroll-artificial-intelligence-center/). The\\nSeattle Times. December 13, 2015. Archived (https://web.archive.org/web/2016010518155\\n2/http://www.seattletimes.com/business/technology/silicon-valley-investors-to-bankroll-artifici\\nal-intelligence-center/) from the original on January 5, 2016. Retrieved December 19, 2015.\\n106. Bordoloi, Pritam (May 9, 2022). \"OpenAI gets a new president, CTO & COO in the latest\\nrejig\" (https://analyticsindiamag.com/openai-gets-a-new-president-cto-coo-in-the-latest-reji\\ng//). AIM. Archived (https://web.archive.org/web/20221016010242/https://analyticsindiamag.\\ncom/openai-gets-a-new-president-cto-coo-in-the-latest-rejig/) from the original on October\\n16, 2022. Retrieved October 11, 2022.\\n107. \"OpenAI hires former Nextdoor CEO Sarah Friar as first CFO\" (https://www.reuters.com/tech\\nnology/openai-hires-sarah-friar-cfo-2024-06-10/). Reuters. June 10, 2024. Archived (https://\\nweb.archive.org/web/20240611124940/https://www.reuters.com/technology/openai-hires-sar\\nah-friar-cfo-2024-06-10/) from the original on June 11, 2024. Retrieved June 11, 2024.\\n108. Peters, Jay (June 13, 2024). \"Former head of NSA joins OpenAI board\" (https://www.thever\\nge.com/2024/6/13/24178079/openai-board-paul-nakasone-nsa-safety). The Verge. Archived\\n(https://web.archive.org/web/20240614060521/https://www.theverge.com/2024/6/13/241780\\n79/openai-board-paul-nakasone-nsa-safety) from the original on June 14, 2024. Retrieved\\nJune 14, 2024.\\n109. \"OpenAI Names Computer Scientist Zico Kolter as New Board Member\" (https://www.bloom\\nberg.com/news/articles/2024-08-08/openai-names-computer-scientist-zico-kolter-as-new-bo\\nard-member). Bloomberg.com. August 8, 2024. Retrieved August 14, 2024.\\n110. \"Who are OpenAI\\'s new board members?\" (https://www.reuters.com/technology/who-are-op\\nenais-new-board-members-2024-03-11/). Reuters. March 11, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 30, 'page_label': '31'}, page_content='111. Liedtke, Michael. \"Elon Musk, Peter Thiel, Reid Hoffman, others back $1 billion OpenAI\\nresearch center\" (http://www.mercurynews.com/business/ci_29256196/elon-musk-peter-thiel\\n-reid-hoffman-others-back). Mercury News. Archived (https://web.archive.org/web/20151222\\n152100/http://www.mercurynews.com/business/ci_29256196/elon-musk-peter-thiel-reid-hoff\\nman-others-back) from the original on December 22, 2015. Retrieved December 19, 2015.\\n112. Vincent, James (July 22, 2019). \"Microsoft invests $1 billion in OpenAI to pursue holy grail\\nof artificial intelligence\" (https://www.theverge.com/2019/7/22/20703578/microsoft-openai-in\\nvestment-partnership-1-billion-azure-artificial-general-intelligence-agi). The Verge. Archived\\n(https://web.archive.org/web/20190723011910/https://www.theverge.com/2019/7/22/207035\\n78/microsoft-openai-investment-partnership-1-billion-azure-artificial-general-intelligence-agi)\\nfrom the original on July 23, 2019. Retrieved July 23, 2019.\\n113. \"About OpenAI\" (https://openai.com/about/). OpenAI. December 11, 2015. Archived (https://\\nweb.archive.org/web/20171222181056/https://openai.com/about/) from the original on\\nDecember 22, 2017. Retrieved December 23, 2022.\\n114. \"Elon Musk, Infosys, others back OpenAI with $1 bn\" (https://www.business-standard.com/a\\nrticle/news-ians/elon-musk-infosys-others-back-openai-with-1-bn-115121200862_1.html).\\nBusiness Standard India. Indo-Asian News Service. December 12, 2015. Archived (https://w\\neb.archive.org/web/20190830090926/https://www.business-standard.com/article/news-ians/\\nelon-musk-infosys-others-back-openai-with-1-bn-115121200862_1.html) from the original\\non August 30, 2019. Retrieved August 30, 2019.\\n115. Shontell, Alyson (December 2, 2023). \"The rise of Joshua Kushner: How the young VC\\nquietly built a $5.3 billion firm, Thrive Capital\" (https://archive.today/20231206150706/https://\\nfortune.com/longform/josh-kushner-net-worth-thrive-capital-investments/). Fortune. Archived\\nfrom the original (https://fortune.com/longform/josh-kushner-net-worth-thrive-capital-investm\\nents/) on December 6, 2023.\\n116. Graham, Paul (December 2, 2023). \"Paul Graham on X\" (https://twitter.com/paulg/status/17\\n96174494026879040).\\n117. Metz, Cade (October 2, 2024). \"OpenAI Completes Deal That Values Company at $157\\nBillion\" (https://www.nytimes.com/2024/10/02/technology/openai-valuation-150-billion.html).\\nThe New York Times.\\n118. Piper, Kelsey (November 2, 2018). \"Why Elon Musk fears artificial intelligence\" (https://www.\\nvox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmi\\nnd-openai). Vox. Archived (https://web.archive.org/web/20210423095445/https://www.vox.co\\nm/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-ope\\nnai) from the original on April 23, 2021. Retrieved March 10, 2021.\\n119. Lewontin, Max (December 14, 2015). \"Open AI: Effort to democratize artificial intelligence\\nresearch?\" (http://www.csmonitor.com/Technology/2015/1214/Open-AI-Effort-to-democratize\\n-artificial-intelligence-research). The Christian Science Monitor. Archived (https://web.archiv\\ne.org/web/20151219071134/http://www.csmonitor.com/Technology/2015/1214/Open-AI-Effo\\nrt-to-democratize-artificial-intelligence-research) from the original on December 19, 2015.\\nRetrieved December 19, 2015.\\n120. Cade Metz (April 27, 2016). \"Inside OpenAI, Elon Musk\\'s Wild Plan to Set Artificial\\nIntelligence Free\" (https://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-se\\nt-artificial-intelligence-free/). Wired. Archived (https://web.archive.org/web/20160427162700/\\nhttp://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-set-artificial-intelligenc\\ne-free/) from the original on April 27, 2016. Retrieved April 28, 2016.\\n121. Mendoza, Jessica. \"Tech leaders launch nonprofit to save the world from killer robots\" (htt\\np://www.csmonitor.com/Science/2015/1214/Tech-leaders-launch-nonprofit-to-save-the-world\\n-from-killer-robots). The Christian Science Monitor. Archived (https://web.archive.org/web/20\\n180703170556/https://www.csmonitor.com/Science/2015/1214/Tech-leaders-launch-nonprof\\nit-to-save-the-world-from-killer-robots) from the original on July 3, 2018. Retrieved\\nDecember 19, 2015.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 31, 'page_label': '32'}, page_content='122. Metz, Cade (December 15, 2015). \"Elon Musk\\'s Billion-Dollar AI Plan Is About Far More\\nThan Saving the World\" (https://www.wired.com/2015/12/elon-musks-billion-dollar-ai-plan-is-\\nabout-far-more-than-saving-the-world/). Wired. Archived (https://web.archive.org/web/20151\\n219155928/http://www.wired.com/2015/12/elon-musks-billion-dollar-ai-plan-is-about-far-mor\\ne-than-saving-the-world/) from the original on December 19, 2015. Retrieved December 19,\\n2015. \"Altman said they expect this decades-long project to surpass human intelligence.\"\\n123. Vishal Sikka (December 14, 2015). \"OpenAI: AI for All\" (https://web.archive.org/web/201512\\n22094518/http://www.infosysblogs.com/infytalk/2015/12/openai_ai_for_all.html). InfyTalk.\\nInfosys. Archived from the original (http://www.infosysblogs.com/infytalk/2015/12/openai_ai_\\nfor_all.html) on December 22, 2015. Retrieved December 22, 2015.\\n124. \"Sam Altman\\'s Manifest Destiny\" (http://www.newyorker.com/magazine/2016/10/10/sam-alt\\nmans-manifest-destiny). The New Yorker. No. October 10, 2016. Archived (https://web.archi\\nve.org/web/20161004091200/http://www.newyorker.com/magazine/2016/10/10/sam-altmans\\n-manifest-destiny) from the original on October 4, 2016. Retrieved October 4, 2016.\\n125. Vincent, James (February 21, 2019). \"AI researchers debate the ethics of sharing potentially\\nharmful programs\" (https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researc\\nhers-harmful-programs-openai). The Verge. Archived (https://web.archive.org/web/2021020\\n9123243/https://www.theverge.com/2019/2/21/18234500/ai-ethics-debate-researchers-harm\\nful-programs-openai) from the original on February 9, 2021. Retrieved March 6, 2020.\\n126. \"Our approach to alignment research\" (https://openai.com/blog/our-approach-to-alignment-r\\nesearch). openai.com. Archived (https://web.archive.org/web/20230426111006/https://open\\nai.com/blog/our-approach-to-alignment-research) from the original on April 26, 2023.\\nRetrieved April 26, 2023.\\n127. Sorkin, Andrew Ross; Mattu, Ravi; Warner, Bernhard; Kessler, Sarah; Merced, Michael J. de\\nla; Hirsch, Lauren; Livni, Ephrat (November 22, 2023). \"The Fallout From Sam Altman\\'s\\nReturn to OpenAI\" (https://web.archive.org/web/20231217010304/https://www.nytimes.com/\\n2023/11/22/business/dealbook/openai-altman-microsoft-board.html). The New York Times.\\nISSN 0362-4331 (https://search.worldcat.org/issn/0362-4331). Archived from the original (htt\\nps://www.nytimes.com/2023/11/22/business/dealbook/openai-altman-microsoft-board.html)\\non December 17, 2023. Retrieved February 17, 2024.\\n128. Wiggers, Kyle (July 16, 2021). \"OpenAI disbands its robotics research team\" (https://venture\\nbeat.com/business/openai-disbands-its-robotics-research-team/). VentureBeat. Archived (htt\\nps://web.archive.org/web/20230212150930/https://venturebeat.com/business/openai-disban\\nds-its-robotics-research-team/) from the original on February 12, 2023. Retrieved\\nFebruary 12, 2023.\\n129. Lee, Dave (October 15, 2019). \"Robot solves Rubik\\'s cube, but not grand challenge\" (http\\ns://www.bbc.com/news/technology-50064225). BBC News. Archived (https://web.archive.or\\ng/web/20200403202722/https://www.bbc.com/news/technology-50064225) from the original\\non April 3, 2020. Retrieved February 29, 2020.\\n130. Greg Brockman; John Schulman (April 27, 2016). \"OpenAI Gym Beta\" (https://openai.com/bl\\nog/openai-gym-beta/). OpenAI Blog. OpenAI. Archived (https://web.archive.org/web/201902\\n26173517/https://openai.com/blog/openai-gym-beta/) from the original on February 26,\\n2019. Retrieved April 29, 2016.\\n131. \"openai/gym\" (https://github.com/openai/gym). GitHub. Archived (https://web.archive.org/we\\nb/20240823033813/https://github.com/openai/gym) from the original on August 23, 2024.\\nRetrieved August 29, 2024. \"The team that has been maintaining Gym since 2021 has\\nmoved all future development to Gymnasium, a drop in replacement for Gym (import\\ngymnasium as gym), and Gym will not be receiving any future updates.\"\\n132. \"Announcing The Farama Foundation - The future of open source reinforcement learning\" (h\\nttps://farama.org/Announcing-The-Farama-Foundation). The Farama Foundation. October\\n25, 2022. Archived (https://web.archive.org/web/20240829061404/https://farama.org/Annou\\nncing-The-Farama-Foundation) from the original on August 29, 2024. Retrieved August 29,\\n2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 32, 'page_label': '33'}, page_content='133. \"Gym Retro\" (https://openai.com/blog/gym-retro/). OpenAI. May 25, 2018. Archived (https://\\nweb.archive.org/web/20230212155510/https://openai.com/blog/gym-retro/) from the original\\non February 12, 2023. Retrieved February 12, 2023.\\n134. \"AI Sumo Wrestlers Could Make Future Robots More Nimble\" (https://www.wired.com/story/\\nai-sumo-wrestlers-could-make-future-robots-more-nimble/). Wired. October 11, 2017.\\nArchived (https://web.archive.org/web/20171107024652/https://www.wired.com/story/ai-sum\\no-wrestlers-could-make-future-robots-more-nimble/) from the original on November 7, 2017.\\nRetrieved November 2, 2017.\\n135. \"OpenAI\\'s Goofy Sumo-Wrestling Bots Are Smarter Than They Look\" (https://www.technolog\\nyreview.com/the-download/609117/openais-goofy-sumo-wrestling-bots-are-smarter-than-the\\ny-look/). MIT Technology Review. Archived (https://web.archive.org/web/20181109021143/ht\\ntps://www.technologyreview.com/the-download/609117/openais-goofy-sumo-wrestling-bots-\\nare-smarter-than-they-look/) from the original on November 9, 2018. Retrieved November 2,\\n2017.\\n136. Savov, Vlad (August 14, 2017). \"My favorite game has been invaded by killer AI bots and\\nElon Musk hype\" (https://www.theverge.com/2017/8/14/16141938/dota-2-openai-bots-elon-\\nmusk-artificial-intelligence). The Verge. Archived (https://web.archive.org/web/20180626030\\n145/https://www.theverge.com/2017/8/14/16141938/dota-2-openai-bots-elon-musk-artificial-i\\nntelligence) from the original on June 26, 2018. Retrieved June 25, 2018.\\n137. Frank, Blair Hanley. \"OpenAI\\'s bot beats top Dota 2 player so badly that he quits\" (https://we\\nb.archive.org/web/20170812065202/https://venturebeat.com/2017/08/11/openais-bot-beats-\\ntop-dota-2-player-so-badly-that-he-quits/). Venture Beat. Archived from the original (https://v\\nenturebeat.com/2017/08/11/openais-bot-beats-top-dota-2-player-so-badly-that-he-quits/) on\\nAugust 12, 2017. Retrieved August 12, 2017.\\n138. \"Dota 2\" (https://blog.openai.com/dota-2/). blog.openai.com. August 11, 2017. Archived (http\\ns://web.archive.org/web/20170811235617/https://blog.openai.com/dota-2/) from the original\\non August 11, 2017. Retrieved August 12, 2017.\\n139. \"More on Dota 2\" (https://blog.openai.com/more-on-dota-2/). blog.openai.com. August 16,\\n2017. Archived (https://web.archive.org/web/20190223171009/https://blog.openai.com/more\\n-on-dota-2/) from the original on February 23, 2019. Retrieved August 16, 2017.\\n140. Simonite, Tom. \"Can Bots Outwit Humans in One of the Biggest Esports Games?\" (https://w\\nww.wired.com/story/can-bots-outwit-humans-in-one-of-the-biggest-esports-games/). Wired.\\nArchived (https://web.archive.org/web/20180625213810/https://www.wired.com/story/can-bo\\nts-outwit-humans-in-one-of-the-biggest-esports-games/) from the original on June 25, 2018.\\nRetrieved June 25, 2018.\\n141. Kahn, Jeremy (June 25, 2018). \"A Bot Backed by Elon Musk Has Made an AI Breakthrough\\nin Video Game World\" (https://www.bloomberg.com/news/articles/2018-06-25/musk-backed-\\nbot-conquers-e-gamer-teams-in-ai-breakthrough). Bloomberg.com. Bloomberg L.P. Archived\\n(https://web.archive.org/web/20180627144300/https://www.bloomberg.com/news/articles/20\\n18-06-25/musk-backed-bot-conquers-e-gamer-teams-in-ai-breakthrough) from the original\\non June 27, 2018. Retrieved June 27, 2018.\\n142. Clifford, Catherine (June 28, 2018). \"Bill Gates says gamer bots from Elon Musk-backed\\nnonprofit are \\'huge milestone\\' in A.I.\" (https://www.cnbc.com/2018/06/27/bill-gates-openai-ro\\nbots-beating-humans-at-dota-2-is-ai-milestone.html) CNBC. Archived (https://web.archive.or\\ng/web/20180628231125/https://www.cnbc.com/2018/06/27/bill-gates-openai-robots-beating-\\nhumans-at-dota-2-is-ai-milestone.html) from the original on June 28, 2018. Retrieved\\nJune 29, 2018.\\n143. \"OpenAI Five Benchmark\" (https://blog.openai.com/openai-five-benchmark/).\\nblog.openai.com. July 18, 2018. Archived (https://web.archive.org/web/20190213165905/htt\\nps://blog.openai.com/openai-five-benchmark/) from the original on February 13, 2019.\\nRetrieved August 25, 2018.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 33, 'page_label': '34'}, page_content='144. Vincent, James (June 25, 2018). \"AI bots trained for 180 years a day to beat humans at\\nDota 2\" (https://www.theverge.com/2018/6/25/17492918/openai-dota-2-bot-ai-five-5v5-matc\\nhes). The Verge. Archived (https://web.archive.org/web/20180625183203/https://www.thever\\nge.com/2018/6/25/17492918/openai-dota-2-bot-ai-five-5v5-matches) from the original on\\nJune 25, 2018. Retrieved June 25, 2018.\\n145. Savov, Vlad (August 6, 2018). \"The OpenAI Dota 2 bots just defeated a team of former pros\"\\n(https://www.theverge.com/2018/8/6/17655086/dota2-openai-bots-professional-gaming-ai).\\nThe Verge. Archived (https://web.archive.org/web/20180807113227/https://www.theverge.co\\nm/2018/8/6/17655086/dota2-openai-bots-professional-gaming-ai) from the original on\\nAugust 7, 2018. Retrieved August 7, 2018.\\n146. Simonite, Tom. \"Pro Gamers Fend off Elon Musk-Backed AI Bots—for Now\" (https://www.wir\\ned.com/story/pro-gamers-fend-off-elon-musks-ai-bots/). Wired. Archived (https://web.archiv\\ne.org/web/20180824120523/https://www.wired.com/story/pro-gamers-fend-off-elon-musks-ai\\n-bots/) from the original on August 24, 2018. Retrieved August 25, 2018.\\n147. Quach, Katyanna. \"Game over, machines: Humans defeat OpenAI bots once again at video\\ngames Olympics\" (https://www.theregister.co.uk/2018/08/24/openai_bots_eliminated_dota_\\n2/). The Register. Archived (https://web.archive.org/web/20180825110329/https://www.there\\ngister.co.uk/2018/08/24/openai_bots_eliminated_dota_2/) from the original on August 25,\\n2018. Retrieved August 25, 2018.\\n148. \"The International 2018: Results\" (https://blog.openai.com/the-international-2018-results/).\\nblog.openai.com. August 24, 2018. Archived (https://web.archive.org/web/20180824131639/\\nhttps://blog.openai.com/the-international-2018-results/) from the original on August 24,\\n2018. Retrieved August 25, 2018.\\n149. Statt, Nick (April 13, 2019). \"OpenAI\\'s Dota 2 AI steamrolls world champion e-sports team\\nwith back-to-back victories\" (https://www.theverge.com/2019/4/13/18309459/openai-five-dot\\na-2-finals-ai-bot-competition-og-e-sports-the-international-champion). The Verge. Archived\\n(https://web.archive.org/web/20190415011925/https://www.theverge.com/2019/4/13/183094\\n59/openai-five-dota-2-finals-ai-bot-competition-og-e-sports-the-international-champion) from\\nthe original on April 15, 2019. Retrieved July 20, 2019.\\n150. \"How to Train Your OpenAI Five\" (https://openai.com/blog/how-to-train-your-openai-five/).\\nOpenAI Blog. April 15, 2019. Archived (https://web.archive.org/web/20190630013455/http\\ns://openai.com/blog/how-to-train-your-openai-five/) from the original on June 30, 2019.\\nRetrieved July 20, 2019.\\n151. Wiggers, Kyle (April 22, 2019). \"OpenAI\\'s Dota 2 bot defeated 99.4% of players in public\\nmatches\" (https://venturebeat.com/2019/04/22/openais-dota-2-bot-defeated-99-4-of-players-\\nin-public-matches/). Venture Beat. Archived (https://web.archive.org/web/20190711151127/\\nhttps://venturebeat.com/2019/04/22/openais-dota-2-bot-defeated-99-4-of-players-in-public-\\nmatches/) from the original on July 11, 2019. Retrieved April 22, 2019.\\n152. Fangasadha, Edbert Felix; Soeroredjo, Steffi; Anderies; Gunawan, Alexander Agung\\nSantoso (September 17, 2022). \"Literature Review of OpenAI Five\\'s Mechanisms in Dota\\n2\\'s Bot Player\". 2022 International Seminar on Application for Technology of Information and\\nCommunication (ISemantic) (https://ieeexplore.ieee.org/document/9920480). IEEE.\\npp. 183–190. doi:10.1109/iSemantic55962.2022.9920480 (https://doi.org/10.1109%2FiSem\\nantic55962.2022.9920480). ISBN 978-1-6654-8837-2. S2CID 253047170 (https://api.seman\\nticscholar.org/CorpusID:253047170). Archived (https://web.archive.org/web/2023051222413\\n9/https://ieeexplore.ieee.org/document/9920480) from the original on May 12, 2023.\\nRetrieved August 7, 2023.\\n153. Vincent, James (July 30, 2018). \"OpenAI sets new benchmark for robot dexterity\" (https://w\\nww.theverge.com/2018/7/30/17621112/openai-robot-dexterity-dactyl-artificial-intelligence).\\nThe Verge. Archived (https://web.archive.org/web/20230212152342/https://www.theverge.co\\nm/2018/7/30/17621112/openai-robot-dexterity-dactyl-artificial-intelligence) from the original\\non February 12, 2023. Retrieved February 12, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 34, 'page_label': '35'}, page_content='154. OpenAI; Andrychowicz, Marcin; Baker, Bowen; Chociej, Maciek; Józefowicz, Rafał; McGrew,\\nBob; Pachocki, Jakub; Petron, Arthur; Plappert, Matthias; Powell, Glenn; Ray, Alex;\\nSchneider, Jonas; Sidor, Szymon; Tobin, Josh; Welinder, Peter; Weng, Lilian; Zaremba,\\nWojciech (2019). \"Learning Dexterous In-Hand Manipulation\". arXiv:1808.00177v5 (https://a\\nrxiv.org/abs/1808.00177v5) [cs.LG (https://arxiv.org/archive/cs.LG)].\\n155. OpenAI; Akkaya, Ilge; Andrychowicz, Marcin; Chociej, Maciek; Litwin, Mateusz; McGrew,\\nBob; Petron, Arthur; Paino, Alex; Plappert, Matthias; Powell, Glenn; Ribas, Raphael (2019).\\n\"Solving Rubik\\'s Cube with a Robot Hand\". arXiv:1910.07113v1 (https://arxiv.org/abs/1910.0\\n7113v1) [cs.LG (https://arxiv.org/archive/cs.LG)].\\n156. \"OpenAI API\" (https://openai.com/blog/openai-api/). OpenAI. June 11, 2020. Archived (http\\ns://web.archive.org/web/20200611150951/https://openai.com/blog/openai-api/) from the\\noriginal on June 11, 2020. Retrieved June 14, 2020. \"Why did OpenAI choose to release an\\nAPI instead of open-sourcing the models?\\nThere are three main reasons we did this. First, commercializing the technology helps us\\npay for our ongoing AI research, safety, and policy efforts. Second, many of the models\\nunderlying the API are very large, taking a lot of expertise to develop and deploy and\\nmaking them very expensive to run. This makes it hard for anyone except larger companies\\nto benefit from the underlying technology. We\\'re hopeful that the API will make powerful AI\\nsystems more accessible to smaller businesses and organizations. Third, the API model\\nallows us to more easily respond to misuse of the technology. Since it is hard to predict the\\ndownstream use cases of our models, it feels inherently safer to release them via an API\\nand broaden access over time, rather than release an open source model where access\\ncannot be adjusted if it turns out to have harmful applications.\"\\n157. \"TechCrunch Startup and Technology News\" (https://techcrunch.com/2020/06/11/openai-ma\\nkes-an-all-purpose-api-for-its-text-based-ai-capabilities/). TechCrunch. June 11, 2020.\\nArchived (https://web.archive.org/web/20200612041855/https://techcrunch.com/2020/06/11/\\nopenai-makes-an-all-purpose-api-for-its-text-based-ai-capabilities/) from the original on June\\n12, 2020. Retrieved June 11, 2020. \"If you\\'ve ever wanted to try out OpenAI\\'s vaunted\\nmachine learning toolset, it just got a lot easier. The company has released an API that lets\\ndevelopers call its AI tools in on \"virtually any English language task.\"\"\\n158. \"GPT-1 to GPT-4: Each of OpenAI\\'s GPT Models Explained and Compared\" (https://www.m\\nakeuseof.com/gpt-models-explained-and-compared/). April 11, 2023. Archived (https://web.a\\nrchive.org/web/20230415175013/https://www.makeuseof.com/gpt-models-explained-and-co\\nmpared/) from the original on April 15, 2023. Retrieved April 29, 2023.\\n159. \"Improving Language Understanding by Generative Pre-Training\" (https://cdn.openai.com/re\\nsearch-covers/language-unsupervised/language_understanding_paper.pdf) (PDF). Archived\\n(https://web.archive.org/web/20210126024542/https://cdn.openai.com/research-covers/lang\\nuage-unsupervised/language_understanding_paper.pdf) (PDF) from the original on January\\n26, 2021. Retrieved June 9, 2020.\\n160. Hern, Alex (February 14, 2019). \"New AI fake text generator may be too dangerous to\\nrelease, say creators\" (https://www.theguardian.com/technology/2019/feb/14/elon-musk-bac\\nked-ai-writes-convincing-news-fiction). The Guardian. Archived (https://web.archive.org/web/\\n20190214173112/https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-\\nai-writes-convincing-news-fiction) from the original on February 14, 2019. Retrieved\\nFebruary 14, 2019.\\n161. Schwartz, Oscar (July 4, 2019). \"Could \\'fake text\\' be the next global political threat?\" (https://\\nwww.theguardian.com/technology/2019/jul/04/ai-fake-text-gpt-2-concerns-false-information).\\nThe Guardian. Archived (https://web.archive.org/web/20190716035703/https://www.theguar\\ndian.com/technology/2019/jul/04/ai-fake-text-gpt-2-concerns-false-information) from the\\noriginal on July 16, 2019. Retrieved July 16, 2019.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 35, 'page_label': '36'}, page_content='162. Vincent, James (February 14, 2019). \"OpenAI\\'s new multitalented AI writes, translates, and\\nslanders\" (https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-m\\nodels-read-write-openai-gpt2). The Verge. Archived (https://web.archive.org/web/202012180\\n91707/https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-model\\ns-read-write-openai-gpt2) from the original on December 18, 2020. Retrieved July 16, 2019.\\n163. \"GPT-2: 1.5B Release\" (https://openai.com/blog/gpt-2-1-5b-release/). OpenAI. November 5,\\n2019. Archived (https://web.archive.org/web/20191114074358/https://openai.com/blog/gpt-2\\n-1-5b-release/) from the original on November 14, 2019. Retrieved November 14, 2019.\\n164. \"Write With Transformer\" (https://transformer.huggingface.co/). Archived (https://web.archiv\\ne.org/web/20191204060111/https://transformer.huggingface.co/) from the original on\\nDecember 4, 2019. Retrieved December 4, 2019.\\n165. \"Talk to Transformer\" (https://talktotransformer.com/). Archived (https://web.archive.org/web/\\n20191204015009/https://talktotransformer.com/) from the original on December 4, 2019.\\nRetrieved December 4, 2019.\\n166. \"CreativeEngines\" (https://creativeengines.ai/). Archived (https://web.archive.org/web/20230\\n203201104/https://creativeengines.ai/) from the original on February 3, 2023. Retrieved\\nJune 25, 2021.\\n167. Language Models are Unsupervised Multitask Learners (https://d4mucfpksywv.cloudfront.ne\\nt/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\\n(PDF), archived (https://web.archive.org/web/20191212223916/https://d4mucfpksywv.cloudf\\nront.net/better-language-models/language_models_are_unsupervised_multitask_learners.p\\ndf) (PDF) from the original on December 12, 2019, retrieved December 4, 2019\\n168. \"openai/gpt-3\" (https://github.com/openai/gpt-3). OpenAI. May 29, 2020. Archived (https://we\\nb.archive.org/web/20201114165742/https://github.com/openai/gpt-3) from the original on\\nNovember 14, 2020. Retrieved May 29, 2020.\\n169. Sagar, Ram (June 3, 2020). \"OpenAI Releases GPT-3, The Largest Model So Far\" (https://a\\nnalyticsindiamag.com/open-ai-gpt-3-language-model/). Analytics India Magazine. Archived\\n(https://web.archive.org/web/20200804173452/https://analyticsindiamag.com/open-ai-gpt-3-l\\nanguage-model/) from the original on August 4, 2020. Retrieved June 14, 2020.\\n170. Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal,\\nPrafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal,\\nSandhini (June 1, 2020). \"Language Models are Few-Shot Learners\". p. appendix.\\narXiv:2005.14165 (https://arxiv.org/abs/2005.14165) [cs.CL (https://arxiv.org/archive/cs.CL)].\\n171. Language Models are Unsupervised Multitask Learners (https://d4mucfpksywv.cloudfront.ne\\nt/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\\n(PDF), archived (https://web.archive.org/web/20191212223916/https://d4mucfpksywv.cloudf\\nront.net/better-language-models/language_models_are_unsupervised_multitask_learners.p\\ndf) (PDF) from the original on December 12, 2019, retrieved December 4, 2019, \"GPT-2, is\\na 1.5B parameter Transformer\"\\n172. Brown, Tom; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal,\\nPrafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal,\\nSandhini (June 1, 2020). \"Language Models are Few-Shot Learners\". arXiv:2005.14165 (htt\\nps://arxiv.org/abs/2005.14165) [cs.CL (https://arxiv.org/archive/cs.CL)]. \"Since we increase\\nthe capacity by over two orders of magnitude from GPT-2 to GPT-3\"\\n173. Ray, Tiernan (2020). \"OpenAI\\'s gigantic GPT-3 hints at the limits of language models for AI\"\\n(https://www.zdnet.com/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models\\n-for-ai/). ZDNet. Archived (https://web.archive.org/web/20200601081629/https://www.zdnet.c\\nom/article/openais-gigantic-gpt-3-hints-at-the-limits-of-language-models-for-ai/) from the\\noriginal on June 1, 2020. Retrieved June 5, 2020.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 36, 'page_label': '37'}, page_content='174. Amodei, Dario; Hernandez, Danny (May 16, 2018). \"AI and Compute\" (https://openai.com/bl\\nog/ai-and-compute/#fn2). Archived (https://web.archive.org/web/20200617200602/https://op\\nenai.com/blog/ai-and-compute/#fn2) from the original on June 17, 2020. Retrieved\\nAugust 30, 2020. \"A petaflop/s-day (pfs-day) consists of performing 1015 neural net\\noperations per second for one day, or a total of about 1020 operations. The compute-time\\nproduct serves as a mental convenience, similar to kW-hr for energy.\"\\n175. Eadicicco, Lisa. \"The artificial intelligence company that Elon Musk helped found is now\\nselling the text-generation software it previously said was too dangerous to launch\" (https://\\nwww.businessinsider.com/elon-musk-openai-sell-text-tool-it-said-was-dangerous-2020-6).\\nBusiness Insider. Archived (https://web.archive.org/web/20201114205255/https://www.busin\\nessinsider.com/elon-musk-openai-sell-text-tool-it-said-was-dangerous-2020-6) from the\\noriginal on November 14, 2020. Retrieved July 6, 2020.\\n176. \"OpenAI is giving Microsoft exclusive access to its GPT-3 language model\" (https://www.tec\\nhnologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-\\ngpt-3-language-model/). MIT Technology Review. Archived (https://web.archive.org/web/202\\n10205121656/https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-micr\\nosoft-exclusive-access-to-its-gpt-3-language-model/) from the original on February 5, 2021.\\nRetrieved September 24, 2020.\\n177. \"Microsoft gets exclusive license for OpenAI\\'s GPT-3 language model\" (https://venturebeat.c\\nom/2020/09/22/microsoft-gets-exclusive-license-for-openais-gpt-3-language-model/).\\nVentureBeat. September 22, 2020. Archived (https://web.archive.org/web/20201108090524/\\nhttps://venturebeat.com/2020/09/22/microsoft-gets-exclusive-license-for-openais-gpt-3-lang\\nuage-model/) from the original on November 8, 2020. Retrieved September 24, 2020.\\n178. Alford, Anthony (August 31, 2021). \"OpenAI Announces 12 Billion Parameter Code-\\nGeneration AI Codex\" (https://www.infoq.com/news/2021/08/openai-codex/). InfoQ. Archived\\n(https://web.archive.org/web/20220709221205/https://www.infoq.com/news/2021/08/openai-\\ncodex/) from the original on July 9, 2022. Retrieved September 3, 2021.\\n179. Wiggers, Kyle (July 8, 2021). \"OpenAI warns AI behind GitHub\\'s Copilot may be susceptible\\nto bias\" (https://venturebeat.com/2021/07/08/openai-warns-ai-behind-githubs-copilot-may-be\\n-susceptible-to-bias/). VentureBeat. Archived (https://web.archive.org/web/2023020320191\\n2/https://venturebeat.com/business/openai-warns-ai-behind-githubs-copilot-may-be-suscepti\\nble-to-bias/) from the original on February 3, 2023. Retrieved September 3, 2021.\\n180. Zaremba, Wojciech (August 10, 2021). \"OpenAI Codex\" (https://openai.com/blog/openai-cod\\nex/). OpenAI. Archived (https://web.archive.org/web/20230203201912/https://openai.com/bl\\nog/openai-codex/) from the original on February 3, 2023. Retrieved September 3, 2021.\\n181. Dickson, Ben (August 16, 2021). \"What to expect from OpenAI\\'s Codex API\" (https://venture\\nbeat.com/2021/08/16/what-to-expect-from-openais-codex-api/). VentureBeat. Archived (http\\ns://web.archive.org/web/20230203201913/https://venturebeat.com/ai/what-to-expect-from-o\\npenais-codex-api/) from the original on February 3, 2023. Retrieved September 3, 2021.\\n182. Claburn, Thomas (August 25, 2021). \"GitHub\\'s Copilot may steer you into dangerous waters\\nabout 40% of the time – study\" (https://www.theregister.com/2021/08/25/github_copilot_stud\\ny/). The Register. Archived (https://web.archive.org/web/20230203201913/https://www.there\\ngister.com/2021/08/25/github_copilot_study/) from the original on February 3, 2023.\\nRetrieved September 3, 2021.\\n183. \"GitHub Copilot: The Latest in the List of AI Generative Models Facing Copyright\\nAllegations\" (https://analyticsindiamag.com/github-copilot-the-latest-in-the-list-of-ai-generativ\\ne-models-facing-copyright-allegations/). Analytics India Magazine. October 23, 2022.\\nArchived (https://web.archive.org/web/20230322031738/https://analyticsindiamag.com/githu\\nb-copilot-the-latest-in-the-list-of-ai-generative-models-facing-copyright-allegations/) from the\\noriginal on March 22, 2023. Retrieved March 23, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 37, 'page_label': '38'}, page_content='184. \"OpenAI Might Invite Legal Trouble\" (https://analyticsindiamag.com/openai-might-invite-legal\\n-trouble/). Analytics India Magazine. March 21, 2023. Archived (https://web.archive.org/web/\\n20230323205407/https://analyticsindiamag.com/openai-might-invite-legal-trouble/) from the\\noriginal on March 23, 2023. Retrieved March 23, 2023.\\n185. Vincent, James (March 14, 2023). \"OpenAI announces GPT-4—the next generation of its AI\\nlanguage model\" (https://www.theverge.com/2023/3/14/23638033/openai-gpt-4-chatgpt-mult\\nimodal-deep-learning). The Verge. Archived (https://web.archive.org/web/20230314195326/\\nhttps://www.theverge.com/2023/3/14/23638033/openai-gpt-4-chatgpt-multimodal-deep-learn\\ning) from the original on March 14, 2023. Retrieved March 14, 2023.\\n186. Wiggers, Kyle (March 14, 2023). \"OpenAI releases GPT-4, a multimodal AI that it claims is\\nstate-of-the-art\" (https://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-i\\ns-state-of-the-art/). TechCrunch. Archived (https://web.archive.org/web/20230315003723/htt\\nps://techcrunch.com/2023/03/14/openai-releases-gpt-4-ai-that-it-claims-is-state-of-the-art/)\\nfrom the original on March 15, 2023. Retrieved March 14, 2023.\\n187. Belfield, Haydn (March 25, 2023). \"If your AI model is going to sell, it has to be safe\" (https://\\nwww.vox.com/future-perfect/2023/3/25/23655082/ai-openai-gpt-4-safety-microsoft-facebook\\n-meta). Vox. Archived (https://web.archive.org/web/20230328192017/https://www.vox.com/f\\nuture-perfect/2023/3/25/23655082/ai-openai-gpt-4-safety-microsoft-facebook-meta) from the\\noriginal on March 28, 2023. Retrieved March 30, 2023.\\n188. Roose, Kevin (September 28, 2023). \"The New ChatGPT Can \\'See\\' and \\'Talk.\\' Here\\'s What\\nIt\\'s Like\" (https://www.nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html).\\nThe New York Times. Archived (https://web.archive.org/web/20231031055345/https://www.n\\nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html) from the original on\\nOctober 31, 2023. Retrieved December 1, 2023.\\n189. Vincent, James (March 15, 2023). \"OpenAI co-founder on company\\'s past approach to\\nopenly sharing research: \"We were wrong\"\" (https://www.theverge.com/2023/3/15/2364018\\n0/openai-gpt-4-launch-closed-research-ilya-sutskever-interview). The Verge. Archived (http\\ns://web.archive.org/web/20230317210900/https://www.theverge.com/2023/3/15/23640180/o\\npenai-gpt-4-launch-closed-research-ilya-sutskever-interview) from the original on March 17,\\n2023. Retrieved March 18, 2023.\\n190. Wiggers, Kyle (May 13, 2024). \"OpenAI debuts GPT-4o \\'omni\\' model now powering\\nChatGPT\" (https://techcrunch.com/2024/05/13/openais-newest-model-is-gpt-4o/).\\nTechCrunch. Archived (https://web.archive.org/web/20240522094111/https://techcrunch.co\\nm/2024/05/13/openais-newest-model-is-gpt-4o/) from the original on May 22, 2024.\\nRetrieved May 13, 2024.\\n191. van Rijmenam, Mark (May 13, 2024). \"OpenAI Launched GPT-4o: The Future of AI\\nInteractions Is Here\" (https://www.thedigitalspeaker.com/openai-gpt4o-future-ai-interaction\\ns/). The Digital Speaker. Archived (https://web.archive.org/web/20240517013424/https://ww\\nw.thedigitalspeaker.com/openai-gpt4o-future-ai-interactions/) from the original on May 17,\\n2024. Retrieved May 17, 2024.\\n192. Daws, Ryan (May 14, 2024). \"GPT-4o delivers human-like AI interaction with text, audio,\\nand vision integration\" (https://www.artificialintelligence-news.com/2024/05/14/gpt-4o-human\\n-like-ai-interaction-text-audio-vision-integration/). AI News. Archived (https://web.archive.org/\\nweb/20240518131148/https://www.artificialintelligence-news.com/2024/05/14/gpt-4o-human\\n-like-ai-interaction-text-audio-vision-integration/) from the original on May 18, 2024.\\nRetrieved May 18, 2024.\\n193. \"Hello GPT-4o\" (https://openai.com/index/hello-gpt-4o/). OpenAI. Archived (https://web.archi\\nve.org/web/20240514024319/https://openai.com/index/hello-gpt-4o/) from the original on\\nMay 14, 2024. Retrieved July 14, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 38, 'page_label': '39'}, page_content='194. Franzen, Carl (July 18, 2024). \"OpenAI unveils GPT-4o mini — a smaller, much cheaper\\nmultimodal AI model\" (https://venturebeat.com/ai/openai-unveils-gpt-4o-mini-a-smaller-much\\n-cheaper-multimodal-ai-model/). VentureBeat. Archived (https://web.archive.org/web/202407\\n18185315/https://venturebeat.com/ai/openai-unveils-gpt-4o-mini-a-smaller-much-cheaper-m\\nultimodal-ai-model/) from the original on July 18, 2024. Retrieved July 18, 2024.\\n195. Knight, Will. \"OpenAI Announces a New AI Model, Code-Named Strawberry, That Solves\\nDifficult Problems Step by Step\" (https://www.wired.com/story/openai-o1-strawberry-problem\\n-reasoning/). Wired. ISSN 1059-1028 (https://search.worldcat.org/issn/1059-1028). Archived\\n(https://web.archive.org/web/20240914005013/https://www.wired.com/story/openai-o1-straw\\nberry-problem-reasoning/) from the original on September 14, 2024. Retrieved\\nSeptember 14, 2024.\\n196. Robison, Kylie (September 12, 2024). \"OpenAI releases o1, its first model with \\'reasoning\\'\\nabilities\" (https://www.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-straw\\nberry-chatgpt). The Verge. Archived (https://web.archive.org/web/20240913134303/https://w\\nww.theverge.com/2024/9/12/24242439/openai-o1-model-reasoning-strawberry-chatgpt)\\nfrom the original on September 13, 2024. Retrieved September 17, 2024.\\n197. \"CLIP: Connecting Text and Images\" (https://openai.com/blog/clip/). January 5, 2021.\\nArchived (https://web.archive.org/web/20210325202038/https://openai.com/blog/clip/) from\\nthe original on March 25, 2021. Retrieved March 27, 2021.\\n198. \"DALL·E: Creating Images from Text\" (https://openai.com/blog/dall-e/). January 5, 2021.\\nArchived (https://web.archive.org/web/20210327133043/https://openai.com/blog/dall-e/)\\nfrom the original on March 27, 2021. Retrieved March 27, 2021.\\n199. \"DALL·E 2\" (https://openai.com/dall-e-2/). OpenAI. Archived (https://web.archive.org/web/20\\n220406141035/https://openai.com/dall-e-2/) from the original on April 6, 2022. Retrieved\\nApril 6, 2022.\\n200. \"ChatGPT: A scientist explains the hidden genius and pitfalls of OpenAI\\'s chatbot\" (https://w\\nww.sciencefocus.com/news/chatgpt-scientist-openai-chatbot/). BBC Science Focus\\nMagazine. 2022. Archived (https://web.archive.org/web/20230203201910/https://www.scien\\ncefocus.com/news/chatgpt-scientist-openai-chatbot/) from the original on February 3, 2023.\\nRetrieved December 30, 2022.\\n201. \"OpenAI\\'s new AI image generator pushes the limits in detail and prompt fidelity\" (https://we\\nb.archive.org/web/20231116152507/https://arstechnica.com/information-technology/2023/0\\n9/openai-announces-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/). Archived\\nfrom the original (https://arstechnica.com/information-technology/2023/09/openai-announce\\ns-dall-e-3-a-next-gen-ai-image-generator-based-on-chatgpt/) on November 16, 2023.\\nRetrieved November 21, 2023.\\n202. \"DALL·E 3 is now available in ChatGPT Plus and Enterprise\" (https://web.archive.org/web/2\\n0231120155249/https://openai.com/blog/dall-e-3-is-now-available-in-chatgpt-plus-and-enter\\nprise). Archived from the original (https://openai.com/blog/dall-e-3-is-now-available-in-chatg\\npt-plus-and-enterprise) on November 20, 2023. Retrieved November 21, 2023.\\n203. Metz, Cade (February 15, 2024). \"OpenAI Unveils A.I. That Instantly Generates Eye-\\nPopping Videos\" (https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.htm\\nl). The New York Times. Archived (https://web.archive.org/web/20240215220626/https://ww\\nw.nytimes.com/2024/02/15/technology/openai-sora-videos.html) from the original on\\nFebruary 15, 2024. Retrieved February 16, 2024.\\n204. \"Video generation models as world simulators\" (https://openai.com/research/video-generatio\\nn-models-as-world-simulators). OpenAI. February 15, 2024. Archived (https://web.archive.or\\ng/web/20240216072133/https://openai.com/research/video-generation-models-as-world-sim\\nulators) from the original on February 16, 2024. Retrieved February 16, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 39, 'page_label': '40'}, page_content='205. Brooks, Tim; Peebles, Bill; Holmes, Connor; DePue, Will; Guo, Yufei; Jing, Li; Schnurr,\\nDavid; Taylor, Joe; Luhman, Troy; Luhman, Eric; Ng, Clarence Wing Yin; Wang, Ricky;\\nRamesh, Aditya (February 15, 2024). \"Video generation models as world simulators\" (http\\ns://openai.com/research/video-generation-models-as-world-simulators). Openai.com.\\nOpenAI. Archived (https://web.archive.org/web/20240216072133/https://openai.com/researc\\nh/video-generation-models-as-world-simulators) from the original on February 16, 2024.\\nRetrieved February 16, 2024.\\n206. Pequeño IV, Antonio (February 15, 2024). \"OpenAI Reveals \\'Sora\\': AI Video Model Capable\\nOf Realistic Text-To-Video Prompts\" (https://www.forbes.com/sites/antoniopequenoiv/2024/0\\n2/15/openai-reveals-sora-ai-video-model-capable-of-realistic-text-to-video-prompts/).\\nForbes. Archived (https://web.archive.org/web/20240215220634/https://www.forbes.com/site\\ns/antoniopequenoiv/2024/02/15/openai-reveals-sora-ai-video-model-capable-of-realistic-text\\n-to-video-prompts/) from the original on February 15, 2024. Retrieved February 16, 2024.\\n207. Clark, Elijah. \"Tyler Perry Warns Of AI Threat After Sora Debut Halts An $800 Million Studio\\nExpansion\" (https://www.forbes.com/sites/elijahclark/2024/02/23/tyler-perry-warns-of-ai-thre\\nat-to-jobs-after-viewing-openai-sora/). Forbes. Retrieved March 24, 2024.\\n208. Wiggers, Kyle (September 21, 2022). \"OpenAI open-sources Whisper, a multilingual speech\\nrecognition system\" (https://techcrunch.com/2022/09/21/openai-open-sources-whisper-a-mu\\nltilingual-speech-recognition-system/). TechCrunch. Archived (https://web.archive.org/web/2\\n0230212154543/https://techcrunch.com/2022/09/21/openai-open-sources-whisper-a-multilin\\ngual-speech-recognition-system/) from the original on February 12, 2023. Retrieved\\nFebruary 12, 2023.\\n209. Radford, Alec; Kim, Jong Wook; Xu, Tao; Brockman, Greg; McLeavey, Christine; Sutskever,\\nIlya (2022). \"Robust Speech Recognition via Large-Scale Weak Supervision\".\\narXiv:2212.04356 (https://arxiv.org/abs/2212.04356) [eess.AS (https://arxiv.org/archive/eess.\\nAS)].\\n210. \"OpenAI\\'s MuseNet generates AI music at the push of a button\" (https://www.theverge.com/\\n2019/4/26/18517803/openai-musenet-artificial-intelligence-ai-music-generation-lady-gaga-h\\narry-potter-mozart). The Verge. April 2019. Archived (https://web.archive.org/web/20190628\\n164236/https://www.theverge.com/2019/4/26/18517803/openai-musenet-artificial-intelligenc\\ne-ai-music-generation-lady-gaga-harry-potter-mozart) from the original on June 28, 2019.\\nRetrieved June 8, 2020.\\n211. \"MuseNet\" (https://openai.com/blog/musenet/). OpenAI. April 25, 2019. Archived (https://we\\nb.archive.org/web/20200613055143/https://openai.com/blog/musenet/) from the original on\\nJune 13, 2020. Retrieved June 8, 2020.\\n212. \"Arcade Attack Podcast – September (4 of 4) 2020 - Alex Hall (Ben Drowned) - Interview\" (h\\nttps://www.arcadeattack.co.uk/podcast-september-4-2020/). Arcade Attack. September 28,\\n2020. Archived (https://web.archive.org/web/20230203201108/https://www.arcadeattack.co.\\nuk/podcast-september-4-2020/) from the original on February 3, 2023. Retrieved\\nJanuary 29, 2023.\\n213. \"Archived copy\" (https://mobile.twitter.com/alexanderdhall/status/1276186528264392707).\\nArchived (https://web.archive.org/web/20230203201107/https://mobile.twitter.com/alexander\\ndhall/status/1276186528264392707) from the original on February 3, 2023. Retrieved\\nJanuary 29, 2023.\\n214. \"OpenAI introduces Jukebox, a new AI model that generates genre-specific music\" (https://w\\nww.theverge.com/2020/4/30/21243038/openai-jukebox-model-raw-audio-lyrics-ai-generated\\n-copyright). The Verge. April 30, 2020. Archived (https://web.archive.org/web/202006080436\\n59/https://www.theverge.com/2020/4/30/21243038/openai-jukebox-model-raw-audio-lyrics-a\\ni-generated-copyright) from the original on June 8, 2020. Retrieved June 8, 2020.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 40, 'page_label': '41'}, page_content='215. Stephen, Bijan (April 30, 2020). \"OpenAI introduces Jukebox, a new AI model that\\ngenerates genre-specific music\" (https://www.businessinsider.com/jukebox-ai-music-generat\\nor-realistic-songs-machine-learning-algorithm-deepfakes-2020-5). Business Insider.\\nArchived (https://web.archive.org/web/20200608043703/https://www.businessinsider.com/ju\\nkebox-ai-music-generator-realistic-songs-machine-learning-algorithm-deepfakes-2020-5)\\nfrom the original on June 8, 2020. Retrieved June 8, 2020.\\n216. \"Jukebox\" (https://openai.com/blog/jukebox/). OpenAI. April 30, 2020. Archived (https://web.\\narchive.org/web/20200608043703/https://openai.com/blog/jukebox/) from the original on\\nJune 8, 2020. Retrieved June 8, 2020.\\n217. Greene, Tristan (May 4, 2018). \"OpenAI\\'s Debate Game teaches you and your friends how\\nto lie like robots\" (https://thenextweb.com/artificial-intelligence/2018/05/04/openais-debate-g\\name-teaches-you-and-your-friends-how-to-lie-like-robots/). The Next Web. Archived (https://\\nweb.archive.org/web/20180505005129/https://thenextweb.com/artificial-intelligence/2018/0\\n5/04/openais-debate-game-teaches-you-and-your-friends-how-to-lie-like-robots/) from the\\noriginal on May 5, 2018. Retrieved May 31, 2018.\\n218. \"Why Scientists Think AI Systems Should Debate Each Other\" (https://www.fastcompany.co\\nm/40569116/why-scientists-think-ai-systems-should-debate-each-other). Fast Company.\\nMay 8, 2018. Archived (https://web.archive.org/web/20180519140829/https://www.fastcomp\\nany.com/40569116/why-scientists-think-ai-systems-should-debate-each-other) from the\\noriginal on May 19, 2018. Retrieved June 2, 2018.\\n219. \"OpenAI Microscope\" (https://openai.com/blog/microscope/). April 14, 2020. Archived (http\\ns://web.archive.org/web/20230203201911/https://openai.com/blog/microscope/) from the\\noriginal on February 3, 2023. Retrieved March 27, 2021.\\n220. Johnson, Khari (April 14, 2020). \"OpenAI launches Microscope to visualize the neurons in\\npopular machine learning models\" (https://venturebeat.com/ai/openai-launches-microscope-\\nto-visualize-the-neurons-in-popular-machine-learning-models/). VentureBeat. Archived (http\\ns://web.archive.org/web/20230212154926/https://venturebeat.com/ai/openai-launches-micro\\nscope-to-visualize-the-neurons-in-popular-machine-learning-models/) from the original on\\nFebruary 12, 2023. Retrieved February 12, 2023.\\n221. \"OpenAI Microscope\" (https://microscope.openai.com/models). OpenAI Microscope.\\nArchived (https://web.archive.org/web/20230203191623/https://microscope.openai.com/mod\\nels) from the original on February 3, 2023. Retrieved March 27, 2021.\\n222. \"Mira Murati via Twitter\" (https://twitter.com/miramurati/status/1599796191243669504). Mira\\nMurati. December 5, 2022. Archived (https://web.archive.org/web/20221214090229/https://t\\nwitter.com/miramurati/status/1599796191243669504) from the original on December 14,\\n2022. Retrieved December 15, 2022.\\n223. \"Pricing\" (https://openai.com/pricing). OpenAI. Archived (https://web.archive.org/web/20230\\n320175036/https://openai.com/pricing) from the original on March 20, 2023. Retrieved\\nMarch 20, 2023.\\n224. Edwards, Benj (March 14, 2023). \"OpenAI\\'s GPT-4 exhibits \"human-level performance\" on\\nprofessional benchmarks\" (https://arstechnica.com/information-technology/2023/03/openai-a\\nnnounces-gpt-4-its-next-generation-ai-language-model/). Ars Technica. Archived (https://we\\nb.archive.org/web/20230314225236/https://arstechnica.com/information-technology/2023/0\\n3/openai-announces-gpt-4-its-next-generation-ai-language-model/) from the original on\\nMarch 14, 2023. Retrieved March 15, 2023.\\n225. OpenAI (February 1, 2023). \"Introducing ChatGPT Plus\" (https://openai.com/blog/chatgpt-pl\\nus). OpenAI Blog. Archived (https://web.archive.org/web/20230320005603/https://openai.co\\nm/blog/chatgpt-plus) from the original on March 20, 2023. Retrieved March 20, 2023.\\n226. OpenAI. \"OpenAI API\" (https://platform.openai.com/). platform.openai.com. Archived (https://\\nweb.archive.org/web/20230320023933/https://platform.openai.com/) from the original on\\nMarch 20, 2023. Retrieved March 20, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 41, 'page_label': '42'}, page_content='227. OpenAI. \"GPT-4 API waitlist\" (https://openai.com/waitlist/gpt-4-api). openai.com. Archived (h\\nttps://web.archive.org/web/20230320174149/https://openai.com/waitlist/gpt-4-api) from the\\noriginal on March 20, 2023. Retrieved March 20, 2023.\\n228. Wiggers, Kyle (February 1, 2023). \"OpenAI launches ChatGPT Plus, starting at $20 per\\nmonth\" (https://techcrunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-\\nmonth/). TechCrunch. Archived (https://web.archive.org/web/20230212090719/https://techcr\\nunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-month/) from the\\noriginal on February 12, 2023. Retrieved February 12, 2023.\\n229. Lawler, Richard (July 25, 2023). \"ChatGPT for Android is now available\" (https://www.thever\\nge.com/2023/7/25/23807012/chatgpt-android-ai-chatbot-openai-llm). The Verge. Archived (h\\nttps://web.archive.org/web/20230816172328/https://www.theverge.com/2023/7/25/2380701\\n2/chatgpt-android-ai-chatbot-openai-llm) from the original on August 16, 2023. Retrieved\\nAugust 17, 2023.\\n230. \"Introducing the ChatGPT app for iOS\" (https://openai.com/blog/introducing-the-chatgpt-app\\n-for-ios). openai.com. Archived (https://web.archive.org/web/20230605064406/https://opena\\ni.com/blog/introducing-the-chatgpt-app-for-ios) from the original on June 5, 2023. Retrieved\\nAugust 17, 2023.\\n231. \"ChatGPT Android app FAQ | OpenAI Help Center\" (https://help.openai.com/en/articles/814\\n2208-chatgpt-android-app-faq). help.openai.com. Archived (https://web.archive.org/web/202\\n30817093628/https://help.openai.com/en/articles/8142208-chatgpt-android-app-faq) from\\nthe original on August 17, 2023. Retrieved August 17, 2023.\\n232. \"ChatGPT can now see, hear, and speak\" (https://openai.com/blog/chatgpt-can-now-see-he\\nar-and-speak). openai.com. Archived (https://web.archive.org/web/20231107080728/https://\\nopenai.com/blog/chatgpt-can-now-see-hear-and-speak) from the original on November 7,\\n2023. Retrieved October 16, 2023.\\n233. Roose, Kevin (September 27, 2023). \"The New ChatGPT Can \\'See\\' and \\'Talk.\\' Here\\'s What\\nIt\\'s Like\" (https://www.nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html).\\nThe New York Times. Archived (https://web.archive.org/web/20231031055345/https://www.n\\nytimes.com/2023/09/27/technology/new-chatgpt-can-see-hear.html) from the original on\\nOctober 31, 2023. Retrieved October 16, 2023 – via NYTimes.com.\\n234. David, Emilia (September 20, 2023). \"OpenAI releases third version of DALL-E\" (https://ww\\nw.theverge.com/2023/9/20/23881241/openai-dalle-third-version-generative-ai). The Verge.\\nArchived (https://web.archive.org/web/20230920192429/https://www.theverge.com/2023/9/2\\n0/23881241/openai-dalle-third-version-generative-ai) from the original on September 20,\\n2023. Retrieved September 23, 2023.\\n235. Metz, Cade; Hsu, Tiffany (September 20, 2023). \"ChatGPT Can Now Generate Images,\\nToo\" (https://www.nytimes.com/2023/09/20/technology/chatgpt-dalle3-images-openai.html).\\nThe New York Times. ISSN 0362-4331 (https://search.worldcat.org/issn/0362-4331).\\nArchived (https://web.archive.org/web/20230923154712/https://www.nytimes.com/2023/09/2\\n0/technology/chatgpt-dalle3-images-openai.html) from the original on September 23, 2023.\\nRetrieved September 23, 2023.\\n236. Fried, Ina (December 1, 2023). \"Scoop: OpenAI delays launch of custom GPT store until\\nearly 2024\" (https://www.axios.com/2023/12/01/openai-delays-launch-custom-gpt-store-202\\n4). Axios. Archived (https://web.archive.org/web/20231203003716/https://www.axios.com/20\\n23/12/01/openai-delays-launch-custom-gpt-store-2024) from the original on December 3,\\n2023. Retrieved December 3, 2023.\\n237. Halper, Evan (September 20, 2024). \"Microsoft deal would reopen Three Mile Island nuclear\\nplant to power AI\" (https://www.washingtonpost.com/business/2024/09/20/microsoft-three-mi\\nle-island-nuclear-constellation/). Washington Post. ISSN 0190-8286 (https://search.worldcat.\\norg/issn/0190-8286). Retrieved October 7, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 42, 'page_label': '43'}, page_content='238. Robison, Kylie (July 25, 2024). \"OpenAI announces SearchGPT, its AI-powered search\\nengine\" (https://www.theverge.com/2024/7/25/24205701/openai-searchgpt-ai-search-engine\\n-google-perplexity-rival). The Verge. Archived (https://web.archive.org/web/2024072618005\\n0/https://www.theverge.com/2024/7/25/24205701/openai-searchgpt-ai-search-engine-googl\\ne-perplexity-rival) from the original on July 26, 2024. Retrieved July 27, 2024.\\n239. Wiggers, Kyle (July 25, 2024). \"With Google in its sights, OpenAI unveils SearchGPT\" (http\\ns://techcrunch.com/2024/07/25/with-google-in-its-sights-openai-unveils-searchgpt/).\\nTechCrunch. Archived (https://web.archive.org/web/20240726010010/https://techcrunch.co\\nm/2024/07/25/with-google-in-its-sights-openai-unveils-searchgpt/) from the original on July\\n26, 2024. Retrieved July 26, 2024.\\n240. Bajwa, Simao, Gregorio, Arsheeya, Paul, David (March 29, 2024). \"Microsoft, OpenAI plan\\n$100 billion data-center project, media report says\" (https://www.reuters.com/technology/mic\\nrosoft-openai-planning-100-billion-data-center-project-information-reports-2024-03-29/).\\nReuters. Archived (https://web.archive.org/web/20240620203250/https://www.reuters.com/te\\nchnology/microsoft-openai-planning-100-billion-data-center-project-information-reports-2024\\n-03-29/) from the original on June 20, 2024. Retrieved June 6, 2024.\\n241. \"Microsoft and OpenAI Plot $100 Billion Stargate AI Supercomputer\" (https://www.theinform\\nation.com/articles/microsoft-and-openai-plot-100-billion-stargate-ai-supercomputer). The\\nInformation. March 29, 2024. Retrieved June 6, 2024.\\n242. \"OpenAI announces leadership transition\" (https://openai.com/blog/openai-announces-leade\\nrship-transition). openai.com. Archived (https://web.archive.org/web/20231117212221/http\\ns://openai.com/blog/openai-announces-leadership-transition) from the original on November\\n17, 2023. Retrieved November 17, 2023.\\n243. Montgomery, Blake; Anguiano, Dani (November 17, 2023). \"OpenAI fires co-founder and\\nCEO Sam Altman for allegedly lying to company board\" (https://www.theguardian.com/techn\\nology/2023/nov/17/openai-ceo-sam-altman-fired). The Guardian. Archived (https://web.archi\\nve.org/web/20231117210649/https://www.theguardian.com/technology/2023/nov/17/openai-\\nceo-sam-altman-fired) from the original on November 17, 2023. Retrieved November 17,\\n2023.\\n244. Peters, Jay (November 18, 2023). \"OpenAI co-founder Greg Brockman is leaving, too\" (http\\ns://www.theverge.com/2023/11/17/23966277/openai-co-founder-greg-brockman-leaving).\\nThe Verge. Archived (https://web.archive.org/web/20231118003259/https://www.theverge.co\\nm/2023/11/17/23966277/openai-co-founder-greg-brockman-leaving) from the original on\\nNovember 18, 2023. Retrieved November 18, 2023.\\n245. \"Three Senior OpenAI Researchers Resign as Crisis Deepens\" (https://www.theinformation.\\ncom/articles/three-senior-openai-researchers-resign-as-crisis-deepens). The Information.\\nArchived (https://web.archive.org/web/20231118065056/https://www.theinformation.com/arti\\ncles/three-senior-openai-researchers-resign-as-crisis-deepens) from the original on\\nNovember 18, 2023. Retrieved November 18, 2023.\\n246. Edwards, Benj (November 18, 2023). \"Details emerge of surprise board coup that ousted\\nCEO Sam Altman at OpenAI\" (https://arstechnica.com/information-technology/2023/11/repor\\nt-sutskever-led-board-coup-at-openai-that-ousted-altman-over-ai-safety-concerns/). Ars\\nTechnica. Archived (https://web.archive.org/web/20231119020114/https://arstechnica.com/in\\nformation-technology/2023/11/report-sutskever-led-board-coup-at-openai-that-ousted-altma\\nn-over-ai-safety-concerns/) from the original on November 19, 2023. Retrieved\\nNovember 19, 2023.\\n247. Seetharaman, Keach Hagey, Berber Jin and Deepa. \"OpenAI Investors Trying to Get Sam\\nAltman Back as CEO After Sudden Firing\" (https://www.wsj.com/tech/openai-trying-to-get-sa\\nm-altman-back-4b728049). WSJ. Archived (https://web.archive.org/web/20231118233153/ht\\ntps://www.wsj.com/tech/openai-trying-to-get-sam-altman-back-4b728049) from the original\\non November 18, 2023. Retrieved November 19, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 43, 'page_label': '44'}, page_content='248. Metz, Cade; Isaac, Mike; Griffith, Erin (November 19, 2023). \"Sam Altman Is Said to Be\\nDiscussing Return to OpenAI With Company\\'s Board\" (https://www.nytimes.com/2023/11/1\\n8/technology/sam-altman-openai-board.html?smid=nytcore-ios-share&referringSource=articl\\neShare). The New York Times. Archived (https://web.archive.org/web/20231119014720/http\\ns://www.nytimes.com/2023/11/18/technology/sam-altman-openai-board.html?smid=nytcore-i\\nos-share&referringSource=articleShare) from the original on November 19, 2023. Retrieved\\nNovember 19, 2023.\\n249. Patel, Nilay (November 18, 2023). \"OpenAI board in discussions with Sam Altman to return\\nas CEO\" (https://www.theverge.com/2023/11/18/23967199/breaking-openai-board-in-discus\\nsions-with-sam-altman-to-return-as-ceo). The Verge. Archived (https://web.archive.org/web/\\n20231118225333/https://www.theverge.com/2023/11/18/23967199/breaking-openai-board-i\\nn-discussions-with-sam-altman-to-return-as-ceo) from the original on November 18, 2023.\\nRetrieved November 19, 2023.\\n250. Heath, Alex (November 19, 2023). \"The deal to bring Sam Altman back to OpenAI has fallen\\napart\" (https://www.theverge.com/2023/11/20/23967515/sam-altman-openai-board-fired-ne\\nw-ceo). The Verge. Archived (https://web.archive.org/web/20231120053238/https://www.the\\nverge.com/2023/11/20/23967515/sam-altman-openai-board-fired-new-ceo) from the original\\non November 20, 2023. Retrieved November 20, 2023.\\n251. Dastin, Jeffrey (November 21, 2023). \"OpenAI\\'s board approached Anthropic CEO about top\\njob and merger\" (https://www.reuters.com/technology/openais-board-approached-anthropic-\\nceo-about-top-job-merger-sources-2023-11-21/). Reuters. Archived (https://web.archive.org/\\nweb/20231121203624/https://www.reuters.com/technology/openais-board-approached-anth\\nropic-ceo-about-top-job-merger-sources-2023-11-21/) from the original on November 21,\\n2023. Retrieved November 21, 2023.\\n252. Warren, Tom (November 20, 2023). \"Microsoft hires former OpenAI CEO Sam Altman\" (http\\ns://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-e\\nmployees-openai). The Verge. Archived (https://web.archive.org/web/20231120133123/http\\ns://www.theverge.com/2023/11/20/23968829/microsoft-hires-sam-altman-greg-brockman-e\\nmployees-openai) from the original on November 20, 2023. Retrieved November 20, 2023.\\n253. Patel, Nilay (November 20, 2023). \"Sam Altman is still trying to return as OpenAI CEO\" (http\\ns://www.theverge.com/2023/11/20/23969586/sam-altman-plotting-return-open-ai-microsoft).\\nThe Verge. Archived (https://web.archive.org/web/20231120191138/https://www.theverge.co\\nm/2023/11/20/23969586/sam-altman-plotting-return-open-ai-microsoft) from the original on\\nNovember 20, 2023. Retrieved November 20, 2023.\\n254. \"OpenAI Staff Near Total Mutiny With Threat to Jump to Microsoft\" (https://news.bloombergl\\naw.com/us-law-week/openai-staff-threaten-to-go-to-microsoft-if-board-doesnt-quit).\\nBloomberg. Archived (https://web.archive.org/web/20231120201119/https://news.bloomberg\\nlaw.com/us-law-week/openai-staff-threaten-to-go-to-microsoft-if-board-doesnt-quit) from the\\noriginal on November 20, 2023. Retrieved November 20, 2023.\\n255. Knight, Will. \"OpenAI Staff Threaten to Quit Unless Board Resigns\" (https://www.wired.com/\\nstory/openai-staff-walk-protest-sam-altman/). Wired. Archived (https://web.archive.org/web/2\\n0231120150129/https://www.wired.com/story/openai-staff-walk-protest-sam-altman/) from\\nthe original on November 20, 2023. Retrieved November 20, 2023.\\n256. Tong, Anna; Hu, Krystal; Tong, Anna; Hu, Krystal (November 20, 2023). \"Exclusive: OpenAI\\ninvestors considering suing the board after CEO\\'s abrupt firing\" (https://www.reuters.com/tec\\nhnology/openai-investors-considering-suing-board-after-ceos-abrupt-firing-sources-2023-11-\\n20/). Reuters. Archived (https://web.archive.org/web/20231120230652/https://www.reuters.c\\nom/technology/openai-investors-considering-suing-board-after-ceos-abrupt-firing-sources-2\\n023-11-20/) from the original on November 20, 2023. Retrieved November 20, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 44, 'page_label': '45'}, page_content='257. Lawler, Richard (November 21, 2023). \"OpenAI exec to employees: \"our number one goal\\nremains to reunify OpenAI.\"\" (https://www.theverge.com/2023/11/21/23970550/openai-exec\\n-to-employees-our-number-one-goal-remains-to-reunify-openai). The Verge. Archived (http\\ns://web.archive.org/web/20231121180928/https://www.theverge.com/2023/11/21/23970550/\\nopenai-exec-to-employees-our-number-one-goal-remains-to-reunify-openai) from the\\noriginal on November 21, 2023. Retrieved November 21, 2023.\\n258. Heath, Alex (November 22, 2023). \"Breaking: Sam Altman to return as CEO of OpenAI\" (htt\\nps://www.theverge.com/2023/11/22/23967223/sam-altman-returns-ceo-open-ai). The Verge.\\nArchived (https://web.archive.org/web/20231122060733/https://www.theverge.com/2023/11/\\n22/23967223/sam-altman-returns-ceo-open-ai) from the original on November 22, 2023.\\nRetrieved November 22, 2023.\\n259. Anna Tong; Jeffrey Dastin; Krystal Hu (November 22, 2023). \"Exclusive: OpenAI\\nresearchers warned board of AI breakthrough ahead of CEO ouster, sources say\" (https://w\\nww.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-ab\\nout-ai-breakthrough-2023-11-22). Reuters. Archived (https://web.archive.org/web/20231123\\n045247/https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-b\\ny-letter-board-about-ai-breakthrough-2023-11-22/) from the original on November 23, 2023.\\nRetrieved November 23, 2023. \"Some at OpenAI believe Q* (pronounced Q-Star) could be\\na breakthrough in the startup\\'s search for what\\'s known as artificial general intelligence\\n(AGI), one of the people told Reuters. OpenAI defines AGI as autonomous systems that\\nsurpass humans in most economically valuable tasks.\"\\n260. \"openai/prm800k\" (https://github.com/openai/prm800k). January 16, 2024. Archived (https://\\nweb.archive.org/web/20231124210953/https://github.com/openai/prm800k) from the original\\non November 24, 2023. Retrieved December 4, 2023 – via GitHub.\\n261. Lightman, Hunter; Kosaraju, Vineet; Burda, Yura; Edwards, Harri; Baker, Bowen; Lee,\\nTeddy; Leike, Jan; Schulman, John; Sutskever, Ilya; Cobbe, Karl (2023). \"Let\\'s Verify Step\\nby Step\". arXiv:2305.20050 (https://arxiv.org/abs/2305.20050) [cs.LG (https://arxiv.org/archiv\\ne/cs.LG)].\\n262. Tong, Anna; Dastin, Jeffrey; Hu, Krystal (November 23, 2023). \"Exclusive: OpenAI\\nresearchers warned board of AI breakthrough ahead of CEO ouster, sources say\" (https://w\\nww.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-ab\\nout-ai-breakthrough-2023-11-22/). Reuters. Archived (https://web.archive.org/web/20231211\\n175826/https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-b\\ny-letter-board-about-ai-breakthrough-2023-11-22/) from the original on December 11, 2023.\\nRetrieved November 23, 2023.\\n263. Heath, Alex (November 30, 2023). \"Microsoft joins OpenAI\\'s board with Sam Altman\\nofficially back as CEO\" (https://www.theverge.com/2023/11/29/23981848/sam-altman-back-\\nopen-ai-ceo-microsoft-board). The Verge. Archived (https://web.archive.org/web/202312140\\n33303/https://www.theverge.com/2023/11/29/23981848/sam-altman-back-open-ai-ceo-micr\\nosoft-board) from the original on December 14, 2023. Retrieved December 14, 2023.\\n264. \"Microsoft ditches OpenAI board observer seat amid regulatory scrutiny\" (https://www.chann\\nelnewsasia.com/business/microsoft-ditches-openai-board-observer-seat-amid-regulatory-scr\\nutiny-4469086). CNA. Archived (https://web.archive.org/web/20240711100155/https://www.c\\nhannelnewsasia.com/business/microsoft-ditches-openai-board-observer-seat-amid-regulato\\nry-scrutiny-4469086) from the original on July 11, 2024. Retrieved July 10, 2024.\\n265. Perrigo, Billy (January 18, 2023). \"Exclusive: The $2 Per Hour Workers Who Made\\nChatGPT Safer\" (https://time.com/6247678/openai-chatgpt-kenya-workers/). Time. Archived\\n(https://web.archive.org/web/20230119152814/https://time.com/6247678/openai-chatgpt-ke\\nnya-workers/) from the original on January 19, 2023. Retrieved August 5, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 45, 'page_label': '46'}, page_content='266. Vincent, James (March 15, 2023). \"OpenAI co-founder on company\\'s past approach to\\nopenly sharing research: \"We were wrong\"\" (https://www.theverge.com/2023/3/15/2364018\\n0/openai-gpt-4-launch-closed-research-ilya-sutskever-interview). The Verge. Archived (http\\ns://web.archive.org/web/20230317210900/https://www.theverge.com/2023/3/15/23640180/o\\npenai-gpt-4-launch-closed-research-ilya-sutskever-interview) from the original on March 17,\\n2023. Retrieved August 20, 2023.\\n267. Piper, Kelsey (May 17, 2024). \"ChatGPT can talk, but OpenAI employees sure can\\'t\" (http\\ns://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employ\\nees-chatgpt-release). Vox. Archived (https://web.archive.org/web/20240518221431/https://w\\nww.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-\\nchatgpt-release) from the original on May 18, 2024. Retrieved May 18, 2024.\\n268. Christian, Jon (May 18, 2024). \"OpenAI Employees Forced to Sign NDA Preventing Them\\nFrom Ever Criticizing Company\" (https://futurism.com/the-byte/openai-nda-criticism).\\nFuturism. Archived (https://web.archive.org/web/20240518221433/https://futurism.com/the-b\\nyte/openai-nda-criticism) from the original on May 18, 2024. Retrieved May 18, 2024.\\n269. Getahun, Hannah. \"Sam Altman addresses \\'potential equity cancellation\\' in OpenAI exit\\nagreements after 2 high-profile departures\" (https://www.businessinsider.com/sam-altman-o\\npenai-nda-clause-vested-equity-ilya-sutskever-2024-5). Business Insider. Archived (https://w\\neb.archive.org/web/20240519130546/https://www.businessinsider.com/sam-altman-openai-\\nnda-clause-vested-equity-ilya-sutskever-2024-5) from the original on May 19, 2024.\\nRetrieved May 19, 2024.\\n270. Piper, Kelsey (May 22, 2024). \"Leaked OpenAI documents reveal aggressive tactics toward\\nformer employees\" (https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-s\\nam-altman-documents-employees). Vox. Archived (https://web.archive.org/web/2024060116\\n1425/https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-doc\\numents-employees) from the original on June 1, 2024. Retrieved June 2, 2024.\\n271. Field, Hayden (May 24, 2024). \"OpenAI sends internal memo releasing former employees\\nfrom controversial exit agreements\" (https://www.cnbc.com/2024/05/24/openai-sends-intern\\nal-memo-releasing-former-employees-from-non-disparagement-agreements-sam-altman.ht\\nml). CNBC. Archived (https://web.archive.org/web/20240530032655/https://www.cnbc.com/2\\n024/05/24/openai-sends-internal-memo-releasing-former-employees-from-non-disparageme\\nnt-agreements-sam-altman.html) from the original on May 30, 2024. Retrieved June 2,\\n2024.\\n272. Belanger, Ashley (July 10, 2023). \"Sarah Silverman sues OpenAI, Meta for being \"industrial-\\nstrength plagiarists\"\" (https://arstechnica.com/information-technology/2023/07/book-authors\\n-sue-openai-and-meta-over-text-used-to-train-ai/). Ars Technica. Archived (https://web.archiv\\ne.org/web/20230921134950/https://arstechnica.com/information-technology/2023/07/book-a\\nuthors-sue-openai-and-meta-over-text-used-to-train-ai/) from the original on September 21,\\n2023. Retrieved September 21, 2023.\\n273. Krithika, K. L. (August 21, 2023). \"Legal Challenges Surround OpenAI: A Closer Look at the\\nLawsuits\" (https://analyticsindiamag.com/all-the-lawsuits-filed-against-openai/). Analytics\\nIndia Magazine. Archived (https://web.archive.org/web/20230823225353/https://analyticsindi\\namag.com/all-the-lawsuits-filed-against-openai/) from the original on August 23, 2023.\\nRetrieved August 23, 2023.\\n274. Abshire, Elisha (July 6, 2023). \"OpenAI faces copyright lawsuit from authors Mona Awad\\nand Paul Tremblay\" (https://dailyai.com/2023/07/openai-faces-copyright-lawsuit-from-author\\ns-mona-awad-and-paul-tremblay/). Dailyai.com. Archived (https://web.archive.org/web/2023\\n0718091105/https://dailyai.com/2023/07/openai-faces-copyright-lawsuit-from-authors-mona-\\nawad-and-paul-tremblay/) from the original on July 18, 2023. Retrieved July 19, 2023.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 46, 'page_label': '47'}, page_content='275. Belanger, Ashley (September 20, 2023). \"Grisham, Martin join authors suing OpenAI: \"There\\nis nothing fair about this\" [Updated]\" (https://arstechnica.com/tech-policy/2023/09/george-r-r-\\nmartin-joins-authors-suing-openai-over-copyright-infringement/). Ars Technica. Archived (htt\\nps://web.archive.org/web/20230921164338/https://arstechnica.com/tech-policy/2023/09/geo\\nrge-r-r-martin-joins-authors-suing-openai-over-copyright-infringement/) from the original on\\nSeptember 21, 2023. Retrieved September 21, 2023.\\n276. Korn, Jennifer (September 20, 2023). \"George R. R. Martin, Jodi Picoult and other famous\\nwriters join Authors Guild in class action lawsuit against OpenAI\" (https://www.cnn.com/202\\n3/09/20/tech/authors-guild-openai-lawsuit/index.html). CNN Business. Archived (https://web.\\narchive.org/web/20230921141901/https://www.cnn.com/2023/09/20/tech/authors-guild-open\\nai-lawsuit/index.html) from the original on September 21, 2023. Retrieved September 21,\\n2023.\\n277. \"NY Times sues OpenAI, Microsoft for infringing copyrighted works\" (https://www.reuters.co\\nm/legal/transactional/ny-times-sues-openai-microsoft-infringing-copyrighted-work-2023-12-2\\n7). Reuters. December 27, 2023. Archived (https://web.archive.org/web/20231230163134/ht\\ntps://www.reuters.com/legal/transactional/ny-times-sues-openai-microsoft-infringing-copyrig\\nhted-work-2023-12-27/) from the original on December 30, 2023. Retrieved December 27,\\n2023.\\n278. Chowdhury, Darius Rafieyan, Hasan. \"OpenAI destroyed a trove of books used to train AI\\nmodels. The employees who collected the data are gone\" (https://www.businessinsider.com/\\nopenai-destroyed-ai-training-datasets-lawsuit-authors-books-copyright-2024-5). Business\\nInsider. Archived (https://web.archive.org/web/20240507235039/https://www.businessinside\\nr.com/openai-destroyed-ai-training-datasets-lawsuit-authors-books-copyright-2024-5) from\\nthe original on May 7, 2024. Retrieved May 8, 2024. \"…Authors Guild said … the datasets\\nlikely contained \"more than 100,000 published books\" … central to its allegations that\\nOpenAI used copyrighted materials to train AI models\"\\n279. Metz, Cade; Kang, Cecilia; Frenkel, Sheera; Thompson, Stuart A.; Grant, Nico (April 6,\\n2024). \"How Tech Giants Cut Corners to Harvest Data for A.I.\" (https://www.nytimes.com/20\\n24/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html) The New York\\nTimes. Archived (https://web.archive.org/web/20240505040123/https://www.nytimes.com/20\\n24/04/06/technology/tech-giants-harvest-data-artificial-intelligence.html) from the original on\\nMay 5, 2024. Retrieved May 21, 2024.\\n280. Brittain, Blake (February 29, 2024). \"OpenAI hit with new lawsuits from news outlets over AI\\ntraining\" (https://www.reuters.com/legal/litigation/openai-hit-with-new-lawsuits-news-outlets-\\nover-ai-training-2024-02-28/). Reuters. Retrieved March 24, 2024.\\n281. OpenAI RAW STORY LAWSUIT INTERCEPT (https://fingfx.thomsonreuters.com/gfx/legaldo\\ncs/xmpjrjwjrpr/OPENAI%20RAW%20STORY%20LAWSUIT%20intercept.pdf) Archived (http\\ns://web.archive.org/web/20240328115412/https://fingfx.thomsonreuters.com/gfx/legaldocs/x\\nmpjrjwjrpr/OPENAI%20RAW%20STORY%20LAWSUIT%20intercept.pdf) March 28, 2024,\\nat the Wayback Machine - from Reuters\\n282. \"The Intercept charts a new legal strategy for digital publishers suing OpenAI\" (https://www.\\nniemanlab.org/2024/03/the-intercept-charts-a-new-legal-strategy-for-digital-publishers-suing\\n-openai/). Nieman Lab. Archived (https://web.archive.org/web/20240328115412/https://www.\\nniemanlab.org/2024/03/the-intercept-charts-a-new-legal-strategy-for-digital-publishers-suing\\n-openai/) from the original on March 28, 2024. Retrieved March 28, 2024.\\n283. Baron, Ethan (April 30, 2024). \"Mercury News and other papers sue Microsoft, OpenAI over\\nthe new artificial intelligence\" (https://www.eastbaytimes.com/2024/04/30/mercury-news-and\\n-other-papers-sue-microsoft-openai-over-the-new-artificial-intelligence/). East Bay Times.\\nArchived (https://web.archive.org/web/20240430162841/https://www.eastbaytimes.com/202\\n4/04/30/mercury-news-and-other-papers-sue-microsoft-openai-over-the-new-artificial-intellig\\nence/) from the original on April 30, 2024. Retrieved April 30, 2024.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 47, 'page_label': '48'}, page_content='284. \"EDPB resolves dispute on transfers by Meta and creates task force on Chat GPT\" (https://e\\ndpb.europa.eu/news/news/2023/edpb-resolves-dispute-transfers-meta-and-creates-task-for\\nce-chat-gpt_en). EDPB resolves dispute on transfers by Meta and creates task force on\\nChat GPT. Archived (https://web.archive.org/web/20231122215754/https://edpb.europa.eu/n\\news/news/2023/edpb-resolves-dispute-transfers-meta-and-creates-task-force-chat-gpt_en)\\nfrom the original on November 22, 2023. Retrieved November 22, 2023.\\n285. \"ChatGPT verbreitet falsche Infos über Personen – und OpenAI kann nichts tun\" (https://noy\\nb.eu/de/chatgpt-provides-false-information-about-people-and-openai-cant-correct-it).\\nnoyb.eu. Archived (https://web.archive.org/web/20240429054826/https://noyb.eu/de/chatgpt\\n-provides-false-information-about-people-and-openai-cant-correct-it) from the original on\\nApril 29, 2024. Retrieved April 29, 2024.\\n286. \"OpenAI Quietly Deletes Ban on Using ChatGPT for \"Military and Warfare\"\" (https://theinter\\ncept.com/2024/01/12/open-ai-military-ban-chatgpt/). January 12, 2024. Archived (https://we\\nb.archive.org/web/20240125091034/https://theintercept.com/2024/01/12/open-ai-military-ba\\nn-chatgpt/) from the original on January 25, 2024. Retrieved January 13, 2024.\\n287. \"OpenAI Is Working With US Military on Cybersecurity Tools\" (https://www.bloomberg.com/n\\news/articles/2024-01-16/openai-working-with-us-military-on-cybersecurity-tools-for-veteran\\ns). Bloomberg.com. January 16, 2024. Archived (https://web.archive.org/web/202401261317\\n13/https://www.bloomberg.com/news/articles/2024-01-16/openai-working-with-us-military-on\\n-cybersecurity-tools-for-veterans) from the original on January 26, 2024. Retrieved\\nJanuary 23, 2024.\\n288. \"OpenAI | AIxCC\" (https://aicyberchallenge.com/openai/). Artificial Intelligence Cyber\\nChallenge. Archived (https://web.archive.org/web/20240330202438/https://aicyberchallenge.\\ncom/openai/) from the original on March 30, 2024. Retrieved March 30, 2024.\\n289. Bond, Shannon (May 30, 2024). \"In a first, OpenAI removes influence operations tied to\\nRussia, China and Israel\" (https://www.npr.org/2024/05/30/g-s1-1670/openai-influence-oper\\nations-china-russia-israel). NPR. Archived (https://web.archive.org/web/20240530183810/htt\\nps://www.npr.org/2024/05/30/g-s1-1670/openai-influence-operations-china-russia-israel)\\nfrom the original on May 30, 2024. Retrieved May 30, 2024.\\n290. Bergengruen, Vera (May 30, 2024). \"OpenAI Says Russia, China, and Israel Are Using Its\\nTools for Foreign Influence Campaigns\" (https://time.com/6983903/openai-foreign-influence-\\ncampaigns-artificial-intelligence/). Time. Archived (https://web.archive.org/web/2024071101\\n4517/https://time.com/6983903/openai-foreign-influence-campaigns-artificial-intelligence/)\\nfrom the original on July 11, 2024. Retrieved July 11, 2024.\\n291. \"How OpenAI Leaving China Will Reshape the Country\\'s AI Scene\" (https://time.com/69920\\n26/openai-china-ai/). Time. June 26, 2024. Archived (https://web.archive.org/web/20240711\\n014517/https://time.com/6992026/openai-china-ai/) from the original on July 11, 2024.\\nRetrieved July 11, 2024.\\n292. Xiang, Chloe (June 29, 2023). \"OpenAI and Microsoft Sued for $3 Billion Over Alleged\\nChatGPT \\'Privacy Violations\\'\" (https://www.vice.com/en/article/wxjxgx/openai-and-microsoft-\\nsued-for-dollar3-billion-over-alleged-chatgpt-privacy-violations). Vice. Archived (https://web.a\\nrchive.org/web/20240713042222/https://www.vice.com/en/article/wxjxgx/openai-and-micros\\noft-sued-for-dollar3-billion-over-alleged-chatgpt-privacy-violations) from the original on July\\n13, 2024. Retrieved July 13, 2024.\\nLevy, Steven (September 5, 2023). \"What OpenAI Really Wants\" (https://www.wired.com/sto\\nry/what-openai-really-wants). Wired. ISSN 1078-3148 (https://search.worldcat.org/issn/1078\\n-3148). Archived (https://web.archive.org/web/20230906191725/https://www.wired.com/stor\\ny/what-openai-really-wants/) from the original on September 6, 2023. Retrieved\\nSeptember 6, 2023.\\nFurther reading'),\n",
       " Document(metadata={'producer': 'Skia/PDF m128', 'creator': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/128.0.0.0 Safari/537.36', 'creationdate': '2024-10-18T10:40:29+00:00', 'title': 'OpenAI - Wikipedia', 'moddate': '2024-10-18T10:40:29+00:00', 'source': 'data/OpenAI.pdf', 'total_pages': 49, 'page': 48, 'page_label': '49'}, page_content='Duhigg, Charles (December 1, 2023). \"The Inside Story of Microsoft\\'s Partnership with\\nOpenAI\" (https://www.newyorker.com/magazine/2023/12/11/the-inside-story-of-microsofts-p\\nartnership-with-openai). The New Yorker. ISSN 0028-792X (https://search.worldcat.org/issn/\\n0028-792X). Archived (https://archive.today/20231222230940/https://www.newyorker.com/m\\nagazine/2023/12/11/the-inside-story-of-microsofts-partnership-with-openai) from the original\\non December 22, 2023. Retrieved January 15, 2024.\\nOfficial website (https://openai.com/) \\nOpenAI (https://x.com/OpenAI) on Twitter\\nOpenAI (https://www.instagram.com/OpenAI/) on Instagram\\nOpenAI (https://youtube.com/OpenAI) on Youtube\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=OpenAI&oldid=1251657287\"\\nExternal links'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 0, 'page_label': 'Cover'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 1, 'page_label': 'i'}, page_content='Building Data Science \\nApplications with \\nFastAPI\\nDevelop, manage, and deploy efficient machine \\nlearning applications with Python\\nFrançois Voron\\nBIRMINGHAM—MUMBAI'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 2, 'page_label': 'ii'}, page_content='Building Data Science Applications \\nwith FastAPI\\nCopyright © 2021 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, \\nor transmitted in any form or by any means, without the prior written permission of the \\npublisher, except in the case of brief quotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of the \\ninformation presented. However, the information contained in this book is sold without \\nwarranty, either express or implied. Neither the author, nor Packt Publishing or its dealers and \\ndistributors, will be held liable for any damages caused or alleged to have been caused directly \\nor indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies \\nand products mentioned in this book by the appropriate use of capitals. However, Packt \\nPublishing cannot guarantee the accuracy of this information.\\nPublishing Product Manager: Ashish Tiwari\\nSenior Editor: Mohammed Yusuf Imaratwale\\nContent Development Editor: Nazia Shaikh\\nTechnical Editor: Manikandan Kurup\\nCopy Editor: Safis Editing\\nProject Coordinator: Aparna Nair\\nProofreader: Safis Editing\\nIndexer: Subalakshmi Govindhan\\nProduction Designer: Ponraj Dhandapani\\nFirst published: October 2021\\nProduction reference: 1250821\\nPublished by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirmingham\\nB3 2PB, UK.\\nISBN 978-1-80107-921-1\\nwww.packt.com'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 3, 'page_label': 'iii'}, page_content=\"Contributors\\nAbout the author\\nFrançois Voron is graduated from the University of Saint-Étienne (France) and the \\nUniversity of Alicante (Spain) with a master's degree in Machine Learning and Data \\nMining. A full-stack web developer and a data scientist, François has a proven track \\nrecord working in the SaaS industry, with a special focus on Python backends and  \\nREST API. \\nHe is also the creator and maintainer of FastAPI Users, the #1 authentication library for \\nFastAPI, and is one of the top experts in the FastAPI community.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 4, 'page_label': 'iv'}, page_content=\"About the reviewers\\nPrajjwal Nijhara is an electrical engineering student at Aligarh Muslim University and a \\nmember of the AUV-ZHCET club where he works on computer vision. He is a mentor at \\nAMU-OSS and has worked with DeepSource as the developer-relation intern for  \\nsix months.\\nI dedicate my contribution in this book to my mentor Areeb Jamal who recently  \\npassed away recently. Areeb is forever in our hearts and our memory. \\nI also dedicate this book to Vivek Duneja, who taught me Python programming during my \\nschool days; in case, if you find any issues with my review, Mr. Duneja will be responsible.\\n Finally, I'd like to thank my brother, parents, and family, for being supportive.\\nIzabela dos Santos Guerreiro graduated in information technology management and \\nsystems analysis and development. A machine learning enthusiast, Izabela is currently \\na postgraduate in artificial intelligence, machine learning, and data science. A software \\ndeveloper for 7 years, always working with Python, Izabela worked with FastAPI for about \\n1 year and became an enthusiast of the framework, collaborating on the translation of the \\ndocumentation to her native language, Portuguese. She has already organized PyLadies and \\nDjango Girls events and is currently in charge of the Django Girls São Paulo organization.\\nRichad Becker leads data science at Revantage, a real estate shared service organization in \\nthe Blackstone family. Richad grew his career in real estate and finance data science – first \\nat CoreLogic, focusing on text mining applications, then at Greystone, leading all things \\ndata in an innovation lab, before joining Revantage. Richad is self-taught in data science and \\ncredits his career progress to focusing on impact and deeply understanding the business \\ncase. Richad has a BA in neuroscience and anthropology and a master's in commerce  \\n(with a concentration in marketing and management) from the University of Virginia.\\nI'd like to thank my wife for her patience and support, and Jon for giving me advice on  \\ngood technical reviewing!\\nWilliam Jamir Silva is a software developer with a BSc in electrical engineering with \\nmore than 5 years of experience working in scientific software development with Python. \\nHe is skilled in desktop development and web applications.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 5, 'page_label': 'i'}, page_content='Table of Contents\\nPreface\\nSection 1:  \\nIntroduction to Python and FastAPI\\n1\\nPython Development Environment Setup\\nTechnical requirements 4\\nInstalling a Python distribution \\nusing pyenv 4\\nCreating a Python virtual \\nenvironment 7\\nInstalling Python packages  \\nwith pip 8\\nInstalling the HTTPie  \\ncommand-line utility 9\\nSummary 11\\n2\\nPython Programming Specificities\\nTechnical requirements 14\\nBasics of Python programming 14\\nRunning Python scripts 14\\nIndentation matters 16\\nWorking with built-in types 17\\nWorking with data structures – lists, \\ntuples, dictionaries, and sets 18\\nPerforming Boolean logic and  \\nchecking for existence 23\\nControlling the flow of a program 25\\nDefining functions 29\\nWriting and using packages and modules 31\\nOperating over sequences – list \\ncomprehensions and generators 34\\nList comprehensions 34\\nGenerators 36\\nWriting object-oriented \\nprograms 39\\nDefining a class 39\\nImplementing magic methods 41\\nReusing logic and avoiding repetition \\nwith inheritance 45'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 6, 'page_label': 'ii'}, page_content='ii  Table of Contents\\nType hinting and type checking \\nwith mypy 48\\nGetting started 48\\nThe typing module 50\\nType function signatures with Callable 53\\nAny and cast 54\\nAsynchronous I/O 56\\nSummary 60\\n3\\nDeveloping a RESTful API with FastAPI\\nTechnical requirements 62\\nCreating the first endpoint  \\nand running it locally 62\\nHandling request parameters 65\\nPath parameters 65\\nQuery parameters 72\\nThe request body 74\\nForm data and file uploads 79\\nHeaders and cookies 85\\nThe request object 87\\nCustomizing the response 88\\nPath operation parameters 88\\nThe response parameter 95\\nRaising HTTP errors 99\\nBuilding a custom response 102\\nStructuring a bigger project \\nwith multiple routers 107\\nSummary 111\\n4\\nManaging Pydantic Data Models in FastAPI\\nTechnical requirements 114\\nDefining models and their field \\ntypes with Pydantic 114\\nStandard field types 114\\nOptional fields and default values 120\\nField validation 122\\nValidating email addresses and URLs \\nwith Pydantic types 124\\nCreating model variations  \\nwith class inheritance 126\\nAdding custom data validation \\nwith Pydantic 129\\nApplying validation at a field level 129\\nApplying validation at an object level 130\\nApplying validation before  \\nPydantic parsing 131\\nWorking with Pydantic objects 132\\nConverting an object into a dictionary 132\\nCreating an instance from  \\na sub-class object 135\\nUpdating an instance with a partial one 137\\nSummary 139'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 7, 'page_label': 'iii'}, page_content='Table of Contents  iii\\n5\\nDependency Injections in FastAPI\\nTechnical requirements 142\\nWhat is dependency injection? 142\\nCreating and using a function \\ndependency 143\\nGet an object or raise a 404 error 147\\nCreating and using a \\nparameterized dependency \\nwith a class 148\\nUse class methods as dependencies 150\\nUsing dependencies at a path, \\nrouter, and global level 152\\nUse a dependency on a path decorator 153\\nUse a dependency on a whole router 154\\nUse a dependency on a whole \\napplication  156\\nSummary 157\\nSection 2:  \\nBuild and Deploy a Complete Web Backend \\nwith FastAPI\\n6\\nDatabases and Asynchronous ORMs\\nTechnical requirements 162\\nAn overview of relational and \\nNoSQL databases 162\\nRelational databases 163\\nNoSQL databases 164\\nWhich one should you choose? 165\\nCommunicating with a SQL \\ndatabase with SQLAlchemy 166\\nCreating the table schema 168\\nConnecting to a database 169\\nMaking insert queries 171\\nMaking select queries 173\\nMaking update and delete queries 175\\nAdding relationships 177\\nSetting up a database migration \\nsystem with Alembic 180\\nCommunicating with a SQL \\ndatabase with Tortoise ORM 186\\nCreating database models 186\\nSetting up the Tortoise engine 188\\nCreating objects 190\\nUpdating and deleting objects 193\\nAdding relationships 194\\nSetting up a database migration \\nsystem with Aerich 198\\nCommunicating with a \\nMongoDB database using Motor 200\\nCreating models compatible with \\nMongoDB ID 201'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 8, 'page_label': 'iv'}, page_content='iv  Table of Contents\\nConnecting to a database 202\\nInserting documents 203\\nGetting documents  204\\nUpdating and deleting documents 207\\nNesting documents 208\\nSummary 210\\n7\\nManaging Authentication and Security in FastAPI\\nTechnical requirements 212\\nSecurity dependencies  \\nin FastAPI 212\\nStoring a user and their \\npassword securely in a \\ndatabase 216\\nCreating models and tables 217\\nHashing passwords 218\\nImplementing registration routes 219\\nRetrieving a user and \\ngenerating an access token 220\\nImplementing a database access token 220\\nImplementing a login endpoint 222\\nSecuring endpoints with  \\naccess tokens 225\\nConfiguring CORS and \\nprotecting against CSRF attacks 227\\nUnderstanding CORS and  \\nconfiguring it in FastAPI 228\\nImplementing double-submit  \\ncookies to prevent CSRF attacks 233\\nSummary 239\\n8\\nDefining WebSockets for Two-Way Interactive \\nCommunication in FastAPI\\nTechnical requirements 242\\nUnderstanding the principles of \\ntwo-way communication with \\nWebSockets 242\\nCreating a WebSocket with \\nFastAPI 243\\nHandling concurrency 247\\nUsing dependencies 250\\nHandling multiple WebSocket \\nconnections and broadcasting \\nmessages 253\\nSummary 260\\n9\\nTesting an API Asynchronously with pytest and HTTPX\\nTechnical requirements 262\\nIntroduction to unit testing \\nwith pytest 263\\nGenerating tests with parametrize 265\\nReusing test logic by creating fixtures 267'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 9, 'page_label': 'v'}, page_content='Table of Contents  v\\nSetting up testing tools for \\nFastAPI with HTTPX 270\\nWriting tests for REST API \\nendpoints 275\\nWriting tests for POST endpoints 276\\nTesting with a database 278\\nWriting tests for WebSocket \\nendpoints 286\\nSummary 289\\n10\\nDeploying a FastAPI Project\\nTechnical requirements 292\\nSetting and using environment \\nvariables 292\\nUsing a .env file 296\\nManaging Python dependencies 297\\nAdding Gunicorn as a server process \\nfor deployment 299\\nDeploying a FastAPI application \\non a serverless platform 300\\nAdding database servers 303\\nDeploying a FastAPI  \\napplication with Docker 304\\nWriting a Dockerfile 304\\nBuilding a Docker image 306\\nRunning a Docker image locally 307\\nDeploying a Docker image 307\\nDeploying a FastAPI application \\non a traditional server 309\\nSummary 310\\nSection 3:  \\nBuild a Data Science API with Python and \\nFastAPI\\n11\\nIntroduction to NumPy and pandas \\nTechnical requirements 314\\nGetting started with NumPy 314\\nCreating arrays 315\\nAccessing elements and sub-arrays 318\\nManipulating arrays with \\nNumPy – computation, \\naggregations, comparisons 320\\nAdding and multiplicating arrays  322\\nAggregating arrays – sum, min,  \\nmax, mean… 324\\nComparing arrays 325\\nGetting started with pandas 326\\nUsing pandas Series for  \\none-dimensional data 326\\nUsing pandas DataFrames for  \\nmulti-dimensional data 328\\nImporting and exporting CSV data 331\\nSummary 332'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 10, 'page_label': 'vi'}, page_content='vi  Table of Contents\\n12\\nTraining Machine Learning Models with scikit-learn\\nTechnical requirements 334\\nWhat is machine learning? 334\\nSupervised versus unsupervised \\nlearning 334\\nModel validation 335\\nBasics of scikit-learn 337\\nTraining models and predicting 337\\nChaining pre-processors and \\nestimators with pipelines 340\\nValidating the model with  \\ncross-validation 344\\nClassifying data with  \\nNaive Bayes models 346\\nIntuition 346\\nClassifying data with Gaussian  \\nNaive Bayes 347\\nClassifying data with Multinomial \\nNaive Bayes 350\\nClassifying data with  \\nsupport vector machines 351\\nIntuition 352\\nUsing SVM in scikit-learn 355\\nFinding the best parameters 356\\nSummary 358\\n13\\nCreating an Efficient Prediction API Endpoint with FastAPI\\nTechnical requirements 360\\nPersisting a trained model  \\nwith Joblib 360\\nDumping a trained model 360\\nLoading a dumped model 362\\nImplementing an efficient \\nprediction endpoint 363\\nCaching results with Joblib 366\\nChoosing between standard or  \\nasync functions 369\\nSummary 372\\n14\\nImplement a Real-Time Face Detection System Using \\nWebSockets with FastAPI and OpenCV\\nTechnical requirements 374\\nGetting started with OpenCV 374\\nImplementing an HTTP \\nendpoint to perform face \\ndetection on a single image 378\\nImplementing a WebSocket to \\nperform face detection on a \\nstream of images 381'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 11, 'page_label': 'vii'}, page_content='Table of Contents  vii\\nSending a stream of images \\nfrom the browser in a \\nWebSocket 383\\nShowing the face detection \\nresults in the browser 387\\nSummary 390\\nOther Books You May Enjoy\\nIndex'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 12, 'page_label': 'viii'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 13, 'page_label': 'ix'}, page_content=\"Preface\\nFastAPI is a web framework for building APIs with Python 3.6 and its later versions based \\non standard Python type hints. With this book, you'll be able to create fast and reliable \\ndata science API backends using practical examples.\\nThis book starts with the basics of the FastAPI framework and associated modern \\nPython programming concepts. Y ou'll then be taken through all the aspects of the \\nframework, including its powerful dependency injection system and how you can use it to \\ncommunicate with databases, implement authentication, and integrate machine learning \\nmodels. Later, you will cover best practices relating to testing and deployment to run a \\nhigh-quality and robust application. Y ou'll also be introduced to the extensive ecosystem \\nof Python data science packages. As you progress, you'll learn how to build data science \\napplications in Python using FastAPI. The book also demonstrates how to develop fast \\nand efficient machine learning prediction backends and test them to achieve the best \\nperformance. Finally, you'll see how to implement a real-time face detection system using \\nWebSockets and a web browser as a client.\\nBy the end of this FastAPI book, you'll have not only learned how to implement \\nPython in data science projects but also how to maintain and design them to meet high \\nprogramming standards with the help of FastAPI.\\nWho this book is for\\nThis book is for data scientists and software developers interested in gaining knowledge \\nof FastAPI and its ecosystem to build data science applications. Basic knowledge \\nof data science and machine learning concepts and how to apply them in Python is \\nrecommended.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 14, 'page_label': 'x'}, page_content=\"x     Preface\\nWhat this book covers\\nChapter 1, Python Development Environment Setup, is aimed at setting up the development \\nenvironment so that you can start working with Python and FastAPI. We'll introduce the \\nvarious tools that are commonly used in the Python community to ease development.\\nChapter 2, Python Programming Specificities, introduces you to the specificities of \\nprogramming in Python, specifically, block indentation, control flow statements, \\nexceptions handling, and the object-oriented paradigm. We'll also cover features such as \\nlist comprehensions and generators. Finally, we'll see how type hinting and asynchronous \\nI/O work.\\xa0\\nChapter 3, Developing a RESTful API with FastAPI, covers the basics of the creation of a \\nRESTful API with FastAPI: routing, parameters, request body validation, and response. \\nWe'll also show how to properly structure a FastAPI project with dedicated modules and \\nseparate routers.\\nChapter 4, Managing pydantic Data Models in FastAPI, covers in more detail the definition \\nof data models with Pydantic, the underlying data validation library used by FastAPI. \\nWe'll explain how to implement variations of the same model without repeating ourselves \\nthanks to class inheritance. Finally, we'll show how to implement custom data validation \\nlogic on those models.\\nChapter 5, Dependency Injections in FastAPI, explains how dependency injection works \\nand how we can define our own dependencies to reuse logic across different routers  \\nand endpoints.\\nChapter 6, Databases and Asynchronous ORMs, demonstrates how we can set up a \\nconnection with a database to read and write data. We'll cover how to use two libraries to \\nwork asynchronously with SQL databases and how they interact with the Pydantic model. \\nFinally, we'll also show you how to work with MongoDB, a NoSQL database.\\nChapter 7, Managing Authentication and Security in FastAPI, shows us how to implement \\na basic authentication system to protect our API endpoints and return the relevant data \\nfor the authenticated user. We'll also talk about the best practices around CORS and how \\nto be safe from CSRF attacks.\\nChapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI, is \\naimed at understanding WebSockets and how to create them and handle the messages \\nreceived with FastAPI.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 15, 'page_label': 'xi'}, page_content=\"To get the most out of this book     xi\\nChapter 9, Testing an API Asynchronously with pytest and HTTPX, shows us how to write \\ntests for our REST API endpoints.\\nChapter 10, Deploying a FastAPI Project, covers the common configuration for running \\nFastAPI applications smoothly in production. We'll also explore several deployment \\noptions: DigitalOcean App Platform, Docker, and the traditional server setup.\\nChapter 11, Introduction to NumPy and pandas, introduces two core libraries for data \\nscience in Python: NumPy and pandas. We'll see how to create and manipulate arrays with \\nNumPy and how we can do efficient operations on them. We'll then show how to manage \\nlarge datasets with pandas.\\nChapter 12, Training Machine Learning Models with scikit-learn, gives a quick introduction \\nto machine learning before moving on to the scikit-learn library, a set of ready-to-\\nuse tools to perform machine learning tasks in Python. We'll review some of the most \\ncommon algorithms and train prediction models.\\nChapter 13, Creating an Efficient Prediction API Endpoint with FastAPI, shows us how we \\ncan efficiently store a trained machine learning model using Joblib. Then, we'll integrate it \\nin a FastAPI backend, considering some technical details of FastAPI internals to achieve \\nmaximum performance. Finally, we'll show a way to cache results using Joblib.\\nChapter 14, Implementing a Real-Time Face Detection System Using WebSockets with FastAPI \\nand OpenCV, implements a simple application to perform face detection in the browser, \\nbacked by a FastAPI WebSocket and OpenCV , a popular library for computer vision.\\nTo get the most out of this book\\nIn this book, we'll mainly work with the Python programming language. The first chapter \\nwill explain how to set up\\xa0a proper Python environment on your operating system. Some \\nexamples also involve running web pages with JavaScript, so you'll need a modern browser \\nlike Google Chrome or Mozilla Firefox.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 16, 'page_label': 'xii'}, page_content='xii     Preface\\nDownload the example code files\\nY ou can download the example code files for this book from GitHub at  \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI. If there\\'s an update to the code, it will be updated  \\nin the GitHub repository.\\nWe also have other code bundles from our rich catalog of books and videos available at \\nhttps://github.com/PacktPublishing/. Check them out!\\nDownload the color images\\nWe also provide a PDF file that has color images of the screenshots and diagrams used \\nin this book. Y ou can download it here: https://static.packt-cdn.com/\\ndownloads/9781801079211_ColorImages.pdf\\nConventions used\\nThere are a number of text conventions used throughout this book.\\nCode in text: Indicates code words in text, database table names, folder names, \\nfilenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. \\nHere is an example: \"Obviously, if everything is okay, we get a Person instance and have \\naccess to the properly parsed fields.\"\\nA block of code is set as follows:\\nfrom pydantic import BaseModel\\nclass Person(BaseModel):\\n    first_name: str\\n    last_name: str\\n    age: int\\nWhen we wish to draw your attention to a particular part of a code block, the relevant \\nlines or items are set in bold:\\nclass PostBase(BaseModel):\\n    title: str\\n    content: str\\n    def excerpt(self) -> str:\\n        return f\"{self.content[:140]}...\"'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 17, 'page_label': 'xiii'}, page_content='Get in touch     xiii\\nAny command-line input or output is written as follows:\\n1 validation error for Person\\nbirthdate\\n  invalid date format (type=value_error.date)\\nTips or important notes \\nAppear like this.\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: If you have questions about any aspect of this book, email us at \\ncustomercare@packtpub.com and mention the book title in the subject of  \\nyour message.\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes \\ndo happen. If you have found a mistake in this book, we would be grateful if you would \\nreport this to us. Please visit www.packtpub.com/support/errata and fill in the \\nform.\\nPiracy: If you come across any illegal copies of our works in any form on the internet, \\nwe would be grateful if you would provide us with the location address or website name. \\nPlease contact us at copyright@packt.com with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have expertise in \\nand you are interested in either writing or contributing to a book, please visit authors.\\npacktpub.com.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 18, 'page_label': 'xiv'}, page_content='xiv     Preface\\nShare Your Thoughts\\nOnce you’ve read Building Data Science Applications with FastAPI, we’ d love to hear your \\nthoughts! Please click here to go straight to the Amazon review \\npage for this book and share your feedback.\\nY our review is important to us and the tech community and will help us make sure we’re \\ndelivering excellent quality content.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 19, 'page_label': '1'}, page_content='Section 1: \\nIntroduction to \\nPython and FastAPI\\nAfter setting up the development environment, we’ll introduce the specificities of Python \\nbefore starting to explore the basic features of FastAPI and running our first REST API.\\nThis section comprises the following chapters:\\n• Chapter 1, Python Development Environment Setup \\n• Chapter 2, Python Programming Specificities \\n• Chapter 3, Developing a RESTful API with FastAPI \\n• Chapter 4, Managing pydantic Data Models in FastAPI\\n• Chapter 5, Dependency Injections in FastAPI'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 20, 'page_label': '2'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 21, 'page_label': '3'}, page_content=\"1\\nPython \\nDevelopment \\nEnvironment Setup\\nBefore we can get started on our FastAPI journey, we need to configure a clean and \\nefficient Python environment. This chapter will show you the best practices and \\nconventions that Python developers use daily to run their projects.\\nBy the end of this chapter, you'll be able to run Python projects and install third-party \\ndependencies in a contained environment that won't raise conflicts if you happen to \\nwork on another project that uses different versions of the Python language or different \\ndependencies.\\nIn this chapter, we're going to cover the following main topics:\\n• Installing a Python distribution using pyenv\\n• Creating a Python virtual environment\\n• Installing Python packages with pip\\n• Installing the HTTPie command-line utility\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 22, 'page_label': '4'}, page_content=\"4     Python Development Environment Setup\\nTechnical requirements\\nThroughout this book, we'll assume you have access to a Unix-based environment, such as \\na Linux distribution or macOS.\\nIf they haven't done so already, macOS users should install the Homebrew package \\n(https://brew.sh), which helps a lot in installing command-line tools.\\nIf you are a Windows user, you should enable Windows Subsystem for Linux \\n(https://docs.microsoft.com/windows/wsl/install-win10), WSL, \\nand install a Linux distribution (such as Ubuntu) that will run alongside the Windows \\nenvironment, which should give you access to all the required tools. There are currently \\ntwo versions of WSL, WSL and WSL2. Depending on your Windows version, you might \\nnot be able to install the newest version. However, we do recommend using WSL2 if your \\nWindows installation supports it.\\nInstalling a Python distribution using pyenv\\nPython is already bundled with most Unix environments. To ensure this is the case, you can \\nrun this command in a command line to show the version of the currently installed Python:\\n$ python3 --version\\nThe output version displayed will vary depending on your system. Y ou may think that \\nthis is enough to get started, but it poses an important issue: you can't choose the Python \\nversion for your project. Each Python version introduces new features and breaking \\nchanges. Thus, it's important to be able to switch to a recent version for new projects to \\ntake advantage of the new features but still be able to run older projects that may not be \\ncompatible. This is why we need pyenv.\\npyenv (https://github.com/pyenv/pyenv) is a tool that helps you manage and \\nswitch between multiple Python versions on your system. It allows you to set a default \\nPython version for your whole system but also per project.\\nBeforehand, you need to install several build dependencies on your system to allow \\npyenv to compile Python on your system. The official documentation provides clear \\nguidance on this (https://github.com/pyenv/pyenv/wiki#suggested-\\nbuild-environment), but here are the commands you should run:\\n1. Install the build dependencies:\\n• macOS users, use this:\\n$ brew install openssl readline sqlite3 xz zlib\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 23, 'page_label': '5'}, page_content='Installing a Python distribution using pyenv     5\\n• Ubuntu users, use this:\\n$ sudo apt-get update; sudo apt-get install --no-install-\\nrecommends make build-essential libssl-dev zlib1g-dev \\nlibbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\nlibncurses5-dev xz-utils tk-dev libxml2-dev libxmlsec1-\\ndev libffi-dev liblzma-dev\\nPackage managers\\nBrew and APT are what are commonly known as package managers. Their role \\nis to automate the installation and management of software on your system. \\nThus, you don\\'t have to worry about where to download them and how to \\ninstall and uninstall them. The commands just tell the package manager to \\nupdate its internal package index and then install the list of required packages.\\n2. Install pyenv:\\n$ curl https://pyenv.run | bash\\nTip\\nIf you are a macOS user, you can also install it with Homebrew: brew \\ninstall pyenv.\\n3. This will download and execute an installation script that will handle everything for \\nyou. At the end, it\\'ll prompt you with some instructions to add some lines to your \\nshell scripts so that pyenv is discovered properly by your shell:\\na. Open your ~/.profile script in nano, a simple command-line text editor:\\n$ nano ~/.profile\\nb. Add the following lines before the block containing ~/.bashrc:\\nexport PYENV_ROOT=\"$HOME/.pyenv\"\\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\\neval \"$(pyenv init --path)\"\\nc. Save by using the keyboard shortcut Ctrl + O and confirm by pressing Enter. \\nThen, quit by using the keyboard shortcut Ctrl + X.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 24, 'page_label': '6'}, page_content='6     Python Development Environment Setup\\nd. Open your ~/.bashrc script in nano. If you are using zsh instead of Bash (the \\ndefault on the latest macOS), the file is named ~/.zshrc:\\n$ nano ~/.bashrc\\ne. Add the following line at the end:\\neval \"$(pyenv init -)\"\\nf. Save by using the keyboard shortcut Ctrl + O and confirm by pressing Enter.  \\nThen, quit by using the keyboard shortcut Ctrl + X.\\n4. Reload your shell configuration to apply those changes:\\n$ source ~/.profile && exec $SHELL\\n5. If everything went well, you should now be able to invoke the pyenv tool:\\n$ pyenv\\npyenv 1.2.21\\nUsage: pyenv <command> [<args>]\\n6. We can now install the Python distribution of our choice. Even though FastAPI is \\ncompatible with Python 3.6 and later, we\\'ll use Python 3.7 throughout this book, \\nwhich has more mature handling of the asynchronous paradigm. All the examples \\nin the book were tested with this version but should work flawlessly with newer \\nversions. Let\\'s install Python 3.7:\\n$ pyenv install 3.7.10\\nThis may take a few minutes since your system will have to compile Python from \\nthe source.\\n7. Finally, you can set the default Python version with the following command:\\n$ pyenv global 3.7.10\\nThis will tell your system to always use Python 3.7.10 by default, unless \\nspecified otherwise in a specific project.\\n8. To make sure everything is in order, run the following command to check the \\nPython version that is invoked by default:\\n$ python --version\\nPython 3.7.10'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 25, 'page_label': '7'}, page_content=\"Creating a Python virtual environment     7\\nCongratulations! Y ou can now handle any version of Python on your system and switch it \\nwhenever you like!\\nCreating a Python virtual environment\\nAs for many programming languages of today, the power of Python comes from the \\nvast ecosystem of third-party libraries, including FastAPI, of course, that help you build \\ncomplex and high-quality software very quickly. The Python Package Index (https://\\npypi.org), PyPi, is the public repository that hosts all those packages. This is the default \\nrepository that will be used by the built-in Python package manager, pip.\\nBy default, when you install a third-party package with pip, it will install it for the whole \\nsystem. This is different from some other languages, such as Node.js' npm, which by default \\ncreates a local directory for the current project to install those dependencies. Obviously, \\nthis may cause issues when you work on several Python projects with dependencies \\nthat have conflicting versions. It also makes it difficult to retrieve only the dependencies \\nnecessary to deploy a project properly on a server.\\nThis is why Python developers generally use virtual environments. Basically, a virtual \\nenvironment is just a directory in your project containing a copy of your Python \\ninstallation and the dependencies of your project. It's quite similar to the node_modules \\ndirectory in Node.js. This pattern is so common that the tool to create them is bundled \\nwith Python:\\n1. Create a directory that will contain your project:\\n$ mkdir fastapi-data-science\\n$ cd fastapi-data-science\\nTip\\nIf you are on Windows with WSL, we recommend that you create your working \\nfolder on the Windows drive rather than the virtual filesystem of the Linux \\ndistribution. It'll allow you to edit your source code files in Windows with your \\nfavorite text editor or IDE while running them in Linux.\\nTo do this, you can actually access your C: drive in the Linux command \\nline through /mnt/c. Y ou can thus access your personal documents \\nusing the usual Windows path, for example, cd /mnt/c/Users/\\nYourUsername/Documents.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 26, 'page_label': '8'}, page_content=\"8     Python Development Environment Setup\\n2. Yo u  can now create a virtual environment:\\n$ python -m venv\\nBasically, this command tells Python to run the venv package of the standard \\nlibrary to create a virtual environment in the venv directory. The name of this \\ndirectory is a convention, but you can choose another name if you wish.\\n3. Once this is done, you have to activate this virtual environment. It'll tell your shell \\nsession to use the Python interpreter and the dependencies in the local directory \\ninstead of the global ones. Simply run the following command:\\n$ source venv/bin/activate\\nAfter doing this, you may notice that the prompt adds the name of the virtual \\nenvironment:\\n(venv) $\\nRemember that the activation of this virtual environment is only available for the current \\nsession. If you close it or open other command prompts, you'll have to activate it again. \\nThis is quite easy to forget, but it will become natural after some practice with Python.\\nY ou are now ready to install Python packages safely in your project!\\nInstalling Python packages with pip\\nAs we said earlier, pip is the built-in Python package manager that will help us install \\nthird-party libraries. To get started, let's install FastAPI and Uvicorn:\\n$ pip install fastapi uvicorn[standard]\\nWe'll talk about it in later chapters, but Uvicorn is required to run a FastAPI project.\\nTip\\nY ou have probably noticed the word standard inside square brackets  \\njust after uvicorn. Sometimes, some libraries have sub-dependencies  \\nthat are not required to make the library work. Usually, they are needed for \\noptional features or specific project requirements. The square brackets are  \\nhere to indicate that we want to install the standard sub-dependencies  \\nof uvicorn.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 27, 'page_label': '9'}, page_content=\"Installing the HTTPie command-line utility     9\\nTo make sure the installation worked, we can open a Python interactive shell and try to \\nimport the FastAPI package:\\n$ python\\n>>> from fastapi import FastAPI\\nIf it passes without any errors, congratulations, FastAPI is installed and ready to use!\\nInstalling the HTTPie command-line utility\\nBefore getting into the heart of the topic, there is one last tool that we'll install. FastAPI is, as \\nyou probably know, mainly about building REST APIs. To do so, you have several options:\\n• FastAPI automatic documentation (we'll talk about this later in the book)\\n• Postman, a GUI tool to perform HTTP requests\\n• cURL, the well-known and widely used command-line tool to perform  \\nnetwork requests\\nEven if visual tools are nice and easy to use, they sometimes lack some flexibility and  \\nmay not be as productive as command-line tools. On the other hand, cURL is a very \\npowerful tool with thousands of options but can be complex and verbose for testing \\nsimple REST APIs.\\nThis is why we'll introduce HTTPie, a command-line tool aimed at making HTTP \\nrequests with an intuitive syntax, JSON support, and syntax highlighting. It's available to \\ninstall from most package managers:\\n• macOS users, use this:\\n$ brew install httpie\\n• Ubuntu users, use this:\\n$ sudo apt-get update; sudo apt-get install httpie\\nLet's see how to perform simple requests on a dummy API:\\n1. First, let's retrieve data:\\n$ http GET https://603cca51f4333a0017b68509.mockapi.io/\\ntodos\\nHTTP/1.1 200 OK\\nContent-Length: 195\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 28, 'page_label': '10'}, page_content='10     Python Development Environment Setup\\nContent-Type: application/json\\n[\\n    {\\n        \"id\": \"1\",\\n        \"text\": \"Island\"\\n    }\\n]\\nAs you can see, you can invoke HTTPie with the http command and simply type \\nthe HTTP method and the URL. It outputs both the HTTP headers and the JSON \\nbody in a clean and formatted way.\\n2. HTTPie also supports sending JSON data in a request body very quickly without \\nhaving to format the JSON yourself:\\n$ http -v POST https://603cca51f4333a0017b68509.mockapi.\\nio/todos text=\"My new task\"\\nPOST /todos HTTP/1.1\\nAccept: application/json, */*;q=0.5\\nUser-Agent: HTTPie/2.3.0\\n{\\n    \"text\": \"My new task\"\\n}\\nHTTP/1.1 201 Created\\nContent-Length: 31\\nContent-Type: application/json\\n{\\n    \"id\": \"6\",\\n    \"text\": \"My new task\"\\n}\\nBy simply typing the property name and its value separated by =, HTTPie will \\nunderstand that it\\'s part of the request body in JSON. Notice here that we specified \\nthe -v option, which tells HTTPie to output the request before the response, which \\nis very useful to check that we properly specified the request.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 29, 'page_label': '11'}, page_content='Summary     11\\n3. Finally, let\\'s see how we can specify request headers:\\n$ http -v GET https://603cca51f4333a0017b68509.mockapi.\\nio/todos \"My-Header: My-Header-Value\"\\nGET /todos HTTP/1.1\\nAccept: */*\\nMy-Header: My-Header-Value\\nUser-Agent: HTTPie/2.3.0\\nHTTP/1.1 200 OK\\nContent-Length: 227\\nContent-Type: application/json\\n[\\n    {\\n        \"id\": \"1\",\\n        \"text\": \"Island\"\\n    }\\n]\\nThat\\'s it! Just type your header name and value separated by a colon to tell HTTPie it\\'s  \\na header.\\nSummary\\nY ou now have all the tools and setup required to confidently run the examples of this \\nbook and all your future Python projects. Understanding how to work with pyenv and \\nvirtual environments is a key skill to ensure everything goes smoothly when you switch to \\nanother project or when you have to work on somebody else\\'s code. Y ou also learned how \\nto install third-party Python libraries using pip. Finally, you saw how to use HTTPie, a \\nsimple and efficient way to run HTTP queries that will make you more productive while \\ntesting your REST APIs.\\nIn the next chapter, we\\'ll highlight some of Python\\'s peculiarities as a programming \\nlanguage and get a grasp of what it means to be Pythonic.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 30, 'page_label': '12'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 31, 'page_label': '13'}, page_content=\"2\\nPython \\nProgramming \\nSpecificities\\nThe Python language was designed to emphasize code readability. As such, it provides \\nsyntaxes and constructs that allow developers to quickly express complex concepts in \\nfew and readable lines. However, this makes it quite different from other programming \\nlanguages.\\nThe goal of this chapter is thus to get you acquainted with its specificities, but we expect \\nyou already have some experience with programming. We'll first get started with the \\nbasics of the language, the standard types, and the flow control syntaxes. Y ou'll also be \\nintroduced to the list comprehension and generator concepts, which are very powerful \\nways to go through and transform sequences of data. Y ou'll also see that Python can be \\nused as an object-oriented language, still through a very lightweight yet powerful syntax. \\nBefore moving on, we'll also review the concepts of type hinting and asynchronous I/O, \\nwhich are quite new in Python but are at the core of the FastAPI framework.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 32, 'page_label': '14'}, page_content=\"14     Python Programming Specificities\\nIn this chapter, we're going to cover the following main topics:\\n• Basics of Python programming\\n• List comprehensions and generators\\n• Classes and objects\\n• Type hinting and type checking with mypy\\n• Asynchronous I/O\\nTechnical requirements\\nY ou'll need a Python virtual environment, as we set up in Chapter 1, Python Development \\nEnvironment Setup.\\nY ou'll find all the code examples of this chapter in the dedicated GitHub repository: \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter2.\\nBasics of Python programming\\nFirst of all, let's review some of the key aspects of Python:\\n• It's an interpreted language. Contrary to languages such as C or Java, it doesn't \\nneed to be compiled, which allows us to run Python code interactively.\\n• It's dynamically typed. The type of values is determined at runtime.\\n• It supports several programming paradigms: procedural, object-oriented, and \\nfunctional programming.\\nThis makes Python quite a versatile language, from simple automation scripts to complex \\ndata science projects.\\nLet's now write and run some Python!\\nRunning Python scripts\\nAs we said, Python is an interpreted language. Hence, the simplest and quickest way to \\nrun some Python code is to launch an interactive shell. Just run the following command \\nto start a session:\\n$ python\\nPython 3.7.10 (default, Mar  7 2021, 10:12:14)\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 33, 'page_label': '15'}, page_content='Basics of Python programming     15\\n[Clang 12.0.0 (clang-1200.0.32.29)] on darwin\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more \\ninformation.\\n>>> \\nThis shell makes it very easy to run some simple statements and make some experiments:\\n>>> 1 + 1\\n2\\n>>> x = 100\\n>>> x * 2\\n200\\nTo exit the shell, use the Ctrl + D keyboard shortcut.\\nObviously, this can become tedious when you start to have more statements or if you just \\nwish to keep your work to reuse it later. Python scripts are saved in files with the .py \\nextension. Let\\'s create a file named chapter2_basics_01.py in our project directory \\nand add this code:\\nchapter2_basics_01.py\\nprint(\"Hello world!\")\\nx = 100\\nprint(f\"Double of {x} is {x * 2}\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_01.py\\nQuite simply, this script prints Hello world in the console, assigns the value 100 to a \\nvariable named x, and prints a string with the value of x and its double. To run it, simply \\nadd the path of your script as a parameter of the python command:\\n$ python chapter2_basics_01.py\\nHello world!\\nDouble of 100 is 200'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 34, 'page_label': '16'}, page_content=\"16     Python Programming Specificities\\nf-strings\\nY ou have probably noticed the string starting with f. This syntax, called \\nf-strings, is a very convenient and neat way to perform string interpolation. \\nWithin, you can simply insert variables between curly braces; they will \\nautomatically be converted into strings to build the resulting string. We'll use it \\nquite often in our examples.\\nThat's it! Y ou are now able to write and run simple Python scripts. Let's now dive deeper \\ninto the Python syntax.\\nIndentation matters\\nOne of the most iconic aspects of Python is that code blocks are not defined using curly \\nbraces like many other programming languages, but rather with whitespace indentation. \\nThis may sound a bit strange, but it's at the heart of the readability philosophy of Python. \\nLet's see how you can write a script that finds the even numbers in a list:\\nchapter2_basics_02.py\\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\neven = []\\nfor number in numbers:\\n    if number % 2 == 0:\\n        even.append(number)\\nprint(even)  # [2, 4, 6, 8, 10]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_02.py\\nIn this script, we define numbers, a list of numbers from 1 to 10, and even, an empty list \\nthat will contain the even numbers.\\nThen, we define a for loop statement to go through each element of numbers. As you \\nsee, we open a block with a colon, :, break a line, and start writing the next statement with \\nan indentation.\\nThe next line is a conditional statement to check the parity of the current number. Once \\nagain, we open a block with a colon, :, and write the next statement with an additional \\nindentation level. This statement adds the even number to the even list.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 35, 'page_label': '17'}, page_content='Basics of Python programming     17\\nAfter that, the next statements are not intended. This means that we are out of the for \\nloop block; they should be executed after the iteration is finished.\\nLet\\'s run it:\\n$ python chapter2_basics_02.py\\n[2, 4, 6, 8, 10]\\nIndentation style and size\\nY ou can choose the indentation style (tabs or spaces) and size (2, 4, 6…) you \\nprefer; the only constraint is that you should be consistent within a block. \\nHowever, by convention, Python developers usually go for a four-space \\nindentation.\\nThis aspect of Python may sound weird but with some practice, you\\'ll find that it enforces \\nclear formatting and greatly improves the readability of your scripts.\\nWe\\'ll now review the built-in types and data structures.\\nWorking with built-in types\\nPython is quite conventional regarding scalar types. There are six of them:\\n• int, to store integer values, such as x = 1\\n• float, for floating-point numbers, such as x = 1.5\\n• complex, for complex numbers, such as x = 1 + 2j\\n• bool, for Boolean values, either True or False\\n• str, for string values, such as x = \"abc\"\\n• NoneType, to indicate null values, such as x = None\\nIt\\'s worth noting that Python is strongly typed, meaning that the interpreter will limit \\nimplicit type conversions. For example, trying to add an int value and a str value will \\nraise an error, as you can see in the following example:\\n>>> 1 + \"abc\"\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nTypeError: unsupported operand type(s) for +: \\'int\\' and \\'str\\''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 36, 'page_label': '18'}, page_content=\"18     Python Programming Specificities\\nStill, adding an int value and a float value will automatically upcast the result to \\nfloat:\\n>>> 1 + 1.5\\n2.5\\nAs you may have noticed, Python is quite traditional regarding those standard types.  \\nLet's see now how basic data structures are handled.\\nWorking with data structures – lists, tuples, \\ndictionaries, and sets\\nBesides the scalar types, Python also provides handy data structures: an array structure, \\nof course, called a list in Python, but also tuples, dictionaries, and sets, which are very \\nconvenient in lots of cases. Let's start with lists.\\nLists\\nLists are the equivalent in Python of the classic array structure. Defining a list is quite \\nstraightforward:\\n>>> l = [1, 2, 3, 4, 5]\\nAs you can see, wrapping a suite of elements in square brackets denotes a list. Y ou can of \\ncourse access single elements by index:\\n>>> l[0]\\n1\\n>>> l[2]\\n3\\nIt also supports negative indexing, which allows retrieving elements from the end of the \\nlist: index -1 is the last element, -2 is the second-last element, and so on:\\n>>> l[-1]\\n5\\n>>> l[-4]\\n2\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 37, 'page_label': '19'}, page_content='Basics of Python programming     19\\nAnother useful syntax is slicing, which quickly allows you to retrieve a sub-list:\\n>>> l[1:3]\\n[2, 3]\\nThe first number is the start index (inclusive) and the second one is the end index \\n(exclusive), separated by a colon. Y ou can omit the first one; in this case, 0 is assumed:\\n>>> l[:3]\\n[1, 2, 3] \\nY ou can also omit the second one; in this case, the length of the list is assumed:\\n>>> l[1:]\\n[2, 3, 4, 5]\\nFinally, this syntax also supports a third argument to specify the step size. It can be useful \\nto select every second element of the list:\\n>>> l[::2]\\n[1, 3, 5]\\nA useful trick with this syntax is to use -1 to reverse the list:\\n>>> l[::-1]\\n[5, 4, 3, 2, 1]\\nLists are mutable. This means that you can reassign elements or add new ones:\\n>>> l[1] = 10\\n>>> l\\n[1, 10, 3, 4, 5]\\n>>> l.append(6)\\n[1, 10, 3, 4, 5, 6]\\nThis is different from their cousin, tuples, which are immutable.\\nTuples\\nTuples are very similar to lists. Instead of square brackets, they are defined using \\nparentheses:\\n>>> t = (1, 2, 3, 4, 5)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 38, 'page_label': '20'}, page_content='20     Python Programming Specificities\\nThey support the same syntax as lists to access elements or slicing:\\n>>> t[2]\\n3\\n>>> t[1:3]\\n(2, 3)\\n>>> t[::-1]\\n(5, 4, 3, 2, 1)\\nHowever, tuples are immutable. This means that you can\\'t add new elements or change \\nexisting ones. Trying to do so will raise an error:\\n>>> t[1] = 10\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nTypeError: \\'tuple\\' object does not support item assignment\\n>>> t.append(6)\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nAttributeError: \\'tuple\\' object has no attribute \\'append\\'\\nA common way to use them is for functions that have multiple return values. In the \\nfollowing example, we define a function to compute and return both the quotient and \\nremainder of the Euclidean division:\\nchapter2_basics_03.py\\ndef euclidean_division(dividend, divisor):\\n    quotient = dividend // divisor\\n    remainder = dividend % divisor\\n    return (quotient, remainder)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_03.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 39, 'page_label': '21'}, page_content='Basics of Python programming     21\\nThis function simply returns the quotient and remainder wrapped in a tuple. Let\\'s now \\ncompute the Euclidean division of 3 and 2:\\nchapter2_basics_03.py\\nt = euclidean_division(3, 2)\\nprint(t[0])  # 1\\nprint(t[1])  # 1\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_03.py\\nIn this case, we assign the result to a tuple named t, and simply retrieve the quotient and \\nremainder by index. However, we can do something better than that. Let\\'s compute the \\nEuclidean division of 42 and 4:\\nchapter2_basics_03.py\\nq, r = euclidean_division(42, 4)\\nprint(q)  # 10\\nprint(r)  # 2\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_03.py\\nY ou see here that we directly assign the quotient and remainder to the q and r variables, \\nrespectively. This syntax is called unpacking and is very convenient to assign variables \\nfrom list or tuple elements. It\\'s worth noting that since t is a tuple, it\\'s immutable, so you \\ncan\\'t reassign the values. On the other hand, q and r are new variables and therefore are \\nmutable.\\nDictionaries\\nA dictionary is also a widely used data structure in Python, to map keys to values.  \\nIt is defined using curly braces, with a list of keys and values separated by a colon:\\n>>> d = {\"a\": 1, \"b\": 2, \"c\": 3}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 40, 'page_label': '22'}, page_content='22     Python Programming Specificities\\nElements can be accessed by key:\\n>>> d[\"a\"]\\n1\\nDictionaries are mutable, so you can reassign or add elements in the mapping:\\n>>> d[\"a\"] = 10\\n>>> d\\n{\\'a\\': 10, \\'b\\': 2, \\'c\\': 3}\\n>>> d[\"d\"] = 4\\n>>> d\\n{\\'a\\': 10, \\'b\\': 2, \\'c\\': 3, \\'d\\': 4}\\nSets\\nA set is a convenient data structure to store a collection of unique items. It is defined using \\ncurly braces:\\n>>> s = {1, 2, 3, 4, 5}\\nElements can be added to the set, but the structure ensures elements appear only once:\\n>>> s.add(1)\\n>>> s\\n{1, 2, 3, 4, 5}\\n>>> s.add(6)\\n{1, 2, 3, 4, 5, 6}\\nConvenient methods are also provided to perform operations such as unions or \\nintersections on two sets:\\n>>> s.union({4, 5, 6})\\n{1, 2, 3, 4, 5, 6}\\n>>> s.intersection({4, 5, 6})\\n{4, 5}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 41, 'page_label': '23'}, page_content=\"Basics of Python programming     23\\nThat's all for this overview of the Python data structures. Y ou'll probably use them quite \\noften in your programs, so take some time to get acquainted with them. Obviously, we \\ndidn't cover all of their methods and specificities, but you can have a look at the official \\nPython documentation for exhaustive information: https://docs.python.org/3/\\nlibrary/stdtypes.html.\\nLet's now talk about the different types of operators available in Python that will allow us \\nto perform some logic on this data.\\nPerforming Boolean logic and checking for existence\\nPredictably, Python provides operators to perform Boolean logic. However, we'll also \\nsee that there are other operators that are less common but make Python a very efficient \\nlanguage to work with.\\nPerforming Boolean logic\\nBoolean logic is performed with the and, or, and not keywords. Let's review some \\nsimple examples:\\n>>> x = 10\\n>>> x > 0 and x < 100\\nTrue\\n>>> x > 0 or (x % 2 == 0)\\nTrue\\n>>> not (x > 0)\\nFalse\\nY ou'll probably use them quite often in your programs, especially with conditional blocks. \\nLet's now review the identity operators.\\nChecking whether two variables are the same\\nThe is and is not identity operators check whether two variables refer to the same \\nobject. This is different from the == and != comparison operators, which check whether \\ntwo variables have the same value.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 42, 'page_label': '24'}, page_content=\"24     Python Programming Specificities\\nInternally, Python stores variables in pointers. The goal of the identity operators is thus \\nto check whether two variables actually point to the same pointer. Let's review some \\nexamples:\\n>>> a = [1, 2, 3]\\n>>> b = [1, 2, 3]\\n>>> a is b\\nFalse\\nEven though lists a and b are identical, they don't share the same pointer, so a is b is \\nfalse. However, a == b is true. Let's see what happen if we assign a to b:\\n>>> a = [1, 2, 3]\\n>>> b = a\\n>>> a is b\\nTrue\\nIn this case, the b variable will now refer to the same pointer as a, that is, the same list in \\nmemory. Thus, the identity operator is true.\\nis None or == None?\\nTo check whether a variable is null, you could write a == None. While it \\nwill work most of the time, it's generally advised to write a is None.\\nWhy? In Python, classes can implement custom comparison operators, so the \\nresult of a == None may be unpredictable in some cases, since a class can \\nchoose to attach a special meaning to the None value.\\nWe'll now review the membership operators.\\nChecking whether a value is present in a data structure\\nThe membership operators, in and not in, are very useful to check whether an element \\nis present in data structures such as lists or dictionaries. They are idiomatic of Python and \\nmake this operation very efficient and easy to write. Let's review some examples:\\n>>> l = [1, 2, 3]\\n>>> 2 in l\\nTrue\\n>>> 5 not in l\\nTrue\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 43, 'page_label': '25'}, page_content='Basics of Python programming     25\\nWith the membership operators, we can check in one statement whether an element is \\npresent or not in a list. It also works with tuples and sets:\\n>>> t = (1, 2, 3)\\n>>> 2 in t\\nTrue \\n>>> s = {1, 2, 3}\\n>>> 2 in s\\nTrue\\nFinally, it also works with dictionaries. In this case, the membership operators check \\nwhether the key is present, not the value:\\n>>> d = {\"a\": 1, \"b\": 2, \"c\": 3}\\n>>> \"b\" in d\\nTrue\\n>>> 3 in d\\nFalse\\nWe are now clear about those common operations. We\\'ll now put them to use with \\nconditional statements.\\nControlling the flow of a program\\nA programming language would not be a programming language without its control flow \\nstatements. Once again, you\\'ll see that Python is a bit different from other languages. Let\\'s \\nstart with conditional statements.\\nExecuting operations conditionally – if, elif, else\\nClassically, those statements are there to perform some logic based on some Boolean \\nconditions. In the following example, we\\'ll consider a situation where we have a dictionary \\ncontaining information about an e-commerce website order. We\\'ll write a function that \\nwill change the order status to the next step given the current status:\\nchapter2_basics_04.py\\ndef forward_order_status(order):\\n    if order[\"status\"] == \"NEW\":\\n        order[\"status\"] = \"IN_PROGRESS\"\\n    elif order[\"status\"] == \"IN_PROGRESS\":'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 44, 'page_label': '26'}, page_content='26     Python Programming Specificities\\n        order[\"status\"] = \"SHIPPED\"\\n    else:\\n        order[\"status\"] = \"DONE\"\\n    return order\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_04.py\\nThe first condition is noted as if, followed by a Boolean condition. We then open an \\nindented block, as we explained in the Indentation matters section of this chapter.\\nThe alternate conditions are noted as elif (not else if) and the fallback block is noted \\nas else. Of course, those are optional if you don\\'t need alternate or fallback conditions.\\nIt\\'s also worth noting that, contrary to many other languages, Python does not provide a \\nswitch statement.\\nWe\\'ll now move on to another classic control flow statement: the for loop. Y ou can repeat \\noperations over a sequence using the for loop statement.\\nWe already saw an example of the for loop in action in the Indentation matters section \\nof this chapter. As you probably understood, this statement is useful for repeating the \\nexecution of a code block.\\nY ou also may have noticed that it works a bit differently than other languages. Usually, \\nprogramming languages define for loops like this: for (i = 0; i <= 10; i++). \\nThey give you the responsibility to define and control the variable used for the iteration.\\nPython doesn\\'t work this way. Instead, it expects you to feed the loop with an iterator. \\nAn iterator can be seen as a sequence of elements that you can retrieve one by one. Lists, \\ntuples, dictionaries, and sets can behave like an iterator and be used in a for loop. Let\\'s \\nsee some examples:\\n>>> for i in [1,2,3]:\\n...     print(i)\\n... \\n1\\n2\\n3\\n>>> for k in {\"a\": 1, \"b\": 2, \"c\": 3}:\\n...     print(k)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 45, 'page_label': '27'}, page_content=\"Basics of Python programming     27\\n... \\na\\nb\\nc\\nBut what if you just wish to iterate a certain number of times? Luckily, Python has built-in \\nfunctions that generate some useful iterators. The most known is range, which precisely \\ncreates a sequence of numbers. Let's see how it works:\\n>>> for i in range(3):\\n...     print(i)\\n... \\n0\\n1\\n2\\nrange will generate a sequence of the size you provided in the first argument, starting \\nwith 0.\\nY ou could also be more precise by specifying two arguments: the start index (inclusive) \\nand the last index (exclusive):\\n>>> for i in range(1, 3):\\n...     print(i)\\n... \\n1\\n2\\nFinally, you may even provide a step as the third argument:\\n>>> for i in range(0, 5, 2):\\n...     print(i)\\n... \\n0\\n2\\n4\\nNote that this syntax is quite similar to the slicing syntax we saw earlier in this chapter in \\nthe sections dedicated to Lists and Tuples.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 46, 'page_label': '28'}, page_content='28     Python Programming Specificities\\nrange output is not a list\\nA common misconception is to think that range returns a list. It\\'s actually a \\nsequence object that only stores the start, end, and step arguments. That\\'s \\nwhy you could write range(1000000000) without blowing up your \\nmemory; the millions of integers are not assigned in memory all at once.\\nAs you see, the for loop syntax in Python is quite straightforward to understand and \\nemphasizes readability. We\\'ll now have a word about its cousin, the while loop.\\nRepeating operations until a condition is met – the while loop \\nstatement\\nThe classical while loop is also available in Python. At the risk of disappointing you, \\nthere is nothing truly special about this one. Classically, this statement allows you to \\nrepeat instructions until a condition is met. We\\'ll review an example in which we use a \\nwhile loop to retrieve paginated elements until we reach the end:\\nchapter2_basics_05.py\\ndef retrieve_page(page):\\n    if page > 3:\\n        return {\"next_page\": None, \"items\": []}\\n    return {\"next_page\": page + 1, \"items\": [\"A\", \"B\", \"C\"]}\\nitems = []\\npage = 1\\nwhile page is not None:\\n    page_result = retrieve_page(page)\\n    items += page_result[\"items\"]\\n    page = page_result[\"next_page\"]\\nprint(items)  # [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_05.py\\nThe retrieve_page function is a dummy function that returns a dictionary with \\nthe items for the page passed in an argument and the next page number or None if \\nwe reached the last page. A priori, we don\\'t know how many pages there are. Thus, we \\nrepeatedly call retrieve_page until page is None. At each iteration, we save the \\ncurrent page items in an accumulator, items.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 47, 'page_label': '29'}, page_content=\"Basics of Python programming     29\\nThis kind of use case is quite common when you are dealing with third-party REST APIs \\nand you wish to retrieve all items available, and while loops perfectly help with this.\\nIt may happen though that you wish for finer control of the loop behavior. This is where \\nbreak and continue come in.\\nFinely controlling a loop – break and continue\\nThere are cases where you wish to prematurely end the loop or skip an iteration. To solve \\nthis, Python implements the classic break and continue statements.\\nDefining functions\\nNow that we know how to use the common operators and control the flow of our \\nprogram, let's put it in reusable logic. As you may have guessed, we'll look at functions \\nand how to define them. We already saw them in some of our previous examples, but let's \\nintroduce them more formally.\\nIn Python, functions are defined using the def keyword followed by the name of the \\nfunction. Then, you have the list of supported arguments in parentheses, before a colon \\nthat indicates the start of the function body. Let's see a simple example:\\n>>> def f(a):\\n...     return a\\n... \\n>>> f(2)\\n2\\nThat's it! Python also supports default values on arguments:\\n>>> def f(a, b = 1):\\n...     return a, b\\n... \\n>>> f(2)\\n(2, 1)\\n>>> f(2, 3)\\n(2, 3)\\nWhen calling a function, you can specify the values of arguments using their name:\\n>>> f(a=2, b=3)\\n(2, 3)\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 48, 'page_label': '30'}, page_content='30     Python Programming Specificities\\nThose arguments are called keyword arguments. They are especially useful if you have \\nseveral default arguments but only wish to set one of them:\\n>>> def f(a = 1, b = 2, c = 3):\\n...     return a, b, c\\n...\\n>>> f(c=1)\\n(1, 2, 1)\\nFunction naming\\nBy convention, functions should be named using snake case: my_\\nwonderful_function but not MyWonderfulFunction.\\nBut there is more! Y ou can actually define functions accepting a dynamic number  \\nof arguments.\\nAccepting arguments dynamically with *args and **kwargs\\nSometimes, you may need a function that supports a dynamic number of arguments. \\nThose arguments are then handled in your function logic at runtime. To do this, you have \\nto use the *args and **kwargs syntax. Let\\'s define a function that uses this syntax and \\nprints the value of those arguments:\\n>>> def f(*args, **kwargs):\\n...     print(\"args\", args)\\n...     print(\"kwargs\", kwargs)\\n... \\n>>> f(1, 2, 3, a=4, b=5)\\nargs (1, 2, 3)\\nkwargs {\\'a\\': 4, \\'b\\': 5}\\nAs you see, standard arguments are placed in a tuple , in the same order as they have \\nbeen called. Keyword arguments, on the other hand, have been placed in a dictionary , \\nwith the key being the name of the argument. It\\'s up to you then to use this data to \\nperform your logic!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 49, 'page_label': '31'}, page_content='Basics of Python programming     31\\nInterestingly, you can mix both approaches so that you have hardcoded arguments and \\ndynamic ones:\\n>>> def f(a, *args):\\n...     print(\"a\", a)\\n...     print(\"arg\", args)\\n... \\n>>> f(1, 2, 3)\\na 1\\narg (2, 3)\\nWell done! Y ou have learned how to write functions in Python to organize the logic of \\nyour program. The next step now is to organize those functions into modules and import \\nthem into other modules to take advantage of them!\\nWriting and using packages and modules\\nY ou probably already know that, apart from small scripts, your source code shouldn\\'t live \\nin one big file with thousands of lines. Instead, you should split it into logical blocks of \\nreasonable size that are easy to maintain. That\\'s exactly what packages and modules are \\nfor! We\\'ll see how they work and how you can define your own.\\nFirst of all, Python comes with its own set of modules, the standard library, that are \\ndirectly importable in a program: \\n>>> import datetime\\n>>> datetime.date.today()\\ndatetime.date(2021, 3, 12)\\nWith just the import keyword, you can use the datetime module and access all its \\ncontent by referring to its namespace, datetime.date, which is the built-in class to work \\nwith dates. However, you may wish sometimes to explicitly import a part of this module:\\n>>> from datetime import date\\n>>> date.today()\\ndatetime.date(2021, 3, 12)\\nHere, we explicitly import the date class to use it directly.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 50, 'page_label': '32'}, page_content='32     Python Programming Specificities\\nThe same principles apply to third-party packages installed with pip, such as FastAPI.\\nUsing existing packages and modules is nice but writing your own is even better. In \\nPython, a module is a single file containing declarations but can also contain instructions \\nthat will be executed when the module is first imported. Y ou\\'ll find in the following \\nexample the definition of a very simple module:\\nchapter2_basics_module.py\\ndef module_function():\\n    return \"Hello world\"\\nprint(\"Module is loaded\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_basics_\\nmodule.py\\nThis module only contains a function, module_function, and a print statement. \\nCreate a file containing this code at the root of your project directory and name it \\nmodule.py. Then, open a Python interpreter and run this command:\\n>>> import module\\nModule is loaded\\nNotice that the print statement was executed when you imported it. Y ou can now use \\nthe following function:\\n>>> module.module_function()\\n\\'Hello world\\'\\nCongratulations! Y ou\\'ve just written your first Python module!\\nNow, let\\'s see how to structure a package. A package is a way to organize modules in a \\nhierarchy that you can then import using its namespace.\\nAt the root of your project, create a directory named package. Inside, create another \\ndirectory named subpackage and move module.py into it. Y our project structure \\nshould look like the one shown in Figure 2.1:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 51, 'page_label': '33'}, page_content=\"Basics of Python programming     33\\nFigure 2.1 – Python package sample hierarchy\\nY ou can then import your module using the full namespace:\\n>>> import package.subpackage.module\\nModule is loaded\\nIt works! However, to define a proper Python package, it's strongly recommended to create \\nan empty __init__.py file at the root of each package and sub-package. In older \\nPython versions, it was compulsory to make a package recognizable by the interpreter. \\nThis became optional in more recent versions, but there are actually some subtle \\ndifferences between a package with an __init__.py file (a package) and one without \\n(a namespace package). We won't explain it further in this book, but you could check the \\ndocumentation about namespace packages here if you wish for more details: https://\\npackaging.python.org/guides/packaging-namespace-packages/.\\nTherefore, you generally should always create __init__.py files. In our example, our \\nproject structure would finally look like this:\\nFigure 2.2 – Python package hierarchy with __init__.py files\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 52, 'page_label': '34'}, page_content=\"34     Python Programming Specificities\\nIt's worth noting that even if empty __init__.py files are perfectly fine, you can \\nactually write some code in them. In this case, it is executed the first time you import the \\npackage or one of its sub-modules. It's useful to perform some initialization logic for your \\npackage. Y ou now have a good overview of how to write some Python code. Feel free to \\nwrite some small scripts to get acquainted with its peculiar syntax. We'll now explore more \\nadvanced topics about the language that will prove useful during our journey with FastAPI.\\nOperating over sequences – list \\ncomprehensions and generators\\nIn this section, we'll cover what are probably the most idiomatic constructions in Python: \\nlist comprehensions and generators. Y ou'll see that they are very useful for reading and \\ntransforming sequences of data with very minimal syntax.\\nList comprehensions\\nIn programming, a very common task is to transform a sequence (let's say a list) into \\nanother, for example, to filter out or transform elements. Usually, you would write an \\noperation as we did in one of the previous examples of this chapter:\\nchapter2_basics_02.py\\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\neven = []\\n \\nfor number in numbers:\\n    if number % 2 == 0:\\n        even.append(number)\\n \\nprint(even)  # [2, 4, 6, 8, 10]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nbasics_02.py\\nWith this approach, we simply iterate over each element, check a condition, and add the \\nelement in an accumulator if it passes this condition.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 53, 'page_label': '35'}, page_content=\"Operating over sequences – list comprehensions and generators     35\\nTo go further into its readability philosophy, Python supports a neat syntax to perform \\nthis operation in only one statement: list comprehensions. Let's see what our previous \\nexample looks like with this syntax:\\nchapter2_list_comprehensions_01.py\\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\neven = [number for number in numbers if number % 2 == 0]\\nprint(even)  # [2, 4, 6, 8, 10]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_01.py\\nThat's it! Basically, a list comprehension works by packing a for loop and wrapping it \\nwith square brackets. The element to add to the result list appears first, followed by the \\niteration. Optionally, we can add a condition, as we did here, to filter some elements of the \\nlist input.\\nActually, the result element can be any valid Python expression. In the following example, \\nwe use the randint function of the random standard module to generate a list of \\nrandom integers:\\nchapter2_list_comprehensions_02.py\\nfrom random import randint, seed\\n \\nseed(10)  # Set random seed to make examples reproducible\\nrandom_elements = [randint(1, 10) for i in range(5)]\\nprint(random_elements)  # [10, 1, 7, 8, 10]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_02.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 54, 'page_label': '36'}, page_content=\"36     Python Programming Specificities\\nThis syntax is widely used by Python programmers and you'll probably grow quite fond \\nof it. The nice thing about this syntax is that it also works for sets and dictionaries. Quite \\nsimply, just replace the square brackets with curly braces to generate a set:\\nchapter2_list_comprehensions_03.py\\nfrom random import randint, seed\\n \\nseed(10)  # Set random seed to make examples reproducible\\nrandom_unique_elements = {randint(1, 10) for i in range(5)}\\nprint(random_unique_elements)  # {8, 1, 10, 7}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_03.py\\nTo create a dictionary, specify both the key and the value separated by a colon:\\nchapter2_list_comprehensions_04.py\\nfrom random import randint, seed\\n \\nseed(10)  # Set random seed to make examples reproducible\\nrandom_dictionary = {i: randint(1, 10) for i in range(5)}\\nprint(random_dictionary)  # {0: 10, 1: 1, 2: 7, 3: 8, 4: 10}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_04.py\\nGenerators\\nY ou might think that if you replace the square brackets with parentheses, you could obtain \\na tuple. Actually, you get a generator object. The main difference between generators and \\nlist comprehensions is that elements are generated on demand and not computed and \\nstored all at once in memory. Y ou could see a generator as a recipe to generate values.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 55, 'page_label': '37'}, page_content=\"Operating over sequences – list comprehensions and generators     37\\nAs we said, a generator can be defined simply by using the same syntax as list \\ncomprehensions, with parentheses:\\nchapter2_list_comprehensions_05.py\\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\neven_generator = (number for number in numbers if number % 2 == \\n0)\\neven = list(even_generator)\\neven_bis = list(even_generator)\\n \\nprint(even)  # [2, 4, 6, 8, 10]\\nprint(even_bis)  # []\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_05.py\\nIn this example, we define even_generator to output the even numbers of the \\nnumbers list. Then, we call the list constructor with this generator and assign it to the \\nvariable named even. This constructor will exhaust the iterator passed in the parameter \\nand build a proper list. We do it a second time and assign it to even_bis.\\nAs you see, even is a list with all the even numbers. However, even_bis is an empty list. \\nThis simple example is here to show you that a generator can be used only once. Once all \\nthe values have been produced, it's over.\\nThis can be useful because you can start to iterate on the generator, stop to do something \\nelse, and then resume iterating.\\nAnother way to create generators is to define generator functions. In the following \\nexample, we'll define a generator function that outputs even numbers from 2 to the limit \\npassed in the argument:\\nchapter2_list_comprehensions_06.py\\ndef even_numbers(max):\\n    for i in range(2, max + 1):\\n        if i % 2 == 0:\\n            yield i\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 56, 'page_label': '38'}, page_content='38     Python Programming Specificities\\n \\neven = list(even_numbers(10))\\nprint(even)  # [2, 4, 6, 8, 10]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_06.py\\nAs you see in this function, we use the yield keyword instead of return. When the \\ninterpreter reaches this statement, it pauses the function execution and yields the value to \\nthe generator consumer. When the main program asks for another value, the function is \\nresumed in order to yield again.\\nThis allows us to implement complex generators, even ones that will output different types \\nof values over their course. Another interesting property of generator functions is that \\nthey allow us to execute some instructions when they have finished to generate values. \\nLet\\'s add a print statement at the end of the function we just reviewed:\\nchapter2_list_comprehensions_07.py\\ndef even_numbers(max):\\n    for i in range(2, max + 1):\\n        if i % 2 == 0:\\n            yield i\\n    print(\"Generator exhausted\")\\n \\n \\neven = list(even_numbers(10))\\nprint(even)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_list_\\ncomprehensions_07.py\\nIf you execute it in a Python interpreter, you\\'ll get this output:\\n$ python chapter2_list_comprehensions_07.py\\nGenerator exhausted\\n[2, 4, 6, 8, 10] \\nWe get Generator exhausted in the output, which means that our code after the last \\nyield statement is well executed.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 57, 'page_label': '39'}, page_content='Writing object-oriented programs     39\\nThis is especially useful when you want to perform some cleanup operations after your \\ngenerator has been exhausted: close a connection, remove temporary files, and so on.\\nWriting object-oriented programs\\nAs we said in the first section of this chapter, Python is a multi-paradigm language, and \\none among those paradigms is object-oriented programming. In this section, we\\'ll \\nreview how you can define classes and how you can instantiate and use objects. Y ou\\'ll see \\nthat Python syntax is once again very lightweight.\\nDefining a class\\nDefining a class in Python is straightforward: use the class keyword, type the name \\nof your class, and begin a new block. Y ou can then define methods under it just like you \\nwould for regular functions. Let\\'s review an example:\\nchapter2_classes_objects_01.py\\nclass Greetings:\\n    def greet(self, name):\\n        return f\"Hello, {name}\"\\n \\n \\nc = Greetings()\\nprint(c.greet(\"John\"))  # \"Hello, John\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_01.py\\nNotice that the first argument of each method must be self, which is a reference to the \\ncurrent object instance (the equivalent of this in other languages).\\nTo instantiate a class, simply call the class as you would for a function and assign it to a \\nvariable. Y ou can then access the methods using dot notation.\\nClass and method naming\\nBy convention, classes should be named using camel case: \\nMyWonderfulClass but not my_wonderful_class. Methods \\nshould use snake case like regular functions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 58, 'page_label': '40'}, page_content='40     Python Programming Specificities\\nObviously, you can also set class properties. To do this, we\\'ll implement the __init__ \\nmethod, whose goal is to initialize values:\\nchapter2_classes_objects_02.py\\nclass Greetings:\\n    def __init__(self, default_name):\\n        self.default_name = default_name\\n \\n    def greet(self, name=None):\\n        return f\"Hello, {name if name else self.default_name}\"\\n \\n \\nc = Greetings(\"Alan\")\\nprint(c.default_name)  # \"Alan\"\\nprint(c.greet())  # \"Hello, Alan\"\\nprint(c.greet(\"John\"))  # \"Hello, John\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_02.py\\nIn this example, __init__ allows us to set a default_name property, which will be \\nused by the greet method if no name is provided in the argument. As you see, you can \\nsimply access this property through dot notation.\\nBe careful though: __init__ is not a constructor. In typical object-oriented languages, \\na constructor is a method to actually create the object in memory. In Python, when __\\ninit__ is called, the object is already created in memory (notice we have access to the \\nself instance). Actually, there is a method to define the constructor, __new__, but it\\'s \\nrarely used in Python.\\nPrivate methods and properties\\nIn Python, there is no such thing as private methods or properties. Everything \\nwill always be accessible from the outside. However, by convention, you can \\nprefix your private methods and properties with an underscore to suggest that \\nthey should be considered as private: _private_method.\\nY ou now know the basics of object-oriented programming in Python! We\\'ll now focus on \\nmagic methods, which will allow us to do clever things with objects.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 59, 'page_label': '41'}, page_content='Writing object-oriented programs     41\\nImplementing magic methods\\nMagic methods are a set of predefined methods that bear a special meaning in the \\nlanguage. They are easy to recognize as they start and end with two underscores. Actually, \\nwe already saw one of those magic methods: __init__! Those methods are not called \\ndirectly but are used by the interpreter when using other constructs such as standard \\nfunctions or operators.\\nTo understand how they are useful, we\\'ll review the most used. Let\\'s start with __repr__ \\nand __str__.\\nObject representations – __repr__ and __str__\\nWhen you define a class, it\\'s generally useful to be able to get a readable and clear string \\nrepresentation of an instance. For this purpose, Python provides two magic methods: __\\nrepr__ and __str__. Let\\'s see how they work on a class representing a temperature in \\neither degrees Celsius or degrees Fahrenheit:\\nchapter2_classes_objects_03.py\\nclass Temperature:\\n    def __init__(self, value, scale):\\n        self.value = value\\n        self.scale = scale\\n \\n    def __repr__(self):\\n        return f\"Temperature({self.value}, {self.scale!r})\"\\n \\n    def __str__(self):\\n        return f\"Temperature is {self.value} °{self.scale}\"\\n \\n \\nt = Temperature(25, \"C\")\\nprint(repr(t))  # \"Temperature(25, \\'C\\')\"\\nprint(str(t))  # \"Temperature is 25 °C\"\\nprint(t)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_03.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 60, 'page_label': '42'}, page_content='42     Python Programming Specificities\\nIf you run this example, you\\'ll notice that print(t) prints the same thing as \\nprint(str(t)). Through print, the interpreter called the __str__ method to get \\nthe string representation of our object. This is what __str__ is for: giving a nice string \\nrepresentation of an object for the end user. \\nOn the other hand, you saw that even if very similar, we implemented __repr__ in a \\ndifferent way. The purpose of this method is to give an internal representation of the object \\nthat is unambiguous. By convention, this should give the exact statement that would allow \\nus to recreate the very same object.\\nNow that we can represent temperatures with our class, what would happen if we tried to \\ncompare them?\\nComparison methods – __eq__, __gt__, __lt__, and so on\\nOf course, comparing two temperatures with different units would lead to unexpected \\nresults. Fortunately, magic methods allow us to overload the default operators to perform \\nmeaningful comparisons. Let\\'s expand our previous example:\\nchapter2_classes_objects_04.py\\nclass Temperature:\\n    def __init__(self, value, scale):\\n        self.value = value\\n        self.scale = scale\\n        if scale == \"C\":\\n            self.value_kelvin = value + 273.15\\n        elif scale == \"F\":\\n            self.value_kelvin = (value - 32) * 5/9 + 273.15\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_04.py\\nIn the __init__ method, we convert the temperature value in Kelvin given the current \\nscale. This will help us to make comparisons. Then, let\\'s define __eq__ and __lt__:\\nchapter2_classes_objects_04.py\\n    def __eq__(self, other):\\n        return self.value_kelvin == other.value_kelvin'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 61, 'page_label': '43'}, page_content='Writing object-oriented programs     43\\n    def __lt__(self, other):\\n        return self.value_kelvin < other.value_kelvin\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_04.py\\nAs you see, those methods simply accept another argument, which is the other object \\ninstance to compare with. We then just have to perform our comparison logic. By doing \\nthis, we can perform comparison just as we would for any variable:\\nchapter2_classes_objects_04.py\\ntc = Temperature(25, \"C\")\\ntf = Temperature(77, \"F\")\\ntf2 = Temperature(100, \"F\")\\nprint(tc == tf)  # True\\nprint(tc < tf2)  # True\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_04.py\\nThat\\'s it! If you wish to have all the comparison operators available, you should also \\nimplement all the other comparison magic methods: __le__, __gt__, and __ge__.\\nThe type of the other instance is not guaranteed\\nIn this example, we assumed that the other variable was also a \\nTemperature object. In the real world, however, this is not guaranteed and \\ndevelopers could try to compare Temperature with another object, which \\nwould likely lead to errors or weird behaviors. To prevent this, you should \\ncheck the type of the other variable using isinstance to ensure we \\nhandle Temperature, or raise a proper exception otherwise.\\nOperators – __add__, __sub__, __mul__, and so on\\nSimilarly, you could also define what would happen when trying to add or multiply two \\nTemperature objects. We won\\'t go into much detail here as it works exactly the same as \\nthe comparison operators.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 62, 'page_label': '44'}, page_content=\"44     Python Programming Specificities\\nCallable object – __call__\\nThe last magic method we'll review is __call__. This one is a bit special because it \\nenables you to call your object instance like a regular function. Let's take an example:\\nchapter2_classes_objects_05.py\\nclass Counter:\\n    def __init__(self):\\n        self.counter = 0\\n \\n    def __call__(self, inc=1):\\n        self.counter += inc\\n \\n \\nc = Counter()\\nprint(c.counter)  # 0\\nc()\\nprint(c.counter)  # 1\\nc(10)\\nprint(c.counter)  # 11\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_05.py\\n__call__ can be defined like any other method, with any argument you wish. The only \\ndifference is how you call it: you just pass the argument directly on the object instance \\nvariable as you would do for a regular function.\\nThis pattern can be useful if you want to define a function that maintains some kind of \\nlocal state, as we did here in our example, or in cases where you need to provide a callable \\nobject but have to set some parameters. Actually, this is the use case we'll encounter when \\ndefining class dependencies for FastAPI.\\nAs we saw, magic methods are an excellent way to implement operations for our custom \\nclasses and make them easy to use in a pure object-oriented way. We haven't covered every \\nmagic method available but you can find the complete list on the official documentation: \\nhttps://docs.python.org/3/reference/datamodel.html#special-\\nmethod-names.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 63, 'page_label': '45'}, page_content='Writing object-oriented programs     45\\nWe\\'ll now focus on another essential characteristic of object-oriented programming: \\ninheritance.\\nReusing logic and avoiding repetition with inheritance\\nInheritance is one of the core concepts of object-oriented programming: it allows you to \\nderive a new class from existing ones, enabling you to reuse some logic and overload the \\nparts that are specific to this new one. Of course, this is supported in Python. We\\'ll take \\nvery simple examples to understand the mechanism underneath.\\nFirst of all, let\\'s take an example of a very simple inheritance:\\nchapter2_classes_objects_06.py\\nclass A:\\n    def f(self):\\n        return \"A\"\\nclass Child(A):\\n    pass\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_06.py\\nThe Child class inherits from the A class. The syntax is simple: the class we want to \\ninherit from is specified between parentheses after the child class name.\\nThe pass statement\\npass is a statement that does nothing. Since Python relies only on indentation \\nto denote blocks, it\\'s a useful statement to create an empty block, as you would \\ndo with curly braces in other programming languages.\\nIn this example, we don\\'t want to add some logic to the Child class, so we \\njust write pass. \\nAnother way to do it is to add a docstring just below the class definition.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 64, 'page_label': '46'}, page_content='46     Python Programming Specificities\\nIf you wish to overload a method but still want to get the result of the parent method, you \\ncan call the super function:\\nchapter2_classes_objects_07.py\\nclass A:\\n    def f(self):\\n        return \"A\"\\nclass Child(A):\\n    def f(self):\\n        parent_result = super().f()\\n        return f\"Child {parent_result}\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_07.py\\nY ou now know how to create basic inheritance in Python. But there is more: we can also \\nhave multiple inheritance!\\nMultiple inheritance\\nAs its name suggests, multiple inheritance allows you to derive a child class from  \\nmultiple classes. This way, you can combine the logic of several classes into one.  \\nLet\\'s take an example:\\nchapter2_classes_objects_08.py\\nclass A:\\n    def f(self):\\n        return \"A\"\\nclass B:\\n    def g(self):\\n        return \"B\"\\nclass Child(A, B):\\n    pass'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 65, 'page_label': '47'}, page_content='Writing object-oriented programs     47\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_08.py\\nOnce again, the syntax is quite straightforward: just list all the parent classes with a \\ncomma. Now, the Child class can call both the f and g methods.\\nMixins\\nMixins are a common pattern in Python that take advantage of the multiple \\ninheritance feature. Basically, mixins are short classes containing a single \\nfeature that you often want to reuse. Y ou can then compose concrete classes by \\ncombining the mixins.\\nHowever, what would happen if both A and B classes implemented a method named f? \\nLet\\'s try it out:\\nchapter2_classes_objects_09.py\\nclass A:\\n    def f(self):\\n        return \"A\"\\nclass B:\\n    def f(self):\\n        return \"B\"\\nclass Child(A, B):\\n    pass\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_classes_\\nobjects_09.py\\nIf you call method f of Child, you\\'ll get the value \"A\". In this simple case, Python will \\nconsider the first matching method following the order of the parent classes. However, \\nfor more complex hierarchies, the resolution may not be so obvious: this is the purpose of \\nthe Method Resolution Order (MRO) algorithm. We won\\'t go into much detail here but \\njust know that it follows the C3 linearization principles. If you wish to know more, you \\ncan have a look at the official document explaining the algorithm implemented in Python: \\nhttps://www.python.org/download/releases/2.3/mro/.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 66, 'page_label': '48'}, page_content='48     Python Programming Specificities\\nIf you are confused about the MRO of your class, you can call the mro method on your \\nclass to get a list of considered classes in order:\\n>>> Child.mro()\\n[<class \\'chapter2.chapter2_classes_objects_08.Child\\'>, <class \\n\\'chapter2.chapter2_classes_objects_08.A\\'>, <class \\'chapter2.\\nchapter2_classes_objects_08.B\\'>, <class \\'object\\'>]\\nWell done! Y ou now have a good overview of object-oriented programming in Python. \\nThose concepts will be helpful when defining dependencies in FastAPI.\\nWe\\'ll now review some of the most recent and trending features in Python upon which \\nFastAPI relies heavily. We\\'ll start with type hinting.\\nType hinting and type checking with mypy\\nIn the first section of this chapter, we said that Python was a dynamically typed language: \\nthe interpreter doesn\\'t check types at compile time but rather at runtime. This makes the \\nlanguage a bit more flexible and the developer a bit more efficient. However, if you are \\nexperienced with that kind of language, you probably know that it\\'s easy to produce errors \\nand bugs in this context: forgetting arguments and type mismatch.\\nThis is why Python introduced type hinting starting with version 3.5. The goal is to provide \\na syntax to annotate the source code with type annotations: each variable, function, and \\nclass can be annotated to give indications about the types they expect. This doesn\\'t mean \\nthat Python becomes a statically typed language. Those annotations remain completely \\noptional and are ignored by the interpreter. However, those annotations can be used by \\nstatic-type checkers, which will check whether your code is valid and consistent following \\nthe annotations. Hence, it greatly helps you to reduce errors and write self-explanatory code. \\nOne of those tools, mypy, is widely used by the community in this context.\\nGetting started\\nTo understand how type annotations work, we\\'ll review a simple annotated function:\\nchapter2_type_hints_01.py\\ndef greeting(name: str) -> str:\\n    return f\"Hello, {name}\"'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 67, 'page_label': '49'}, page_content='Type hinting and type checking with mypy     49\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_01.py\\nAs you see here, we simply added the type of the name argument after a colon. We also \\nspecified the return type after an arrow. For built-in types, such as str or int, we \\ncan simply use them as type annotations. We\\'ll see a little further in this section how to \\nannotate more complex types such as lists or dictionaries.\\nWe\\'ll now install mypy to perform a type check on this file. This can be done like any \\nother Python package:\\n$ pip install mypy\\nThen, you can run a type check on your source file:\\n$ mypy chapter2_type_hints_01.py\\nSuccess: no issues found in 1 source file\\nAs you see, mypy tells us that everything is good with our typing. Let\\'s try to modify our \\ncode a bit to provoke a type error:\\ndef greeting(name: str) -> int:\\n    return f\"Hello, {name}\"\\nQuite simply, we just said that the return type of our function is now int, but we are \\nstill returning a string. If you run this code, it\\'ll execute perfectly well: as we said, the \\ninterpreter ignores type annotations. However, let\\'s see what mypy tells us about it:\\n$ mypy chapter2_type_hints_01.py\\nchapter2/chapter2_type_hints_01.py:2: error: Incompatible \\nreturn value type (got \"str\", expected \"int\")\\nFound 1 error in 1 file (checked 1 source file)\\nThis time, it complains. It clearly tells us what is wrong here: the return value is a string, \\nwhile an integer was expected!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 68, 'page_label': '50'}, page_content='50     Python Programming Specificities\\nCode editors and IDE integration\\nHaving type checking is good, but it may be a bit tedious to run mypy \\nmanually in the command line. Fortunately, it integrates well with the most \\npopular code editors and IDEs. Once configured, it\\'ll perform type checking \\nwhile you type and show you the errors directly on the faulty lines. Type \\nannotations also help the IDE to perform clever things such as auto-completion.\\nY ou can check on the official documentation of mypy how to set it up for \\nyour favorite editor: https://github.com/python/mypy#ide-\\nlinter-integrations-and-pre-commit.\\nY ou understand the basics of type hinting in Python. We\\'ll now review more advanced \\nexamples, especially with non-scalar types.\\nThe typing module\\nSo far, we\\'ve seen how to annotate variables for scalar types such as str or int. But we\\'ve \\nseen that there are data structures such as lists and dictionaries that are widely used in \\nPython. For those and other types of utilities, Python introduced the typing module. \\nIn the following example, we\\'ll show how to type hint basic data structures in Python:\\nchapter2_type_hints_02.py\\nfrom typing import Dict, List, Set, Tuple\\nl: List[int] = [1, 2, 3, 4, 5]\\nt: Tuple[int, str, float] = (1, \"hello\", 3.14)\\ns: Set[int] = {1, 2, 3, 4, 5}\\nd: Dict[str, int] = {\"a\": 1, \"b\": 2, \"c\": 3}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_02.py\\nThe typing module contains classes for type hinting lists, tuples, sets, and dictionaries. \\nY ou simply have to import it and use it in your annotations. In this case, those classes \\nexpect you to provide the type of the values composing your structure. It\\'s the same as \\nthe well-known concept of generics in object-oriented programming. In Python, they are \\ndefined using square brackets.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 69, 'page_label': '51'}, page_content='Type hinting and type checking with mypy     51\\nBuilt-in type annotations have changed in Python 3.9\\nStarting with Python 3.9, the method to annotate lists, tuples, sets, and \\ndictionaries shown here is deprecated. This newer version of Python actually \\nsupports type hinting with a regular class, without the need to import another \\nversion from typing: l: list[int] = [1, 2, 3, 4, 5].\\nBesides generic types, the typing module contains other utilities to cover more complex \\nannotations. For example, having a list with elements of different types is perfectly valid in \\nPython. To make this work with type checkers, we\\'ll use the Union type:\\nchapter2_type_hints_03.py\\nfrom typing import List, Union\\nl: List[Union[int, float]] = [1, 2.5, 3.14, 5]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_03.py\\nUnion is also a generic type accepting any number of types. In this case, our list will \\naccept either integers or floating-point numbers. Of course, mypy will complain if you try \\nto add an element to this list that is neither an int nor a float value.\\nThere is also another case where this is useful. Quite often, you\\'ll have function arguments or \\nreturn types that return either a value or None. Thus, you could write something like this:\\nchapter2_type_hints_04.py\\nfrom typing import Union\\ndef greeting(name: Union[str, None] = None) -> str:\\n    return f\"Hello, {name if name else \\'Anonymous\\'}\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_04.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 70, 'page_label': '52'}, page_content='52     Python Programming Specificities\\nThe allowed value is either a string or None. This works perfectly. However, this case  \\nis so common that typing provides a shortcut for this: Optional. So, you can write  \\nthe following:\\nchapter2_type_hints_05.py\\nfrom typing import Optional\\ndef greeting(name: Optional[str] = None) -> str:\\n    return f\"Hello, {name if name else \\'Anonymous\\'}\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_05.py\\nWhen dealing with complex types, it may be useful to alias them and reuse them at will \\nwithout the need to rewrite them each time. To do this, you can simply assign them as you \\nwould do for any variable:\\nchapter2_type_hints_06.py\\nfrom typing import Tuple\\nIntStringFloatTuple = Tuple[int, str, float]\\nt: IntStringFloatTuple = (1, \"hello\", 3.14)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_06.py\\nBy convention, types should be named using camel case, like classes. Talking about classes, \\nlet\\'s see how type hinting works with them:\\nchapter2_type_hints_07.py\\nfrom typing import List\\nclass Post:\\n    def __init__(self, title: str) -> None:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 71, 'page_label': '53'}, page_content='Type hinting and type checking with mypy     53\\n        self.title = title\\n    def __str__(self) -> str:\\n        return self.title\\nposts: List[Post] = [Post(\"Post A\"), Post(\"Post B\")]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_07.py\\nActually, there is nothing special about class type hinting. Y ou just annotate the methods \\nas you would for a regular function. If you need to use your class in an annotation, like \\nhere for a list of posts, you just have to use the class name.\\nSometimes, you\\'ll have to write a function or method that accepts another function in an \\nargument. In this case, you\\'ll need to give the type signature of this function. This is what \\nthe Callable class is for.\\nType function signatures with Callable\\nIt can be useful to have types for function signatures, especially when you need to pass \\nfunctions as arguments of other functions. For this task, the typing module provides the \\nCallable class. In the following example, we\\'ll implement a function called filter_\\nlist that expects argument a list of integers and a function that returns a Boolean given \\nan integer:\\nchapter2_type_hints_08.py\\nfrom typing import Callable, List\\nConditionFunction = Callable[[int], bool]\\ndef filter_list(l: List[int], condition: ConditionFunction) -> \\nList[int]:\\n    return [i for i in l if condition(i)]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_08.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 72, 'page_label': '54'}, page_content=\"54     Python Programming Specificities\\nY ou see here that we define a ConditionFunction type alias thanks to Callable. \\nOnce again, this is a generic class that expects two things: first, the list of argument types, \\nand then the return type. Here, we expect a single integer argument and the return type is \\na Boolean.\\nWe can then use this type in the annotation of the filter_list function. mypy \\nwill then ensure that the condition function passed in the argument conforms to this \\nsignature. For example, we could write a simple function to check the parity of an integer, \\nas shown in the next sample:\\nchapter2_type_hints_08.py\\ndef is_even(i: int) -> bool:\\n    return i % 2 == 0\\n \\n \\nfilter_list([1, 2, 3, 4, 5], is_even)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_08.py\\nIt's worth noting, however, that there is no syntax to indicate optional or keyword \\narguments. In this case, you can write Callable[..., bool], the ellipsis, ..., \\nmeaning here any number of arguments.\\nAny and cast\\nIn some situations, the code is so dynamic or complicated that it won't be possible to \\nannotate it correctly or the type checker may not correctly infer the type. To help with \\nthis, the typing module provides two utilities: Any and cast.\\nThe first one is a type annotation that tells the type checker that the variable or argument \\ncan be anything. In this case, any type of value will be valid for the type checker:\\nchapter2_type_hints_09.py\\nfrom typing import Any\\ndef f(x: Any) -> Any:\\n    return x\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 73, 'page_label': '55'}, page_content='Type hinting and type checking with mypy     55\\nf(\"a\")\\nf(10)\\nf([1, 2, 3])\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_09.py\\nThe second one, cast, is a function that lets you override the type inferred by the type \\nchecker. It\\'ll force the type checker to consider the type you specify:\\nchapter2_type_hints_10.py\\nfrom typing import Any, cast\\ndef f(x: Any) -> Any:\\n    return x\\na = f(\"a\")  # inferred type is \"Any\"\\na = cast(str, f(\"a\"))  # forced type to be \"str\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_type_\\nhints_10.py\\nBe careful though: the cast function is only meaningful for type checkers. As for \\nevery other type of annotation, the interpreter completely ignores it and doesn\\'t  \\nperform a real cast.\\nWhile convenient, try to refrain from using those utilities too often. If everything is Any \\nor casted to a different type, you completely miss the benefits of static type checking.\\nAs we saw, type hinting and type checking are really helpful to help reduce errors while \\ndeveloping and keep high-quality code. But that\\'s not all. Actually, Python allows you to \\nretrieve type annotations at runtime and perform some logic based on them. This enables \\nyou to do clever things such as dependency injection: just by type hinting an argument \\nin a function, a library can automatically interpret it and inject the corresponding value at \\nruntime. This concept is at the heart of FastAPI.\\nAnother key approach in FastAPI is asynchronous I/O. This will be the last subject we\\'ll \\ncover in this chapter.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 74, 'page_label': '56'}, page_content=\"56     Python Programming Specificities\\nAsynchronous I/O\\nIf you have already worked with JavaScript and Node.js, you have probably come across \\nthe concepts of promises and the async/await keywords, which are characteristic of the \\nasynchronous I/O paradigm. Basically, this is a way to make I/O operations non-blocking \\nand allow the program to perform other tasks while the read or write operation is \\nongoing. The main motivation behind this is that I/O operations are slow: reading from \\ndisk, network requests are\\xa0a million times slower\\xa0than reading from RAM or processing \\ninstructions. In the following example, we have a simple script that reads a file on disk:\\nchapter2_asyncio_01.py\\nwith open(__file__) as f:\\n    data = f.read()\\n# The program will block here until the data has been read\\nprint(data)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nasyncio_01.py\\nWe see that the script will block until we have retrieved the data from the disk and, as \\nwe said, this can be long. 99% percent of the execution time of the program is spent \\non waiting for the disk. Usually, it's not an issue for simple scripts like this because you \\nprobably don't have to perform other operations in the meantime.\\nHowever, in other situations, there could have been the opportunity to perform other \\ntasks. The typical case that is of great interest in this book is web servers. Imagine we have \\na first user that makes a request performing a 10-seconds-long database query before \\nsending the response. If a second user makes another request in the meantime, they'll have \\nto wait for the first response to finish before getting their answer.\\nTo solve this, traditional Python web servers based on the Web Server Gateway Interface \\n(WSGI), such as Flask or Django, spawn several workers. Those are sub-processes of the \\nweb server that are all able to answer requests. If one is busy processing a long request, \\nothers can answer new coming requests.\\nWith asynchronous I/O, a single process won't block when processing a request with a long \\nI/O operation. While it waits for this operation to finish, it can answer other requests. When \\nthe I/O operation is done, it resumes the request logic and can finally answer the request.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 75, 'page_label': '57'}, page_content=\"Asynchronous I/O     57\\nTechnically, this is achieved through the concept of an event loop. Think of it as a \\nconductor that will manage all the asynchronous tasks you'll send to it. When data is \\navailable or when the write operation is done for one of those tasks, it'll ping the main \\nprogram so that it can perform the next operations. Underneath, it relies upon the \\noperating system select and poll calls, which are precisely there to ask for events \\nabout I/O operations at an operating system level. Y ou can read very interesting details \\nabout this in the article Async IO on Linux: select, poll, and epoll by Julia Evans: https://\\njvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-\\nepoll/.\\nPython first implemented asynchronous I/O in version 3.4 and has since greatly evolved, \\nnotably with the introduction of the async/await keywords in version 3.6. All the \\nutilities to manage this paradigm are available through the standard asyncio module. \\nNot long after, the spiritual successor of WSGI for asynchronous-enabled web servers, \\nAsynchronous Server Gateway Interface (ASGI), was introduced. FastAPI relies on this, \\nand this is one of the reasons why it shows such great performance.\\nWe'll now review the basics of asynchronous programming in Python. The following \\nexample is a simple Hello world using asyncio:\\nchapter2_asyncio_02.py\\nimport asyncio\\nasync def main():\\n    print('Hello ...')\\n    await asyncio.sleep(1)\\n    print('... World!')\\nasyncio.run(main())\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nasyncio_02.py\\nWhen you wish to define an asynchronous function, you just have to add the async \\nkeyword before def. This allows you to use the await keyword inside it. Such async \\nfunctions are called coroutines.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 76, 'page_label': '58'}, page_content='58     Python Programming Specificities\\nInside it, we perform a first print statement and then call the asyncio.sleep \\ncoroutine. This is the async equivalent of time.sleep that blocks the program for a \\ngiven number of seconds. Notice that we prefixed the call with the await keyword. This \\nmeans that we want to wait for this coroutine to finish before proceeding. This is the main \\nbenefit of async/await keywords: writing code that looks like synchronous code. If we \\nomitted await, the coroutine object would have been created but never executed.\\nFinally, notice that we use the asyncio.run function. This is the machinery that will \\ncreate a new event loop, execute your coroutine, and return its result. It should be the \\nmain entry point of your async program.\\nThis example is nice but not very interesting from an asynchronous point of view; since \\nwe are waiting for only one operation, this is not very impressive. Let\\'s see an example \\nwhere we execute two coroutines concurrently:\\nchapter2_asyncio_03.py\\nimport asyncio\\nasync def printer(name: str, times: int) -> None:\\n    for i in range(times):\\n        print(name)\\n        await asyncio.sleep(1)\\nasync def main():\\n    await asyncio.gather(\\n        printer(\"A\", 3),\\n        printer(\"B\", 3),\\n    )\\nasyncio.run(main())\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter2/chapter2_\\nasyncio_03.py\\nHere, we have a printer coroutine that prints its name a given number of times. \\nBetween each print, it sleeps for 1 second.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 77, 'page_label': '59'}, page_content=\"Asynchronous I/O     59\\nThen, our main coroutine uses the asyncio.gather utility, which schedules several \\ncoroutines for concurrent execution. If you run this script, you'll get the following result:\\n$ python chapter2_asyncio_03.py\\nA\\nB\\nA\\nB\\nA\\nB\\nA succession of A and B means that our coroutines were executed concurrently and that \\nwe didn't wait for the first one to finish before starting the second one.\\nY ou might wonder why we added the asyncio.sleep call in this example. Actually, if \\nwe removed it, we would have obtained this result:\\nA\\nA\\nA\\nB\\nB\\nB\\nThat doesn't look very concurrent, and indeed it's not. This is one of the main pitfalls of \\nasyncio: writing code in a coroutine doesn't necessarily mean that it won't block. Regular \\noperations such as computations are blocking and will block the event loop. Usually, this \\nis not a problem since those operations are fast. The only operations that won't block are \\nproper I/O operations that are designed to work asynchronously. This is different from \\nmultiprocessing where operations are executed on child processes, which, by nature, \\ndoesn't block the main one.\\nBecause of this, you'll have to be careful when choosing a third-party library for \\ninteracting with databases, APIs, and so on. Some have been adapted to work \\nasynchronously, or some alternatives have been developed in parallel to the standard ones. \\nWe'll see some of them in the next chapters, especially when working with databases.\\nWe'll end this quick introduction to asynchronous I/O here. There are some other \\nsubtleties underneath but, generally, the basics we've seen here will allow you to leverage \\nthe power of asyncio with FastAPI.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 78, 'page_label': '60'}, page_content=\"60     Python Programming Specificities\\nSummary\\nCongratulations! Through this chapter, you've discovered the basics of the Python \\nlanguage, a very clean and efficient language to work with. Y ou've then been introduced to \\nthe more advanced concepts of list comprehensions and generators, which are idiomatic \\nways of handling sequences of data. Python is also a multi-paradigm language and you've \\nseen how to leverage the object-oriented syntax.\\nFinally, you've discovered some of the most recent features of the language: type hinting, \\nwhich allows static-type checking to reduce errors and speed up development, and \\nasynchronous I/O, a set of new tools and syntax to maximize performance and allow \\nconcurrency while doing I/O-bound operations.\\nY ou're now ready to begin your journey with FastAPI! Y ou'll see that the framework  \\ntakes advantage of all those Python features to propose a fast and enjoyable development  \\nexperience. In the next chapter, you'll learn how to write your very first REST API  \\nwith FastAPI.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 79, 'page_label': '61'}, page_content=\"3\\nDeveloping a RESTful \\nAPI with FastAPI\\nNow it's time to begin learning about FastAPI! In this chapter, we'll cover the basics of \\nFastAPI. We'll go through very simple and focused examples that will demonstrate the \\ndifferent features of FastAPI. Each example will lead to a working API endpoint that you'll \\nbe able to test yourself using HTTPie. In the final section of this chapter, we'll show you \\na more complex FastAPI project, with routes split across several files. It will give you an \\noverview of how you can structure your own application.\\nBy the end of this chapter, you'll know how to start a FastAPI application and how to write \\nan API endpoint. Y ou'll also be able to handle request data and build a response according \\nto your own logic. Finally, you'll learn a way to structure a FastAPI project into several \\nmodules that will be easier to maintain and work with in the long term.\\nIn this chapter, we'll cover the following main topics:\\n• Creating the first endpoint and running it locally\\n• Handling request parameters\\n• Customizing the response\\n• Structuring a bigger project with multiple routers\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 80, 'page_label': '62'}, page_content='62     Developing a RESTful API with FastAPI\\nTechnical requirements\\nFor the examples in this chapter, you\\'ll require a Python virtual environment, just as we \\nset up in Chapter 1, Python Development Environment Setup.\\nY ou can find all the code examples of this chapter in the dedicated GitHub repository \\nat https://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter3.\\nCreating the first endpoint and running it \\nlocally\\nFastAPI is a framework that aims at being easy to use and quick to write. In the following \\nexample, you\\'ll realize that this is not just a promise. In fact, creating an API endpoint \\ninvolves just a few lines:\\nchapter3_first_endpoint_01.py\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@app.get(\"/\")\\nasync def hello_world():\\n    return {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_first_\\nendpoint_01.py\\nIn this example, we define a GET endpoint at the root path, which always returns the \\n{\"hello\": \"world\"} JSON response. To do this, we first instantiate a FastAPI object, \\napp. This will be the main application object that will wire all of the API routes.\\nThen, we simply define a coroutine that contains our route logic, the path operation \\nfunction. Its return value is automatically handled by FastAPI to produce a proper HTTP \\nresponse with a JSON payload.\\nHere, the most important part of the code is probably the line starting with @, which \\ncan be found above the coroutine definition, the decorator. In Python, a decorator is \\nsyntactic sugar that allows you to wrap a function or class with common logic without \\ncompromising readability. It\\'s roughly equivalent to app.get(\"/\")(hello_world).'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 81, 'page_label': '63'}, page_content='Creating the first endpoint and running it locally     63\\nFastAPI exposes one decorator per HTTP method to add new routes to the application. The \\none that is shown here adds a GET endpoint with the path as the first argument.\\nNow, let\\'s run this API. Copy the example to the root of your project and run the \\nfollowing command:\\n$ uvicorn chapter3_first_endpoint_01:app\\nINFO:     Started server process [13300]\\nINFO:     Waiting for application startup.\\nINFO:     Application startup complete.\\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press \\nCTRL+C to quit)\\nAs we mentioned in the Asynchronous I/O section of Chapter 2, Python Programming \\nSpecificities, FastAPI exposes an ASGI-compatible application. To run it, we require a web \\nserver compatible with this protocol. Uvicorn is a good option to use. It gives a command \\nto quickly start a web server. In the first argument, it expects the dotted namespace of the \\nPython module, which contains your app instance, followed by a colon, :, and, finally, the \\nvariable name of your ASGI app instance (in our example, this is app). Afterward, it takes \\ncare of instantiating the application and exposing it on your local machine.\\nLet\\'s try our endpoint with HTTPie:\\n$ http http://localhost:8000\\nHTTP/1.1 200 OK\\ncontent-length: 17\\ncontent-type: application/json\\ndate: Tue, 23 Mar 2021 07:35:16 GMT\\nserver: uvicorn\\n{\\n    \"hello\": \"world\"\\n}\\nIt works! As you can see, we did get a JSON response with the payload that we wanted, \\nusing just a few lines of Python and a command!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 82, 'page_label': '64'}, page_content=\"64     Developing a RESTful API with FastAPI\\nOne of the most beloved features of FastAPI is the automatic interactive documentation. \\nIf you open http://localhost:8000/docs in your browser, you should get a web \\ninterface that looks similar to the following screenshot:\\nFigure 3.1 – The FastAPI automatic interactive documentation\\nFastAPI will automatically list all of your defined endpoints and provide documentation \\nabout the expected inputs and outputs. Y ou can even try each endpoint directly in this \\nweb interface. Under the hood, it relies on the OpenAPI Specification and the associated \\ntools by Swagger. Y ou can read more about this on their official website at https://\\nswagger.io/.\\nThat's it! Y ou've created your very first API with FastAPI. Of course, this is just a very \\nsimple example, but next, you'll learn how to handle input data and start making \\nmeaningful things!\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 83, 'page_label': '65'}, page_content='Handling request parameters     65\\nOn the shoulders of giants\\nIt\\'s worth noting that FastAPI is built upon two main Python libraries: Starlette, \\na low-level ASGI web framework (https://www.starlette.io/), \\nand pydantic, a data validation library that is based on type hints (https://\\npydantic-docs.helpmanual.io/).\\nHandling request parameters\\nThe main goal of a REST API is to provide a structured way in which to interact with data. \\nAs such, it\\'s crucial for the end user to send some information to tailor the response they \\nneed, such as path parameters, query parameters, body payloads, or headers.\\nTo handle them, usually, web frameworks ask you to manipulate a request object to \\nretrieve the parts you are interested in and manually apply validation. However, that\\'s \\nnot necessary with FastAPI! Indeed, it allows you to define all of your parameters \\ndeclaratively. Then, it\\'ll automatically retrieve them in the request and apply validations \\nbased on the type hints. This is why we introduced type hinting in Chapter 2, Python \\nProgramming Specificities; it\\'s used by FastAPI to perform data validation!\\nNext, we\\'ll explore how you can use this feature to retrieve and validate this input data \\nfrom different parts of the request.\\nPath parameters\\nThe API path is the main thing that the end user will interact with. Therefore, it\\'s a good \\nspot for dynamic parameters. A typical example is to put the unique identifier of an object \\nwe want to retrieve, such as /users/123. Let\\'s examine how to define this with FastAPI:\\nchapter3_path_parameters_01.py\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@app.get(\"/users/{id}\")\\nasync def get_user(id: int):\\n    return {\"id\": id}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 84, 'page_label': '66'}, page_content='66     Developing a RESTful API with FastAPI\\nIn this example, we defined an API that expects an integer in the last part of its path. We \\ndid this by putting the parameter name in the path around curly braces. Then, we defined \\nthis same parameter as an argument for our path operation function. Notice that we add a \\ntype hint to specify the parameter is an integer.\\nLet\\'s run this example. Y ou can refer to the previous section, Creating the first endpoint \\nand running it locally, to learn how to run a FastAPI app with uvicorn.\\nFirst, we\\'ll try to make a request by omitting our path parameter:\\n$ http http://localhost:8000/users\\nHTTP/1.1 404 Not Found\\ncontent-length: 22\\ncontent-type: application/json\\ndate: Wed, 24 Mar 2021 07:23:47 GMT\\nserver: uvicorn\\n{\\n    \"detail\": \"Not Found\"\\n}\\nWe get a response with a 404 status. That\\'s expected: our route awaits a parameter  \\nafter /users, so if we omit it, it simply doesn\\'t match any pattern.\\nNow, let\\'s try it using a proper integer parameter:\\n$ http http://localhost:8000/users/123\\nHTTP/1.1 200 OK\\ncontent-length: 10\\ncontent-type: application/json\\ndate: Wed, 24 Mar 2021 07:26:23 GMT\\nserver: uvicorn\\n{\\n    \"id\": 123\\n}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 85, 'page_label': '67'}, page_content='Handling request parameters     67\\nIt works! We get a 200 status, and the response does contain the integer we passed in the \\nparameter. Notice that it has been properly cast as an integer.\\nSo, what happens if we pass a value that\\'s not a valid integer? Let\\'s find out:\\n$ http http://localhost:8000/users/abc\\nHTTP/1.1 422 Unprocessable Entity\\ncontent-length: 99\\ncontent-type: application/json\\ndate: Wed, 24 Mar 2021 07:28:11 GMT\\nserver: uvicorn\\n{\\n    \"detail\": [\\n        {\\n            \"loc\": [\\n                \"path\",\\n                \"id\"\\n            ],\\n            \"msg\": \"value is not a valid integer\",\\n            \"type\": \"type_error.integer\"\\n        }\\n    ]\\n}\\nWe get a response with a 422 status! Since abc is not a valid integer, the validation fails \\nand outputs an error. Notice that we have a very detailed and structured error response \\ntelling us exactly which element caused the error and why. All we need to do to trigger \\nthis validation is to type hint our parameter!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 86, 'page_label': '68'}, page_content='68     Developing a RESTful API with FastAPI\\nOf course, you are not limited to just one path parameter. Y ou can have as many as you \\nwant, with different types. In the following example, we\\'ve added a type parameter of the \\nstring type:\\nchapter3_path_parameters_02.py\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@app.get(\"/users/{type}/{id}/\")\\nasync def get_user(type: str, id: int):\\n    return {\"type\": type, \"id\": id}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_02.py\\nThis works well, but the endpoint will accept any string as the type parameter.\\nLimiting allowed values\\nSo, what if we just want to accept a limited set of values? Once again, we\\'ll lean on type \\nhinting. Python has a very useful class for this: Enum. An enumeration is a way to list \\nall the valid values for a specific kind of data. Let\\'s define an Enum class that will list the \\ndifferent types of users:\\nchapter3_path_parameters_03.py\\nfrom enum import Enum\\nfrom fastapi import FastAPI\\nclass UserType(str, Enum):\\n    STANDARD = \"standard\"\\n    ADMIN = \"admin\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_03.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 87, 'page_label': '69'}, page_content='Handling request parameters     69\\nTo define a string enumeration, we inherit from both the str type and the Enum class. \\nThen, we simply list the allowed values as class properties: the property name and its \\nactual string value. Finally, we need to type hint the type argument using this class:\\nchapter3_path_parameters_03.py\\n@app.get(\"/users/{type}/{id}/\")\\nasync def get_user(type: UserType, id: int):\\n    return {\"type\": type, \"id\": id}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_03.py\\nIf you run this example and call the endpoint with a type that is not in the enumeration, \\nyou\\'ll get the following response:\\n$ http http://localhost:8000/users/hello/123/\\nHTTP/1.1 422 Unprocessable Entity\\ncontent-length: 184\\ncontent-type: application/json\\ndate: Thu, 25 Mar 2021 06:30:36 GMT\\nserver: uvicorn\\n{\\n    \"detail\": [\\n        {\\n            \"ctx\": {\\n                \"enum_values\": [\\n                    \"standard\",\\n                    \"admin\"\\n                ]\\n            },\\n            \"loc\": [\\n                \"path\",\\n                \"type\"\\n            ],\\n            \"msg\": \"value is not a valid enumeration member;'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 88, 'page_label': '70'}, page_content='70     Developing a RESTful API with FastAPI\\npermitted: \\'standard\\', \\'admin\\'\",\\n            \"type\": \"type_error.enum\"\\n        }\\n    ]\\n}\\nAs you can see, you get a nice validation error, with the allowed values for this parameter!\\nAdvanced validation\\nWe can take one step further by defining more advanced validation rules, particularly for \\nnumbers and strings. In this case, the type of hint is no longer enough. We\\'ll rely on the \\nfunctions provided by FastAPI, allowing us to set some options on each of our parameters. \\nFor path parameters, the function is named Path. In the following example, we\\'ll only \\nallow an id argument that is greater than or equal to 1:\\nchapter3_path_parameters_04.py\\nfrom fastapi import FastAPI, Path\\napp = FastAPI()\\n@app.get(\"/users/{id}\")\\nasync def get_user(id: int = Path(..., ge=1)):\\n    return {\"id\": id}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_04.py\\nThere are several things to pay attention to here: the result of Path is used as a default \\nvalue for the id argument in the path operation function.\\nAdditionally, you can see that we use the ellipsis syntax as the first parameter of Path. \\nIndeed, it expects the default value for the parameter as the first argument. In this \\nscenario, we don\\'t want a default value: the parameter is required. Therefore, ellipses are \\nhere to tell FastAPI that we don\\'t want a default value.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 89, 'page_label': '71'}, page_content='Handling request parameters     71\\nThen, we can add the keyword arguments that we are interested in. In our example, this \\nis ge, greater than or equal to, and its associated value. There are a number of validations \\navailable, as follows:\\n• gt: Greater than\\n• ge: Greater than or equal to\\n• lt: Less than\\n• le: Less than or equal to\\nThere are also validation options for string values, which are based on the length and the \\nuse of a regular expression. In the following example, we want to define a path parameter \\nthat accepts license plates in the form of AB-123-CD (French license plates). The first \\napproach would be to force the string to be of length 9 (that is, two letters, a dash, three \\ndigits, a dash, and two letters):\\nchapter3_path_parameters_05.py\\n@app.get(\"/license-plates/{license}\")\\nasync def get_license_plate(license: str = Path(..., min_\\nlength=9, max_length=9)):\\n    return {\"license\": license}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_05.py\\nNow we just have to define the min_length and max_length keyword arguments, just \\nas we did for the number of validations. Of course, a better solution for this use case is to \\nuse a regular expression to validate the license plate number:\\nchapter3_path_parameters_06.py\\n@app.get(\"/license-plates/{license}\")\\nasync def get_license_plate(license: str = Path(..., regex=r\"^\\\\\\nw{2}-\\\\d{3}-\\\\w{2}$\")):\\n    return {\"license\": license}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_path_\\nparameters_06.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 90, 'page_label': '72'}, page_content='72     Developing a RESTful API with FastAPI\\nThanks to this regular expression, we only accept strings that exactly match the license \\nplate format. Notice that the regular expression is prefixed with r. Just like f-strings, \\nthis is a Python syntax that is used to indicate that the following string should be \\nconsidered as a regular expression.\\nParameter metadata\\nData validation is not the only option accepted by the parameter function. \\nY ou can also set options that will add information about the parameter \\nin the automatic documentation, such as title, description, and \\ndeprecated.\\nNow you should be able to define path parameters and apply some validation to them. \\nOther useful parameters to put inside the URL are query parameters. We\\'ll discuss  \\nthem next.\\nQuery parameters\\nQuery parameters are a common way to add some dynamic parameters to a URL. Y ou \\nfind them at the end of the URL in the following form: ?param1=foo&param2=bar. \\nIn a REST API, they are commonly used on read endpoints to apply pagination, a filter, a \\nsorting order, or selecting fields.\\nY ou\\'ll discover that they are quite straightforward to define with FastAPI. In fact, they use \\nthe exact same syntax as path parameters:\\nchapter3_query_parameters_01.py\\n@app.get(\"/users\")\\nasync def get_user(page: int = 1, size: int = 10):\\n    return {\"page\": page, \"size\": size}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_query_\\nparameters_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 91, 'page_label': '73'}, page_content='Handling request parameters     73\\nY ou simply have to declare them as arguments of your path operation function. If they \\ndon\\'t appear in the path pattern, as they do for path parameters, FastAPI automatically \\nconsiders them to be query parameters. Let\\'s try it:\\n$ http \"http://localhost:8000/users?page=5&size=50\"\\nHTTP/1.1 200 OK\\ncontent-length: 20\\ncontent-type: application/json\\ndate: Thu, 25 Mar 2021 07:17:01 GMT\\nserver: uvicorn\\n{\\n    \"page\": 5,\\n    \"size\": 50\\n}\\nHere, you can see that we have defined a default value for those arguments, which means \\nthey are optional when calling the API. Of course, if you wish to define a required query \\nparameter, simply leave out the default value:\\nchapter3_query_parameters_01.py\\nfrom enum import Enum\\nfrom fastapi import FastAPI\\nclass UsersFormat(str, Enum):\\n    SHORT = \"short\"\\n    FULL = \"full\"\\napp = FastAPI()\\n@app.get(\"/users\")\\nasync def get_user(format: UsersFormat):\\n    return {\"format\": format}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_query_\\nparameters_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 92, 'page_label': '74'}, page_content='74     Developing a RESTful API with FastAPI\\nNow, if you omit the format parameter in the URL, you\\'ll get a 422 error response. \\nAdditionally, notice that, in this example, we defined a UsersFormat enumeration to \\nlimit the number of allowed values for this parameter; this is exactly what we did in the \\nprevious section for path parameters.\\nWe also have access to more advanced validations through the Query function. It works \\nin the same way that we demonstrated in the Path parameters section:\\nchapter3_query_parameters_01.py\\n@app.get(\"/users\")\\nasync def get_user(page: int = Query(1, gt=0), size: int = \\nQuery(10, le=100)):\\n    return {\"page\": page, \"size\": size}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_query_\\nparameters_01.py\\nHere, we force the page to be greater than 0 and the size to be less than or equal to 100. \\nNotice how the default parameter value is the first argument of the Query function.\\nNaturally, when it comes to sending request data, the most obvious way is to use the \\nrequest body. Let\\'s examine how it works with FastAPI next.\\nThe request body\\nThe body is the part of the HTTP request that contains raw data, representing documents, \\nfiles, or form submissions. In a REST API, it\\'s usually encoded in JSON and used to create \\nstructured objects in a database.\\nFor the simplest cases, retrieving data from the body works exactly like query parameters. \\nThe only difference is that you always have to use the Body function; otherwise, FastAPI \\nwill look for it inside the query parameters by default. Let\\'s explore a simple example \\nwhere we want to post some user data:\\nchapter3_request_body_01.py\\n@app.post(\"/users\")\\nasync def create_user(name: str = Body(...), age: int = \\nBody(...)):\\n    return {\"name\": name, \"age\": age}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 93, 'page_label': '75'}, page_content='Handling request parameters     75\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_request_\\nbody_01.py\\nIn the same way as query parameters, we define each argument with a type hint along  \\nwith the Body function with no default value to make them required. Let\\'s try the \\nfollowing endpoint:\\n$ http -v POST http://localhost:8000/users name=\"John\" age=30\\nPOST /users HTTP/1.1\\nAccept: application/json, */*;q=0.5\\nAccept-Encoding: gzip, deflate\\nConnection: keep-alive\\nContent-Length: 29\\nContent-Type: application/json\\nHost: localhost:8000\\nUser-Agent: HTTPie/2.4.0\\n{\\n    \"age\": \"30\",\\n    \"name\": \"John\"\\n}\\nHTTP/1.1 200 OK\\ncontent-length: 24\\ncontent-type: application/json\\ndate: Sun, 28 Mar 2021 08:17:26 GMT\\nserver: uvicorn\\n{\\n    \"age\": 30,\\n    \"name\": \"John\"\\n}\\nHere, we\\'ve shown  the request payload with the -v option so that you can clearly see  \\nthe JSON payload we sent. FastAPI successfully retrieves the data for each field from  \\nthe payload. If you send a request with a missing or invalid field, you\\'ll get a 422 status \\nerror response.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 94, 'page_label': '76'}, page_content='76     Developing a RESTful API with FastAPI\\nY ou also have access to more advanced validation through the Body function. It works in \\nthe same way as we demonstrated in the Path parameters section.\\nHowever, defining payload validations like this has some major drawbacks. First, it\\'s quite \\nverbose and makes the path operation function prototype huge, especially for bigger \\nmodels. Second, usually, you\\'ll need to reuse the data structure on other endpoints or in \\nother parts of your application.\\nThis is why FastAPI uses pydantic models for data validation. Pydantic is a Python library \\nfor data validation and is based on classes and type hints. In fact, the Path, Query, and \\nBody functions that we\\'ve learned about so far use pydantic under the hood!\\nBy defining your own pydantic models and using them as type hints in your path \\narguments, FastAPI will automatically instantiate a model instance and validate the data. \\nLet\\'s rewrite our previous example using this method:\\nchapter3_request_body_02.py\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\nclass User(BaseModel):\\n    name: str\\n    age: int\\napp = FastAPI()\\n@app.post(\"/users\")\\nasync def create_user(user: User):\\n    return user\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_request_\\nbody_02.py\\nFirst, we import BaseModel from pydantic. This is the base class that every model \\nshould inherit from. Then, we define our User class and simply list all of the properties as \\nclass properties. Each one of them should have a proper type hint: this is how Pydantic \\nwill be able to validate the type of the field.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 95, 'page_label': '77'}, page_content='Handling request parameters     77\\nFinally, we just declare user as an argument for our path operation function with the \\nUser class as a type hint. FastAPI automatically understands that the user data can be \\nfound in the request payload. Inside the function, you have access to a proper User object \\ninstance, where you can access individual properties by simply using the dot notation, \\nsuch as user.name.\\nNotice that if you just return the object, FastAPI is smart enough to convert it \\nautomatically into JSON to produce the HTTP response.\\nIn the following chapter, that is, Chapter 4, Managing pydantic Data Models in FastAPI, we\\'ll \\nexplore, in more detail, the possibilities of pydantic, particularly in terms of validation.\\nMultiple objects\\nSometimes, you might find that you have several objects that you wish to send in the \\nsame payload all at once. For example, both user and company. In this scenario, you \\ncan simply add several arguments that have been type hinted by a pydantic model, and \\nFastAPI will automatically understand that there are several objects. In this configuration, \\nit will expect a body containing each object indexed by its argument name:\\nchapter3_request_body_03.py\\n@app.post(\"/users\")\\nasync def create_user(user: User, company: Company):\\n    return {\"user\": user, \"company\": company}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_request_\\nbody_03.py\\nHere, Company is a simple pydantic model with a single string name property. In this \\nconfiguration, FastAPI expects a payload that looks similar to the following:\\n{\\n    \"user\": {\\n        \"name\": \"John\",\\n        \"age\": 30\\n    },\\n    \"company\": {\\n        \"name\": \"ACME\"\\n    }\\n}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 96, 'page_label': '78'}, page_content='78     Developing a RESTful API with FastAPI\\nFor more complex JSON structures, it\\'s advised that you pipe a formatted JSON into \\nHTTPie rather than use parameters. Let\\'s try this as follows:\\n$ echo \\'{\"user\": {\"name\": \"John\", \"age\": 30}, \"company\": \\n{\"name\": \"ACME\"}}\\' | http POST http://localhost:8000/users\\nHTTP/1.1 200 OK\\ncontent-length: 59\\ncontent-type: application/json\\ndate: Sun, 28 Mar 2021 08:54:11 GMT\\nserver: uvicorn\\n{\\n    \"company\": {\\n        \"name\": \"ACME\"\\n    },\\n    \"user\": {\\n        \"age\": 30,\\n        \"name\": \"John\"\\n    }\\n}\\nAnd that\\'s it!\\nY ou can even add singular body values with the Body function, just as we saw at the \\nbeginning of this section. This is useful if you wish to have a single property that\\'s not  \\npart of any model:\\nchapter3_request_body_04.py\\n@app.post(\"/users\")\\nasync def create_user(user: User, priority: int = Body(..., \\nge=1, le=3)):\\n    return {\"user\": user, \"priority\": priority}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_request_\\nbody_04.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 97, 'page_label': '79'}, page_content='Handling request parameters     79\\nThe priority property is an integer between 1 and 3, which is expected beside the  \\nuser object:\\n$ echo \\'{\"user\": {\"name\": \"John\", \"age\": 30}, \"priority\": 1}\\' | \\nhttp POST http://localhost:8000/users\\nHTTP/1.1 200 OK\\ncontent-length: 46\\ncontent-type: application/json\\ndate: Sun, 28 Mar 2021 09:02:59 GMT\\nserver: uvicorn\\n{\\n    \"priority\": 1,\\n    \"user\": {\\n        \"age\": 30,\\n        \"name\": \"John\"\\n    }\\n}\\nY ou now have a good overview of how to handle JSON payload data. However, sometimes, \\nyou\\'ll find that you need to accept more traditional-form data or even file uploads. Let\\'s \\nfind out how to do this next!\\nForm data and file uploads\\nEven if REST APIs work most of the time with JSON, sometimes, you might have \\nto handle form-encoded data or file uploads, which have been encoded either as \\napplication/x-www-form-urlencoded or multipart/form-data.\\nOnce again, FastAPI allows you to implement this case very easily. However, you\\'ll need \\nan additional Python dependency, python-multipart, to handle this kind of data.  \\nAs usual, you can install it with pip:\\n$ pip install python-multipart\\nThen, you can use the FastAPI features that are dedicated to form data. First, let\\'s take a \\nlook at how you can handle simple form data.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 98, 'page_label': '80'}, page_content='80     Developing a RESTful API with FastAPI\\nForm data\\nThe method to retrieve form data fields is similar to the one we discussed in the The \\nrequest body section to retrieve singular JSON properties. The following example is \\nroughly the same as the one you explored there. However, this example expects  \\nform-encoded data instead of JSON:\\nchapter3_form_data_01.py\\n@app.post(\"/users\")\\nasync def create_user(name: str = Form(...), age: int = \\nForm(...)):\\n    return {\"name\": name, \"age\": age}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_form_\\ndata_01.py\\nThe only difference here is that we use the Form function instead of Body. Y ou can try \\nthis endpoint with HTTPie and the --form option to force the data to be form-encoded:\\n$ http -v --form POST http://localhost:8000/users name=John \\nage=30    \\nPOST /users HTTP/1.1\\nAccept: */*\\nAccept-Encoding: gzip, deflate\\nConnection: keep-alive\\nContent-Length: 16\\nContent-Type: application/x-www-form-urlencoded; charset=utf-8\\nHost: localhost:8000\\nUser-Agent: HTTPie/2.4.0\\nname=John&age=30\\nHTTP/1.1 200 OK\\ncontent-length: 24\\ncontent-type: application/json\\ndate: Sun, 28 Mar 2021 14:50:07 GMT\\nserver: uvicorn'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 99, 'page_label': '81'}, page_content='Handling request parameters     81\\n{\\n    \"age\": 30,\\n    \"name\": \"John\"\\n}\\nPay attention to how the Content-Type header and the body data representation have \\nchanged in the request. Y ou can also see that the response is still provided in JSON. Unless \\nspecified otherwise, FastAPI will always output a JSON response by default, no matter the \\nform of the input data.\\nOf course, the validation options we saw for Path, Query, and Body are still available. \\nY ou can find a description for each of them in the Path parameters section.\\nIt\\'s worth noting that, contrary to JSON payloads, FastAPI doesn\\'t allow to you define \\npydantic models to validate form data. Instead, you have to manually define each field as \\nan argument for the path operation function.\\nNow, let\\'s go on to discuss how to handle file uploads.\\nFile uploads\\nUploading files is a common requirement for web applications, whether this is images or \\ndocuments. FastAPI provides a parameter function, File, that enables this.\\nLet\\'s take a look at a simple example where you can directly retrieve a file as a bytes object:\\n Chapter3_file_uploads_01.py\\nfrom fastapi import FastAPI, File\\napp = FastAPI()\\n@app.post(\"/files\")\\nasync def upload_file(file: bytes = File(...)):\\n    return {\"file_size\": len(file)}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_file_\\nuploads_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 100, 'page_label': '82'}, page_content='82     Developing a RESTful API with FastAPI\\nOnce again, you can see that the approach is still the same: we define an argument for the \\npath operation function, file, we add a type of hint, bytes, and then we use the File \\nfunction as a default value for this argument. By doing this, FastAPI understands that it \\nwill have to retrieve raw data in a part of the body named file and return it as bytes.\\nWe simply return the size of this file by calling the len function on this bytes object.\\nIn the code example repository, you should be able to find a picture of a cat: https://\\ngithub.com/PacktPublishing/Building-Data-Science-Applications-\\nwith-FastAPI/blob/main/assets/cat.jpg.\\nLet\\'s upload it on our endpoint using HTTPie. To upload a file, type in the name of the file \\nupload field (here, it is file), followed by @ and the path of the file you want to upload. \\nDon\\'t forget to set the --form option:\\n$ http --form POST http://localhost:8000/files file@./assets/\\ncat.jpg \\nHTTP/1.1 200 OK\\ncontent-length: 19\\ncontent-type: application/json\\ndate: Mon, 29 Mar 2021 06:42:02 GMT\\nserver: uvicorn\\n{\\n    \"file_size\": 71457\\n}\\nIt works! We have correctly got the size of the file in bytes.\\nOne drawback to this approach is that the uploaded file is entirely stored in memory. So, \\nwhile it\\'ll work for small files, it is likely that you\\'ll run into issues for larger files. Besides, \\nmanipulating a bytes object is not always convenient for file handling.\\nTo fix this problem, FastAPI provides an UploadFile class. This class will store the data \\nin memory up to a certain threshold and, after this, will automatically store it on disk in \\na temporary location. This allows you to accept much larger files without running out of \\nmemory. Furthermore, the exposed object instance exposes useful metadata, such as the \\ncontent type, and a file-like interface. This means that you can manipulate it as a regular \\nfile in Python and that you can feed it to any function that expects a file.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 101, 'page_label': '83'}, page_content='Handling request parameters     83\\nTo use it, you simply have to specify it as a type hint instead of bytes:\\nchapter3_file_uploads_02.py\\nfrom fastapi import FastAPI, File, UploadFile\\napp = FastAPI()\\n@app.post(\"/files\")\\nasync def upload_file(file: UploadFile = File(...)):\\n    return {\"file_name\": file.filename, \"content_type\": file.\\ncontent_type}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_file_\\nuploads_02.py\\nNotice that, here, we return the filename and content_type properties. The content \\ntype is especially useful for checking the type of the uploaded file and possibly rejecting it if \\nit\\'s not one of the types that you expect.\\nHere is the result with HTTPie:\\n$ http --form POST http://localhost:8000/files file@./assets/\\ncat.jpg\\nHTTP/1.1 200 OK\\ncontent-length: 51\\ncontent-type: application/json\\ndate: Mon, 29 Mar 2021 06:58:20 GMT\\nserver: uvicorn\\n{\\n    \"content_type\": \"image/jpeg\",\\n    \"file_name\": \"cat.jpg\"\\n}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 102, 'page_label': '84'}, page_content='84     Developing a RESTful API with FastAPI\\nY ou can even accept multiple files by type hinting the argument as a list of UploadFile:\\nchapter3_file_uploads_03.py\\n@app.post(\"/files\")\\nasync def upload_multiple_files(files: List[UploadFile] = \\nFile(...)):\\n    return [\\n        {\"file_name\": file.filename, \"content_type\": file.\\ncontent_type}\\n        for file in files\\n    ]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_file_\\nuploads_03.py\\nTo upload several files with HTTPie, simply repeat the argument. It should appear  \\nas follows:\\n$ http --form POST http://localhost:8000/files files@./assets/\\ncat.jpg files@./assets/cat.jpg \\nHTTP/1.1 200 OK\\ncontent-length: 105\\ncontent-type: application/json\\ndate: Mon, 29 Mar 2021 12:52:45 GMT\\nserver: uvicorn\\n[\\n    {\\n        \"content_type\": \"image/jpeg\",\\n        \"file_name\": \"cat.jpg\"\\n    },\\n    {\\n        \"content_type\": \"image/jpeg\",\\n        \"file_name\": \"cat.jpg\"\\n    }\\n]\\nNow, you should be able to handle form data and file uploads in a FastAPI application.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 103, 'page_label': '85'}, page_content='Handling request parameters     85\\nSo far, you\\'ve learned how to manage user-facing data. However, there are also very \\ninteresting pieces of information that are less visible: headers. We\\'ll explore them next. \\nHeaders and cookies\\nBesides the URL and the body, another major part of the HTTP request are the headers. \\nThey contain all sorts of metadata that can be useful when handling requests. A common \\nusage is to use them for authentication, for example, via the famous cookies.\\nOnce again, retrieving them in FastAPI only involves a type hint and a parameter function. \\nLet\\'s take a look at a simple example where we want to retrieve a header named Hello:\\nchapter3_headers_cookies_01.py\\n@app.get(\"/\")\\nasync def get_header(hello: str = Header(...)):\\n    return {\"hello\": hello}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_headers_\\ncookies_01.py\\nHere, you can see that we simply have to use the Header function as a default value for \\nthe hello argument. The name of the argument determines the key of the header that we \\nwant to retrieve. Let\\'s see this in action:\\n$ http GET http://localhost:8000 \\'Hello: World\\'\\nHTTP/1.1 200 OK\\ncontent-length: 17\\ncontent-type: application/json\\ndate: Mon, 29 Mar 2021 13:28:36 GMT\\nserver: uvicorn\\n{\\n    «hello»: «World»\\n}\\nFastAPI was able to retrieve the header value. Since there was no default value specified \\n(we put in an ellipsis), the header is required. If it\\'s missing, once again, you\\'ll get a 422 \\nstatus error response.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 104, 'page_label': '86'}, page_content='86     Developing a RESTful API with FastAPI\\nAdditionally, notice that FastAPI automatically converts the header name to lowercase. \\nBesides that, since header names are separated by a hyphen, -, most of the time, it also \\nautomatically converts it to snake case. Therefore, it works out of the box with any valid \\nPython variable name. The following example shows this behavior by retrieving the \\nUser-Agent header:\\nchapter3_headers_cookies_02.py\\n@app.get(\"/\")\\nasync def get_header(user_agent: str = Header(...)):\\n    return {\"user_agent\": user_agent}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_headers_\\ncookies_02.py\\nNow, let\\'s make a very simple request. We\\'ll keep the default user agent of HTTPie to see \\nwhat happens:\\n$ http -v GET http://localhost:8000\\nGET / HTTP/1.1\\nAccept: */*\\nAccept-Encoding: gzip, deflate\\nConnection: keep-alive\\nHost: localhost:8000\\nUser-Agent: HTTPie/2.4.0\\nHTTP/1.1 200 OK\\ncontent-length: 29\\ncontent-type: application/json\\ndate: Mon, 29 Mar 2021 13:37:57 GMT\\nserver: uvicorn\\n{\\n    «user_agent»: \"HTTPie/2.4.0\"\\n}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 105, 'page_label': '87'}, page_content='Handling request parameters     87\\nOne very special case of header is cookies. Y ou could retrieve them by parsing the \\nCookie header yourself, but that would be a bit tedious. FastAPI provides another \\nparameter function that automatically does it for you.\\nThe following example simply retrieves a cookie named hello:\\nchapter3_headers_cookies_03.py\\n@app.get(\"/\")\\nasync def get_cookie(hello: Optional[str] = Cookie(None)):\\n    return {\"hello\": hello}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_headers_\\ncookies_03.py\\nNotice that we type hinted the argument as Optional, and we set a default value of \\nNone to the Cookie function. This way, even if the cookie is not set in the request, \\nFastAPI will proceed and not generate a 422 status error response.\\nHeaders and cookies can be very useful tools in which to implement some authentication \\nfeatures. In Chapter 7, Managing Authentication and Security in FastAPI, you\\'ll learn \\nthat there are built-in security functions that can help you to implement common \\nauthentication schemes.\\nThe request object\\nSometimes, you might find that you need to access a raw request object with all of the data \\nassociated with it. That\\'s possible. Simply declare an argument on your path operation \\nfunction type hinted with the Request class:\\nchapter3_request_object_01.py\\nfrom fastapi import FastAPI, Request\\napp = FastAPI()\\n@app.get(\"/\")\\nasync def get_request_object(request: Request):\\n    return {\"path\": request.url.path}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 106, 'page_label': '88'}, page_content=\"88     Developing a RESTful API with FastAPI\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_request_\\nobject_01.py\\nUnder the hood, this is the Request object from Starlette, which is a library that provides \\nall the core server logic for FastAPI. Y ou can view a complete description of the methods \\nand properties of this object in the official documentation of Starlette (https://www.\\nstarlette.io/requests/).\\nCongratulations! Y ou have now learned all of the basics regarding how to handle request \\ndata in FastAPI. As you learned, the logic is the same no matter what part of the HTTP \\nrequest you want to look at. Simply name the argument you want to retrieve, add a type \\nhint, and use a parameter function to tell FastAPI where it should look. Y ou can even add \\nsome validation logic!\\nIn the next section, we'll explore the other side of a REST API job: returning a response.\\nCustomizing the response\\nIn the previous sections, you learned that directly returning a dictionary or a pydantic \\nobject in your path operation function was enough for FastAPI to return a JSON response.\\nMost of the time, you'll want to customize this response a bit further; for instance, by \\nchanging the status code, raising validation errors, and setting cookies. FastAPI offers \\ndifferent ways to do this, from the simplest case to the most advanced one. First, we'll \\nlearn how to customize the response declaratively by using path operation parameters.\\nPath operation parameters\\nIn the Creating the first endpoint and running it locally section, you learned that in order to \\ncreate a new endpoint, you had to put a decorator on top of the path operation function. \\nThis decorator accepts a lot of options, including ones to customize the response.\\nThe status code\\nThe most obvious thing to customize in an HTTP response is the status code. By default, \\nFastAPI will always set a 200 status when everything goes well during your path operation \\nfunction execution.\\nSometimes, it might be useful to change this status. For example, it's good practice in a \\nREST API to return a 201 Created status when the execution of the endpoint ends up \\nin the creation of a new object.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 107, 'page_label': '89'}, page_content='Customizing the response     89\\nTo set this, simply specify the status_code argument on the path decorator:\\nchapter3_response_path_parameters_01.py\\nfrom fastapi import FastAPI, status\\nfrom pydantic import BaseModel\\nclass Post(BaseModel):\\n    title: str\\napp = FastAPI()\\n@app.post(\"/posts\", status_code=status.HTTP_201_CREATED)\\nasync def create_post(post: Post):\\n    return post\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_path_parameters_01.py\\nThe decorator arguments come right after the path as keyword arguments. The status_\\ncode option simply expects an integer representing the status code. So, we could \\nhave written status_code=201, but FastAPI provides a useful list in the status \\nsub-module that improves code comprehensiveness, as you can see here.\\nWe can try this endpoint to obtain the resulting status code:\\n$ http POST http://localhost:8000/posts title=\"Hello\"\\nHTTP/1.1 201 Created\\ncontent-length: 17\\ncontent-type: application/json\\ndate: Tue, 30 Mar 2021 07:56:22 GMT\\nserver: uvicorn\\n{\\n    \"title\": \"Hello\"\\n}\\nWe have got our 201 status code.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 108, 'page_label': '90'}, page_content='90     Developing a RESTful API with FastAPI\\nIt\\'s important to understand that this option to override the status code is only useful \\nwhen everything goes well. Even if your input data was invalid, you would still get a  \\n422 status error response.\\nAnother interesting scenario for this option is when you have nothing to return, such as \\nwhen you typically delete an object. In this case, the 204 No content status code is a \\ngood fit. In the following example, we implement a simple DELETE endpoint that sets this \\nresponse status code:\\nchapter3_response_path_parameters_02.py\\n# Dummy database\\nposts = {\\n    1: Post(title=\"Hello\", nb_views=100),\\n}\\n@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_\\nCONTENT)\\nasync def delete_post(id: int):\\n    posts.pop(id, None)\\n    return None\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_path_parameters_02.py\\nNotice that you can very well return None in your path operation function. FastAPI will \\ntake care of it and return a response with an empty body.\\nIn the Setting the status code dynamically section, you\\'ll learn how to customize the status \\ncode dynamically inside the path operation logic.\\nThe response model\\nWith FastAPI, the main use case is to directly return a pydantic model that automatically \\ngets turned into properly formatted JSON. However, quite often, you\\'ll find that there are \\nsome differences between the input data, the data you store in your database, and the data \\nyou want to show to the end user. For instance, perhaps some fields are private or only for \\ninternal use, or perhaps some fields are only useful during the creation process and then \\ndiscarded afterward.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 109, 'page_label': '91'}, page_content='Customizing the response     91\\nNow, let\\'s consider a simple example. Assume you have a database containing blog \\nposts. Those blog posts have several properties, such as a title, content, or creation date. \\nAdditionally, you store the number of views of each one, but you don\\'t want the end user \\nto see any of this.\\nY ou could take the standard approach as follows:\\nchapter3_response_path_parameters_03.py\\nfrom fastapi import FastAPI\\nfrom pydantic import BaseModel\\nclass Post(BaseModel):\\n    title: str\\n    nb_views: int\\napp = FastAPI()\\n# Dummy database\\nposts = {\\n    1: Post(title=\"Hello\", nb_views=100),\\n}\\n@app.get(\"/posts/{id}\")\\nasync def get_post(id: int):\\n    return posts[id]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_path_parameters_03.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 110, 'page_label': '92'}, page_content='92     Developing a RESTful API with FastAPI\\nAnd then call this endpoint:\\n$ http GET http://localhost:8000/posts/1            \\nHTTP/1.1 200 OK\\ncontent-length: 32\\ncontent-type: application/json\\ndate: Tue, 30 Mar 2021 08:11:11 GMT\\nserver: uvicorn\\n{\\n    \"nb_views\": 100,\\n    \"title\": \"Hello\"\\n}\\nThe nb_views property is in the output. However, we don\\'t want this. This is exactly \\nwhat the response_model option is for: to specify another model that only outputs \\nthe properties we want. First, let\\'s define another pydantic model with only the title \\nproperty:\\nchapter3_response_path_parameters_04.py\\nclass PublicPost(BaseModel):\\n    title: str\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_path_parameters_04.py\\nTip\\nY ou might have noticed that we repeat ourselves a lot when defining those \\nmodels. In Chapter 4, Managing pydantic Data Models in FastAPI, you\\'ll learn \\nhow to avoid this.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 111, 'page_label': '93'}, page_content='Customizing the response     93\\nThen, the only change is to add the response_model option as a keyword argument for \\nthe path decorator:\\nchapter3_response_path_parameters_04.py\\n@app.get(\"/posts/{id}\", response_model=PublicPost)\\nasync def get_post(id: int):\\n    return posts[id]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_path_parameters_04.py\\nNow, let\\'s try to call this endpoint:\\n$ http GET http://localhost:8000/posts/1\\nHTTP/1.1 200 OK\\ncontent-length: 17\\ncontent-type: application/json\\ndate: Tue, 30 Mar 2021 08:29:45 GMT\\nserver: uvicorn\\n{\\n    \"title\": \"Hello\"\\n}\\nThe nb_views property is no longer there! Thanks to the response_model option, \\nFastAPI automatically converted our Post instance into a PublicPost instance before \\nserializing it. Now our private data is safe!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 112, 'page_label': '94'}, page_content=\"94     Developing a RESTful API with FastAPI\\nThe good thing is that this option is also considered by the interactive documentation, \\nwhich will show the correct output schema to the end user, as you can see in Figure 3.2:\\nFigure 3.2 – The response model schema in the interactive documentation\\nSo far, you've looked at options that can help you to quickly customize the response \\ngenerated by FastAPI. Now, we'll introduce another approach that will open up  \\nmore possibilities.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 113, 'page_label': '95'}, page_content='Customizing the response     95\\nThe response parameter\\nThe body and status code are not the only interesting parts of an HTTP response. \\nSometimes, it might be useful to return some custom headers or set cookies. This can be \\ndone dynamically using FastAPI directly within the path operation logic. How so? By \\ninjecting the Response object as an argument of the path operation function.\\nSetting headers\\nAs usual, this only involves setting the proper type hinting to the argument. The following \\nexample shows you how to set a custom header:\\nchapter3_response_parameter_01.py\\nfrom fastapi import FastAPI, Response\\napp = FastAPI()\\n@app.get(\"/\")\\nasync def custom_header(response: Response):\\n    response.headers[\"Custom-Header\"] = \"Custom-Header-Value\"\\n    return {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_parameter_01.py\\nThe Response object gives you access to a set of properties, including headers. It\\'s a \\nsimple dictionary where the key is the name of the header, and the value is its associated \\nvalue. Therefore, it\\'s relatively straightforward to set your own custom header.\\nAlso, notice that you don\\'t have to return the Response object. Y ou can still return \\nJSON-encodable data and FastAPI will take care of forming a proper response, including \\nthe headers you\\'ve set. Therefore, the response_model and status_code options we \\ndiscussed in the Path operation parameters section are still honored.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 114, 'page_label': '96'}, page_content='96     Developing a RESTful API with FastAPI\\nLet\\'s view the result:\\n$ http GET http://localhost:8000        \\nHTTP/1.1 200 OK\\ncontent-length: 17\\ncontent-type: application/json\\ncustom-header: Custom-Header-Value\\ndate: Wed, 31 Mar 2021 06:22:03 GMT\\nserver: uvicorn\\n{\\n    \"hello\": \"world\"\\n}\\nOur custom header is part of the response.\\nAs we mentioned earlier, the good thing about this approach is that it\\'s within your \\npath operation logic. That means you can dynamically set headers depending on what\\'s \\nhappening in your business logic.\\nSetting cookies\\nCookies can also be particularly useful when you want to maintain the user\\'s state within \\nthe browser between each of their visits.\\nTo prompt the browser to save some cookies in your response, you could, of course, \\nbuild your own Set-Cookie header and set it in the headers dictionary, just as we \\nsaw in the preceding command block. However, since this can be quite tricky to do, the \\nResponse object exposes a convenient set_cookie method:\\nchapter3_response_parameter_02.py\\n@app.get(\"/\")\\nasync def custom_cookie(response: Response):\\n    response.set_cookie(\"cookie-name\", \"cookie-value\", max_\\nage=86400)\\n    return {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_parameter_02.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 115, 'page_label': '97'}, page_content='Customizing the response     97\\nHere, we simply set a cookie, named cookie-name, with the value of cookie-value. \\nIt\\'ll be valid for 86\\'400 seconds before the browser removes it.\\nLet\\'s try it:\\n$ http GET http://localhost:8000\\nHTTP/1.1 200 OK\\ncontent-length: 17\\ncontent-type: application/json\\ndate: Wed, 31 Mar 2021 06:37:18 GMT\\nserver: uvicorn\\nset-cookie: cookie-name=cookie-value; Max-Age=86400; Path=/; \\nSameSite=lax\\n{\\n    \"hello\": \"world\"\\n}\\nHere, you can see that we have a nice Set-Cookie header with all of the properties of \\nour cookie.\\nAs you may know, cookies have a lot more options than the ones we have shown here; \\nfor instance, path, domain, and HTTP-only. The set_cookie method supports all of \\nthem. Y ou can read about the full list of options in the official Starlette documentation \\n(since Response is also borrowed from Starlette) at https://www.starlette.io/\\nresponses/#set-cookie.\\nIf you\\'re not familiar with the Set-Cookie header, we also recommend that you to refer \\nto MDN Web Docs, which can be accessed at https://developer.mozilla.org/\\nen-US/docs/Web/HTTP/Headers/Set-Cookie.\\nOf course, if you need to set several cookies, you can call this method several times.\\nSetting the status code dynamically\\nIn the Path operation parameters section, we discussed a way to declaratively set the status \\ncode of the response. The drawback to this approach is that it\\'ll always be the same no \\nmatter what\\'s happening inside.\\nLet\\'s assume that we have an endpoint that updates an object in the database or creates it \\nif it doesn\\'t exist. A good approach would be to return a 200 OK status when the object \\nalready exists or a 201 Created status when the object has to be created.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 116, 'page_label': '98'}, page_content='98     Developing a RESTful API with FastAPI\\nTo do this, you can simply set the status_code property on the Response object:\\nchapter3_response_parameter_03.py\\n# Dummy database\\nposts = {\\n    1: Post(title=\"Hello\", nb_views=100),\\n}\\n@app.put(\"/posts/{id}\")\\nasync def update_or_create_post(id: int, post: Post, response: \\nResponse):\\n    if id not in posts:\\n        response.status_code = status.HTTP_201_CREATED\\n    posts[id] = post\\n    return posts[id]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_\\nresponse_parameter_03.py\\nFirst, we check whether the ID in the path exists in the database. If not, we change the \\nstatus code to 201. Then, we simply assign the post at this ID in the database.\\nLet\\'s try with an existing post first:\\n$ http PUT http://localhost:8000/posts/1 title=\"Updated title\"             \\nHTTP/1.1 200 OK\\ncontent-length: 25\\ncontent-type: application/json\\ndate: Wed, 31 Mar 2021 07:02:41 GMT\\nserver: uvicorn\\n{\\n    \"title\": \"Updated title\"\\n}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 117, 'page_label': '99'}, page_content='Customizing the response     99\\nThe post with an ID of 1 already exists, so we get a 200 status. Now, let\\'s try with a \\nnon-existing ID:\\n$ http PUT http://localhost:8000/posts/2 title=\"Updated title\"\\nHTTP/1.1 201 Created\\ncontent-length: 25\\ncontent-type: application/json\\ndate: Wed, 31 Mar 2021 07:03:56 GMT\\nserver: uvicorn\\n{\\n    \"title\": \"Updated title\"\\n}\\nWe get a 201 status!\\nNow you have a way to dynamically set the status code in your logic. Bear in mind, \\nthough, that they won\\'t be detected by the automatic documentation. Therefore, they won\\'t \\nappear as a possible response status code in it.\\nY ou might be tempted to use this approach to set error status codes, such as 400 Bad \\nRequest or 404 Not Found. In fact, you shouldn\\'t do that. FastAPI provides a \\ndedicated way to do this: HTTPException.\\nRaising HTTP errors\\nWhen calling a REST API, quite frequently, you might find that things don\\'t go very well; \\nyou might come across the wrong parameters, invalid payloads, or objects that don\\'t exist \\nanymore. Errors can happen for a lot of reasons.\\nThat\\'s why it\\'s critical to detect them and raise a clear and unambiguous error message \\nto the end user so that they can correct their mistake. In a REST API, there are two very \\nimportant things that you can use to return an informative message: the status code and \\nthe payload.\\nThe status code can give you a precious hint about the nature of the error. Since HTTP \\nprotocols provide a wide range of error status codes, your end user might not even need to \\nread the payload to understand what\\'s wrong.\\nOf course, it\\'s always better to provide a clear error message at the same time in order to \\ngive further details and add some useful information regarding how the end user can solve \\nthe issue.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 118, 'page_label': '100'}, page_content='100     Developing a RESTful API with FastAPI\\nError status codes are crucial\\nSome APIs choice to always return a 200 status code with the payload \\ncontaining a property stating whether the request was successful or not, \\nsuch as {\"success\": false}. Don\\'t do that. The RESTful philosophy \\nencourages you to use the HTTP semantic to give meaning to the data. Having \\nto parse the output and look for a property to determine whether the call was \\nsuccessful is a bad design.\\nTo raise an HTTP error in FastAPI, you\\'ll have to raise a Python exception, \\nHTTPException. This exception class will allow us to set a status code and an error \\nmessage. It is caught by FastAPI error handlers that take care of forming a proper  \\nHTTP response.\\nIn the following example, we\\'ll raise a 400 Bad Request error if the password and \\npassword_confirm payload properties don\\'t match:\\nchapter3_raise_errors_01.py\\n@app.post(\"/password\")\\nasync def check_password(password: str = Body(...), password_\\nconfirm: str = Body(...)):\\n    if password != password_confirm:\\n        raise HTTPException(\\n            status.HTTP_400_BAD_REQUEST,\\n            detail=\"Passwords don\\'t match.\",\\n        )\\n    return {\"message\": \"Passwords match.\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_raise_\\nerrors_01.py\\nAs you can see here, if the passwords are not equal, we directly raise HTTPException. \\nThe first argument is the status code, and the detail keyword argument lets us write an \\nerror message.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 119, 'page_label': '101'}, page_content='Customizing the response     101\\nLet\\'s examine how it works:\\n$ http POST http://localhost:8000/password password=\"aa\" \\npassword_confirm=\"bb\"\\nHTTP/1.1 400 Bad Request\\ncontent-length: 35\\ncontent-type: application/json\\ndate: Wed, 31 Mar 2021 11:58:45 GMT\\nserver: uvicorn\\n{\\n    \"detail\": \"Passwords don\\'t match.\"\\n}\\nHere, we do get a 400 status code and our error message has been wrapped nicely in a \\nJSON object with the detail key. This is how FastAPI handles errors by default.\\nIn fact, you are not limited to a simple string for the error message: you can return a \\ndictionary or a list in order to get structured information about the error. For example, \\ntake a look at the following code snippet:\\nchapter3_raise_errors_02.py\\nraise HTTPException(\\n    status.HTTP_400_BAD_REQUEST,\\n    detail={\\n        \"message\": \"Passwords don\\'t match.\",\\n        \"hints\": [\\n            \"Check the caps lock on your keyboard\",\\n            \"Try to make the password visible by clicking on \\nthe eye icon to check your typing\",\\n        ],\\n    },\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_raise_\\nerrors_02.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 120, 'page_label': '102'}, page_content=\"102     Developing a RESTful API with FastAPI\\nAnd that's it! Y ou now have the power to raise errors and give meaningful information \\nabout them to the end user. \\nSo far, all of the methods you have seen should cover the majority of cases you'll \\nencounter during the development of an API. Sometimes, however, you'll have scenarios \\nwhere you'll need to build a complete HTTP response yourself. This is the subject of the \\nnext section.\\nBuilding a custom response\\nMost of the time, you'll let FastAPI take care of building an HTTP response by simply \\nproviding it with some data to serialize. Under the hood, FastAPI uses a subclass of \\nResponse, called JSONResponse. Quite predictably, this response class takes care of \\nserializing some data to JSON and adding the correct Content-Type header.\\nHowever, there are other response classes that cover common cases:\\n• HTMLResponse: This can be used to return an HTML response.\\n• PlainTextResponse: This can be used to return raw text.\\n• RedirectResponse: This can be used to make a redirection.\\n• StreamingResponse: This can be used to stream a flow of bytes.\\n• FileResponse: This can be used to automatically build a proper file response \\ngiven the path of a file on the local disk.\\nY ou have two ways of using them: either setting the response_class argument on the \\npath decorator or directly returning a response instance.\\nUsing the response_class argument\\nThis is the simplest and most straightforward way to return a custom response. Indeed, by \\ndoing this, you won't even have to create a class instance: you'll just have to return the data \\nas you do usually for standard JSON responses.\\nThis is well suited for HTMLResponse and PlainTextResponse:\\nchapter3_custom_response_01.py\\nfrom fastapi import FastAPI\\nfrom fastapi.responses import HTMLResponse, PlainTextResponse\\napp = FastAPI()\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 121, 'page_label': '103'}, page_content='Customizing the response     103\\n@app.get(\"/html\", response_class=HTMLResponse)\\nasync def get_html():\\n    return \"\"\"\\n        <html>\\n            <head>\\n                <title>Hello world!</title>\\n            </head>\\n            <body>\\n                <h1>Hello world!</h1>\\n            </body>\\n        </html>\\n    \"\"\"\\n@app.get(\"/text\", response_class=PlainTextResponse)\\nasync def text():\\n    return \"Hello world!\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_custom_\\nresponse_01.py\\nBy setting the response_class argument on the decorator, you can change the class \\nthat will be used by FastAPI to build the response. Then, you can simply return valid \\ndata for this kind of response. Notice that the responses classes are imported through the \\nfastapi.responses module.\\nThe nice thing about this is that you can combine this option with the ones we saw in the \\nPath operation parameters section. Using the Response parameter that we described in \\nthe The response parameter section also works perfectly!\\nFor the other response classes, however, you\\'ll have to build the instance yourself and then \\nreturn it.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 122, 'page_label': '104'}, page_content='104     Developing a RESTful API with FastAPI\\nMaking a redirection\\nAs mentioned earlier, RedirectResponse is a class that helps you build an HTTP \\nredirection, which simply is an HTTP response with a Location header pointing to \\nthe new URL and a status code in the 3xx range. It simply expects the URL you wish to \\nredirect to as the first argument:\\nchapter3_custom_response_02.py\\n@app.get(\"/redirect\")\\nasync def redirect():\\n    return RedirectResponse(\"/new-url\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_custom_\\nresponse_02.py\\nBy default, it\\'ll use the 307 Temporary Redirect status code, but you can change \\nthis through the status_code argument:\\nchapter3_custom_response_03.py\\n@app.get(\"/redirect\")\\nasync def redirect():\\n    return RedirectResponse(\"/new-url\", status_code=status.\\nHTTP_301_MOVED_PERMANENTLY)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_custom_\\nresponse_03.py\\nServing a file\\nNow, let\\'s examine how FileResponse works. This will be useful if you wish to propose \\nsome files to download. This response class will automatically take care of opening the file \\non disk and streaming the bytes along with the proper HTTP headers.\\nLet\\'s take a look at how we can use an endpoint to download a picture of a cat. Y ou\\'ll find \\nthis in the code examples repository at https://github.com/PacktPublishing/\\nBuilding-Data-Science-Applications-with-FastAPI/blob/main/\\nassets/cat.jpg.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 123, 'page_label': '105'}, page_content='Customizing the response     105\\nFor this class to work, first, you\\'ll need another extra dependency, aiofiles:\\n$ pip install aiofiles\\nThen, we just need to return an instance of FileResponse with the path of the file we \\nwant to serve as the first argument:\\nchapter3_custom_response_04.py\\n@app.get(\"/cat\")\\nasync def get_cat():\\n    root_directory = path.dirname(path.dirname(__file__))\\n    picture_path = path.join(root_directory, \"assets\", \"cat.\\njpg\")\\n    return FileResponse(picture_path)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_custom_\\nresponse_04.py\\nThe os.path module\\nPython provides a module to help you work with file paths, os.path. It\\'s \\nthe recommended way to manipulate paths, as it takes care of handling them \\ncorrectly depending on the OS you are running. Y ou can read about the \\nfunctions of this module in the official documentation at https://docs.\\npython.org/3/library/os.path.html.\\nLet\\'s examine what the HTTP response looks like:\\n$ http GET http://localhost:8000/cat\\nHTTP/1.1 200 OK\\ncontent-length: 71457\\ncontent-type: image/jpeg\\ndate: Thu, 01 Apr 2021 06:50:34 GMT\\netag: 243d3de0ca74453f0c2d120e2f064e58\\nlast-modified: Mon, 29 Mar 2021 06:40:29 GMT\\nserver: uvicorn'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 124, 'page_label': '106'}, page_content='106     Developing a RESTful API with FastAPI\\n+-----------------------------------------+\\n| NOTE: binary data not shown in terminal |\\n+-----------------------------------------+\\nAs you can see, we have the right Content-Length and Content-Type headers for \\nour image. The response even sets the Etag and Last-Modified headers so that the \\nbrowser can properly cache the resource. HTTPie doesn\\'t show the binary data in the \\nbody; however, if you open the endpoint in your browser, you\\'ll see the cat appear!\\nCustom responses\\nFinally, if you really have a case that\\'s not covered by the provided classes, you always have \\nthe option to use the Response class to build exactly what you need. With this class, you \\ncan set everything, including the body content and the headers.\\nThe following example shows you how to return an XML response:\\nchapter3_custom_response_05.py\\n@app.get(\"/xml\")\\nasync def get_xml():\\n    content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n        <Hello>World</Hello>\\n    \"\"\"\\n    return Response(content=content, media_type=\"application/\\nxml\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3/chapter3_custom_\\nresponse_05.py\\nY ou can view the complete list of arguments in the Starlette documentation at https://\\nwww.starlette.io/responses/#response.\\nPath operation parameters and response parameters won\\'t have any effect\\nBear in mind that when you directly return a Response class (or one of \\nits subclasses), the parameters you set on the decorator or the operations \\nyou make on the injected Response object won\\'t have any effect. They are \\ncompletely overridden by the Response object you return. If you need to \\ncustomize the status code or the headers, then use the status_code and \\nheaders arguments when instantiating your class.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 125, 'page_label': '107'}, page_content='Structuring a bigger project with multiple routers     107\\nWell done! Now you have all of the knowledge required to create the response you need \\nfor your REST API. Y ou\\'ve learned that FastAPI comes with sensible defaults that can help \\nyou create proper JSON responses in no time. At the same time, it also gives you access to \\nmore advanced objects and options to allow you to make custom responses.\\nSo far, all of the examples we\\'ve looked at have been quite short and simple. However, \\nwhen you\\'re developing a real application, you\\'ll probably have dozens of endpoints and \\nmodels. In the final section of this chapter, we\\'ll examine how to organize such projects to \\nmake them modular and easier to maintain.\\nStructuring a bigger project with multiple \\nrouters\\nWhen building a real-world web application, you\\'re likely to have lot of code and logic: \\ndata models, API endpoints, and services. Of course, all of those can\\'t live in a single file; \\nwe have to structure the project so that it\\'s easy to maintain and evolve.\\nFastAPI supports the concept of routers. They are \"sub-parts\" of your API and are usually \\ndedicated to a single type of object, such as users or posts, that are defined in their own file. \\nY ou can then include them in your main FastAPI app so that it can route it accordingly.\\nIn this section, we\\'ll explore how to use routers and how you can structure a FastAPI \\nproject. While this structure is one way to do it, and works quite well, it\\'s not a golden rule \\nand can be adapted to your own needs.\\nIn the code examples repository, there is a folder named chapter3_project, \\nwhich contains a sample project with this structure: https://github.com/\\nPacktPublishing/Building-Data-Science-Applications-with-\\nFastAPI/tree/main/chapter3_project.\\nHere is the project structure:\\n.\\n└── chapter3_project/\\n    ├── models/\\n    │   ├── __init__.py\\n    │   ├── post.py\\n    │   └── user.py\\n    ├── routers/\\n    │   ├── __init__.py\\n    │   ├── posts.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 126, 'page_label': '108'}, page_content='108     Developing a RESTful API with FastAPI\\n    │   └── users.py\\n    ├── __init__.py\\n    ├── app.py\\n    └── db.py\\nHere, you can see that we chose to have packages that contain pydantic models on one \\nside and routers on the other side. At the root of the project, we have a file named app.py \\nthat will expose the main FastAPI application. The db.py file defines a dummy database \\nfor the sake of the example.\\nThe __init__.py files are there to properly define our directories as Python packages. \\nY ou can read more details about this in the Packages, modules, and imports section of \\nChapter 2, Python Programming Specificities.\\nFirst, let\\'s examine what a FastAPI router looks like:\\nusers.py\\nfrom typing import List\\nfrom fastapi import APIRouter, HTTPException, status\\nfrom chapter3_project.models.user import User, UserCreate\\nfrom chapter3_project.db import db\\nrouter = APIRouter()\\n@router.get(\"/\")\\nasync def all() -> List[User]:\\n    return list(db.users.values())\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3_project/routers/\\nusers.py\\nAs you can see here, instead of instantiating the FastAPI class, you instantiate the \\nAPIRouter class. Then, you can use it exactly the same way to decorate your path \\noperation functions.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 127, 'page_label': '109'}, page_content='Structuring a bigger project with multiple routers     109\\nAlso, notice that we import the pydantic models from the relevant module in the  \\nmodels package.\\nWe won\\'t go into detail about the logic of the endpoints, but we invite you to read about it. \\nIt uses all the FastAPI features that we\\'ve explored so far.\\nNow, let\\'s take a look at how to import this router and include it within a FastAPI \\napplication:\\napp.py\\nfrom fastapi import FastAPI\\nfrom chapter3_project.routers.posts import router as posts_\\nrouter\\nfrom chapter3_project.routers.users import router as users_\\nrouter\\napp = FastAPI()\\napp.include_router(posts_router, prefix=\"/posts\", \\ntags=[\"posts\"])\\napp.include_router(users_router, prefix=\"/users\", \\ntags=[\"users\"])\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter3_project/app.py\\nAs usual, we instantiate the FastAPI class. Then, we use the include_router method \\nto add our sub-router. Y ou can see that we simply imported the router from its relevant \\nmodule and used it as the first argument of include_router. Notice that we used the \\nsynta as while importing. Since both users and posts routers are named  \\nthe same inside their module, this syntax allows us to alias their name and, thus, avoid \\nname collision.\\nAdditionally, you can see that we set the keyword argument as prefix. This allows us \\nto prefix the path of all the endpoints of this router. This way, you don\\'t have to hardcode \\nit in the router logic and can easily change it for the whole router. It can also be used to \\nprovide versioned paths of your API, such as /v1.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 128, 'page_label': '110'}, page_content=\"110     Developing a RESTful API with FastAPI\\nFinally, the tags argument helps you to group endpoints in the interactive \\ndocumentation for better readability. By doing this, the posts and users endpoints will \\nbe clearly separated in the documentation.\\nAnd that's all you need to do! Y ou can run this whole application, as usual, with uvicorn:\\n$ uvicorn chapter3_project.app:app\\nIf you open the interactive documentation at http://localhost:8000/docs,  \\nyou'll see that all the routes are there, grouped by the tags we specified when including  \\nthe router:\\nFigure 3.3 – Tagged routers in the interactive documentation\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 129, 'page_label': '111'}, page_content=\"Summary     111\\nOnce again, you can see that FastAPI is both powerful and very lightweight to use. The \\ngood thing about routers is that you can even nest them, that include sub-routers in \\nrouters that include other routers themselves. Therefore, you can have a quite complex \\nrouting hierarchy with very low effort.\\nSummary\\nWell done! Y ou're now acquainted with all the basic features of FastAPI. Throughout this \\nchapter, you've learned how to create and run API endpoints where you can validate and \\nretrieve data from all parts of an HTTP request: the path, the query, the parameters, the \\nheaders, and, of course, the body. Y ou've also learned how to tailor the HTTP response \\nto your needs, whether it is a simple JSON response, an error, or a file to download. \\nFinally, you looked at how to define separate API routers and include them in your main \\napplication to keep a clean and maintainable project structure.\\nY ou have enough knowledge now to start building your own API with FastAPI. In the \\nnext chapter, we'll focus on pydantic models. Y ou now know that they are at the core of \\nthe data validation features of FastAPI, so it's crucial to fully understand how they work \\nand how to manipulate them efficiently.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 130, 'page_label': '112'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 131, 'page_label': '113'}, page_content=\"4\\nManaging Pydantic \\nData Models in \\nFastAPI\\nThis chapter will cover in more detail the definition of a data model with Pydantic, the \\nunderlying data validation library used by FastAPI. We'll explain how to implement \\nvariations of the same model without repeating the same code again and again, thanks to \\nclass inheritance. Finally, we'll show how to implement custom data validation logic into \\nPydantic models.\\nIn this chapter, we're going to cover the following main topics:\\n• Defining models and their field types with Pydantic\\n• Creating model variations with class inheritance\\n• Adding custom data validation with Pydantic\\n• Working with Pydantic objects\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 132, 'page_label': '114'}, page_content=\"114     Managing Pydantic Data Models in FastAPI\\nTechnical requirements\\nTo run the code examples, you'll need a Python virtual environment, which we set up in \\nChapter 1, Python Development Environment Setup.\\nY ou'll find all the code examples of this chapter in the dedicated GitHub repository \\nat https://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter4.\\nDefining models and their field types  \\nwith Pydantic\\nPydantic is a powerful library for defining data models using Python classes and type \\nhints. This approach makes those classes completely compatible with static type checking. \\nBesides, since there are regular Python classes, we can use inheritance and also define our \\nvery own methods to add custom logic.\\nIn Chapter 3, Developing a RESTful API with FastAPI, you learned the basics of defining a \\ndata model with Pydantic: you have to define a class inheriting from BaseModel and list \\nall the fields as class properties, each one with a proper type hint to enforce their type.\\nIn this section, we'll focus on model definition and see all the possibilities we have to \\ndefine the fields.\\nStandard field types\\nWe'll begin by defining fields with standard types, which only involve simple type hints. \\nLet's review a simple model representing information about a person. Y ou can see this in \\nthe following code sample:\\nchapter4_standard_field_types_01.py\\nfrom pydantic import BaseModel\\nclass Person(BaseModel):\\n    first_name: str\\n    last_name: str\\n    age: int\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_01.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 133, 'page_label': '115'}, page_content='Defining models and their field types with Pydantic      115\\nAs we said, you just have to write the name of the fields and type-hint it with the intended \\ntype. Of course, we are not limited to scalar types: we can actually use compound types \\nsuch as lists, tuples, or datetime classes. In the following example, you can see a model \\nusing those more complex types:\\nchapter4_standard_field_types_02.py\\nfrom datetime import date\\nfrom enum import Enum\\nfrom typing import List\\nfrom pydantic import BaseModel, ValidationError\\nclass Gender(str, Enum):\\n    MALE = \"MALE\"\\n    FEMALE = \"FEMALE\"\\n    NON_BINARY = \"NON_BINARY\"\\nclass Person(BaseModel):\\n    first_name: str\\n    last_name: str\\n    gender: Gender\\n    birthdate: date\\n    interests: List[str]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_02.py\\nThere are three things to notice in this example.\\nFirst, we used the standard Python Enum class as a type for the gender field. This allows \\nus to specify a set of valid values. If we input a value that\\'s not in this enumeration, \\nPydantic will raise an error, as illustrated in the following example:\\nchapter4_standard_field_types_02.py\\n# Invalid gender\\ntry:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 134, 'page_label': '116'}, page_content='116     Managing Pydantic Data Models in FastAPI\\n    Person(\\n        first_name=\"John\",\\n        last_name=\"Doe\",\\n        gender=\"INVALID_VALUE\",\\n        birthdate=\"1991-01-01\",\\n        interests=[\"travel\", \"sports\"],\\n    )\\nexcept ValidationError as e:\\n    print(str(e))\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_02.py\\nIf you run the preceding example, you\\'ll get this output:\\n1 validation error for Person\\ngender\\n  value is not a valid enumeration member; permitted: \\n\\'MALE\\', \\'FEMALE\\', \\'NON_BINARY\\' (type=type_error.enum; enum_\\nvalues=[<Gender.MALE: \\'MALE\\'>, <Gender.FEMALE: \\'FEMALE\\'>, \\n<Gender.NON_BINARY: \\'NON_BINARY\\'>])\\nActually, this is exactly what we already did in Chapter 3, Developing a RESTful API with \\nFastAPI, to limit the allowed values of the path parameter.\\nThen, we used the date Python class as a type for the birthdate field. Pydantic is \\nable to automatically parse dates and datetimes given as an International Organization \\nfor Standardization (ISO) format string or a timestamp integer and instantiate a proper \\ndate or datetime object. Of course, if the parsing fails, you\\'ll also get an error. Y ou can \\nexperiment with this in the following example:\\nchapter4_standard_field_types_02.py\\n# Invalid birthdate\\ntry:\\n    Person(\\n        first_name=\"John\",\\n        last_name=\"Doe\",\\n        gender=Gender.MALE,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 135, 'page_label': '117'}, page_content='Defining models and their field types with Pydantic      117\\n        birthdate=\"1991-13-42\",\\n        interests=[\"travel\", \"sports\"],\\n    )\\nexcept ValidationError as e:\\n    print(str(e))\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_02.py\\nAnd here is the output:\\n1 validation error for Person\\nbirthdate\\n  invalid date format (type=value_error.date)\\nFinally, we defined interests as a list of strings. Once again, Pydantic will check if the \\nfield is a valid list of strings.\\nObviously, if everything is okay, we get a Person instance and have access to the properly \\nparsed fields. This is what we show in the following code sample:\\nchapter4_standard_field_types_02.py\\n# Valid\\nperson = Person(\\n    first_name=\"John\",\\n    last_name=\"Doe\",\\n    gender=Gender.MALE,\\n    birthdate=\"1991-01-01\",\\n    interests=[\"travel\", \"sports\"],\\n)\\n# first_name=\\'John\\' last_name=\\'Doe\\' gender=<Gender.\\nMALE: \\'MALE\\'> birthdate=datetime.date(1991, 1, 1) \\ninterests=[\\'travel\\', \\'sports\\']\\nprint(person)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_02.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 136, 'page_label': '118'}, page_content='118     Managing Pydantic Data Models in FastAPI\\nAs you see, this is quite powerful, and we can have quite complex field types. But that\\'s \\nnot all: fields can be Pydantic models themselves, allowing you to have sub-objects! In the \\nfollowing code example, we expand the previous one to add an address field:\\nchapter4_standard_field_types_03.py\\nclass Address(BaseModel):\\n    street_address: str\\n    postal_code: str\\n    city: str\\n    country: str\\nclass Person(BaseModel):\\n    first_name: str\\n    last_name: str\\n    gender: Gender\\n    birthdate: date\\n    interests: List[str]\\n    address: Address\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_03.py\\nWe just have to define another Pydantic model and use it as a type hint. Now, you can \\neither instantiate a Person instance with an already valid Address instance or, even \\nbetter, with a dictionary. In this case, Pydantic will automatically parse it and validate it \\nagainst the address model.\\nIn the following code sample, we try to input an invalid address:\\nchapter4_standard_field_types_03.py\\ntry:\\n    Person(\\n        first_name=\"John\",\\n        last_name=\"Doe\",\\n        gender=\"INVALID_VALUE\",\\n        birthdate=\"1991-01-01\",'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 137, 'page_label': '119'}, page_content='Defining models and their field types with Pydantic      119\\n        interests=[\"travel\", \"sports\"],\\n        address={\\n            \"street_address\": \"12 Squirell Street\",\\n            \"postal_code\": \"424242\",\\n            \"city\": \"Woodtown\",\\n            # Missing country\\n        }\\n    )\\nexcept ValidationError as e:\\n    print(str(e))\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_03.py\\nThis will generate the following validation error:\\n1 validation error for Person\\naddress -> country\\n  field required (type=value_error.missing)\\nPydantic clearly shows the missing field in the sub-object. Once again, if everything \\ngoes well, we get a Person instance and its associated Address, as you can see in the \\nfollowing extract:\\nchapter4_standard_field_types_03.py\\n# Valid\\nperson = Person(\\n    first_name=\"John\",\\n    last_name=\"Doe\",\\n    gender=Gender.MALE,\\n    birthdate=\"1991-01-01\",\\n    interests=[\"travel\", \"sports\"],\\n    address={\\n        \"street_address\": \"12 Squirell Street\",\\n        \"postal_code\": \"424242\",\\n        \"city\": \"Woodtown\",'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 138, 'page_label': '120'}, page_content='120     Managing Pydantic Data Models in FastAPI\\n        \"country\": \"US\",\\n    },\\n)\\nprint(person)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\nstandard_field_types_03.py\\nOptional fields and default values\\nUp to now, we\\'ve assumed that each field had to be provided when instantiating the \\nmodel. Quite often, however, there are values that we want to be optional because they \\nmay not be relevant for each object instance. Sometimes, we also wish to set a default \\nvalue for a field when it\\'s not specified.\\nAs you may have guessed, this is done quite simply, with the Optional typing \\nannotation, as illustrated in the following code sample:\\nchapter4_optional_fields_default_values_01.py\\nfrom typing import Optional\\nfrom pydantic import BaseModel\\nclass UserProfile(BaseModel):\\n    nickname: str\\n    location: Optional[str] = None\\n    subscribed_newsletter: bool = True\\nuser = UserProfile(nickname=\"jdoe\")\\nprint(user)  # nickname=\\'jdoe\\' location=None subscribed_\\nnewsletter=True\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\noptional_fields_default_values_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 139, 'page_label': '121'}, page_content='Defining models and their field types with Pydantic      121\\nWhen defining a field with the Optional type hint, it accepts a None value. As you see \\nin the preceding code sample, the default value can be simply assigned by putting the \\nvalue after an equals sign.\\nBe careful, though: don\\'t assign default values such as this for dynamic types such as \\ndatetimes. By doing so, the datetime instantiation will be evaluated only once when the \\nmodel is imported. The effect of this is that all the objects you\\'ll instantiate will then \\nshare the same value instead of having a fresh value. Y ou can observe this behavior in the \\nfollowing example:\\nchapter4_optional_fields_default_values_02.py\\nclass Model(BaseModel):\\n    # Don\\'t do this.\\n    # This example shows you why it doesn\\'t work.\\n    d: datetime = datetime.now()\\n \\n \\no1 = Model()\\nprint(o1.d)\\n \\ntime.sleep(1)  # Wait for a second\\n \\no2 = Model()\\nprint(o2.d)\\n \\nprint(o1.d < o2.d)  # False\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\noptional_fields_default_values_02.py\\nEven though we waited for 1 second between the instantiation of o1 and o2, the d datetime \\nis the same! This means that the datetime is evaluated once when the class is imported.\\nY ou can have the same kind of problem if you want to have a default list, such as l: \\nList[str] = [\"a\", \"b\", \"c\"]. Notice that this is true for every Python object, \\nnot only Pydantic models, so you should bear this in mind.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 140, 'page_label': '122'}, page_content=\"122     Managing Pydantic Data Models in FastAPI\\nSo, how to assign dynamic default values? Fortunately, Pydantic provides a Field \\nfunction that allows us to set some advanced options on our fields, including one to set \\na factory for creating dynamic values. Before showing you this, we'll first introduce the \\nField function.\\nField validation\\nIn Chapter 3, Developing a RESTful API with FastAPI, we showed how to apply some \\nvalidation to the request parameters to check if a number was in a certain range or if a \\nstring was matching a regular expression (regex). Actually, those options directly come \\nfrom Pydantic! We can use the same ones to apply validation to the fields of a model.\\nTo do this, we'll use the Field function from Pydantic and use its result as the default \\nvalue of the field. In the following example, we define a Person model with the  \\nfirst_name and last_name required properties, which should be at least three \\ncharacters long, and an optional age property, which should be an integer between 0  \\nand 120. We show the implementation of this model in the following code sample:\\nchapter4_fields_validation_01.py\\nfrom typing import Optional\\nfrom pydantic import BaseModel, Field, ValidationError\\nclass Person(BaseModel):\\n    first_name: str = Field(..., min_length=3)\\n    last_name: str = Field(..., min_length=3)\\n    age: Optional[int] = Field(None, ge=0, le=120)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_fields_\\nvalidation_01.py\\nAs you see, the syntax is very similar to the one we saw for Path, Query, and Body.  \\nThe first positional argument defines the default value for the field. If the field is required, \\nwe use the ellipsis .... Then, the keyword arguments are there to set options for the field, \\nincluding some basic validation.\\nY ou can view a complete list of the arguments accepted by Field in the official \\nPydantic documentation, at https://pydantic-docs.helpmanual.io/usage/\\nschema/#field-customisation.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 141, 'page_label': '123'}, page_content='Defining models and their field types with Pydantic      123\\nDynamic default values\\nIn the previous section, we warned you about setting dynamic values as defaults. \\nFortunately, Pydantic provides the default_factory argument on the Field \\nfunction to cover this use case. This argument expects you to pass a function that will be \\ncalled during model instantiation. Thus, the resulting object will be evaluated at runtime \\neach time you create a new object. Y ou can see how to use it in the following example:\\nchapter4_fields_validation_02.py\\nfrom datetime import datetime\\nfrom typing import List\\nfrom pydantic import BaseModel, Field\\ndef list_factory():\\n    return [\"a\", \"b\", \"c\"]\\nclass Model(BaseModel):\\n    l: List[str] = Field(default_factory=list_factory)\\n    d: datetime = Field(default_factory=datetime.now)\\n    l2: List[str] = Field(default_factory=list)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_fields_\\nvalidation_02.py\\nY ou simply have to pass a function to this argument. Don\\'t put arguments on it—it\\'ll be \\nPydantic that will automatically call the function for you when instantiating a new object. \\nIf you need to call a function with specific arguments, you\\'ll have to wrap it into your own \\nfunction, as we did for list_factory.\\nNotice also that the first positional argument used for the default value (such as None or \\n...) is completely omitted here. This makes sense: it\\'s not consistent to have both a default \\nvalue and a factory. Pydantic will raise an error if you set those two arguments together.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 142, 'page_label': '124'}, page_content='124     Managing Pydantic Data Models in FastAPI\\nValidating email addresses and URLs with  \\nPydantic types\\nFor convenience, Pydantic provides some classes to use as field types to validate some \\ncommon patterns such as email addresses or Uniform Resource Locators (URLs).\\nIn the following example, we\\'ll use EmailStr and HttpUrl to validate an email address \\nand a HyperText Transfer Protocol (HTTP) URL.\\nFor EmailStr to work, you\\'ll need an optional dependency, email-validator,  \\nwhich you can install with the following command:\\n$ pip install email-validator\\nThose classes work like any other type or class: just use them as a type hint for your field. \\nY ou can see this in the following extract:\\nchapter4_pydantic_types_01.py\\nfrom pydantic import BaseModel, EmailStr, HttpUrl, \\nValidationError\\nclass User(BaseModel):\\n    email: EmailStr\\n    website: HttpUrl\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\npydantic_types_01.py\\nIn the following example, we check that the email address is correctly validated:\\nchapter4_pydantic_types_01.py\\n# Invalid email\\ntry:\\n    User(email=\"jdoe\", website=\"https://www.example.com\")\\nexcept ValidationError as e:\\n    print(str(e))\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\npydantic_types_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 143, 'page_label': '125'}, page_content='Defining models and their field types with Pydantic      125\\nY ou will see the following output:\\n1 validation error for User\\nemail\\n  value is not a valid email address (type=value_error.email)\\nWe also check that the URL is correctly parsed, as follows:\\nchapter4_pydantic_types_01.py\\n# Invalid URL\\ntry:\\n    User(email=\"jdoe@example.com\", website=\"jdoe\")\\nexcept ValidationError as e:\\n    print(str(e))\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\npydantic_types_01.py\\nY ou will see the following output:\\n1 validation error for User\\nwebsite\\n  invalid or missing URL scheme (type=value_error.url.scheme)\\nIf you have a look at a valid example, shown next, you\\'ll see that the URL is parsed into an \\nobject, giving you access to the different parts of it, such as the scheme or hostname:\\nchapter4_pydantic_types_01.py\\n# Valid\\nuser = User(email=»jdoe@example.com», website=»https://www.\\nexample.com»)\\n# email=\\'jdoe@example.com\\' website=HttpUrl(\\'https://www.\\nexample.com\\', scheme=\\'https\\', host=\\'www.example.com\\', \\ntld=\\'com\\', host_type=\\'domain\\')\\nprint(user)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_\\npydantic_types_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 144, 'page_label': '126'}, page_content=\"126     Managing Pydantic Data Models in FastAPI\\nPydantic provides a quite big set of types that can help you in various situations. We invite \\nyou to review a full list of these in the official documentation, at https://pydantic-\\ndocs.helpmanual.io/usage/types/#pydantic-types.\\nY ou now have a better view of how to define finely your Pydantic models, by using more \\nadvanced types or leveraging the validation features. As we said, those models are at \\nthe heart of FastAPI, and you'll probably have to define several variations for the same \\nentity to account for several situations. In the next section, we'll show how to do that with \\nminimum repetition.\\nCreating model variations with class \\ninheritance\\nIn Chapter 3, Developing a RESTful API with FastAPI, we saw a case where we needed to \\ndefine two variations of a Pydantic model in order to split between the data we want to \\nstore in the backend and the data we want to show to the user. This is a common pattern \\nin FastAPI: you define one model for creation, one for the response and one for the data to \\nstore in the database.\\nWe show this basic approach in the following sample:\\nchapter4_model_inheritance_01.py\\nfrom pydantic import BaseModel\\nclass PostCreate(BaseModel):\\n    title: str\\n    content: str\\nclass PostPublic(BaseModel):\\n    id: int\\n    title: str\\n    content: str\\nclass PostDB(BaseModel):\\n    id: int\\n    title: str\\n    content: str\\n    nb_views: int = 0\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 145, 'page_label': '127'}, page_content='Creating model variations with class inheritance     127\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_model_\\ninheritance_01.py\\nWe have three models here, covering three situations. These are outlined as follows:\\n• PostCreate will be used for a POST endpoint to create a new post. We expect \\nthe user to give the title and the content; however, the identifier (ID) will be \\nautomatically determined by the database.\\n• PostPublic will be used when we retrieve the data of a post. We want its title and \\ncontent, of course, but also its associated ID in the database.\\n• PostDB will carry all the data we wish to store in the database. Here, we also want \\nto store the number of views, but we want to keep this secret to make our own \\nstatistics internally.\\nY ou can see here that we are repeating ourselves quite a lot, especially with the title and \\ncontent fields. In bigger examples with lots of fields and lots of validation options, this \\ncould quickly become unmanageable.\\nThe solution here is to leverage model inheritance to avoid this. The approach is simple: \\nidentify the fields that are common to every variation and put them in a model that will \\nbe used as a base for every other. Then, you only have to inherit from that model to create \\nyour variations and add the specific fields. In the following example, we see what our \\nprevious example looks like with this method:\\nchapter4_model_inheritance_02.py\\nfrom pydantic import BaseModel\\nclass PostBase(BaseModel):\\n    title: str\\n    content: str\\nclass PostCreate(PostBase):\\n    pass'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 146, 'page_label': '128'}, page_content='128     Managing Pydantic Data Models in FastAPI\\nclass PostPublic(PostBase):\\n    id: int\\nclass PostDB(PostBase):\\n    id: int\\n    nb_views: int = 0\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_model_\\ninheritance_02.py\\nNow, whenever you need to add a field for the whole entity, all you have to do is to add it \\nto the PostBase model.\\nIt\\'s also very convenient if you wish to define methods on your model. Remember that \\nPydantic models are regular Python classes, so you can implement as many methods as \\nyou wish!\\nchapter4_model_inheritance_03.py\\nclass PostBase(BaseModel):\\n    title: str\\n    content: str\\n    def excerpt(self) -> str:\\n        return f\"{self.content[:140]}...\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_model_\\ninheritance_03.py\\nDefining the excerpt method on PostBase means that this will be available in every \\nmodel variation.\\nWhile not strictly required, this inheritance approach is strongly recommended to avoid \\ncode duplication and, ultimately, bugs. We\\'ll see in the next section that it\\'ll make even \\nmore sense with custom validation methods.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 147, 'page_label': '129'}, page_content='Adding custom data validation with Pydantic     129\\nAdding custom data validation with Pydantic\\nUp to now, we\\'ve seen how to apply basic validation to our models, through the Field \\narguments or the custom types provided by Pydantic. In a real-world project, though, \\nyou\\'ll probably need to add your own custom validation logic for your specific case. \\nPydantic allows this by defining validators, which are methods on the model that can be \\napplied at a field level or an object level.\\nApplying validation at a field level\\nThis is the most common case: have a validation rule for a single field. To define it in \\nPydantic, we\\'ll just have to write a static method on our model and decorate it with \\nthe validator decorator. As a reminder, decorators are syntactic sugar, allowing the \\nwrapping of a function or a class with common logic, without compromising readability.\\nThe following example checks a birth date by verifying that the person is not more than \\n120 years old:\\nchapter4_custom_validation_01.py\\nfrom datetime import date\\nfrom pydantic import BaseModel, validator\\nclass Person(BaseModel):\\n    first_name: str\\n    last_name: str\\n    birthdate: date\\n    @validator(\"birthdate\")\\n    def valid_birthdate(cls, v: date):\\n        delta = date.today() - v\\n        age = delta.days / 365\\n        if age > 120:\\n            raise ValueError(\"You seem a bit too old!\")\\n        return v\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_custom_\\nvalidation_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 148, 'page_label': '130'}, page_content='130     Managing Pydantic Data Models in FastAPI\\nAs you see here, the validator is a static class method (the first argument, cls,  \\nbeing the class itself), with the value to validate as the v argument. It\\'s decorated by  \\nthe validator decorator, which expects the name of the argument to validate as the  \\nfirst argument.\\nPydantic expects two things for this method, detailed as follows:\\n• If the value is not valid according to your logic, you should raise a ValueError \\nerror with an explicit error message.\\n• Otherwise, you should return the value that will be assigned in the model. Notice \\nthat it doesn\\'t need to be the same as the input value: you can very well change it \\nto fit your needs. That\\'s actually what we\\'ll do in an upcoming section, Applying \\nvalidation before Pydantic parsing.\\nApplying validation at an object level\\nIt happens quite often that the validation of one field is dependent on another—for \\nexample, to check if a password confirmation matches the password or to enforce a field \\nto be required in certain circumstances. To allow this kind of validation, we need to access \\nthe whole object data. For this, Pydantic provides the root_validator decorator, \\nwhich is illustrated in the following code example:\\nchapter4_custom_validation_02.py\\nfrom pydantic import BaseModel, EmailStr, ValidationError, \\nroot_validator\\nclass UserRegistration(BaseModel):\\n    email: EmailStr\\n    password: str\\n    password_confirmation: str\\n    @root_validator()\\n    def passwords_match(cls, values):\\n        password = values.get(\"password\")\\n        password_confirmation = values.get(\"password_\\nconfirmation\")\\n        if password != password_confirmation:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 149, 'page_label': '131'}, page_content='Adding custom data validation with Pydantic     131\\n            raise ValueError(\"Passwords don\\'t match\")\\n        return values\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_custom_\\nvalidation_02.py\\nThe usage of this decorator is similar to the validator decorator. The static class \\nmethod is called along with the values argument, which is a dictionary containing all \\nthe fields. Thus, you can retrieve each one of them and implement your logic.\\nOnce again, Pydantic expects two things for this method, outlined as follows:\\n• If the values are not valid according to your logic, you should raise a ValueError \\nerror with an explicit error message.\\n• Otherwise, you should return a values dictionary that will be assigned to the \\nmodel. Notice that you could change some values in this dictionary to fit your needs.\\nApplying validation before Pydantic parsing\\nBy default, your validators are run after Pydantic has done its parsing work. This means \\nthat the value you get already conforms to the type of field you specified. If the type is \\nincorrect, Pydantic raises an error without calling your validator.\\nHowever, you may sometimes wish to provide some custom parsing logic that allows you \\nto transform input values that would have been incorrect for the type you set. In that case, \\nyou would need to run your validator before the Pydantic parser: this is the purpose of the \\npre argument on validator.\\nIn the following example, we show how to transform a string with values separated by a \\ncomma into a proper list:\\nchapter4_custom_validation_03.py\\nfrom typing import List\\nfrom pydantic import BaseModel, validator\\nclass Model(BaseModel):\\n    values: List[int]'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 150, 'page_label': '132'}, page_content='132     Managing Pydantic Data Models in FastAPI\\n    @validator(\"values\", pre=True)\\n    def split_string_values(cls, v):\\n        if isinstance(v, str):\\n            return v.split(\",\")\\n        return v\\nm = Model(values=\"1,2,3\")\\nprint(m.values)  # [1, 2, 3]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_custom_\\nvalidation_03.py\\nY ou see here that our validator first checks whether we have a string. If we do, we split a \\ncomma-separated string and return the resulting list; otherwise, we directly return the \\nvalue. Pydantic will run its parsing logic after, so you can still be sure that an error will be \\nraised if v is an invalid value.\\nWorking with Pydantic objects\\nWhen developing API endpoints with FastAPI, you\\'ll likely get a lot of Pydantic model \\ninstances to handle. It\\'s then up to you to implement the logic to make a link between those \\nobjects and your services, such as your database or your machine learning (ML) model. \\nFortunately, Pydantic provides methods to make this very easy. We\\'ll review common use \\ncases that will be useful for you during development.\\nConverting an object into a dictionary\\nThis is probably the action you\\'ll perform the most on a Pydantic object: convert it to a \\nraw dictionary that\\'ll be easy to send to another API or use in a database, for example.  \\nY ou just have to call the dict method on the object instance.\\nThe following example reuses the Person and Address models we saw in the Standard \\nfield types section of this chapter:\\nchapter4_working_pydantic_objects_01.py\\nperson = Person(\\n    first_name=\"John\",\\n    last_name=\"Doe\",\\n    gender=Gender.MALE,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 151, 'page_label': '133'}, page_content='Working with Pydantic objects     133\\n    birthdate=\"1991-01-01\",\\n    interests=[\"travel\", \"sports\"],\\n    address={\\n        \"street_address\": \"12 Squirell Street\",\\n        \"postal_code\": \"424242\",\\n        \"city\": \"Woodtown\",\\n        \"country\": \"US\",\\n    },\\n)\\nperson_dict = person.dict()\\nprint(person_dict[\"first_name\"])  # \"John\"\\nprint(person_dict[\"address\"][\"street_address\"])  # \"12 Squirell \\nStreet\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_01.py\\nAs you see, calling dict is enough to transform the whole data into a dictionary. \\nSub-objects are also recursively converted: the address key points itself to a dictionary \\nwith the address properties.\\nInterestingly, the dict method supports some arguments, allowing you to select a subset \\nof properties to be converted. Y ou can either state the ones you want to be included or the \\nones you want to exclude, as you can see in the following sample:\\nchapter4_working_pydantic_objects_02.py\\nperson_include = person.dict(include={\"first_name\", \"last_\\nname\"})\\nprint(person_include)  # {\"first_name\": \"John\", \"last_name\": \\n\"Doe\"}\\nperson_exclude = person.dict(exclude={\"birthdate\", \\n\"interests\"})\\nprint(person_exclude)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_02.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 152, 'page_label': '134'}, page_content='134     Managing Pydantic Data Models in FastAPI\\nThe include and exclude arguments expect a set with the keys of the fields you want \\nto include or exclude.\\nFor nested structures such as address here, you can also use a dictionary to specify \\nwhich sub-field you want to include or exclude, as illustrated in the following example:\\nchapter4_working_pydantic_objects_02.py\\nperson_nested_include = person.dict(\\n    include={\\n        \"first_name\": ...,\\n        \"last_name\": ...,\\n        \"address\": {\"city\", \"country\"},\\n    }\\n)\\n# {\"first_name\": \"John\", \"last_name\": \"Doe\", \"address\": \\n{\"city\": \"Woodtown\", \"country\": \"US\"}}\\nprint(person_nested_include)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_02.py\\nThe resulting address dictionary only contains the city and the country. Notice that \\nwhen using this syntax, scalar fields such as first_name or last_name have to be \\nassociated with the ellipsis ....\\nIf you use a conversion quite often, it can be interesting to put it in a method so that you \\ncan reuse it at will, as illustrated in the following example:\\nchapter4_working_pydantic_objects_03.py\\nclass Person(BaseModel):\\n    first_name: str\\n    last_name: str\\n    gender: Gender\\n    birthdate: date\\n    interests: List[str]\\n    address: Address'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 153, 'page_label': '135'}, page_content='Working with Pydantic objects     135\\n    def name_dict(self):\\n        return self.dict(include={\"first_name\", \"last_name\"})\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_03.py\\nCreating an instance from a sub-class object\\nIn the earlier section, Creating model variations with class inheritance, we studied \\nthe common pattern of having specific model classes depending on the situation. In \\nparticular, you\\'ll have a model dedicated for the creation endpoint, with only the required \\nfields for creation, and a database model with all the fields we want to store.\\nLet\\'s take again the Post example, as follows:\\nchapter4_working_pydantic_objects_04.py\\nclass PostBase(BaseModel):\\n    title: str\\n    content: str\\nclass PostCreate(PostBase):\\n    pass\\nclass PostPublic(PostBase):\\n    id: int\\nclass PostDB(PostBase):\\n    id: int\\n    nb_views: int = 0\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_04.py\\nIn our path operation function for our create endpoint, we\\'ll thus get a PostCreate \\ninstance with only title and content. However, we need to build a proper PostDB \\ninstance before storing it in the database.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 154, 'page_label': '136'}, page_content='136     Managing Pydantic Data Models in FastAPI\\nA convenient way to do this is to jointly use the dict method and the unpacking syntax. \\nIn the following example, we implemented a creation endpoint using this approach:\\nchapter4_working_pydantic_objects_04.py\\n@app.post(\"/posts\", status_code=status.HTTP_201_CREATED, \\nresponse_model=PostPublic)\\nasync def create(post_create: PostCreate):\\n    new_id = max(db.posts.keys() or (0,)) + 1\\n    post = PostDB(id=new_id, **post_create.dict())\\n    db.posts[new_id] = post\\n    return post\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_04.py\\nAs you see, the path operation function would give us a valid PostCreate object. Then, \\nwe want to transform it into a PostDB object.\\nWe first determine the missing id property, which is given to us by the database. Here, we \\nuse a dummy database based on a dictionary, so we simply take the maximum key already \\npresent in the database and increment it. In a real-world situation, this would have been \\nautomatically determined by the database.\\nThe most interesting line here is the PostDB instantiation. Y ou see that we first assign the \\nmissing fields by the keyword argument and then unpack the dictionary representation \\nof post_create. As a reminder, the effect of ** in a function call is to transform \\na dictionary such as {\"title\": \"Foo\", \"content\": \"Bar\"} into keyword \\narguments such as this: title=\"Foo\", content=\"Bar\". It\\'s a very convenient and \\ndynamic approach to set all the fields we already have into our new model.\\nNotice also that we set the response_model argument on the path operation \\ndecorator. We already explained this in Chapter 3, Developing a RESTful API with \\nFastAPI, but basically, it prompts FastAPI to build a JSON response with only the fields of \\nPostPublic, even though we return a PostDB instance at the end of the function.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 155, 'page_label': '137'}, page_content='Working with Pydantic objects     137\\nUpdating an instance with a partial one\\nIn some situations, you\\'ll want to allow partial updates. In other words, you\\'ll allow the \\nend user to only send the fields they want to change to your API and omit the ones that \\nshouldn\\'t change. This is the usual way of implementing a PATCH endpoint.\\nTo do this, you would first need a special Pydantic model with all the fields marked as \\noptional so that no error is raised when a field is missing. Let\\'s see what this looks like \\nwith our Post example, as follows:\\nchapter4_working_pydantic_objects_05.py\\nclass PostBase(BaseModel):\\n    title: str\\n    content: str\\nclass PostPartialUpdate(BaseModel):\\n    title: Optional[str] = None\\n    content: Optional[str] = None\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_05.py\\nWe are now able to implement an endpoint that will accept a subset of our Post fields. \\nSince it\\'s an update, we\\'ll retrieve an existing post in the database thanks to its ID. Then, \\nwe\\'ll have to find a way to only update the fields in the payload and keep the others \\nuntouched. Fortunately, Pydantic once again has this covered, with handy methods  \\nand options.\\nLet\\'s see how the implementation of such an endpoint could look in the following example:\\nchapter4_working_pydantic_objects_05.py\\n@app.patch(\"/posts/{id}\", response_model=PostPublic)\\nasync def partial_update(id: int, post_update: \\nPostPartialUpdate):\\n    try:\\n        post_db = db.posts[id]\\n        updated_fields = post_update.dict(exclude_unset=True)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 156, 'page_label': '138'}, page_content=\"138     Managing Pydantic Data Models in FastAPI\\n        updated_post = post_db.copy(update=updated_fields)\\n        db.posts[id] = updated_post\\n        return updated_post\\n    except KeyError:\\n        raise HTTPException(status.HTTP_404_NOT_FOUND)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter4/chapter4_working_\\npydantic_objects_05.py\\nOur path operation function takes two arguments: the id property (from the path), and a \\nPostPartialUpdate instance (from the body).\\nThe first thing to do is to check if this id property exists in the database. Since we use  \\na dictionary for our dummy database, accessing a non-existing key will raise a  \\nKeyError error. If this happens, we simply raise an HTTPException exception with \\nthe 404 status code.\\nNow for the interesting part: updating the existing object. Y ou see that the first thing we \\ndo is transform PostPartialUpdate into a dictionary with the dict method. This \\ntime, however, we set the exclude_unset argument to True. The effect of this is that \\nPydantic won't output the fields that were not provided in the resulting dictionary: we \\nonly get the fields that the user did send in the payload.\\nThen, on our existing post_db database instance, we call the copy method. This is a \\nuseful method to clone a Pydantic object into another instance. The nice thing about this \\nmethod is that it even accepts an update argument. This argument expects a dictionary \\nwith all the fields that should be updated during the copy: that's exactly what we want to \\ndo with our updated_fields dictionary!\\nAnd that's it! We now have an updated post instance with only the changes required in \\nthe payload. Y ou'll probably use the exclude_unset argument and the copy method \\nquite often while developing with FastAPI, so be sure to keep them in mind—they'll make \\nyour life easier!\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 157, 'page_label': '139'}, page_content=\"Summary     139\\nSummary\\nCongratulations! Y ou've learned another important aspect of FastAPI: designing and \\nmanaging data models with Pydantic. Y ou should now be confident about creating \\nmodels and applying validation at a field level, with built-in options and types, and also \\nby implementing your own validation methods. Y ou also know how to apply validation at \\nan object level to check consistency between several fields. Y ou also reviewed a common \\npattern, leveraging model inheritance to avoid code duplication and repetition while \\ndefining your model variations. Finally, you learned how to correctly work with Pydantic \\nmodel instances in order to transform and update them in an efficient and readable way.\\nY ou know almost all the features of FastAPI by now. There is a last very powerful one for \\nyou to learn: dependency injections. These will allow you to define your own logic and \\nvalues to directly inject into your path operation functions, as you do for path parameters \\nand payload objects, which you'll be able to reuse everywhere in your project. That's the \\nsubject of the next chapter.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 158, 'page_label': '140'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 159, 'page_label': '141'}, page_content=\"5\\nDependency \\nInjections in FastAPI\\nIn this chapter, we'll focus on one of the most interesting parts of FastAPI: dependency \\ninjections. Y ou'll see that it is a powerful and readable approach to reuse logic across \\nyour project. Indeed, it will allow you to create complex building blocks for your project \\nthat you'll be able to use everywhere in your logic. An authentication system, a query \\nparameters' validator, or a rate-limiter are typical use cases for dependencies. In FastAPI,  \\na dependency injection can even call another one recursively, allowing you to build  \\nhigh-level blocks from basic features. By the end of this chapter, you'll be able to create \\nyour own dependencies for FastAPI and use them at several levels of your project.\\nIn this chapter, we're going to cover the following main topics:\\n• What is dependency injection?\\n• Creating and using a function dependency\\n• Creating and using a parameterized dependency with a class\\n• Using dependencies at a path, router, and global level\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 160, 'page_label': '142'}, page_content='142     Dependency Injections in FastAPI\\nTechnical requirements\\nY ou\\'ll need a Python virtual environment, as we set up in Chapter 1, Python Development \\nEnvironment Setup.\\nY ou\\'ll find all the code examples of this chapter in the dedicated GitHub repository: \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter5.\\nWhat is dependency injection?\\nGenerally speaking, dependency injection is a system able to automatically instantiate \\nobjects and the ones they depend on. The responsibility of developers is then to only \\nprovide a declaration of how an object should be created, and let the system resolve all  \\nthe dependency chains and create the actual objects at runtime.\\nFastAPI allows you to declare the objects and variables you wish to have at hand only \\nby declaring them in the path operation function arguments. Actually, we already used \\ndependency injection in the previous chapters. In the following example, we use the \\nHeader function to retrieve the user-agent header:\\nchapter5_what_is_dependency_injection_01.py\\nfrom fastapi import FastAPI, Header\\napp = FastAPI()\\n@app.get(\"/\")\\nasync def header(user_agent: str = Header(...)):\\n    return {\"user_agent\": user_agent}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_what_is_\\ndependency_injection_01.py\\nInternally, the Header function has some logic to automatically get the request object, \\ncheck for the required header, return its value, or raise an error if it\\'s not present. From the \\ndeveloper\\'s perspective, however, we don\\'t know how it handled the required objects for \\nthis operation: we just ask for the value we need. That\\'s dependency injection.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 161, 'page_label': '143'}, page_content=\"Creating and using a function dependency     143\\nAdmittedly, you could reproduce this example quite easily in the function body by picking \\nthe user-agent property in the headers dictionary of the Request object. However, \\nthe dependency injection approach has numerous advantages over this:\\n• The intent is clear: you know what the endpoint expects in the request data without \\nreading the function's code.\\n• Y ou have a clear separation of concern between the logic of the endpoint and the more \\ngeneric logic: the header retrieval and the associated error handling doesn't pollute \\nthe rest of the logic; it's self-contained in the dependency function. Besides, it can \\nbe reused easily in other endpoints.\\n• In the case of FastAPI, it's used to generate the OpenAPI schema so that the automatic \\ndocumentation can clearly show which parameters are expected for this endpoint.\\nPut another way, whenever you need utility logic to retrieve or validate data, make \\nsecurity checks or call external logic that you'll need several times across your application, \\na dependency is an ideal choice.\\nFastAPI relies heavily on this dependency injection system and encourages developers to \\nuse it to implement their building blocks. It may be a bit puzzling if you come from other \\nweb frameworks such as Flask or Express, but you'll surely be quickly convinced by its \\npower and relevance.\\nTo convince you, we'll now see how you can create and use your very own dependency, in \\nthe form of a function to begin with.\\nCreating and using a function dependency\\nIn FastAPI, a dependency can be defined either as a function or as a callable class. In this \\nsection, we'll focus on the functions, which are the ones you'll probably work with most of \\nthe time.\\nAs we said, a dependency is a way to wrap some logic that will retrieve some sub-values \\nor sub-objects, make something with them, and finally return a value that will be injected \\ninto the endpoint calling it.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 162, 'page_label': '144'}, page_content='144     Dependency Injections in FastAPI\\nLet\\'s look at a first example where we define a function dependency to retrieve the \\npagination query parameters, skip and limit:\\nchapter5_function_dependency_01.py\\nasync def pagination(skip: int = 0, limit: int = 10) -> \\nTuple[int, int]:\\n    return (skip, limit)\\n@app.get(\"/items\")\\nasync def list_items(p: Tuple[int, int] = Depends(pagination)):\\n    skip, limit = p\\n    return {\"skip\": skip, \"limit\": limit}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_\\nfunction_dependency_01.py\\nThere are two parts of this example:\\n• First, we have the dependency definition, with the pagination function. Y ou see \\nthat we define two arguments, skip and limit, which are integers with default \\nvalues. Those will be the query parameters on our endpoint. We define them exactly \\nlike we would have done on a path operation function. That\\'s the beauty of this \\napproach: FastAPI will recursively handle the arguments on the dependency and \\nmatch them with the request data, such as query parameters or headers, if needed.\\nWe simply return those values as a tuple.\\n• Secondly, we have the path operation function, list_items, that uses the \\npagination dependency. Y ou see here that the usage is quite similar to what we \\nhave done for header or body values: we define the name of our resulting argument \\nand we use a function result as a default value. In the case of a dependency, we use \\nthe Depends function. Its role is to take a function in the argument and execute \\nit when the endpoint is called. The sub-dependencies are automatically discovered \\nand executed.\\nIn the endpoint, we have the pagination directly in the form of a tuple.\\nLet\\'s run this example with the following command:\\n$ uvicorn chapter5.chapter5_function_dependency_01:app'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 163, 'page_label': '145'}, page_content='Creating and using a function dependency     145\\nNow, we\\'ll try to call the /items endpoint and see whether it\\'s able to retrieve the query \\nparameters. Y ou can try this with the following HTTPie command:\\n$ http \"http://localhost:8000/items?limit=5&skip=10\"\\nHTTP/1.1 200 OK\\ncontent-length: 21\\ncontent-type: application/json\\ndate: Sat, 29 May 2021 16:03:36 GMT\\nserver: uvicorn\\n \\n{\\n    \"limit\": 5,\\n    \"skip\": 10\\n}\\nThe limit and skip query parameters have correctly been retrieved thanks to our \\nfunction dependency. Y ou can also try to call the endpoint without the query parameter \\nand notice that it will return you the default values.\\nType hint of a dependency return value\\nY ou may have noticed that we had to type hint the result of our dependency \\nin the path operation arguments, even though we already type hinted the \\ndependency function itself. Unfortunately, this is a limitation of FastAPI and \\nits Depends function, which isn\\'t able to forward the type of the dependency \\nfunction. Therefore, we have to type hint the result by hand, as we did here.\\nAnd that\\'s it! As you see, it\\'s very simple and straightforward to create and use a \\ndependency in FastAPI. Of course, you can now reuse it at will in several endpoints,  \\nas you can see in the rest of our examples:\\nchapter5_function_dependency_01.py\\n@app.get(\"/items\")\\nasync def list_items(p: Tuple[int, int] = Depends(pagination)):\\n    skip, limit = p\\n    return {\"skip\": skip, \"limit\": limit}\\n@app.get(\"/things\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 164, 'page_label': '146'}, page_content='146     Dependency Injections in FastAPI\\nasync def list_things(p: Tuple[int, int] = \\nDepends(pagination)):\\n    skip, limit = p\\n    return {\"skip\": skip, \"limit\": limit}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_\\nfunction_dependency_01.py\\nOf course, we can do more complex things in those dependencies, just like we would in \\na regular path operation function. In the following example, we add some validation to \\nthose pagination parameters and cap the limit at 100:\\nchapter5_function_dependency_02.py\\nasync def pagination(\\n    skip: int = Query(0, ge=0),\\n    limit: int = Query(10, ge=0),\\n) -> Tuple[int, int]:\\n    capped_limit = min(100, limit)\\n    return (skip, capped_limit)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_\\nfunction_dependency_02.py\\nAs you can see, our dependency starts to become more complex:\\n• We added the Query function to our arguments to add a validation constraint: \\nnow, an error 422 will be raised if skip or limit are negative integers.\\n• We ensure that the limit is, at most, 100.\\nThe code on our path operation functions doesn\\'t have to change: we have a clear \\nseparation of concern between the logic of the endpoint and the more generic logic for the \\npagination parameters.\\nLet\\'s see another typical use of dependencies: get an object or raise a 404 error.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 165, 'page_label': '147'}, page_content='Creating and using a function dependency     147\\nGet an object or raise a 404 error\\nIn a REST API, you\\'ll typically have endpoints to get, update, and delete a single object \\ngiven its identifier in the path. On each one, you\\'ll likely have the same logic: try to \\nretrieve this object in the database or raise an error 404 if it doesn\\'t exist. That\\'s a perfect \\nuse case for a dependency! In the following example, you\\'ll see how to implement it:\\nchapter5_function_dependency_03.py\\nasync def get_post_or_404(id: int) -> Post:\\n    try:\\n        return db.posts[id]\\n    except KeyError:\\n        raise HTTPException(status_code=status.HTTP_404_NOT_\\nFOUND)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_\\nfunction_dependency_03.py\\nThe dependency definition is simple: it takes in an argument the ID of the post we want to \\nretrieve. It will be pulled from the corresponding path parameter. Then, we check whether \\nit exists in our dummy dictionary database: if it does, we return it, otherwise, we raise an \\nHTTPException with the status code 404.\\nThat\\'s the key takeaway of this example: you can raise errors in your dependencies. It\\'s \\nextremely useful to check for some pre-conditions before your endpoint logic is executed. \\nAnother typical example for this is authentication: if the endpoint requires a user to be \\nauthenticated, we can raise a 401 error in the dependency by checking for the token or \\nthe cookie.\\nNow, we can use this dependency in each of our API endpoints, as you can see in the \\nfollowing example:\\nchapter5_function_dependency_03.py\\n@app.get(\"/posts/{id}\")\\nasync def get(post: Post = Depends(get_post_or_404)):\\n    return post\\n@app.patch(\"/posts/{id}\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 166, 'page_label': '148'}, page_content='148     Dependency Injections in FastAPI\\nasync def update(post_update: PostUpdate, post: Post = \\nDepends(get_post_or_404)):\\n    updated_post = post.copy(update=post_update.dict())\\n    db.posts[post.id] = updated_post\\n    return updated_post\\n@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_\\nCONTENT)\\nasync def delete(post: Post = Depends(get_post_or_404)):\\n    db.posts.pop(post.id)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_\\nfunction_dependency_03.py\\nAs you can see, we just had to define the post argument and use the Depends function \\non our get_post_or_404 dependency. Then, within the path operation logic, we are \\nguaranteed to have our post object at hand and we can focus on our core logic, which is \\nnow very concise. The get endpoint, for example, just has to return the object.\\nIn this case, the only point of attention is to not forget the ID parameter in the path of \\nthose endpoints. According to the rules of FastAPI, if you don\\'t set this parameter in the \\npath, it will automatically be regarded as a query parameter, which is not what we want \\nhere. Y ou can find more details about this in the Path parameters section of Chapter 3, \\nDeveloping a RESTful API with FastAPI.\\nThat\\'s all for the function dependencies. As we said, those are the main building blocks in \\na FastAPI project. In some cases, however, you\\'ll need to have some parameters on those \\ndependencies, for example, with values coming from environment variables. For this, we \\ncan define class dependencies.\\nCreating and using a parameterized \\ndependency with a class\\nIn the previous section, we defined dependencies as regular functions, which works well \\nin most cases. Still, you may need to set some parameters on a dependency to finely tune \\nits behavior. Since the arguments of the function are set by the dependency injection \\nsystem, we can\\'t add an argument to the function.\\nIn the pagination example, we added some logic to cap the limit value at 100.  \\nIf we wanted to set this maximum limit dynamically, how would we do that?'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 167, 'page_label': '149'}, page_content='Creating and using a parameterized dependency with a class     149\\nThe solution is to create a class that will be used as a dependency. This way, we can set class \\nproperties, with the __init__ method, for example, and use them in the logic of the \\ndependency itself. This logic will be defined in the __call__ method of the class. If you \\nremember what we learned in the Callable object section of Chapter 2, Python Programming \\nSpecificities, you know that it makes the object callable, meaning it can be called like a \\nregular function. Actually, that is all that Depends requires for a dependency: being a \\ncallable. We\\'ll use this property to create a parameterized dependency thanks to a class.\\nIn the following example, we reimplemented the pagination example with a class, allowing \\nus to set the maximum limit dynamically:\\nchapter5_class_dependency_01.py\\nclass Pagination:\\n    def __init__(self, maximum_limit: int = 100):\\n        self.maximum_limit = maximum_limit\\n    async def __call__(\\n        self,\\n        skip: int = Query(0, ge=0),\\n        limit: int = Query(10, ge=0),\\n    ) -> Tuple[int, int]:\\n        capped_limit = min(self.maximum_limit, limit)\\n        return (skip, capped_limit)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_class_\\ndependency_01.py\\nAs you can see, the logic in the __call__ method is the same as in the function \\nwe defined in the previous example. The only difference here is that we can pull our \\nmaximum limit from our class properties that we can set at the object initialization.\\nThen, you can simply create an instance of this class and use it as a dependency with \\nDepends on your path operation function, as you can see in the following code block:\\nchapter5_class_dependency_01.py\\npagination = Pagination(maximum_limit=50)\\n@app.get(\"/items\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 168, 'page_label': '150'}, page_content='150     Dependency Injections in FastAPI\\nasync def list_items(p: Tuple[int, int] = Depends(pagination)):\\n    skip, limit = p\\n    return {\"skip\": skip, \"limit\": limit}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_class_\\ndependency_01.py\\nHere, we hardcoded the value 50, but we could very well pull it from a configuration file \\nor an environment variable.\\nThe other advantage of a class dependency is that it can maintain local values in memory. \\nThis property can be very useful if we have to make some heavy initialization logic, such \\nas loading a machine learning model, for example, that we want to do only once at startup. \\nThen, the callable part just has to call the loaded model to make the prediction, which \\nshould be quite fast.\\nUse class methods as dependencies\\nEven if the __call__ method is the most straightforward way to make a class \\ndependency, you can directly pass a method to Depends. Indeed, as we said, it simply \\nexpects a callable as an argument, and a class method is a perfectly valid callable!\\nThis approach can be very useful if you have common parameters or logic that you need \\nto reuse in slightly different cases. For example, you could have one pre-trained machine \\nlearning model made with Scikit-learn. Before applying the decision function, you may \\nwant to apply different pre-process steps depending on the input data.\\nTo do this, simply write your logic in a class method and pass it to the Depends function \\nthrough the dot notation.\\nY ou can see this in the following example, where we implement another style for our \\npagination dependency, with page and size parameters instead of skip and limit:\\nchapter5_class_dependency_02.py\\nclass Pagination:\\n    def __init__(self, maximum_limit: int = 100):\\n        self.maximum_limit = maximum_limit\\n    async def skip_limit(\\n        self,\\n        skip: int = Query(0, ge=0),'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 169, 'page_label': '151'}, page_content='Creating and using a parameterized dependency with a class     151\\n        limit: int = Query(10, ge=0),\\n    ) -> Tuple[int, int]:\\n        capped_limit = min(self.maximum_limit, limit)\\n        return (skip, capped_limit)\\n    async def page_size(\\n        self,\\n        page: int = Query(1, ge=1),\\n        size: int = Query(10, ge=0),\\n    ) -> Tuple[int, int]:\\n        capped_size = min(self.maximum_limit, size)\\n        return (page, capped_size)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_class_\\ndependency_02.py\\nThe logic of the two methods is quite similar. We just look at different query parameters. \\nThen, on our path operation functions, we set the /items endpoint to work with the \\nskip/limit style, while the /things endpoint will work with the page/size style:\\nchapter5_class_dependency_02.py\\npagination = Pagination(maximum_limit=50)\\n@app.get(«/items»)\\nasync def list_items(p: Tuple[int, int] = Depends(pagination.\\nskip_limit)):\\n    skip, limit = p\\n    return {\"skip\": skip, \"limit\": limit}\\n@app.get(\"/things\")\\nasync def list_things(p: Tuple[int, int] = Depends(pagination.\\npage_size)):\\n    page, size = p\\n    return {\"page\": page, \"size\": size}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_class_\\ndependency_02.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 170, 'page_label': '152'}, page_content='152     Dependency Injections in FastAPI\\nAs you see, we only have to pass the method we wish through the dot notation on the \\npagination object.\\nTo sum up, the class dependency approach is more advanced than the function approach \\nbut can be very useful for cases when you need to set parameters dynamically, perform \\nheavy initialization logic, or reuse common logic on several dependencies.\\nUntil now, we\\'ve assumed that we care about the return value of the dependency. While \\nthis will probably be the case most of the time, you may occasionally need to call a \\ndependency to check for some conditions, but don\\'t really need the returned value. \\nFastAPI allows such use cases, and that\\'s what we\\'ll see now.\\nUsing dependencies at a path, router, and \\nglobal level\\nAs we said, dependencies are the recommended way to create building blocks in a FastAPI \\nproject, allowing you to reuse logic across endpoints while maintaining maximum code \\nreadability. Until now, we\\'ve applied them on a single endpoint, but couldn\\'t we expand \\nthis approach to a whole router? Or even a whole FastAPI application? Actually, we can!\\nThe main motivation for this is to be able to apply some global request validation or \\nperform side logic on several routes without the need to add the dependency on each \\nendpoint. Typically, an authentication method or a rate-limiter could be very good \\ncandidates for this use case.\\nTo show you how it works, we\\'ll implement a simple dependency that we will use across \\nall the following examples. Y ou can see it in the following example:\\nchapter5_path_dependency_01.py\\ndef secret_header(secret_header: Optional[str] = Header(None)) \\n-> None:\\n    if not secret_header or secret_header != \"SECRET_VALUE\":\\n        raise HTTPException(status.HTTP_403_FORBIDDEN)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_path_\\ndependency_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 171, 'page_label': '153'}, page_content='Using dependencies at a path, router, and global level     153\\nThis dependency will simply look for a header in the request named Secret-Header. \\nIf it\\'s missing or not equal to SECRET_VALUE, it will raise a 403 error. Please note that \\nthis approach is only for the sake of the example; there are better ways to secure your API, \\nwhich we\\'ll cover in Chapter 7, Managing Authentication and Security in FastAPI.\\nUse a dependency on a path decorator\\nUntil now, we\\'ve assumed that we were always interested in the return value of the \\ndependency. As our secret_header dependency clearly shows here, this is not always \\nthe case. This is why you can add a dependency on a path operation decorator instead of \\nthe arguments. Y ou can see how in the following example:\\nchapter5_path_dependency_01.py\\n@app.get(\"/protected-route\", dependencies=[Depends(secret_\\nheader)])\\nasync def protected_route():\\n    return {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_path_\\ndependency_01.py\\nThe path operation decorator accepts an argument, dependencies, which expects a list \\nof dependencies. Y ou see that, just like for dependencies you pass in arguments, you need \\nto wrap your function (or callable) with the Depends function.\\nNow, whenever the /protected-route route is called, the dependency will be called \\nand will check for the required header.\\nAs you may have guessed, since dependencies is a list, you can add as many \\ndependencies as you need.\\nThat\\'s interesting, but what if we want to protect a whole set of endpoints? It would be a \\nbit cumbersome and error-prone to add it manually on each one. Fortunately, FastAPI \\nprovides a way to do that.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 172, 'page_label': '154'}, page_content='154     Dependency Injections in FastAPI\\nUse a dependency on a whole router\\nIf you recall the Structure a bigger project with multiple routers section in Chapter 3, \\nDeveloping a RESTful API with FastAPI, you know that you can create several routers in \\nyour project to clearly split the different parts of your API and \"wire\" them to your main \\nFastAPI application. This is done with the APIRouter class and the include_router \\nmethod of the FastAPI class.\\nWith this approach, it can be interesting to inject a dependency on the whole router, so \\nthat it\\'s called for every route of this router. Y ou have two ways of doing this:\\n• Set the dependencies argument on the APIRouter class, as you can see in the \\nfollowing example:\\nchapter5_router_dependency_01.py\\nrouter = APIRouter(dependencies=[Depends(secret_header)])\\n@router.get(\"/route1\")\\nasync def router_route1():\\n    return {\"route\": \"route1\"}\\n@router.get(\"/route2\")\\nasync def router_route2():\\n    return {\"route\": \"route2\"}\\napp = FastAPI()\\napp.include_router(router, prefix=\"/router\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_router_\\ndependency_01.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 173, 'page_label': '155'}, page_content='Using dependencies at a path, router, and global level     155\\n• Set the dependencies argument on the include_router method, as you can \\nsee in the following example:\\nchapter5_router_dependency_02.py\\nrouter = APIRouter()\\n@router.get(\"/route1\")\\nasync def router_route1():\\n    return {\"route\": \"route1\"}\\n@router.get(\"/route2\")\\nasync def router_route2():\\n    return {\"route\": \"route2\"}\\napp = FastAPI()\\napp.include_router(router, prefix=\"/router\", \\ndependencies=[Depends(secret_header)])\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_router_\\ndependency_02.py\\nIn both cases, the dependencies argument expects a list of dependencies. Y ou see \\nthat, just like for dependencies you pass in arguments, you need to wrap your function \\n(or callable) with the Depends function. Of course, since it\\'s a list, you can add several \\ndependencies if you need.\\nNow, how to choose between the two approaches? In both cases, the effect will be exactly \\nthe same, so we could say it doesn\\'t really matter. Philosophically, we could say that we \\nshould declare a dependency on the APIRouter class if it\\'s needed in the context of this \\nrouter. Put another way, we could ask ourselves the question, Does this router work without \\nthis dependency if we run it independently? If the answer to this question is no, then you \\nshould probably set the dependency on the APIRouter class. Otherwise, declaring it in \\nthe include_router method may make more sense. But again, this is an intellectual \\nchoice that won\\'t change the functionality of your API, so feel free to choose the one \\nyou\\'re more comfortable with.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 174, 'page_label': '156'}, page_content='156     Dependency Injections in FastAPI\\nWe are now able to set dependencies for a whole router. In some cases, it could also be \\ninteresting to declare them for a whole application!\\nUse a dependency on a whole application \\nIf you have a dependency that implements some logging or rate-limiting functionality, for \\nexample, it could be interesting to execute it for every endpoint of your API. Fortunately, \\nFastAPI allows this, as you can see in the following example:\\nchapter5_global_dependency_01.py\\napp = FastAPI(dependencies=[Depends(secret_header)])\\n@app.get(\"/route1\")\\nasync def route1():\\n    return {\"route\": \"route1\"}\\n@app.get(\"/route2\")\\nasync def route2():\\n    return {\"route\": \"route2\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter5/chapter5_global_\\ndependency_01.py\\nOnce again, you only have to set the dependencies argument directly on the main \\nFastAPI class. Now, the dependency is applied to every endpoint in your API!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 175, 'page_label': '157'}, page_content='Using dependencies at a path, router, and global level     157\\nIn Figure 5.1, we propose a simple decision tree to determine at which level you should \\ninject your dependency:\\nFigure 5.1 – At which level should I inject my dependency?'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 176, 'page_label': '158'}, page_content=\"158     Dependency Injections in FastAPI\\nSummary\\nWell done! Y ou should now be comfortable with one of the most iconic features of FastAPI: \\ndependency injections. By implementing your own dependencies, you'll be able to keep \\ncommon logic that you wish to reuse across your API separate from the endpoints' logic. \\nThis will make your project clean and maintainable while retaining maximum readability: \\ndependencies just need to be declared as arguments of the path operation functions, which \\nwill help to understand the intent without having to read the body of the function.\\nThose dependencies can be both simple wrappers to retrieve and validate request \\nparameters, or complex services performing machine learning tasks. Thanks to the  \\nclass-based approach, you can indeed set dynamic parameters or keep a local state for \\nyour most advanced tasks.\\nFinally, those dependencies can also be used at a router or global level, allowing you to \\nperform common logic or checks for a set of routes or a whole application.\\nThat's the end of the first part of this book! Y ou're now acquainted with the main  \\nfeatures of FastAPI and should now be able to write clean and performant REST APIs  \\nwith the framework.\\nIn the next part, we'll take your knowledge to the next level and show you how you can \\nimplement and deploy a robust, secure, and tested web backend. The first chapter will be \\ndedicated to databases, a must-have for most APIs to be able to read and write data.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 177, 'page_label': '159'}, page_content='Section 2:  \\nBuild and Deploy \\na Complete Web \\nBackend with FastAPI\\nThe goal of this section is to show you how to build a real-world backend with FastAPI \\nthat can read and write data and authenticate users, and that is properly tested and \\ncorrectly configured for a production environment.\\xa0\\nThis section comprises the following chapters:\\n• \\xa0Chapter 6, Databases and Asynchronous ORMs \\n• \\xa0Chapter 7, Managing Authentication and Security in FastAPI\\n• \\xa0Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI \\n• Chapter 9, Testing an API Asynchronously with pytest and HTTPX \\n• \\xa0Chapter 10, Deploying a FastAPI Project'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 178, 'page_label': '160'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 179, 'page_label': '161'}, page_content=\"6\\nDatabases and \\nAsynchronous ORMs\\nThe main goal of a REST API is, of course, to read and write data. So far, we've solely \\nworked with the tools given by Python and FastAPI, allowing us to build reliable \\nendpoints to process and answer requests. However, we haven't been able to effectively \\nretrieve and persist that information: we missed a database. \\nThe goal of this chapter is to show you how you can interact with different types of \\ndatabases and related libraries inside FastAPI. It's worth noting that FastAPI is completely \\nagnostic regarding databases: you can use any system you want and it's your responsibility \\nto integrate it. This is why we'll review three different approaches to integrate a database, \\nthat is, using basic SQL queries, using Object-Relational Mapping (ORM), and, finally, \\nusing a NoSQL database.\\nIn this chapter, we're going to cover the following main topics:\\n• An overview of relational and NoSQL databases\\n• Communicating with a SQL database with SQLAlchemy\\n• Communicating with a SQL database with Tortoise ORM\\n• Communicating with a MongoDB database using Motor\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 180, 'page_label': '162'}, page_content=\"162     Databases and Asynchronous ORMs\\nTechnical requirements\\nFor this chapter, you'll require a Python virtual environment, just as we set up in Chapter \\n1, Python Development Environment Setup.\\nFor the Communicating with a MongoDB database using Motor section, you'll need  \\na running MongoDB server on your local computer. The easiest way to do this is to run  \\nit as a Docker container. If you've never used Docker before, we recommend that you refer \\nto the Getting started tutorial in the official documentation at https://docs.docker.\\ncom/get-started/. Once you have done this, you'll be able to run a MongoDB server \\nusing this simple command:\\n$ docker run -d --name fastapi-mongo -p 27017:27017 mongo:4.4\\nThe MongoDB server instance will then be available on your local computer at port \\n27017.\\nY ou can find all the code examples for this chapter in the dedicated GitHub repository \\nat https://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter6.\\nAn overview of relational and NoSQL \\ndatabases\\nThe role of a database is to store data in a structured way, preserve the integrity of the \\ndata, and offer a query language that enables you to retrieve this data when an application \\nneeds it.\\nNowadays, when it comes to choosing a database for your web project, you have two main \\nchoices: relational databases, with their associated SQL query language, and NoSQL \\ndatabases, named in opposition to the first category.\\nSelecting the right technology for your project is left up to you, as it greatly depends on \\nyour needs and requirements. In this section, we'll outline the main characteristics and \\nfeatures of those two database families and try to give you some insights into choosing the \\nright one for your project.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 181, 'page_label': '163'}, page_content=\"An overview of relational and NoSQL databases     163\\nRelational databases\\nRelational databases have existed since the 1970s, and they have proved to be very \\nperformant and reliable over time. They are almost inseparable from the SQL query \\nlanguage, which has become the de facto standard for querying such databases. Even  \\nif there are a few differences between one database engine and another, most of the syntax \\nis common, simple to understand, and flexible enough to express complex queries.\\nRelational databases implement the relational model: each entity, or object, of the \\napplication is stored in tables. For example, if we consider a blog application, we could \\nhave tables that represent users, posts, and comments.\\nEach of those tables has several columns representing the attributes of the entity.  \\nIf we consider the posts, we could have a title, a publication date, and content. In those \\ntables, there will be several rows, each one representing a single entity of this type; each \\npost will have its own row.\\nOne of the key points of relational databases is, as their name suggests, relationships. Each \\ntable can be in relation to others, with rows referring to other rows in other tables. In our \\nexample, a post could be related to the user who wrote it. In the same way, a comment \\ncould be linked to the post that it relates to.\\nThe main motivation behind this is to avoid duplication. Indeed, it wouldn't be very \\nefficient to repeat the user's name or email on each of its posts. If it needs to be modified \\nat some point, we would have to go through each post, which is error-prone and puts data \\nconsistency at risk. This is why we prefer to reference the user in the posts. So, how can  \\nwe do this?\\nUsually, each row in a relational database has an identifier, called a primary key. This is \\nunique in the table and will allow you to uniquely identify this row. Therefore, it's possible \\nto use this key in another table to reference it. We call it a foreign key: the key is foreign in \\nthe sense that it refers to another table.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 182, 'page_label': '164'}, page_content='164     Databases and Asynchronous ORMs\\nIn Figure 6.1, you can view a representation of such database schema using an  \\nentity-relationship diagram. Note that each table has its own primary key or id. The \\nPost table refers to a User, through the user_id foreign key. Similarly, the Comment \\ntable refers to both a post and a user through the user_id and post_id foreign keys:\\nFigure 6.1 – A relational database schema example for a blog application\\nIn an application, you\\'ll likely want to retrieve a post with the comments and the user \\nassociated. To do so, we perform a join query, which will return all the relevant records \\nbased on the foreign keys. Relational databases are designed to perform such tasks \\nefficiently; however, those operations can become expensive if the schema is more \\ncomplex. This is why it\\'s important to carefully design a relational schema and its queries.\\nNoSQL databases\\nAll database engines that are not relational fall back into the NoSQL category. In fact, this \\nis a quite vague denomination that regroups different families of databases: key-value \\nstores, such as Redis; graph databases, such as Neo4j; and document-oriented databases, \\nsuch as MongoDB. That said, most of the time when we talk about \"NoSQL databases\",  \\nwe are implicitly referring to document-oriented databases. They are the ones that interest \\nus in this chapter.\\nDocument-oriented databases move away from the relational architecture and try to store \\nall the information of a given object inside a single document. As such, performing a join \\nquery is much rarer and usually more difficult.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 183, 'page_label': '165'}, page_content=\"An overview of relational and NoSQL databases     165\\nThose documents are stored in collections. Contrary to relational databases, documents \\nin a collection might not have all of the same attributes: while tables in relational \\ndatabases have a defined schema, collections accept any kind of document.\\nIn Figure 6.2, you can view a representation of our previous blog example, which has \\nbeen adapted into a document-oriented database structure. In this configuration, we have \\nchosen to have a collection for users and another one for posts. However, notice that the \\ncomments are now part of a post, that is, they are included as a list:\\nFigure 6.2 — A document-oriented schema example for a blog application\\nTo retrieve a post and all of its comments, you don't need to perform a join query: all \\nthe data comes in one query. This was the main motivation behind the development of \\ndocument-oriented databases: increase the query performance by limiting the need to \\nlook at several collections. In particular, this has been adapted for applications with huge \\ndata scales and less structured data, such as social networks.\\nWhich one should you choose?\\nAs we mentioned in the introduction to this section, the choice of database engine \\ngreatly depends on your application and needs. A detailed comparison between relational \\nand document-oriented databases is beyond the scope of this book, but here are some \\nelements for you to think about.\\nRelational databases are very good for storing structured data with a lot of relationships \\nbetween the entities. Besides, they maintain data consistency at all costs, even in the event \\nof errors or hardware failures. However, you'll have to precisely define your schema and \\nconsider a migration system to update your schema if your needs evolve.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 184, 'page_label': '166'}, page_content=\"166     Databases and Asynchronous ORMs\\nOn the other hand, document-oriented databases don't require you to define a schema: \\nthey accept any document structure, so it can be convenient if your data is highly variable \\nor if your project is not mature enough. The downside of this is that they are far less picky \\nin terms of data consistency, which could result in data loss or inconsistencies.\\nFor small and medium-sized applications, the choice doesn't really matter: both relational \\ndatabases and document-oriented databases are very optimized and will deliver awesome \\nperformance at such scales.\\nNow, we'll show you how to work with those different kinds of databases using FastAPI. \\nWhen we introduced asynchronous I/O in Chapter 2, Python Programming Specificities, \\nwe mentioned that it was important to carefully select the libraries you use to perform  \\nI/O operations. Of course, databases are particularly important in this context!\\nWhile working with classic non-async libraries is perfectly possible in FastAPI, you \\ncould miss out one of the key aspects of the framework and might not reach the best \\nperformance it can offer. That's why, in this chapter, we'll only focus on async libraries.\\nCommunicating with a SQL database with \\nSQLAlchemy\\nTo begin, we'll discuss how to work with a relational database using the SQLAlchemy \\nlibrary. SQLAlchemy has been around for years and is the most popular library in Python \\nwhen you wish to work with SQL databases.\\nIn this chapter, it's worth noting that we'll only consider the core part of the library, which \\nonly provides the tools to abstract communication with a SQL database. We won't \\nconsider the ORM part, as, in the next section, we'll focus on another ORM: Tortoise.  \\nAs such, in this section, we'll pay very close attention to the SQL language.\\nRecently, async support has been added in version 1.4 but is not yet considered stable. \\nThat's why, for now, we'll combine it with the databases library by Encode, the same team \\nbehind Starlette, which provides an asynchronous connection layer for SQLAlchemy.  \\nIn Figure 6.3, we have presented a schema for you to better visualize the interaction \\nbetween the different libraries:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 185, 'page_label': '167'}, page_content=\"Communicating with a SQL database with SQLAlchemy     167\\nFigure 6.3 – The interaction between SQLAlchemy Core and the Encode databases\\nThe first step is to install this library:\\n$ pip install databases[sqlite]\\nThis will install the databases library, SQLAlchemy, and the required drivers to work \\nwith SQLite databases. SQLite is a very convenient relational engine that stores all of the \\ndata inside a single file on your computer, which is perfect for testing and experimenting. \\nUnlike PostgreSQL or MySQL, you don't need to install and run a complex server.\\n Each type of SQL server will require its own driver, which provides specific instructions \\non which to communicate with them. Of course, the ones for PostgreSQL and MySQL are \\nprovided by databases, which will be useful when building a real-world project. Y ou \\ncan check the installation instructions in the official documentation at https://www.\\nencode.io/databases/.\\nNow, we'll show you, step by step, how to set up a complete database interaction. Figure \\n6.4 shows you the structure of the project:\\nFigure 6.4 – The FastAPI and SQLAlchemy project structure\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 186, 'page_label': '168'}, page_content='168     Databases and Asynchronous ORMs\\nCreating the table schema\\nFirst, you need to define the SQL schema for your tables: the name, the columns, and their \\nassociated types and properties. SQLAlchemy provides a full set of classes and functions \\nto help you in this task. In the following example, you can view the definition of the \\nposts table:\\nmodels.py\\nmetadata = sqlalchemy.MetaData()\\nposts = sqlalchemy.Table(\\n    \"posts\",\\n    metadata,\\n    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_\\nkey=True, autoincrement=True),\\n    sqlalchemy.Column(\"publication_date\", sqlalchemy.\\nDateTime(), nullable=False),\\n    sqlalchemy.Column(\"title\", sqlalchemy.String(length=255), \\nnullable=False),\\n    sqlalchemy.Column(\"content\", sqlalchemy.Text(), \\nnullable=False),\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/\\nmodels.py\\nFirst, let\\'s create a metadata object. Its role is to keep all the information of a database \\nschema together. This is why you should create it only once in your whole project and \\nalways use the same one throughout.\\nNext, we will define a table using the Table class. The first argument is the name of the \\ntable, followed by the metadata object. Then, we list all of the columns that should be \\ndefined in our table, thanks to the Column class. The first argument is the name of the \\ncolumn, followed by its type and a certain number of options. For example, we define  \\nour id column as a primary key with auto-increment, which is quite common in  \\na SQL database.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 187, 'page_label': '169'}, page_content='Communicating with a SQL database with SQLAlchemy     169\\nNote that we won\\'t go through all the types and options provided by SQLAlchemy.  \\nJust know that they closely follow the ones that are usually provided by SQL databases. \\nY ou can check the complete list in the official documentation, as follows:\\n• Y ou can find the list of types at https://docs.sqlalchemy.org/en/13/\\ncore/type_basics.html#generic-types\\n• Y ou can find the list of Column arguments at https://docs.sqlalchemy.\\norg/en/13/core/metadata.html#:~:text=sqlalchemy.schema.\\nColumn.__init__\\nIf you take a look at the code above the table definition, you\\'ll see that we also defined the \\ncorresponding Pydantic models for our post entity. Since they will be used by FastAPI to \\nvalidate the request payload, they must match the SQL definition to avoid any errors from \\nthe database when we try to insert a new row later.\\nConnecting to a database\\nNow that our table is ready, we have to set up the connection between our FastAPI app \\nand the database engine. To begin, we\\'ll instantiate several objects, as shown in the \\nfollowing example:\\ndatabase.py\\nDATABASE_URL = \"sqlite:///chapter6_sqlalchemy.db\"\\ndatabase = Database(DATABASE_URL)\\nsqlalchemy_engine = sqlalchemy.create_engine(DATABASE_URL)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/\\ndatabase.py\\nHere, you can see that we have set our connection string inside the DATABASE_URL \\nvariable. Generally, it consists of the database engine, followed by authentication \\ninformation and the hostname of the database server. Y ou can find an overview of this \\nformat in the official SQLAlchemy documentation at https://docs.sqlalchemy.\\norg/en/13/core/engines.html#database-urls. In the case of SQLite,  \\nwe simply have to give the path of the file that will store all of the data.\\nThen, we instantiate a Database instance using this URL. This is the connection layer \\nprovided by databases that will allow us to perform asynchronous queries.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 188, 'page_label': '170'}, page_content='170     Databases and Asynchronous ORMs\\nWe also define sqlalchemy_engine, which is the standard synchronous connection \\nobject provided by SQLAlchemy. Y ou might think that it constitutes an overlap with \\ndatabase, and you would be absolutely right. We\\'ll clarify why we need it in our \\nexample later.\\nThen, we define a simple function whose role is to simply return the database instance. \\nThis is shown in the following example:\\ndatabase.py\\ndef get_database() -> Database:\\n    return database\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/\\ndatabase.py\\nWe\\'ll use this function as a dependency to easily retrieve this instance in our path \\noperation functions.\\nUsing a dependency to retrieve a database instance\\nY ou might be wondering why we don\\'t just import the database instance into \\nour app and use it directly rather than passing it through a dependency. In fact, \\nit would totally work. However, it would make our life very hard when trying to \\nimplement unit tests. Indeed, it would be very difficult to replace this instance \\nwith a mock or test database. With a dependency, FastAPI makes it very easy \\nto swap it with another function. We\\'ll view this in more detail in Chapter 9, \\nTesting an API Asynchronously with pytest and HTTPX.\\nNow, we need to tell FastAPI to open the connection with the database when it starts  \\nthe application and then close it when exiting. Fortunately, FastAPI provides two  \\nspecial decorators to perform tasks at startup and shutdown, as you can see in the \\nfollowing example:\\napp.py\\n@app.on_event(\"startup\")\\nasync def startup():\\n    await database.connect()\\n    metadata.create_all(sqlalchemy_engine)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 189, 'page_label': '171'}, page_content='Communicating with a SQL database with SQLAlchemy     171\\n@app.on_event(\"shutdown\")\\nasync def shutdown():\\n    await database.disconnect()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nDecorating functions with the on_event decorators allows us to trigger some useful \\nlogic when FastAPI starts or stops. In this case, we simply call the connect and \\ndisconnect methods of the database accordingly. This will ensure that the database \\nconnection is open and ready to process requests.\\nAdditionally, you can see that we call the create_all method on the metadata object. \\nThis is the same metadata object we defined in the previous section and that we have \\nimported here. The goal of this method is to create the table\\'s schema inside our database. \\nIf we don\\'t do that, our database would be empty and we wouldn\\'t be able to save  \\nor retrieve data. This method is designed to work with a standard SQLAlchemy engine; \\nthis is why we instantiated it earlier. It has no other use in the application.\\nHowever, we only created a schema like this to simplify our example. In a real-world \\napplication, you should have a proper migration system whose role is to make sure  \\nyour database schema is in sync. We\\'ll learn how to set one up for SQLAlchemy later in  \\nthe chapter.\\nMaking insert queries\\nNow we\\'re ready to make queries! Let\\'s start with the INSERT queries to create new rows \\nin our database. In the following example, you can view an implementation of an endpoint \\nto create a new post:\\napp.py\\n@app.post(\"/posts\", response_model=PostDB, status_code=status.\\nHTTP_201_CREATED)\\nasync def create_post(\\n    post: PostCreate, database: Database = Depends(get_\\ndatabase)\\n) -> PostDB:\\n    insert_query = posts.insert().values(post.dict())\\n    post_id = await database.execute(insert_query)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 190, 'page_label': '172'}, page_content=\"172     Databases and Asynchronous ORMs\\n    post_db = await get_post_or_404(post_id, database)\\n    return post_db\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nY ou shouldn't be surprised by the look of it: it's a POST endpoint that accepts a payload \\nfollowing the PostCreate model. It also injects the database thanks to our get_\\ndatabase dependency.\\nInteresting things begin in the body of the function:\\n• On the first line, we build our INSERT query. Rather than writing SQL queries by \\nhand, we rely on the SQLAlchemy expression language, which consists of chained \\nmethod calls. Under the hood, SQLAlchemy will build a proper SQL query for \\nour database engine. This is one of the greatest benefits of such libraries: since it \\nproduces the SQL query for you, you won't have to modify your source code if you \\nchange your database engine.\\n• This query is built directly from the posts object, which is the Table instance that \\nwe defined earlier. By using this object, SQLAlchemy directly understands that the \\nquery concerns this table and builds the SQL accordingly.\\n• We start by calling the insert method. Then, we move ahead with the values \\nmethod. This simply accepts a dictionary that associates the names of the columns \\nwith their values. Hence, we just need to call dict() on our Pydantic object. This \\nis why it's important that our model matches the database schema.\\n• On the second line, we'll actually perform the query. Thanks to database, we can \\nexecute it asynchronously. For an insert query, we'll use the execute method, \\nwhich expects the query in an argument.\\nAn INSERT query will return the id of the newly inserted row. This is very important \\nbecause, since we allow the database to automatically increment this identifier, we don't \\nknow the id of our new post beforehand.\\nIn fact, we need it to retrieve this new row from the database afterward. By doing this, \\nwe ensure we have an exact representation of the current object in the database before \\nreturning it in the response. For this, we use the get_post_or_404 function, which \\nwe'll talk about next.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 191, 'page_label': '173'}, page_content='Communicating with a SQL database with SQLAlchemy     173\\nMaking select queries\\nNow that we can insert new data into our database, we must be able to read it! Typically, \\nyou\\'ll have two kinds of read endpoints in your API: one to list objects and one to get  \\na single object.\\nLet\\'s start with the endpoint to list our blog posts. Y ou can view it in the  \\nfollowing example:\\napp.py\\n@app.get(\"/posts\")\\nasync def list_posts(\\n    pagination: Tuple[int, int] = Depends(pagination),\\n    database: Database = Depends(get_database),\\n) -> List[PostDB]:\\n    skip, limit = pagination\\n    select_query = posts.select().offset(skip).limit(limit)\\n    rows = await database.fetch_all(select_query)\\n    results = [PostDB(**row) for row in rows]\\n    return results\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nOnce again, making a query is a two-step operation: first, we build the query thanks to \\nthe SQLAlchemy query language. Then, we execute it asynchronously using database. \\nIn this case, we perform a SELECT query using the corresponding method on the posts \\ntable. Notice how we use the OFFSET and LIMIT clauses to paginate our list of posts \\nusing the variables provided by the pagination dependency. It\\'s the same dependency that \\nwe defined in Chapter 5, Dependency Injections in FastAPI.\\nThen, we execute this query with the fetch_all method of database. This method \\nwill return a list of rows that match our query.\\nEach row is returned in the form of a dictionary that associates column names and their \\nvalues. Therefore, for each of them, we simply have to instantiate them back to a PostDB \\nmodel by unpacking the dictionary.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 192, 'page_label': '174'}, page_content='174     Databases and Asynchronous ORMs\\nThe other typical endpoint in a REST API is to get a single object. In the following \\nexample, you can see how we implemented this endpoint to retrieve a single post:\\napp.py\\n@app.get(\"/posts/{id}\", response_model=PostDB)\\nasync def get_post(post: PostDB = Depends(get_post_or_404)) -> \\nPostDB:\\n    return post\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nPredictably, this is a GET endpoint that accepts an id in the path parameter. The \\nimplementation itself is very light. Indeed, since the logic of retrieving a post by its id  \\nor raising a 404 error if it doesn\\'t exist will be reused many times, it makes sense to put  \\nit in a dependency, get_post_or_404. Y ou can view its implementation in the \\nfollowing example:\\napp.py\\nasync def get_post_or_404(\\n    id: int, database: Database = Depends(get_database)\\n) -> PostDB:\\n    select_query = posts.select().where(posts.c.id == id)\\n    raw_post = await database.fetch_one(select_query)\\n    if raw_post is None:\\n        raise HTTPException(status_code=status.HTTP_404_NOT_\\nFOUND)\\n    return PostDB(**raw_post)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nOnce again, we start by building a SQL query. This time, we have a WHERE clause, which \\nonly retrieves the row for the id we need. The clause itself might look strange.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 193, 'page_label': '175'}, page_content='Communicating with a SQL database with SQLAlchemy     175\\nThe first part is to set the actual column we want to compare. Each column is accessible \\nvia its name from the c attribute of the table object, that is, posts.c.id.\\nThen, we use the equality operator to compare with our actual id variable. It looks like \\na standard comparison that would result in a Boolean, not a SQL statement! In a general \\nPython context, it would. However, SQLAlchemy developers have done something clever \\nhere: they overloaded the standard operators so that they produce SQL expressions \\ninstead of comparing objects. This is exactly what we saw in the Magic methods section  \\nof Chapter 2, Python Programming Specificities.\\nThen, we simply call fetch_one on the database object. It\\'s a convenient shortcut \\nwhen we only expect one row at most.\\nTwo things can happen: if no row matches our query, the result is None.  \\nIn this case, we can raise a 404 error. Otherwise, we get the data in the form of  \\na dictionary. All we have to do is to instantiate it back into a PostDB model.\\nDependencies are like functions\\nIn our POST endpoint, we used get_post_or_404 as a regular function \\nto retrieve our newly created blog post. This is perfectly okay: dependencies \\ndon\\'t have hidden or magic logic inside, so you can reuse them at will. The only \\nthing to remember is that you have to provide every argument manually since \\nyou are outside of the dependency injection context.\\nMaking update and delete queries\\nFinally, let\\'s examine how to update and delete rows in our database. The main \\ndifference is how you build the query using SQLAlchemy expressions, but the rest of the \\nimplementation is always the same.\\nIn the following example, let\\'s take a look at how to update a blog post:\\napp.py\\n@app.patch(\"/posts/{id}\", response_model=PostDB)\\nasync def update_post(\\n    post_update: PostPartialUpdate,\\n    post: PostDB = Depends(get_post_or_404),\\n    database: Database = Depends(get_database),\\n) -> PostDB:\\n    update_query = ('),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 194, 'page_label': '176'}, page_content='176     Databases and Asynchronous ORMs\\n        posts.update()\\n        .where(posts.c.id == post.id)\\n        .values(post_update.dict(exclude_unset=True))\\n    )\\n    post_id = await database.execute(update_query)\\n    post_db = await get_post_or_404(post_id, database)\\n    return post_db\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nIn this case, we start with an UPDATE statement. Upon this, we add a WHERE clause to \\nonly match the post we want to update. Finally, we set the values we want to update in the \\nform of a dictionary. As we explained in Chapter 4, Managing pydantic Data Models in \\nFastAPI, since we are doing a partial update here, you can see that we use the exclude_\\nunset option to only get the values to update.\\nDeleting an object is not very different, as you can see in the following example:\\napp.py\\n@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_\\nCONTENT)\\nasync def delete_post(\\n    post: PostDB = Depends(get_post_or_404), database: Database \\n= Depends(get_database)\\n):\\n    delete_query = posts.delete().where(posts.c.id == post.id)\\n    await database.execute(delete_query)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy/app.py\\nIt mainly consists of a DELETE statement followed by the adequate WHERE clause.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 195, 'page_label': '177'}, page_content='Communicating with a SQL database with SQLAlchemy     177\\nNow you know how to perform the most common SQL queries using the SQLAlchemy \\nexpression language and databases. We recommend that you go through the SQLAlchemy \\nexpression language tutorial to learn about all the features and the more advanced usage \\nof this powerful tool. Y ou can find the official documentation at https://docs.\\nsqlalchemy.org/en/13/core/tutorial.html.\\nAdding relationships\\nAs we mentioned at the beginning of this chapter, relational databases are all about data \\nand its relationships. Quite often, you\\'ll need to create entities that are linked to others. \\nFor example, in a blog application, comments are linked to the post they relate to. In this \\nsection, we\\'ll examine how you can set up such relationships with SQLAlchemy. Since  \\nit\\'s very close to SQL, you\\'ll discover that there\\'s nothing truly surprising about it.\\nFirst, we need to define the table for the comments, which has a foreign key toward the \\nposts table. Y ou can view its definition in the following example:\\nmodels.py\\ncomments = sqlalchemy.Table(\\n    \"comments\",\\n    metadata,\\n    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_\\nkey=True, autoincrement=True),\\n    sqlalchemy.Column(\\n        \"post_id\", sqlalchemy.ForeignKey(\"posts.id\", \\nondelete=\"CASCADE\"), nullable=False\\n    ),\\n    sqlalchemy.Column(\"publication_date\", sqlalchemy.\\nDateTime(), nullable=False),\\n    sqlalchemy.Column(\"content\", sqlalchemy.Text(), \\nnullable=False),\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/models.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 196, 'page_label': '178'}, page_content='178     Databases and Asynchronous ORMs\\nThe important point here is the post_id column, which is of the ForeignKey type. \\nThis is a special type that tells SQLAlchemy to automatically handle the type of the \\ncolumn and the associated constraint. We simply have to give the table and column names \\nit refers to. Note that we can also specify the ON DELETE action.\\nWe won\\'t go into the details of the Pydantic models for the comments since they are quite \\nstraightforward. However, we want to highlight a new model we created for the posts, that \\nis, PostPublic. This is shown in the following example:\\nmodels.py\\nclass PostPublic(PostDB):\\n    comments: List[CommentDB]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/models.py\\nHere, you can see that we added a comments attribute, which is a list of CommentDB. \\nIndeed, in a REST API, there are some cases where it makes sense to automatically \\nretrieve the associated objects of an entity. Here, it\\'ll be convenient to get the comments \\nof a post in a single request. We\\'ll use this model when getting a single post to serialize the \\ncomments along with the post data.\\nNow, we\\'ll implement an endpoint to create a new comment. This is shown in the \\nfollowing example:\\napp.py\\n@app.post(\"/comments\", response_model=CommentDB, status_\\ncode=status.HTTP_201_CREATED)\\nasync def create_comment(\\n    comment: CommentCreate, database: Database = Depends(get_\\ndatabase)\\n) -> CommentDB:\\n    select_post_query = posts.select().where(posts.c.id == \\ncomment.post_id)\\n    post = await database.fetch_one(select_post_query)\\n    if post is None:\\n        raise HTTPException('),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 197, 'page_label': '179'}, page_content='Communicating with a SQL database with SQLAlchemy     179\\n            status_code=status.HTTP_400_BAD_REQUEST, \\ndetail=f\"Post {id} does not exist\"\\n        )\\n    insert_query = comments.insert().values(comment.dict())\\n    comment_id = await database.execute(insert_query)\\n    select_query = comments.select().where(comments.c.id == \\ncomment_id)\\n    raw_comment = cast(Mapping, await database.fetch_\\none(select_query))\\n    return CommentDB(**raw_comment)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/app.py\\nNote that the endpoint parameters and most of the implementations are very close to  \\nthe create post endpoint. The only difference here is the first part of the function logic \\nwhere we check for the existence of the post before proceeding with the comment \\ncreation. This is important because, since the end user can send any post ID, we could \\nhave a situation where we try to create a comment for a post that doesn\\'t exist, which \\ncould cause a constraint error at the database level. This is why we are trying to get the \\npost first and then show a clear error to prevent this situation.\\nEarlier, we mentioned that we wanted to retrieve a post and its comments at the same \\ntime. To do this, we\\'ll have to make a second query to retrieve the comments and then \\nmerge all the data together in a PostPublic instance. We added this logic in the  \\nget_post_or_404 dependency, as you can see in the following example:\\napp.py\\nasync def get_post_or_404(\\n    id: int, database: Database = Depends(get_database)\\n) -> PostPublic:\\n    select_post_query = posts.select().where(posts.c.id == id)\\n    raw_post = await database.fetch_one(select_post_query)\\n    if raw_post is None:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 198, 'page_label': '180'}, page_content=\"180     Databases and Asynchronous ORMs\\n        raise HTTPException(status_code=status.HTTP_404_NOT_\\nFOUND)\\n    select_post_comments_query = comments.select().\\nwhere(comments.c.post_id == id)\\n    raw_comments = await database.fetch_all(select_post_\\ncomments_query)\\n    comments_list = [CommentDB(**comment) for comment in raw_\\ncomments]\\n    return PostPublic(**raw_post, comments=comments_list)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/app.py\\nHere, you can see that we simply add a fetch_all query with the correct WHERE \\nstatement to collect the comments associated with the post. Then, we only have to \\ntransform them into a list of CommentDB and set it during PostPublic initialization.\\nWhy not make a JOIN query?\\nAdmittedly, by making a JOIN query, we could retrieve the post and the \\ncomments data in one query instead of two. The problem with JOIN queries \\nis that they return as many rows as there are comments, all concatenated with \\nthe post data. While this is possible, clever logic is required to separate the post \\ndata and create a list of comments. For the simplicity of this example, we have \\nchosen to perform two queries.\\nEssentially, that's it for working with relationships with SQLAlchemy. Y ou can see that, \\nsince we are very close to SQL, it's up to you to build the right queries to shape the data as \\nneeded and resolve the relations.\\nSetting up a database migration system with Alembic\\nWhen developing an application, you'll likely make changes to your database schema to \\nadd new tables, add new columns, or modify existing ones. Of course, if your application \\nis already in production, you don't want to erase all your data to recreate the schema from \\nscratch: you want them to be migrated to the new schema. Tools for this task have been \\ndeveloped, and in this section, we'll learn how to set up Alembic, from the creators of \\nSQLAlchemy. Let's install this library:\\n$ pip install alembic\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 199, 'page_label': '181'}, page_content=\"Communicating with a SQL database with SQLAlchemy     181\\nOnce this has been completed, you'll have access to the alembic command to manage \\nthis migration system. When starting a new project, the first thing to do is to initialize  \\nthe migration environment, which includes a set of files and directories where Alembic \\nwill store its configuration and migration files. At the root of your project, run the \\nfollowing command:\\n$ alembic init alembic\\nThis will create a directory, named alembic, at the root of your project. Y ou can view the \\nresult of this command in the example repository shown in Figure 6.5:\\nFigure 6.5 – The Alembic migration environment structure\\nThis folder will contain all the configurations for your migrations and your migration \\nscripts themselves. It should be committed along with your code to keep a record of the \\nversions of  those files.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 200, 'page_label': '182'}, page_content=\"182     Databases and Asynchronous ORMs\\nAdditionally, note that it created an alembic.ini file, which contains all the \\nconfiguration options of Alembic. We'll review two important settings of this file: \\nscript_location and sqlalchemy.url. Y ou can view the first one in the  \\nfollowing example:\\nalembic.ini\\n# path to migration scripts\\nscript_location = chapter6/sqlalchemy_relationship/alembic\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/alembic.ini\\nThis setting expects the path of the alembic directory containing the migration files.  \\nIn most of your projects, this will just be alembic, because it'll simply be at the root of \\nyour project. Here, since we have several projects in our example repository, we had to set \\na sub-folder path.\\nThe second important option is sqlalchemy.url, which you can view in the  \\nfollowing example:\\nalembic.ini\\nsqlalchemy.url = sqlite:///chapter6_sqlalchemy_relationship.db\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/alembic.ini\\nPredictably, this is the connection string of your database that will receive the  \\nmigration queries. It follows the same convention that we saw earlier. Here, we set  \\nour SQLite database.\\nNext, we'll focus on the env.py file. This is a Python script containing all the logic \\nexecuted by Alembic to initialize the migration engine and execute the migrations. Being \\na Python script allows us to finely customize the execution of Alembic. For the time being, \\nwe'll keep the default one except for one thing: we'll import our metadata object. Y ou \\ncan view this in the following example:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 201, 'page_label': '183'}, page_content='Communicating with a SQL database with SQLAlchemy     183\\nenv.py\\nfrom chapter6.sqlalchemy_relationship.models import metadata\\n# this is the Alembic Config object, which provides\\n# access to the values within the .ini file in use.\\nconfig = context.config\\n# Interpret the config file for Python logging.\\n# This line sets up loggers basically.\\nfileConfig(config.config_file_name)\\n# add your model\\'s MetaData object here\\n# for \\'autogenerate\\' support\\n# from myapp import mymodel\\n# target_metadata = mymodel.Base.metadata\\ntarget_metadata = metadata\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/alembic/env.py\\nBy default, the file defines a variable named target_metadata, which is set to None. \\nHere, we changed it so that it refers to the metadata object that we just imported  \\nfrom our models module. But why do we do that? Well, remember that metadata is  \\na SQLAlchemy object that contains all the table definitions. By providing it to Alembic, \\nthe migration system will be able to automatically generate the migration scripts just  \\nby looking at your schema! This way, you won\\'t have to write them from scratch.\\nWhen you have made changes to your database schema, you can run the following \\ncommand to generate a new migration script:\\n$ alembic revision --autogenerate -m \"Initial migration\"'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 202, 'page_label': '184'}, page_content='184     Databases and Asynchronous ORMs\\nIt\\'ll create a new script in the version\\'s directory with the commands reflecting your schema \\nchanges. Y ou can view how it looks in the following example:\\na12742852e8c_initial_migration.py\\ndef upgrade():\\n    # ### commands auto generated by Alembic - please adjust! \\n###\\n    op.create_table(\\n        \"posts\",\\n        sa.Column(\"id\", sa.Integer(), autoincrement=True, \\nnullable=False),\\n        sa.Column(\"publication_date\", sa.DateTime(), \\nnullable=False),\\n        sa.Column(\"title\", sa.String(length=255), \\nnullable=False),\\n        sa.Column(\"content\", sa.Text(), nullable=False),\\n        sa.PrimaryKeyConstraint(\"id\"),\\n    )\\n    op.create_table(\\n        \"comments\",\\n        sa.Column(\"id\", sa.Integer(), autoincrement=True, \\nnullable=False),\\n        sa.Column(\"post_id\", sa.Integer(), nullable=False),\\n        sa.Column(\"publication_date\", sa.DateTime(), \\nnullable=False),\\n        sa.Column(\"content\", sa.Text(), nullable=False),\\n        sa.ForeignKeyConstraint([\"post_id\"], [\"posts.id\"], \\nondelete=\"CASCADE\"),\\n        sa.PrimaryKeyConstraint(\"id\"),\\n    )\\n    # ### end Alembic commands ###\\ndef downgrade():\\n    # ### commands auto generated by Alembic - please adjust! \\n###\\n    op.drop_table(\"comments\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 203, 'page_label': '185'}, page_content='Communicating with a SQL database with SQLAlchemy     185\\n    op.drop_table(\"posts\")\\n    # ### end Alembic commands ###\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/sqlalchemy_\\nrelationship/alembic/versions/a12742852e8c_initial_migration.py\\nHere, we have the required operations to create our posts and comments table, with \\nall of their columns and constraints. Notice that we have two functions: upgrade and \\ndowngrade. The first one is used to apply the migration and the second one is used to roll \\nit back. This is very important because if something goes wrong during the migration,  \\nor if you need to revert to an older version of your application, you\\'ll be able to do so \\nwithout breaking your data.\\nAutogenerate doesn\\'t detect everything\\nBear in mind that, even though autogeneration is very helpful, it\\'s not always \\naccurate, and, sometimes, it\\'s not able to detect ambiguous changes. For \\nexample, if you rename a column, it will delete the old one and create another. \\nAs a result, the data for this column will be lost! This is why you should always \\ncarefully review the migration scripts and make the required changes for edge \\ncases like this.\\nFinally, you can apply the migrations to your database using the following command:\\n$ alembic upgrade head\\nThis will run all the migrations that have not yet been applied to your database until the \\nlatest. It\\'s interesting to know that, in the process, Alembic creates a table in your database \\nso that it can remember all the migrations it has applied: this is how it detects which \\nscripts to run.\\nGenerally speaking, you should be extremely careful when you run such commands on \\nyour database, especially on a production one. Very bad things can happen if you make \\na mistake, and you can lose precious data. Y ou should always test your migrations in \\na test environment and have fresh and working backups before running them on your \\nproduction database.\\nThis is a very quick introduction to Alembic and its powerful migration system.  \\nWe strongly encourage you to go through its documentation to understand all of its \\nmechanisms, especially regarding migration script operations. Please refer to https://\\nalembic.sqlalchemy.org/en/latest/index.html.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 204, 'page_label': '186'}, page_content=\"186     Databases and Asynchronous ORMs\\nThat's it for the SQLAlchemy part of this chapter! If you are used to relational databases \\nand SQL, you shouldn't have been too surprised by its usage; it's very close to SQL. \\nHowever, sometimes, it's quicker and more convenient to move slightly away from SQL \\nand let libraries do the querying for us. This is exactly what ORM is for.\\nCommunicating with a SQL database with \\nTortoise ORM\\nWhen dealing with relational databases, you might wish to abstract away the SQL \\nconcepts and only deal with proper objects from the programming language. That's  \\nthe main motivation behind ORM tools. In this section, we'll examine how to work  \\nwith Tortoise ORM, which is a modern and asynchronous ORM that fits nicely within  \\na FastAPI project. It's greatly inspired by the Django ORM; so, if you've ever worked with \\nit, you'll probably be on familiar ground.\\nAs usual, the first step is to install the library using the following command:\\n$ pip install tortoise-orm\\nIf you need drivers for database engines such as PostgreSQL or MySQL, you can \\ninstall them, as explained in the documentation at https://tortoise-orm.\\nreadthedocs.io/en/latest/getting_started.html#installation. \\nWe're now ready to work!\\nCreating database models\\nThe first step is to create the Tortoise model for your entity. This is a Python class whose \\nattributes represent the columns of your table. This class will provide you static methods \\nin which to perform queries, such as retrieving or creating data. Moreover, the actual \\nentities of your database will be instances of this class, giving you access to its data like any \\nother object. Under the hood, the role of Tortoise is to make the link between this Python \\nobject and the row in the database. Let's take a look at the definition of our blog post \\nmodel in the following example:\\nmodels.py\\nclass PostTortoise(Model):\\n    id = fields.IntField(pk=True, generated=True)\\n    publication_date = fields.DatetimeField(null=False)\\n    title = fields.CharField(max_length=255, null=False)\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 205, 'page_label': '187'}, page_content='Communicating with a SQL database with Tortoise ORM     187\\n    content = fields.TextField(null=False)\\n    class Meta:\\n        table = \"posts\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/models.py\\nOur model is a class that is inheriting from the tortoise.models.Model base class. \\nEach field (or column) is an instance of a class corresponding to the type of the field. Each \\none has its own set of arguments to finely tune the definition in the database. For example, \\nour id field is a primary key that is automatically generated. We won\\'t go through every \\nfield\\'s class, but you can find the complete list in the official Tortoise documentation at \\nhttps://tortoise-orm.readthedocs.io/en/latest/fields.html.\\nNotice that we also have a sub-class called Meta, which allows us to set some options for \\nour table. Here, the table attribute allows us to control the name of the table.\\nIf you look at the code above the table definition, you\\'ll see that we have also defined \\nthe corresponding Pydantic models for our post entity. They will be used by FastAPI to \\nperform data validation and serialization. As you can see in the following example,  \\nwe added a Config sub-class and set an attribute called orm_mode:\\nmodels.py\\nclass PostBase(BaseModel):\\n    title: str\\n    content: str\\n    publication_date: datetime = Field(default_\\nfactory=datetime.now)\\n    class Config:\\n        orm_mode = True\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/models.py\\nThis option will allow us to transform an ORM object instance into a Pydantic object \\ninstance. This is essential because FastAPI is designed to work with Pydantic models,  \\nnot ORM models.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 206, 'page_label': '188'}, page_content='188     Databases and Asynchronous ORMs\\nHere, we hit what is maybe the most confusing part about working with FastAPI and an \\nORM: we\\'ll have to work with both ORM objects and Pydantic models and find ways to \\ntransform them back and forth.\\nIf you refer to the Tortoise documentation, you\\'ll find that it tries to solve this by \\nproviding tools to automatically generate Pydantic models from Tortoise ones. We won\\'t \\nshow this approach in this book because it comes with some pitfalls and is less flexible \\nthan pure Pydantic models. Nevertheless, once you are confident with the concepts we are \\nshowing here, we encourage you to try this approach and see if it fits your needs.\\nSetting up the Tortoise engine\\nNow that we have our model ready, we have to configure the Tortoise engine to set the \\ndatabase connection string and the location of our models. To do this, Tortoise comes \\nwith a utility function for FastAPI that does all the required tasks for you. In particular, \\nit automatically adds event handlers to open and close the connection at startup and \\nshutdown; this is something we had to do by hand with SQLAlchemy.  \\nY ou can see what it looks like in the following example:\\napp.py\\nTORTOISE_ORM = {\\n    \"connections\": {\"default\": \"sqlite://chapter6_tortoise.\\ndb\"},\\n    \"apps\": {\\n        \"models\": {\\n            \"models\": [\"chapter6.tortoise.models\"],\\n            \"default_connection\": \"default\",\\n        },\\n    },\\n}\\nregister_tortoise(\\n    app,\\n    config=TORTOISE_ORM,\\n    generate_schemas=True,\\n    add_exception_handlers=True,\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 207, 'page_label': '189'}, page_content=\"Communicating with a SQL database with Tortoise ORM     189\\nAs you can see, we put the main configuration options in a variable named TORTOISE_\\nORM. Let's review its different fields:\\n• The connections key contains a dictionary associating a database alias to  \\na connection string, which gives access to your database. It follows the standard \\nconvention, as explained in the documentation at https://tortoise-orm.\\nreadthedocs.io/en/latest/databases.html?highlight=db_\\nurl#db-url.\\n In most projects, you'll probably have one database named default, but it allows \\nyou to set several databases if needed.\\n• In the apps key, you'll be able to declare all your modules containing your Tortoise \\nmodels. The first key just below apps, that is, models, will be the prefix with \\nwhich you'll be able to refer to the associated models. Y ou can name it how you \\nwant, but if you place all your models under the same scope, then models is  \\na good candidate. This prefix is especially important when defining foreign keys.  \\nFor example, with this configuration, our PostTortoise model can be referred to \\nby the name models.PostTortoise. It's not the actual path to your module.\\nUnderneath it, you have to list all the modules containing your models. \\nAdditionally, we set the corresponding database connection with the alias  \\nwe defined earlier.\\nThen, we call the register_tortoise function that'll take care of setting up Tortoise \\nfor FastAPI. Let's explain its arguments:\\n• The first one is your FastAPI app instance.\\n• Then, we have the configuration that we defined earlier.\\n• Setting generate_schemas to True will automatically create the table's schema \\nin the database. Otherwise, our database will be empty and we won't be able to \\ninsert any rows.\\nWhile this is useful for testing purposes, in a real-world application, you should \\nhave a proper migration system whose role is to make sure your database schema  \\nis in sync. We'll examine how to set one up for Tortoise later in the chapter.\\n• Finally, the add_exception_handlers option adds custom exception handlers \\nto FastAPI, allowing you to nicely catch Tortoise errors and return proper  \\nerror responses.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 208, 'page_label': '190'}, page_content='190     Databases and Asynchronous ORMs\\nAnd that\\'s all! Always make sure that you call this function at the end of your application \\nfile, to ensure everything has been correctly imported. Apart from that, Tortoise handles \\neverything for us. We\\'re now ready to go!\\nCreating objects\\nLet\\'s start by inserting new objects inside our database. The main challenge is to transform \\nthe Tortoise object instance into a Pydantic model. Let\\'s review this in the following \\nexample:\\napp.py\\n@app.post(\"/posts\", response_model=PostDB, status_code=status.\\nHTTP_201_CREATED)\\nasync def create_post(post: PostCreate) -> PostDB:\\n    post_tortoise = await PostTortoise.create(**post.dict())\\n    return PostDB.from_orm(post_tortoise)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py\\nHere, we have our POST endpoint, which accepts our PostCreate model. The core logic \\nconsists then of two operations.\\nFirst, we create the object in the database. We directly use the PostTortoise class  \\nand its static create method. Conveniently, it accepts a dictionary that maps fields to \\ntheir values, so we just have to call dict on our input object. Of course, this operation  \\nis natively asynchronous!\\nAs a result, we get an instance of a PostTortoise object. This is why the second \\noperation we need to perform is to transform it into a Pydantic model. To do this,  \\nwe use the from_orm method, which is available because we enabled orm_mode.  \\nWe get a proper PostDB instance, which we can return directly.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 209, 'page_label': '191'}, page_content='Communicating with a SQL database with Tortoise ORM     191\\nCan we return a PostTortoise object directly?\\nTechnically, yes, we can. In the case of a Tortoise model, it implements the \\nmagic methods to be transformed into a dictionary, which is the last fallback \\nof FastAPI when it doesn\\'t recognize the object you have returned. However, \\ndoing this would deprive us of all the goodness of using Pydantic models, such \\nas field exclusion or automatic documentation. This is why we recommend here \\nthat you always go back to a Pydantic model.\\nHere, you can see that the implementation is quite straightforward. Now, let\\'s retrieve  \\nthis data!\\nGetting and filtering objects\\nUsually, a REST API provides two types of endpoints to read data: one to list objects  \\nand one to get a specific object. This is exactly what we\\'ll review next!\\nIn the following example, you can see how we implemented the endpoint to list objects:\\napp.py\\n@app.get(\"/posts\")\\nasync def list_posts(pagination: Tuple[int, int] = \\nDepends(pagination)) -> List[PostDB]:\\n    skip, limit = pagination\\n    posts = await PostTortoise.all().offset(skip).limit(limit)\\n    results = [PostDB.from_orm(post) for post in posts]\\n    return results\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py\\nOnce again, this is an operation in two steps: first, we retrieve Tortoise objects using the \\nquery language. Notice that we use the all method, which gives us every object in the \\ntable. Additionally, we\\'re able to apply our pagination parameters through offset  \\nand limit.\\nThen, we have to transform this list of PostTortoise objects into a list of PostDB \\nobjects. Again, thanks to from_orm and a list comprehension, we can do this very easily.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 210, 'page_label': '192'}, page_content='192     Databases and Asynchronous ORMs\\nNow, in the following example, we\\'ll take a look at the endpoint to retrieve a single post:\\napp.py\\n@app.get(\"/posts/{id}\", response_model=PostDB)\\nasync def get_post(post: PostTortoise = Depends(get_post_\\nor_404)) -> PostDB:\\n    return PostDB.from_orm(post)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py\\nThis is a simple GET endpoint that expects the ID of the post in the path parameter.  \\nThe implementation is itself very light: we just have to transform our PostTortoise \\nobject into a PostDB. Most of the logic is in the get_post_or_404 dependency,  \\nwhich we\\'ll reuse often in our application. The following example shows  \\nits implementation:\\napp.py\\nasync def get_post_or_404(id: int) -> PostTortoise:\\n    return await PostTortoise.get(id=id)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py\\nThe role of this dependency is to take the id in the path parameter and retrieve \\na single object from the database that corresponds to this identifier. The get \\nmethod is a convenient shortcut for this: if no matching record is found, it raises \\nthe DoesNotExist exception. If there is more than one matching record, it raises \\nMultipleObjectsReturned.\\nY ou might be wondering where our exception handler is to raise a proper 404 error.  \\nIn fact, it\\'s already there, at a global level! Remember that we set up Tortoise with \\nthe add_exception_handlers option: under the hood, it adds a handler that \\nautomatically catches DoesNotExist and builds a proper 404 error. So, we don\\'t have  \\nto do anything more!'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 211, 'page_label': '193'}, page_content='Communicating with a SQL database with Tortoise ORM     193\\nUpdating and deleting objects\\nWe\\'ll finish by showing you how to update and delete existing objects. The logic is always \\nthe same; we just have to adapt the methods we call on our Tortoise object.\\nIn the following example, you can view the implementation of the update endpoint:\\napp.py\\n@app.patch(\"/posts/{id}\", response_model=PostDB)\\nasync def update_post(\\n    post_update: PostPartialUpdate, post: PostTortoise = \\nDepends(get_post_or_404)\\n) -> PostDB:\\n    post.update_from_dict(post_update.dict(exclude_unset=True))\\n    await post.save()\\n    return PostDB.from_orm(post)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py\\nHere, the main point of attention is that we\\'ll operate directly on the post we want to \\nmodify. This is one of the key aspects when working with ORM: entities are objects that \\ncan be modified as you wish. When you are happy with the data, you can persist it in the \\ndatabase. This is exactly what we do here: we get a fresh representation of our post thanks \\nto get_post_or_404 and apply the update_from_dict utility method to change \\nthe fields that we want. Then, we can persist the changes in the database using save.\\nThe same concept is applied when you wish to delete an object: when you have an \\ninstance, you can call delete to physically remove it from the database. Y ou can view \\nthis in action in the following example:\\napp.py\\n@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_\\nCONTENT)\\nasync def delete_post(post: PostTortoise = Depends(get_post_\\nor_404)):\\n    await post.delete()'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 212, 'page_label': '194'}, page_content='194     Databases and Asynchronous ORMs\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise/app.py\\nThat\\'s almost it for the basics of working with Tortoise ORM. Of course, we only covered \\nthe most basic queries, but you can do far more complex things. Y ou can find a thorough \\noverview of the query language in the official documentation at https://tortoise-\\norm.readthedocs.io/en/latest/query.html#query-api.\\nAdding relationships\\nNow, let\\'s take a look at how to work with relationships. Once again, we\\'ll examine how \\nto implement comments that are linked to posts. One of the main tasks of Tortoise, and \\nORM in general, is to ease the process of working with related entities, by automatically \\nmaking the required JOIN queries and instantiating sub-objects. However, once again, \\nthere are some things that we need to take care of to make sure everything works \\nsmoothly with Pydantic.\\nWe\\'ll begin by creating a model for our comment entity, as shown in the  \\nfollowing example:\\nmodels.py\\nclass CommentTortoise(Model):\\n    id = fields.IntField(pk=True, generated=True)\\n    post = fields.ForeignKeyField(\\n        \"models.PostTortoise\", related_name=\"comments\", \\nnull=False\\n    )\\n    publication_date = fields.DatetimeField(null=False)\\n    content = fields.TextField(null=False)\\n    class Meta:\\n        table = \"comments\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise_\\nrelationship/models.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 213, 'page_label': '195'}, page_content=\"Communicating with a SQL database with Tortoise ORM     195\\nThe main point of interest here is the post field, which is purposely defined as a foreign \\nkey. The first argument is the reference to the associated model. Notice that we use the \\nmodels prefix; this is the same one we defined in the Tortoise configuration that we saw \\nearlier. Additionally, we set the related_name. This is a typical and convenient feature \\nof ORM. By doing this, we'll be able to get all the comments of a given post simply by \\naccessing its comments property. The action of querying the related comments, therefore, \\nbecomes completely implicit.\\nIn the next example, we'll look at the base Pydantic model for a comment, \\nCommentBase:\\nmodels.py\\nclass CommentBase(BaseModel):\\n    post_id: int\\n    publication_date: datetime = Field(default_\\nfactory=datetime.now)\\n    content: str\\n    class Config:\\n        orm_mode = True\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise_\\nrelationship/models.py\\nHere, you can see that we have defined a post_id attribute. This attribute will be used in \\nthe request payload to set the post that we want to attach this new comment to. When you \\nprovide this attribute to Tortoise, it automatically understands that you are referring to the \\nidentifier of the foreign key field, called post.\\nIn a REST API, sometimes, it makes sense to automatically retrieve the associated objects \\nof an entity in one request. Here, we'll ensure that the comments of a post are returned in \\nthe form of a list along with the post data. To do this, we introduce a new Pydantic model, \\nPostPublic. Y ou can view this in the following example:\\nmodels.py\\nclass PostPublic(PostDB):\\n    comments: List[CommentDB]\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 214, 'page_label': '196'}, page_content='196     Databases and Asynchronous ORMs\\n    @validator(\"comments\", pre=True)\\n    def fetch_comments(cls, v):\\n        return list(v)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise_\\nrelationship/models.py\\nPredictably, we simply added a comments attribute, which is a list of CommentDB. \\nHowever, here, you can see something unexpected: a validator for this attribute. Earlier, \\nwe mentioned that thanks to Tortoise, we can retrieve the comments of a post by simply \\ndoing post.comments. This is convenient, but this attribute is not directly a list of data: \\nit\\'s a query set object. If we don\\'t do anything, then, when we try to transform the ORM \\nobject into a PostPublic, Pydantic will try to parse this query set and fail. However, \\ncalling list on this query set forces it to output the data. That is the purpose of this \\nvalidator. Notice that we set it with pre=True to make sure it\\'s called before the built-in \\nPydantic validation.\\nWe\\'ll now implement an endpoint to create a new comment. This is shown in the \\nfollowing example:\\napp.py\\n@app.post(\"/comments\", response_model=CommentDB, status_\\ncode=status.HTTP_201_CREATED)\\nasync def create_comment(comment: CommentBase) -> CommentDB:\\n    try:\\n        await PostTortoise.get(id=comment.post_id)\\n    except DoesNotExist:\\n        raise HTTPException(\\n            status_code=status.HTTP_400_BAD_REQUEST, \\ndetail=f\"Post {id} does not exist\"\\n        )\\n    comment_tortoise = await CommentTortoise.create(**comment.\\ndict())\\n    return CommentDB.from_orm(comment_tortoise)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 215, 'page_label': '197'}, page_content='Communicating with a SQL database with Tortoise ORM     197\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise_\\nrelationship/app.py\\nMost of the logic is very similar to the create post endpoint. The main difference is that \\nwe first check for the existence of the post before proceeding with the comment creation. \\nIndeed, we want to avoid the foreign key constraint error that could occur at the database \\nlevel and show a clear and helpful error message to the end user instead.\\nAs we mentioned earlier, our objective is to output the comments when retrieving a single \\npost. To do this, we made a small change to the get_post_or_404 dependency,  \\nas follows:\\napp.py\\nasync def get_post_or_404(id: int) -> PostTortoise:\\n    try:\\n        return await PostTortoise.get(id=id).prefetch_\\nrelated(\"comments\")\\n    except DoesNotExist:\\n        raise HTTPException(status_code=status.HTTP_404_NOT_\\nFOUND)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise_\\nrelationship/app.py\\nThe only difference here is that we called the prefetch_related method on our query. \\nBy passing in the name of the related entities, it allows you to preload them upfront when \\ngetting the main object. By default, Tortoise is lazy and doesn\\'t make the additional query. \\nIn our case, it\\'s not just an optimization: it\\'s important to ensure our code is working. \\nIndeed, if our validator tries to call list on a query set that hasn\\'t been prefetched,  \\nit\\'ll raise an error. This is because of the asynchronous nature of the ORM: you have to \\nretrieve the data asynchronously, with a proper await statement, before you can operate \\nover the data normally.\\nOther than that, there is nothing more you need to do. The key takeaway here is that you \\nhave to pay attention when trying to work with relationships and make sure you resolve \\nthem correctly before feeding them to Pydantic.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 216, 'page_label': '198'}, page_content='198     Databases and Asynchronous ORMs\\nSetting up a database migration system with Aerich\\nIn the Setting up a database migration system with Alembic section of this chapter,  \\nwe already mentioned the need for a database migration system. When you make changes \\nto your database schema, you want to migrate your existing data in production in a safe \\nand reproducible manner. In this section, we\\'ll demonstrate how to install and configure \\nAerich, which is a database migration tool from the creators of Tortoise. As usual,  \\nwe\\'ll start by installing the library:\\n$ pip install aerich\\nOnce this is done, you\\'ll have access to the aerich command to manage this  \\nmigration system.\\nThe first thing you need to do is declare the Aerich models in your Tortoise configuration. \\nIndeed, Aerich stores some migration state information in your database. Y ou can view \\nwhat the configuration looks like in the following example:\\napp.py\\nTORTOISE_ORM = {\\n    \"connections\": {\"default\": \"sqlite://chapter6_tortoise_\\nrelationship.db\"},\\n    \"apps\": {\\n        \"models\": {\\n            \"models\": [\"chapter6.tortoise_relationship.models\", \\n\"aerich.models\"],\\n            \"default_connection\": \"default\",\\n        },\\n    },\\n}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/tortoise_\\nrelationship/app.py\\nThen, you can initialize the migration environment, which is a set of files and directories \\nwhere Aerich will store its configuration and migration files. The command looks like this:\\n$ aerich init -t chapter6.tortoise_relationship.app.TORTOISE_\\nORM'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 217, 'page_label': '199'}, page_content='Communicating with a SQL database with Tortoise ORM     199\\nThe -t option should refer to the dotted path of your TORTOISE_ORM configuration \\nvariable. This is how Aerich is able to retrieve your database connection information and \\nthe definition of your models. Then, you have to call the following command:\\n$ aerich init-db\\nFollowing this, your project structure should look similar to the one shown in Figure 6.5:\\nFigure 6.6 – The Aerich migration environment structure\\nThe migrations folder will contain all of the migration scripts. Notice that it creates  \\na sub-directory for each of the \"apps\" defined in the configuration. As you can see, we have \\na first migration script that creates all the tables that have already been defined.\\nIt also adds the aerich.ini configuration file, which essentially sets the path to your \\nconfiguration variable and migrations folder.\\nTo apply the migrations to your database, simply run the following command:\\n$ aerich upgrade\\nDuring the life of your project, when you have made changes to your table\\'s schema,  \\nyou\\'ll have to generate new migration scripts to reflect the changes. This is done quite \\neasily using the following command:\\n$ aerich migrate --name added_new_tables\\nThe --name option allows you to set a name for your migration. It will automatically \\ngenerate a new migration file that reflects your changes.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 218, 'page_label': '200'}, page_content=\"200     Databases and Asynchronous ORMs\\nAerich migration scripts are not cross-database compatible\\nContrary to Alembic, Aerich doesn't abstract migration operations through \\ncross-compatible Python scripts. Instead, it directly generates SQL files that are \\ncompatible with the engine you are working with. Since there are significant \\ndifferences between the various SQL implementations, you can't work, for \\nexample, on a SQLite database during development and have a PostgreSQL \\nfor production: the migration scripts generated locally wouldn't work on your \\nproduction server. This is why you should have the same database engine both \\nin local and in production.\\nJust as with any automated migration system, you should always review the generated \\nscripts to make sure they correctly reflect your changes and that you don't lose data in the \\nprocess. Always test your migrations in a test environment and have fresh and working \\nbackups before running them in production.\\nThat's it for this introduction to Tortoise ORM. If you have ever used an ORM before, you \\nshould be already confident with it. The main challenge to tackle with FastAPI is to make \\nit work together with Pydantic models to get the benefit of both worlds. We'll now leave \\nthe world of relational databases to explore how we can work with a document-oriented \\ndatabase, MongoDB.\\nCommunicating with a MongoDB database \\nusing Motor\\nAs we mentioned at the beginning of this chapter, working with a document-oriented \\ndatabase, such as MongoDB, is quite different from a relational database. First and \\nforemost, you don't need to configure a schema upfront: it follows the structure of the data \\nthat you insert into it. In the case of FastAPI, it makes our life slightly easier since we'll \\nonly have to work with Pydantic models. However, there are some subtleties around the \\ndocument identifiers that we need to take into account. We'll review this next.\\nTo begin, we'll install Motor, which is a library that is used to communicate \\nasynchronously with MongoDB and is officially supported by the MongoDB organization. \\nY ou can run the following command:\\n$ pip install motor\\nOnce this is done, we can start working!\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 219, 'page_label': '201'}, page_content='Communicating with a MongoDB database using Motor     201\\nCreating models compatible with MongoDB ID\\nAs we mentioned in the introduction to this section, there are some difficulties with the \\nidentifiers that MongoDB uses to store documents. Indeed, by default, MongoDB assigns \\nevery document an _id property that acts as a unique identifier in a collection. This \\ncauses two issues:\\n• In a Pydantic model, if a property starts with an underscore, it\\'s considered to be \\nprivate and, thus, is not used as a data field for our model.\\n• _id is encoded as a binary object, called ObjectId, instead of a simple \\ninteger or string. It\\'s usually represented in the form of a string such as \\n608d1ee317c3f035100873dc. This type of object is not supported out of the \\nbox by Pydantic or FastAPI.\\nThis is why we\\'ll need some boilerplate code to ensure those identifiers work  \\nwith Pydantic and FastAPI. To begin, in the following example, we have created  \\na MongoBaseModel base class that takes care of defining the id field:\\nmodels.py\\nclass MongoBaseModel(BaseModel):\\n    id: PyObjectId = Field(default_factory=PyObjectId, alias=\"_\\nid\")\\n    class Config:\\n        json_encoders = {ObjectId: str}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/models.py\\nFirst, we need to define an id field, which is of type PyObjectId. This is a custom \\ntype that has been defined in the preceding code. We won\\'t go into the details of its \\nimplementation, but just know that it\\'s a class that makes ObjectId a compatible type \\nfor Pydantic. We define this same class as a default factory for this field. Interestingly, that \\nkind of identifier allows us to generate them on the client side, contrary to traditional \\nauto-incremented integers of relational databases, which could be useful in some cases.\\nThe most interesting argument is alias. It\\'s a Pydantic option allowing us to change the \\nname of the field during serialization. In this example, when we call the dict method on \\none instance of MongoBaseModel, the identifier will be set on the _id key; this is the \\nname expected by MongoDB. That solves the first issue.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 220, 'page_label': '202'}, page_content='202     Databases and Asynchronous ORMs\\nThen, we add the Config sub-class and set the json_encoders option. By default, \\nPydantic is completely unaware of our PyObjectId type, so it won\\'t be able to correctly \\nserialize it to JSON. This option allows us to map custom types with a function that will \\nbe called to serialize them. Here, we simply transform it into a string (it works because \\nObjectId implements the __str__ magic method). That solves the second issue  \\nfor Pydantic.\\nOur base model for Pydantic is complete! We can now use it as a base class \\ninstead of BaseModel for our actual data models. Notice, however, that the \\nPostPartialUpdate doesn\\'t inherit from it. Indeed, we don\\'t want the id field in \\nthis model; otherwise, a PATCH request might be able to replace the ID of the document, \\nwhich could lead to weird issues.\\nConnecting to a database\\nNow that our models are ready, we can set up the connection with a MongoDB  \\nserver. This is quite easy and only involves a class instantiation, as shown in the  \\nfollowing example:\\napp.py\\nmotor_client = AsyncIOMotorClient(\"mongodb://localhost:27017\")  \\n# Connection to the whole server\\ndatabase = motor_client[\"chapter6_mongo\"]  # Single database \\ninstance\\ndef get_database() -> AsyncIOMotorDatabase:\\n    return database\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nHere, you can see that AsyncIOMotorClient simply expects a connection string to \\nyour database. Generally, it consists of the scheme, followed by authentication information \\nand the hostname of the database server. Y ou can find an overview of this format in \\nthe official MongoDB documentation at https://docs.mongodb.com/manual/\\nreference/connection-string/.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 221, 'page_label': '203'}, page_content='Communicating with a MongoDB database using Motor     203\\nHowever, be careful. Contrary to the libraries we\\'ve discussed so far, the client instantiated \\nhere is not bound to any database, that is, it\\'s only a connection to a whole server. That\\'s \\nwhy we need the second line to set the database that we want to work upon directly by its \\nkey. It\\'s worth noting that MongoDB doesn\\'t require you to create the database upfront: \\nit\\'ll create it automatically if it doesn\\'t exist.\\nThen, we create a simple function to return this database instance. We\\'ll use this  \\nfunction as a dependency to retrieve this instance in our path operation functions.  \\nWe explained the benefits of this pattern in the Communicating with a SQL database  \\nwith SQLAlchemy section.\\nThat\\'s it! We can now make queries to our database!\\nInserting documents\\nWe\\'ll start by demonstrating how to implement an endpoint to create posts. Essentially,  \\nwe just have to insert our Pydantic model that has been transformed into a dictionary:\\napp.py\\n@app.post(\"/posts\", response_model=PostDB, status_code=status.\\nHTTP_201_CREATED)\\nasync def create_post(\\n    post: PostCreate, database: AsyncIOMotorDatabase = \\nDepends(get_database)\\n) -> PostDB:\\n    post_db = PostDB(**post.dict())\\n    await database[\"posts\"].insert_one(post_db.dict(by_\\nalias=True))\\n    post_db = await get_post_or_404(post_db.id, database)\\n    return post_db\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nClassically, this is a POST endpoint that accepts a payload in the form of a PostCreate \\nmodel. Additionally, we inject the database instance with the dependency we wrote earlier.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 222, 'page_label': '204'}, page_content='204     Databases and Asynchronous ORMs\\nIn the path operation itself, you can see that we start by instantiating a PostDB from the \\nPostCreate data. This is usually a good practice if you only have fields in PostDB that \\nneed to be initialized.\\nThen, we have the query. To retrieve a collection in our MongoDB database, we simply \\nhave to get it by name, like a dictionary. Once again, MongoDB will take care of creating \\nit if it doesn\\'t exist. As you can see, document-oriented databases are much more \\nlightweight regarding schema than relational databases! In this collection, we can call \\nthe insert_one method to insert a single document. It expects a dictionary to map \\nfields to their values. Therefore, the dict method of Pydantic objects is once again our \\nfriend. However, here, we see something new: we call it with the by_alias argument \\nset to True. By default, Pydantic will serialize the object with the real field name, not the \\nalias name. However, we do need the identifier named as _id in our MongoDB database. \\nUsing this option, Pydantic will use the alias as a key in the dictionary.\\nTo ensure we have a true and fresh representation of our document in the dictionary,  \\nwe retrieve it back from the database thanks to our get_post_or_404 function.  \\nWe\\'ll examine how it works in the next section.\\nGetting documents \\nOf course, retrieving the data from the database is an important part of the job of a REST \\nAPI. Now, we\\'ll demonstrate how to implement two classic endpoints, that is, to list posts \\nand get a single post. Let\\'s start with the first one and take a look at its implementation in \\nthe following example:\\napp.py\\n@app.get(\"/posts\")\\nasync def list_posts(\\n    pagination: Tuple[int, int] = Depends(pagination),\\n    database: AsyncIOMotorDatabase = Depends(get_database),\\n) -> List[PostDB]:\\n    skip, limit = pagination\\n    query = database[\"posts\"].find({}, skip=skip, limit=limit)\\n    results = [PostDB(**raw_post) async for raw_post in query]\\n    return results\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 223, 'page_label': '205'}, page_content='Communicating with a MongoDB database using Motor     205\\nThe most interesting part is the second line where we define the query. After retrieving  \\nthe posts collection, we call the find method. The first argument should be the  \\nfiltering query, following  the MongoDB syntax. Since we want every document,  \\nwe leave it empty. Then, we have keyword arguments that allow us to apply our  \\npagination parameters.\\nMongoDB returns us a result in the form of a list of dictionaries, which maps fields to \\ntheir values. This is why we added a list comprehension construct to transform them back \\ninto PostDB instances so that FastAPI can serialize them properly.\\nY ou might have noticed something quite surprising here: contrary to what we do usually, \\nwe didn\\'t wait for the query directly. Instead, we added the async keyword to our list \\ncomprehension. Indeed, in this case, Motor returns an asynchronous generator. It\\'s the \\nasynchronous counterpart of the classic generator. It works in the same way, aside from \\nthe async keyword we have to add when iterating over it.\\nNow, let\\'s take a look at the endpoint to retrieve a single post. The following example \\nshows its implementation:\\napp.py\\n@app.get(\"/posts/{id}\", response_model=PostDB)\\nasync def get_post(post: PostDB = Depends(get_post_or_404)) -> \\nPostDB:\\n    return post\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nAs you can see, it\\'s a simple GET endpoint that accepts the id post as a path parameter. \\nMost of the logic implementation is in the reusable get_post_or_404 dependency. \\nY ou can view how it looks like in the next example:\\nApp.py\\nasync def get_post_or_404(\\n    id: ObjectId = Depends(get_object_id),\\n    database: AsyncIOMotorDatabase = Depends(get_database),\\n) -> PostDB:\\n    raw_post = await database[\"posts\"].find_one({\"_id\": id})'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 224, 'page_label': '206'}, page_content=\"206     Databases and Asynchronous ORMs\\n    if raw_post is None:\\n        raise HTTPException(status_code=status.HTTP_404_NOT_\\nFOUND)\\n    return PostDB(**raw_post)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nThe logic is quite similar to what we saw for the list endpoint. This time, however, we call \\nthe find_one method with a query to match the post identifier: the key is the name of \\nthe document attribute we want to filter on, and the value is the one we are looking for.\\nThis method returns the document in the form of a dictionary or None if it doesn't exist. \\nIn this case, we raise a proper 404 error.\\nFinally, we transform it back into a PostDB model before returning it.\\nY ou might have noticed that we got the id through a dependency, get_object_id. \\nIndeed, FastAPI will return a string from the path parameter. If we try to make a query \\nwith the id in the form of a string, MongoDB will not match with the actual binary IDs. \\nThat's why we use another dependency that transforms the identifier represented as  \\na string (such as 608d1ee317c3f035100873dc) to a proper ObjectId.\\nOn a side note, here, you have a very nice example of nested dependencies: endpoints use \\nthe get_post_or_404 dependency, which itself gets a value from get_object_id. \\nY ou can view the implementation of this dependency in the following example:\\napp.py\\nasync def get_object_id(id: str) -> ObjectId:\\n    try:\\n        return ObjectId(id)\\n    except (errors.InvalidId, TypeError):\\n        raise HTTPException(status_code=status.HTTP_404_NOT_\\nFOUND)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 225, 'page_label': '207'}, page_content='Communicating with a MongoDB database using Motor     207\\nHere, we simply retrieve the id string from the path parameters and try to instantiate  \\nit back into an ObjectId. If it\\'s not a valid value, we catch the corresponding errors and \\nconsider it as a 404 error.\\nWith this, we have solved every challenge posed by the MongoDB identifiers format.  \\nLet\\'s now discuss how to update and delete documents.\\nUpdating and deleting documents\\nWe\\'ll now review the endpoints to update and delete documents. The logic is still the same \\nand only involves building the proper query from the request payload.\\nLet\\'s start with the PATCH endpoint, which you can view in the following example:\\napp.py\\n@app.patch(\"/posts/{id}\", response_model=PostDB)\\nasync def update_post(\\n    post_update: PostPartialUpdate,\\n    post: PostDB = Depends(get_post_or_404),\\n    database: AsyncIOMotorDatabase = Depends(get_database),\\n) -> PostDB:\\n    await database[\"posts\"].update_one(\\n        {\"_id\": post.id}, {\"$set\": post_update.dict(exclude_\\nunset=True)}\\n    )\\n    post_db = await get_post_or_404(post.id, database)\\n    return post_db\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nHere, you can see that we use the update_one method to update one document. The \\nfirst argument is the filtering query and the second one is the actual operation to apply to \\nthe document. Once again, it follows the MongoDB syntax: the $set operation allows  \\nus to only modify the fields we want to change by passing the update dictionary.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 226, 'page_label': '208'}, page_content='208     Databases and Asynchronous ORMs\\nThe DELETE endpoint is even simpler: it\\'s just a single query, as you can see in the \\nfollowing example:\\napp.py\\n@app.delete(\"/posts/{id}\", status_code=status.HTTP_204_NO_\\nCONTENT)\\nasync def delete_post(\\n    post: PostDB = Depends(get_post_or_404),\\n    database: AsyncIOMotorDatabase = Depends(get_database),\\n):\\n    await database[\"posts\"].delete_one({\"_id\": post.id})\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nThe delete_one method expects the filtering query as the first argument. \\nThat\\'s it! Of course, here, we\\'ve only demonstrated the simplest queries, but MongoDB \\nhas a very powerful query language that\\'ll allow you to do more complex things. If \\nyou\\'re not used to it, we recommend you to read the nice introduction from the official \\ndocumentation. Y ou can find this at https://docs.mongodb.com/manual/crud.\\nNesting documents\\nAt the beginning of this chapter, we mentioned that document-based databases, contrary \\nto relational databases, aim to store all the data related to an entity in a single document. \\nIn our current example, if we wish to store the comments along with the post, we simply \\nhave to add a list containing information regarding each comment.\\nIn this section, we\\'ll implement this behavior. Y ou should see that the functioning of \\nMongoDB makes it very easy and straightforward.\\nWe\\'ll start by adding a new comments attribute on our PostDB model. Y ou can view this \\nin the following example:\\nmodels.py\\nclass PostDB(PostBase):\\n    comments: List[CommentDB] = Field(default_factory=list)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 227, 'page_label': '209'}, page_content='Communicating with a MongoDB database using Motor     209\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb_\\nrelationship/models.py\\nThis field is simply a list of CommentDB. We won\\'t go into the details of the comment \\nmodels, since they are quite straightforward. Notice here that we use the list function  \\nas the default factory for this attribute. This instantiates an empty list by default when  \\nwe create a PostDB without setting any comments.\\nNow that we have our models, we can implement an endpoint to create a new comment. \\nY ou can view it in the following example:\\napp.py\\n@app.post(\\n    \"/posts/{id}/comments\", response_model=PostDB, status_\\ncode=status.HTTP_201_CREATED\\n)\\nasync def create_comment(\\n    comment: CommentCreate,\\n    post: PostDB = Depends(get_post_or_404),\\n    database: AsyncIOMotorDatabase = Depends(get_database),\\n) -> PostDB:\\n    await database[\"posts\"].update_one(\\n        {\"_id\": post.id}, {\"$push\": {\"comments\": comment.\\ndict()}}\\n    )\\n    post_db = await get_post_or_404(post.id, database)\\n    return post_db\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb_\\nrelationship/app.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 228, 'page_label': '210'}, page_content='210     Databases and Asynchronous ORMs\\nThis one is slightly different from what we\\'ve seen so far. Indeed, instead of making \\ncomments a \"first-class\" resource with their own paths, such as for relational databases, \\nhere, we chose to nest it under the path of a single post. The motivation behind this is that, \\nsince those comments are designed to be nested under posts, it doesn\\'t really make sense \\nto consider them as single entities that you can work with independently.\\nSince we have the post ID in the path parameter, you can reuse our get_post_or_404 \\ndependency to retrieve the post.\\nThen, we trigger an update_one query; this time, using the $push operation. This is \\na useful operator for adding elements to a list attribute. Operators to remove elements \\nfrom a list are also available. Y ou can find a description of every update operator in the \\nofficial documentation at https://docs.mongodb.com/manual/reference/\\noperator/update/.\\nAnd that\\'s it! In fact, we don\\'t even have to modify the rest of our code. Because the \\ncomments are included in the whole document, we\\'ll always retrieve them when querying \\nfor a post in the database. Besides, our PostDB model now expects a comments \\nattribute, so Pydantic will take care of serializing them automatically.\\nThat concludes this part regarding MongoDB. Y ou\\'ve seen that its integration into  \\na FastAPI application is very quick, especially because of its very flexible schema.\\nSummary\\nCongratulations! Y ou\\'ve reached another big milestone in your mastering of building  \\na REST API with FastAPI. As you know, databases are an essential part of every system; \\nthey allow you to save data in a structured way and retrieve it precisely and reliably \\nthanks to powerful query languages. Y ou are now able to leverage their power in FastAPI, \\nwhether they are relational databases or document-oriented databases. Additionally, \\nyou\\'ve seen the differences between working with and without an ORM to manage \\nrelational databases, and you have also learned about the importance of a good migration \\nsystem when working with such databases.\\nSerious things can now happen: users can send and retrieve data to and from your system. \\nHowever, this poses a new challenge to tackle. This data needs to be protected so that  \\nit can remain private and secure. This is exactly what we\\'ll discuss in our next chapter: \\nhow to authenticate users and set up FastAPI for maximum security.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 229, 'page_label': '211'}, page_content=\"7\\nManaging \\nAuthentication and \\nSecurity in FastAPI\\nMost of the time, you don't want everyone on the internet to have access to your API, \\nwithout any restrictions on the data they can create or read. That's why you'll need to \\nat least protect your application with a private token or have a proper authentication \\nsystem to manage rights per user. In this chapter, we'll see that FastAPI provides security \\ndependencies to help us retrieve credentials following different standards that are directly \\nintegrated into the automatic documentation. We'll also build a basic user registration and \\nauthentication system to secure our API endpoints. \\nFinally, we'll cover security challenges you must tackle when you want to call your API \\nfrom a web application in a browser – in particular, CORS and CSRF attacks.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 230, 'page_label': '212'}, page_content=\"212     Managing Authentication and Security in FastAPI\\nIn this chapter, we're going to cover the following main topics:\\n• Security dependencies in FastAPI\\n• Retrieving a user and generating an access token\\n• Securing API endpoints for authenticated users\\n• Securing endpoints with access tokens\\n• Configuring CORS and protecting against CSRF attacks\\nTechnical requirements\\nFor this chapter, you'll need the Python virtual environment that we set up in Chapter 1, \\nPython Development Environment Setup.\\nY ou can find all the code examples for this chapter in this book's dedicated GitHub \\nrepository: https://github.com/PacktPublishing/Building-Data-\\nScience-Applications-with-FastAPI/tree/main/chapter7.\\nSecurity dependencies in FastAPI\\nTo protect a REST API and, more generally, HTTP endpoints, lots of standards have been \\nproposed. Here is a non-exhaustive list of the most common ones:\\n• Basic HTTP authentication: In this scheme, user credentials (usually, an identifier \\nsuch as an email address and password) are put into an HTTP header called \\nAuthorization. The value consists of the Basic keyword, followed by the user \\ncredentials encoded in Base64. This is a very simple scheme to implement but not \\nvery secure since the password appears in every request.\\n• Cookies: Cookies are a useful way to store static data on the client side, usually on \\nweb browsers, that is sent in each request to the server. Typically, a cookie can contain \\na session token that can be verified by the server and linked to a specific user.\\n• Tokens in the Authorization header: Probably the most used header in a REST \\nAPI context, this simply consists of sending a token in an HTTP Authorization \\nheader. The token is often prefixed by a method keyword, such as Bearer. On the \\nserver side, this token can be verified and linked to a specific user.\\nEach standard has its pros and cons and is suitable for a specific use case.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 231, 'page_label': '213'}, page_content='Security dependencies in FastAPI     213\\nAs you already know, FastAPI is mainly about dependency injection and callables that are \\nautomatically detected and called at runtime. Authentication methods are no exception: \\nFastAPI provides most of them out of the box as security dependencies.\\nFirst, let\\'s learn how to retrieve an access token in an arbitrary header. For this, we can use \\nthe ApiKeyHeader dependency, as shown in the following example:\\nchapter7_api_key_header.py\\nfrom fastapi import Depends, FastAPI, HTTPException, status\\nfrom fastapi.params import Depends\\nfrom fastapi.security import APIKeyHeader\\nAPI_TOKEN = \"SECRET_API_TOKEN\"\\napp = FastAPI()\\napi_key_header = APIKeyHeader(name=\"Token\")\\n@app.get(\"/protected-route\")\\nasync def protected_route(token: str = Depends(api_key_\\nheader)):\\n\\xa0\\xa0\\xa0\\xa0if token != API_TOKEN:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=status.HTTP_403_\\nFORBIDDEN)\\n\\xa0\\xa0\\xa0\\xa0return {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/chapter7_api_key_\\nheader.py\\nIn this simple example, we hardcoded a token, API_TOKEN, and checked if the one that \\nwas passed in the header is equal to this token, before authorizing the endpoint to be \\ncalled. To do this, we used the APIKeyHeader security dependency, which is designed \\nto retrieve a value from a header. It\\'s a class dependency that can be instantiated with \\narguments. It also accepts the name argument, which will be the name of the header it\\'ll \\nlook for.\\nThen, in our endpoint, we injected this dependency to get the token\\'s value. If it\\'s equal to \\nour token constant, we proceed with the endpoint logic. Otherwise, we raise a 403 error.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 232, 'page_label': '214'}, page_content=\"214     Managing Authentication and Security in FastAPI\\nOur example from the Path, router, and global dependencies section of Chapter 5, \\nDependency Injections in FastAPI, is not very different from this one. We are simply \\nretrieving a value from an arbitrary header and making an equality check. So, why bother \\nwith a dedicated dependency? There are two reasons:\\n• First, the logic to check if the header exists and retrieve its value is included in \\nAPIKeyHeader. When you reach the endpoint, you are sure that a token value  \\nwas retrieved; otherwise, a 403 error will be thrown.\\n• The second, and probably most important thing, is that it's detected by the \\nOpenAPI schema and included in its interactive documentation. This means that \\nendpoints, including this dependency will display a lock icon, showing that it's a \\nprotected endpoint. Furthermore, you'll have access to an interface to input your \\ntoken, as shown in the following screenshot. The token will then be automatically \\nincluded in the requests you are making from the documentation:\\nFigure 7.1 – Token authorization in interactive documentation\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 233, 'page_label': '215'}, page_content='Security dependencies in FastAPI     215\\nOf course, you can wrap the logic that checks the token value in its own dependency to \\nreuse it across your endpoints, as shown in the following example:\\nchapter7_api_key_header_dependency.py\\nasync def api_token(token: str = \\nDepends(APIKeyHeader(name=\"Token\"))):\\n\\xa0\\xa0\\xa0\\xa0if token != API_TOKEN:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=status.HTTP_403_\\nFORBIDDEN)\\n@app.get(\"/protected-route\", dependencies=[Depends(api_token)])\\nasync def protected_route():\\n\\xa0\\xa0\\xa0\\xa0return {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/chapter7_api_key_\\nheader_dependency.py\\nRemember that these kinds of dependencies are very good candidates to be used as router \\nor global dependencies to protect whole sets of routes, as we saw in Chapter 5, Dependency \\nInjections in FastAPI.\\nThis is a very basic example of adding authorization to your API. In this example, we don\\'t \\nhave any user management; we are only checking that a token corresponds to a constant \\nvalue. While it could be useful for private microservices that are not intended to be called \\nby end users, don\\'t consider this approach as a very secure one. First, make sure your API \\nis always served using HTTPS to ensure your token is not exposed in the headers.\\nThen, if it\\'s a private microservice, you should also consider not exposing it publicly on \\nthe internet and making sure only trusted servers can call it. Since you don\\'t need users to \\nmake requests to this service, it\\'s much safer than a simple token key that could be stolen.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 234, 'page_label': '216'}, page_content=\"216     Managing Authentication and Security in FastAPI\\nOf course, most of the time, you'll want to authenticate real users with their own \\nindividual access token so that they can access their own data. Y ou have probably already \\nused a service that implements this very typical pattern:\\n• First, you must register an account on this service, usually by providing your email \\naddress and a password.\\n• Next, you can log into the service using the same email address and password.  \\nThe service checks if the email address exists and that the password is valid.\\n• In exchange, the service provides you with a session token that can be used on \\nsubsequent requests to authenticate yourself. This way, you don't have to provide \\nyour email address and password on each request, which would be annoying and \\ndangerous. Usually, such session tokens have a limited lifetime, which means you'll \\nhave to log in again after some time. This mitigates any security risks if the session \\ntoken is stolen.\\nIn the next section, you'll learn how to implement such a system.\\nStoring a user and their password securely in \\na database\\nStoring a user entity in a database is no different from storing any other entity, and you \\ncan implement this in the same way as we saw in Chapter 6, Databases and Asynchronous \\nORMs. The only thing you must be extremely cautious about is password storage. Y ou \\nmust not store the password as plain text in your database. Why? If, unfortunately, a \\nmalicious person manages to get into your database, they'll be able to get the passwords \\nof all your users. Since many people use the same password several times, the security of \\ntheir accounts on other applications and websites would be seriously compromised.\\nTo avoid a disaster like this, we can apply cryptographic hash functions to the password. \\nThe goal of those functions is to transform the password string into a hash value. They are \\ndesigned to make it near impossible to retrieve the original data from the hash. Hence, \\neven if your database is compromised, the passwords are still safe.\\nWhen users try to log in, we simply compute the hash of the password they input and \\ncompare it with the hash we have in our database. If they match, this means it's the  \\nright password.\\nNow, let's learn how to implement such a system with FastAPI and Tortoise ORM.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 235, 'page_label': '217'}, page_content=\"Storing a user and their password securely in a database     217\\nCreating models and tables\\nThe first thing we must do is create the Pydantic models, as shown in the  \\nfollowing example:\\nmodels.py\\nclass UserBase(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0email: EmailStr\\n\\xa0\\xa0\\xa0\\xa0class Config:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0orm_mode = True\\nclass UserCreate(UserBase):\\n\\xa0\\xa0\\xa0\\xa0password: str\\nclass User(UserBase):\\n\\xa0\\xa0\\xa0\\xa0id: int\\nclass UserDB(User):\\n\\xa0\\xa0\\xa0\\xa0hashed_password: str \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\nmodels.py\\nTo keep this example simple, we're only considering the email address and password in \\nour user model. As you can see, there is a major difference between UserCreate and \\nUserDB: the former accepts the plain text password we'll hash during registration, while \\nthe second will only keep the hashed password in the database.\\nNow, we can define the corresponding Tortoise model, as shown in the following example:\\nmodels.py\\nclass UserTortoise(Model):\\n\\xa0\\xa0\\xa0\\xa0id = fields.IntField(pk=True, generated=True)\\n\\xa0\\xa0\\xa0\\xa0email = fields.CharField(index=True, unique=True, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0null=False, max_length=255)\\n\\xa0\\xa0\\xa0\\xa0hashed_password = fields.CharField(null=False,\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 236, 'page_label': '218'}, page_content='218     Managing Authentication and Security in FastAPI\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0max_length=255)\\n\\xa0\\xa0\\xa0\\xa0class Meta:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0table = \"users\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\nmodels.py\\nNote that we added a unique constraint to the email column to ensure we can\\'t have \\nduplicate emails in our database.\\nHashing passwords\\nBefore we look at the registration endpoint, let\\'s implement some important utility \\nfunctions for hashing passwords. Fortunately, libraries exist that provide the most secure \\nand efficient algorithms for this task. Here, we\\'ll use passlib. Y ou can install it with  \\nits optional bcrypt dependency, which is one of the safest hash functions at the time  \\nof writing:\\n$ pip install \\'passlib[bcrypt]\\'\\nNow, we\\'ll just instantiate the passlib classes and wrap some of their functions to make \\nour lives easier:\\npassword.py\\nfrom passlib.context import CryptContext\\npwd_context = CryptContext(schemes=[\"bcrypt\"], \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0deprecated=\"auto\")\\ndef get_password_hash(password: str) -> str:\\n\\xa0\\xa0\\xa0\\xa0return pwd_context.hash(password)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\npassword.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 237, 'page_label': '219'}, page_content='Storing a user and their password securely in a database     219\\nCryptContext is a very useful class since it allows us to work with different hash \\nalgorithms. If, one day, a better algorithm than bcrypt emerges, we can just add it to our \\nallowed schemes. New passwords will be hashed using the new algorithm, but existing \\npasswords will still be recognized (and optionally upgraded to the new algorithm).\\nImplementing registration routes\\nNow, we have all the elements to create a proper registration route. Once again, it\\'ll be \\nvery similar to what we saw earlier. The only thing we must remember is to hash the \\npassword before inserting it into our database.\\nLet\\'s look at the implementation:\\napp.py\\n@app.post(\"/register\", status_code=status.HTTP_201_CREATED)\\nasync def register(user: UserCreate) -> User:\\n\\xa0\\xa0\\xa0\\xa0hashed_password = get_password_hash(user.password)\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0user_tortoise = await UserTortoise.create(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0**user.dict(), hashed_password=hashed_password\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0except IntegrityError:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0status_code=status.HTTP_400_BAD_REQUEST, \\ndetail=\"Email already exists\"\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0return User.from_orm(user_tortoise)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\napp.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 238, 'page_label': '220'}, page_content=\"220     Managing Authentication and Security in FastAPI\\nAs you can see, we are calling get_password_hash on the input password before \\ninserting the user into the database thanks to Tortoise. Note that we are catching a \\npossible IntegrityError exception, which means we're trying to insert an email  \\nthat already exists.\\nAlso, notice that we took care to return the user with the User model, not the UserDB \\nmodel. By doing this, we're ensuring that hashed_password is not part of the output. \\nEven hashed, it's generally not advised to leak it into the API responses.\\nGreat! We now have a proper user model and users can create a new account with our \\nAPI. The next step is to allow them to log in and give them an access token. \\nRetrieving a user and generating an access \\ntoken\\nAfter successful registration, the next step is being able to log in: the user will send  \\ntheir credentials and receive an authentication token to access the API. In this section, \\nwe'll implement the endpoint that allows this. Basically, we'll get the credentials from  \\nthe request payload, retrieve the user with the given email, and verify their password.  \\nIf the user exists and their password is valid, we'll generate an access token and return  \\nit in the response.\\nImplementing a database access token\\nFirst, let's think about the nature of this access token. It should be a data string that \\nuniquely identifies a user that is impossible to forge by a malicious third party. In this \\nexample, we will take a simple but reliable approach: we'll generate a random string and \\nstore it in a dedicated table in our database, with a foreign key referring to the user.\\nThis way, when an authenticated request arrives, we simply have to check whether it exists \\nin the database and look for the corresponding user. The advantage of this approach is that \\ntokens are centralized and can easily be invalidated if they are compromised; we only need \\nto delete them from the database.\\nThe first step is to implement the Pydantic and Tortoise models for this new entity. Let's \\nhave a look at the Pydantic model first:\\nmodels.py\\nclass AccessToken(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0user_id: int\\n\\xa0\\xa0\\xa0\\xa0access_token: str = Field(default_factory=generate_token)\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 239, 'page_label': '221'}, page_content='Retrieving a user and generating an access token     221\\n\\xa0\\xa0\\xa0\\xa0expiration_date: datetime = Field(default_factory=get_\\nexpiration_date)\\n\\xa0\\xa0\\xa0\\xa0class Config:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0orm_mode = True\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\nmodels.py\\nHere, we have three fields:\\n•  user_id, which will let us identify the user that corresponds to this token.\\n• access_token, the string that will be passed in the requests to authenticate them. \\nNotice that we defined the generate_token function as the default factory; \\nit\\'s a simple function living in password.py that generates a random secure \\npassphrase. Under the hood, it relies on the standard secrets module.\\n• expiration_date, which is the date and time when the access token won\\'t be \\nvalid anymore. It\\'s always a good idea to make access tokens expire to mitigate the \\nrisk if they are stolen. Here, the get_expiration_date factory sets a default \\nvalidity of 24 hours.\\nNow, let\\'s have a look at the corresponding Tortoise model:\\nmodels.py\\nclass AccessTokenTortoise(Model):\\n\\xa0\\xa0\\xa0\\xa0access_token = fields.CharField(pk=True, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0max_length=255)\\n\\xa0\\xa0\\xa0\\xa0user = fields.ForeignKeyField(\"models.UserTortoise\", \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0null=False)\\n\\xa0\\xa0\\xa0\\xa0expiration_date = fields.DatetimeField(null=False)\\n\\xa0\\xa0\\xa0\\xa0class Meta:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0table = \"access_tokens\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\nmodels.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 240, 'page_label': '222'}, page_content='222     Managing Authentication and Security in FastAPI\\nThe implementation here is quite straightforward. Notice that we chose to directly use \\naccess_token as a primary key.\\nImplementing a login endpoint\\nNow, let\\'s think about the login endpoint. Its goal is to take credentials in the request \\npayload, retrieve the corresponding user, check the password, and generate a new access \\ntoken. Its implementation is quite straightforward, apart from one thing: the model that\\'s \\nused to handle the request. Y ou\\'ll see why thanks to the following example:\\napp.py\\n@app.post(\"/token\")\\nasync def create_token(\\n\\xa0\\xa0\\xa0\\xa0form_data: OAuth2PasswordRequestForm = \\nDepends(OAuth2PasswordRequestForm),\\n):\\n\\xa0\\xa0\\xa0\\xa0email = form_data.username\\n\\xa0\\xa0\\xa0\\xa0password = form_data.password\\n\\xa0\\xa0\\xa0\\xa0user = await authenticate(email, password)\\n\\xa0\\xa0\\xa0\\xa0if not user:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=status.HTTP_401_\\nUNAUTHORIZED)\\n\\xa0\\xa0\\xa0\\xa0token = await create_access_token(user)\\n\\xa0\\xa0\\xa0\\xa0return {\"access_token\": token.access_token, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"token_type\": \"bearer\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\napp.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 241, 'page_label': '223'}, page_content=\"Retrieving a user and generating an access token     223\\nAs you can see, we retrieve the request data thanks to the \\nOAuth2PasswordRequestForm module, which is provided by FastAPI in its security \\nmodule. It expects several fields, especially username and password, in a form \\nencoding rather than JSON.\\nWhy do we use this class? The main benefit of using this class is that it's completely \\nintegrated into the OpenAPI schema. This means that the interactive documentation will \\nbe able to automatically detect it and present a proper authentication form behind the \\nAuthorize button, as shown in the following screenshot:\\nFigure 7.2 – OAuth2 authorization in interactive documentation\\nBut that's not all: it will be able to automatically retrieve the returned access token and \\nset the proper authorization header in subsequent requests. The authentication process is \\nhandled transparently by the interactive documentation.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 242, 'page_label': '224'}, page_content=\"224     Managing Authentication and Security in FastAPI\\nThis class follows the OAuth2 protocol, which means you also have fields for the client \\nID and secret. We won't learn how to implement the complete OAuth2 protocol here, but \\nnote that FastAPI provides all the tools needed to do so properly. For our project, we'll \\njust stick with a username and a password. Notice that, following the protocol, the field is \\nnamed username, regardless of whether we are using an email address to identify the user. \\nThis isn't a big deal; we just have to remember it while retrieving it.\\nThe rest of the path operation function is quite simple: first, we try to retrieve a user \\nfrom this email and password. If no corresponding user is found, we raise a 401 \\nerror. Otherwise, we generate a new access token before returning it. Notice that the \\nresponse structure also includes the token_type property. This allows the interactive \\ndocumentation to automatically sets the authorization headers.\\nIn the following example, we'll look at the implementation of the authenticate and \\ncreate_access_token functions. We won't go into too much detail here as they are \\nquite simple:\\nauthentication.py\\nasync def authenticate(email: str, password: str) -> \\nOptional[UserDB]:\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0user = await UserTortoise.get(email=email)\\n\\xa0\\xa0\\xa0\\xa0except DoesNotExist:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return None\\n\\xa0\\xa0\\xa0\\xa0if not verify_password(password, user.hashed_password):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return None\\n\\xa0\\xa0\\xa0\\xa0return UserDB.from_orm(user)\\nasync def create_access_token(user: UserDB) -> AccessToken:\\n\\xa0\\xa0\\xa0\\xa0access_token = AccessToken(user_id=user.id)\\n\\xa0\\xa0\\xa0\\xa0access_token_tortoise = await AccessTokenTortoise.\\ncreate(**access_token.dict())\\n\\xa0\\xa0\\xa0\\xa0return AccessToken.from_orm(access_token_tortoise)\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 243, 'page_label': '225'}, page_content='Securing endpoints with access tokens     225\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\nauthentication.py\\nNotice that we defined a function called verify_password to check the validity of the \\npassword. Once again, it uses passlib under the hood, which takes care of comparing \\nthe hashes of the passwords.\\nPassword Hash Upgrade\\nTo keep this example simple, we implemented a simple password comparison. \\nUsually, it\\'s good practice to implement a mechanism to upgrade the password \\nhash at this stage. Imagine that a new and more robust hash algorithm has \\nbeen introduced. We can take this opportunity to hash the password with \\nthis new algorithm and store it in a database. passlib includes a function \\nfor verifying and upgrading the hash with one operation. Y ou can learn \\nmore about this in the following documentation: https://passlib.\\nreadthedocs.io/en/stable/narr/context-tutorial.\\nhtml#integrating-hash-migration.\\nWe\\'ve almost achieved our goal! Users can now log in and get a new access token. All we \\nneed to do now is implement a dependency to retrieve the Authorization header and \\nverify this token!\\nSecuring endpoints with access tokens\\nPreviously, we learned how to implement a simple dependency to protect an endpoint with \\na header. Here, we\\'ll also retrieve a token from a request header, but then, we\\'ll have to check \\nthe database to see if it\\'s valid. If it is, we\\'ll be able to return the corresponding user.\\nLet\\'s see what our dependency looks like:\\napp.py\\nasync def get_current_user(\\n\\xa0\\xa0\\xa0\\xa0token: str = Depends(OAuth2PasswordBearer(tokenUrl=\"/\\ntoken\")),\\n) -> UserTortoise:\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0access_token: AccessTokenTortoise = await \\nAccessTokenTortoise.get('),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 244, 'page_label': '226'}, page_content='226     Managing Authentication and Security in FastAPI\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0access_token=token, expiration_date__gte=timezone.\\nnow()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0).prefetch_related(\"user\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return cast(UserTortoise, access_token.user)\\n\\xa0\\xa0\\xa0\\xa0except DoesNotExist:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=status.HTTP_401_\\nUNAUTHORIZED)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\napp.py\\nThe first thing to notice is that we used the OAuth2PasswordBearer dependency from \\nFastAPI. It goes hand in hand with OAuth2PasswordRequestForm, which we saw in \\nthe previous section. It not only checks for the access token in the Authorization header, \\nbut it also informs the OpenAPI schema that the endpoint to get a fresh token is /token. \\nThis is the purpose of the tokenUrl argument. This is how the automatic documentation \\ncan automatically call the access token endpoint in the login form we saw earlier.\\nThen we performed a database query with Tortoise. We applied two clauses: one to \\nmatch the token we got and another to ensure that the expiration date is in the future. \\nThe __gte syntax is a filter modifier: it allows us to specify the comparison operator to \\napply when comparing values. Here, gte means \"greater than or equal to.\" Y ou can find \\na list of every filter that\\'s available in Tortoise in the official documentation: https://\\ntortoise-orm.readthedocs.io/en/latest/query.html#filtering. \\nNotice that we also prefetched the related user so that we can directly return it. However, \\nif no corresponding record is found in the database, we raise a 401 error.\\nAnd that\\'s it! Our whole authentication system is complete. Now, we can protect our \\nendpoints simply by injecting this dependency. We even have access to the user data \\nso that we can tailor the response according to the current user. Y ou can see this in the \\nfollowing example:\\napp.py\\n@app.get(\"/protected-route\", response_model=User)\\nasync def protected_route(user: UserDB = Depends(get_current_\\nuser)):\\n\\xa0\\xa0\\xa0\\xa0return User.from_orm(user)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 245, 'page_label': '227'}, page_content=\"Configuring CORS and protecting against CSRF attacks     227\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/authentication/\\napp.py\\nWith that, you've learned how to implement a whole registration and authentication \\nsystem from scratch. We voluntarily kept it simple to focus on the most important points, \\nbut it's a good base you can expand.\\nThe patterns we showed here are good candidates for a REST API, which is called \\nexternally by other client programs. However, you may wish to call your API from a very \\ncommon piece of software: the browser. In this case, there are some additional security \\nconsiderations to take care of.\\nConfiguring CORS and protecting against CSRF \\nattacks\\nNowadays, lot of software are designed to be used in a browser through an interface built \\nwith HTML, CSS, and JavaScript. Traditionally, web servers were responsible for handling \\nbrowser requests and returning an HTML response, ready to be shown. This is a common \\nuse case for frameworks such as Django.\\nFor a few years now, there has been a shift in that pattern. With the emergence of \\nJavaScript frameworks, such as Angular, React, and Vue, we tend to have a clear \\nseparation between the frontend, a highly interactive user interface powered by JavaScript, \\nand the backend. Thus, those backends are now only responsible for data storage and \\nretrieving and executing business logic. This is a task that REST APIs are very good at! \\nFrom the JavaScript code, the user interface can then just spawn requests to your API and \\nhandle the result to present it.\\nHowever, we must still handle authentication: we want our user to be able to log in on \\nthe frontend application and be able to make authenticated requests to the API. While an \\nAuthorization header, as we've seen so far, could work, there is a better way to handle \\nauthentication when working in browsers: cookies!\\nCookies are designed to store user information in browser memory and are sent \\nautomatically in every request made to your server. They have been supported for years, \\nand browsers integrate lots of mechanisms to make them safe and reliable.\\nHowever, this comes with some security challenges. Websites are very common targets for \\nhackers and lots of attacks have emerged over the years.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 246, 'page_label': '228'}, page_content=\"228     Managing Authentication and Security in FastAPI\\nOne of the most typical is Cross-Site Request Forgery (CSRF). In this scenario, an \\nattacker on another website tries to trick a user who is currently authenticated with your \\napplication to perform a request on your server. Since browsers tend to send cookies with \\nevery request, your server wouldn't be able to tell that the request was actually forged. \\nSince it's the users themselves who unintentionally launched the malicious request, these \\nkinds of attacks don't aim to steal data but execute operations that change the state of the \\napplication, such as changing an email address or making a money transfer.\\nObviously, we should be prepared for these kinds of risks and have measures in place to \\nmitigate them.\\nUnderstanding CORS and configuring it in FastAPI\\nWhen you have a clearly separated frontend application and a REST API backend, they \\ntypically are not served from the same sub-domain. For example, the frontend may be \\navailable from www.myapplication.com, while the REST API may be available from \\napi.myapplication.com. As we mentioned in the introduction, we would like to \\nmake requests to this API from our frontend application, in JavaScript.\\nHowever, browsers don't allow cross-origin HTTP requests, meaning domain A can't \\nmake requests to domain B. This follows what is called a same-origin policy. This is a \\ngood thing in general as it's the first barrier to preventing CSRF attacks.\\nTo experience this behavior, we'll run a simple example. In our example repository, the \\nchapter7/cors folder contains a FastAPI app called app_without_cors.py and \\na simple HTML file called index.html that contains some JavaScript for performing \\nHTTP requests.\\nFirst, let's run the FastAPI application using the usual uvicorn command:\\n$ uvicorn chapter7.cors.app_without_cors:app\\nThis will launch the FastAPI application on port 8000 by default. On another terminal, \\nwe'll serve the HTML file using the built-in Python HTTP server. It's a simple server, \\nbut it's ideal for quickly serving static files. We can launch it on port 9000 thanks to the \\nfollowing command:\\n$ python -m http.server --directory chapter7/cors 9000\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 247, 'page_label': '229'}, page_content=\"Configuring CORS and protecting against CSRF attacks     229\\nStarting Several Terminals\\nOn Linux and macOS, you should be able to simply start a new terminal by \\ncreating a new window or tab. On Windows and WSL, you can also have \\nseveral tabs if you're using the Windows terminal application: https://\\nwww.microsoft.com/en-us/p/windows-terminal/9n0dx20\\nhk701?activetab=pivot:overviewtab.\\nOtherwise, you can simply click on the Ubuntu shortcut in your Start menu to \\nstart another terminal.\\nWe now have two running servers – one on localhost:8000 and one on \\nlocalhost:9000. Strictly speaking, since they are on different ports, they are of \\ndifferent origins; so, it's a good setup to try out cross-origin HTTP requests.\\nIn your browser, go to http://localhost:9000. Y ou'll see the simple application \\nimplemented in index.html, as shown in the following screenshot:\\nFigure 7.3 – Simple application for trying out CORS policies\\nThere are two buttons that initiate GET and POST requests to our FastAPI application  \\non port 8000. If you click on either of those, you'll have a message in the error area stating \\nFailed to fetch. If you look at the browser console in the development tools section,  \\nyou'll see that the request has failed because there isn't a CORS policy, as shown in the \\nfollowing screenshot. That's what we wanted – by default, browsers block cross-origin \\nHTTP requests:\\nFigure 7.4 – CORS error in a browser console\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 248, 'page_label': '230'}, page_content=\"230     Managing Authentication and Security in FastAPI\\nHowever, if you look at the terminal running the FastAPI application, you'll see an output \\nsimilar to the following:\\nFigure 7.5 – Uvicorn output when performing simple requests\\nClearly, both the GET and POST requests have been received and processed: we even \\nreturned a 200 status. So, what does this mean? In this case, the browser does send the \\nrequest to the server. The lack of a CORS policy only forbids it to read the response; the \\nrequest is still executed.\\nIt happens for requests that the browser considers as simple requests. Simply put, simple \\nrequests are the ones using the methods GET, POST or HEAD that don't set custom \\nheaders or unusual content types. Y ou can learn more about simple requests and their \\nconditions by going to the following MDN page about CORS: https://developer.\\nmozilla.org/en-US/docs/Web/HTTP/CORS#simple_requests.\\nThis means that, for simple requests, the same-origin policy is not enough to protect us \\nagainst CSRF attacks. \\nY ou may have noticed that our simple web application has a toggle to Enable JSON \\ncontent-type. Enable it and perform the GET and POST requests again. On your FastAPI \\nterminal, you should have an output similar to the following:\\nFigure 7.6 – Uvicorn output when receiving preflight requests\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 249, 'page_label': '231'}, page_content='Configuring CORS and protecting against CSRF attacks     231\\nAs you can see, our server received two strange requests with the OPTIONS method.  \\nThis is what we call preflight requests in the context of CORS policies. Those requests \\nare initiated by the browser before it performs the actual request when it doesn\\'t consider \\nit a \"simple request.\" Here, we added the Content-Type header with a value of \\napplication/json, which is against the conditions of simple requests.\\nBy performing this preflight request, the browser expects the server to provide \\ninformation about what it is and isn\\'t allowed to do in terms of cross-origin HTTP \\nrequests. Since we\\'ve not implemented anything here, our server can\\'t provide a response \\nto this preflight request. Hence, the browser stops there and doesn\\'t proceed with the \\nactual request.\\nAnd that\\'s basically CORS: the server answers preflight queries with a set of HTTP \\nheaders that provide information to the browser about whether it\\'s allowed to make the \\nrequest or not. In that sense, CORS doesn\\'t make your application more secure, it\\'s quite \\nthe contrary: it allows to relax some rules so that a frontend application can make requests \\nto a backend living on another domain. That\\'s why it\\'s crucial to configure them properly, \\nso that they don\\'t expose you to dangerous attacks.\\nFortunately, it\\'s fairly easy to do this with FastAPI. All we need to do is import and add \\nthe CORSMiddleware class provided by Starlette. Y ou can see what it looks like in the \\nfollowing example:\\napp_with_cors.py\\napp.add_middleware(\\n\\xa0\\xa0\\xa0\\xa0CORSMiddleware,\\n\\xa0\\xa0\\xa0\\xa0allow_origins=[\"http://localhost:9000\"],\\n\\xa0\\xa0\\xa0\\xa0allow_credentials=True,\\n\\xa0\\xa0\\xa0\\xa0allow_methods=[\"*\"],\\n\\xa0\\xa0\\xa0\\xa0allow_headers=[\"*\"],\\n\\xa0\\xa0\\xa0\\xa0max_age=-1,\\xa0\\xa0# Only for the sake of the example. \\n# Remove this in your own project.\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/cors/app_with_\\ncors.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 250, 'page_label': '232'}, page_content=\"232     Managing Authentication and Security in FastAPI\\nA middleware is a special class that adds global logic to an ASGI application performing \\nthings before the request is handled by your path operation functions, and also after to \\npossibly alter the response. FastAPI provides the add_middleware method for wiring \\nsuch middleware into your application.\\nHere, CORSMiddleware will catch preflight requests sent by the browser and return the \\nappropriate response with the CORS headers corresponding to your configuration. Y ou \\ncan see that there are options to finely tune the CORS policy to your needs.\\nThe most important one is probably allow_origins, which is the list of origins allowed \\nto make requests to your API. Since our HTML application is served from http://\\nlocalhost:9000, this is what we put here in this argument. If the browser tries to \\nmake requests from any other origin, it will stop as it's not authorized to do so by  \\nCORS headers.\\nThe other interesting argument is allow_credentials. By default, browsers don't \\nsend cookies for cross-origin HTTP requests. If we wish to make authenticated requests to \\nour API, we need to allow this via this option.\\nWe can also finely tune the allowed methods and headers that are sent in the request. \\nY ou can find a complete list of arguments for this middleware in the official Starlette \\ndocumentation: https://www.starlette.io/middleware/#corsmiddleware.\\nLet's quickly talk about the max_age parameter. This parameter allows you to control the \\ncache duration of the CORS responses. Having to perform a preflight request before the \\nactual one is an expensive operation. To improve performance, browsers can cache the \\nresponse so that they don't have to do this every time. Here, we are disabling caching with \\na value of -1 to make sure you see the behavior of the browser in this example. In your \\nprojects, you can remove this argument so that you have a proper cache value.\\nNow, let's see how our web application behaves with this CORS-enabled application.  \\nStop the previous FastAPI app and run this one using the usual command:\\n$ uvicorn chapter7.cors.app_with_cors:app\\nNow, if you try to perform the requests from the HTML application, you should see a \\nworking response in each case, both with and without a JSON content type. If you look at \\nthe FastAPI terminal, you should see an output similar to the following:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 251, 'page_label': '233'}, page_content='Configuring CORS and protecting against CSRF attacks     233\\nFigure 7.7 – Uvicorn output with CORS headers\\nThe two first requests are the \"simple requests,\" which don\\'t need a preflight request \\naccording to the browser rules. Then, we can see the requests that were performed with \\nthe JSON content type enabled. Before the GET and POST requests, an OPTIONS request \\nwas performed: the preflight request!\\nThanks to this configuration, you can now make cross-origin HTTP requests between \\nyour frontend application and your backend living on another origin. Once again, it\\'s not \\nsomething that\\'ll improve the security of your application, but it allows you to make this \\nspecific scenario work while keeping it secure from the rest of the web.\\nEven if those policies can be a first layer of defense against CSRF , this doesn\\'t mitigate the \\nrisk completely. Indeed, the \"simple requests\" are still an issue: POST requests are allowed \\nand, even if the response cannot be read, it\\'s actually executed on the server.\\nNow, let\\'s learn how to implement a pattern so that we\\'re completely safe from such \\nattacks: the double-submit cookie.\\nImplementing double-submit cookies to prevent  \\nCSRF attacks\\nAs we mentioned previously, when relying on cookies to store user credentials, we are \\nexposed to CSRF attacks since browsers will automatically send the cookies to your server. \\nThis is especially true for what the browser considers \"simple requests\", which don\\'t \\nenforce the CORS policy before the request is executed. There are also other attack vectors \\ninvolving traditional HTML form submissions or even the src attribute of the image tag.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 252, 'page_label': '234'}, page_content='234     Managing Authentication and Security in FastAPI\\nFor all these reasons, we need to have another layer of security to mitigate this risk. Once \\nagain, this is only necessary if you plan to use your API from a browser application and \\nuse cookies for authentication.\\nTo help you understand this, we\\'ve built a new example application that uses a cookie \\nto store the user access token. It\\'s very similar to the one we saw at the beginning of this \\nchapter; we only modified it so that it looks for the access token in a cookie rather than in \\na header.\\nTo make this example work, you\\'ll have to install the starlette-csrf library. We\\'ll \\nexplain what it does a bit later in this section. For now, just run the following command:\\n$ pip install starlette-csrf\\nIn the following example, you can see the login endpoint that sets a cookie with the access \\ntoken value:\\napp.py\\n@app.post(\"/login\")\\nasync def login(response: Response, email: str = Form(...), \\npassword: str = Form(...)):\\n\\xa0\\xa0\\xa0\\xa0user = await authenticate(email, password)\\n\\xa0\\xa0\\xa0\\xa0if not user:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=status.HTTP_401_\\nUNAUTHORIZED)\\n\\xa0\\xa0\\xa0\\xa0token = await create_access_token(user)\\n\\xa0\\xa0\\xa0\\xa0response.set_cookie(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0TOKEN_COOKIE_NAME,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0token.access_token,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0max_age=token.max_age(),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0secure=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0httponly=True,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0samesite=\"lax\"\\n\\xa0\\xa0\\xa0\\xa0)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 253, 'page_label': '235'}, page_content='Configuring CORS and protecting against CSRF attacks     235\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/csrf/app.py\\nNotice that we used the Secure and HttpOnly flags for the resulting cookie. This \\nensures that it\\'s sent only through HTTPS connection and that its value can\\'t be read  \\nfrom JavaScript, respectively. While this is not enough to prevent every kind of attack,  \\nit\\'s crucial for such sensitive information.\\nBesides, we also set the SameSite flag to lax. It\\'s a quite recent flag that allows us to \\ncontrol how the cookie is sent in a cross-origin context. lax is the default value in most \\nbrowsers and allows the cookie to be sent to sub-domains of the cookie domain but \\nprevent it for other sites. In a sense, it\\'s designed to be the built-in and standard protection \\nagainst CSRF . However, other CSRF mitigation techniques, like the one we\\'ll implement \\nhere, are still needed currently. Indeed, older browsers that are not compatible with the \\nSameSite flag are still vulnerable.\\nNow, when checking for the authenticated user, we\\'ll just have to retrieve the token \\nfrom the cookie that was sent in the request. Once again, FastAPI provides a security \\ndependency to help with this called APIKeyCookie. Y ou can see it in the following \\nexample:\\napp.py\\nasync def get_current_user(\\n\\xa0\\xa0\\xa0\\xa0token: str = Depends(APIKeyCookie(name=TOKEN_COOKIE_NAME)),\\n) -> UserTortoise:\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0access_token: AccessTokenTortoise = await \\nAccessTokenTortoise.get(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0access_token=token, expiration_date__gte=timezone.\\nnow()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0).prefetch_related(\"user\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return cast(UserTortoise, access_token.user)\\n\\xa0\\xa0\\xa0\\xa0except DoesNotExist:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=status.HTTP_401_\\nUNAUTHORIZED)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/csrf/app.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 254, 'page_label': '236'}, page_content='236     Managing Authentication and Security in FastAPI\\nAnd that\\'s basically it! The rest of the code remains the same. Now, let\\'s implement an \\nendpoint that allows us to update the email address of the authenticated user. Y ou can see \\nthis in the following example:\\napp.py\\n@app.post(\"/me\", response_model=User)\\nasync def update_me(\\n\\xa0\\xa0\\xa0\\xa0user_update: UserUpdate, user: UserTortoise = Depends(get_\\ncurrent_user)\\n):\\n\\xa0\\xa0\\xa0\\xa0user.update_from_dict(user_update.dict(exclude_unset=True))\\n\\xa0\\xa0\\xa0\\xa0await user.save()\\n\\xa0\\xa0\\xa0\\xa0return User.from_orm(user)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/csrf/app.py\\nThe implementation is not very surprising and follows what we\\'ve seen so far. However, \\nit exposes us to a CSRF threat. As you can see, it uses the POST method. If we make a \\nrequest in the browser to this endpoint, without any special header, it will consider it as a \\nsimple request and execute it. Therefore, an attacker could change the email of a currently \\nauthenticated user, which is a major threat.\\nThis is exactly why we need CSRF protection here. In the context of a REST API, the most \\nstraightforward technique is the double submit cookie pattern. Here is how it works:\\n1. The user makes a first request with a method that\\'s considered safe. Typically,  \\nthis is a GET request.\\n2. In response, it receives a cookie containing a secret random value; that is,  \\nthe CSRF token.\\n3. When making an unsafe request, such as POST, the user will read the CSRF token \\nin the cookies and put the exact same value in a header. Since the browser also \\nsends the cookies it has in memory, the request will contain the token both in the \\ncookie and the header. That\\'s why it\\'s called double submit.\\n4. Before processing the request, the server will compare the CSRF token provided \\nin the header with the one present in the cookie. If they match, it can process the \\nrequest. Otherwise, it\\'ll throw an error.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 255, 'page_label': '237'}, page_content='Configuring CORS and protecting against CSRF attacks     237\\nThis is safe for two reasons:\\n• An attacker on a third-party website can\\'t read the cookies for a domain they don\\'t \\nown. Thus, they have no way of retrieving the CSRF token value.\\n• Adding a custom header is against the conditions of \"simple requests\". Hence, the \\nbrowser will have to make a preflight request before sending the request, enforcing \\nthe CORS policy.\\nThis is a widely used pattern that works well to prevent such risks. This is why we installed \\nstarlette-csrf at the beginning of this section: it provides a piece of middleware for \\nimplementing it.\\nWe can use it just like any other middleware, as shown in the following example:\\napp.py\\napp.add_middleware(\\n\\xa0\\xa0\\xa0\\xa0CSRFMiddleware,\\n\\xa0\\xa0\\xa0\\xa0secret=CSRF_TOKEN_SECRET,\\n\\xa0\\xa0\\xa0\\xa0sensitive_cookies={TOKEN_COOKIE_NAME},\\n\\xa0\\xa0\\xa0\\xa0cookie_domain=\"localhost\",\\n) \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter7/csrf/app.py\\nWe set several important arguments here. First, we have the secret, which should be \\na strong passphrase that\\'s used to sign the CSRF token. Then, we have sensitive_\\ncookies, which is a set of cookie names that should trigger the CSRF protection. If no \\ncookie is present or if the provided ones are not critical, we can bypass the CSRF check. \\nIt\\'s also useful if you have other authentication methods available that don\\'t rely on \\ncookies, such as Authorization headers, that are not vulnerable to CSRF . Finally, setting \\na cookie domain will allow you to retrieve the cookie containing the CSRF token, even if \\nyou are on a different subdomain; this is necessary in a cross-origin situation.\\nThat\\'s all you need to have the necessary protection ready. To ease the process of getting \\na fresh CSRF token, we implemented a minimal GET endpoint called /csrf. Its sole \\npurpose is to provide us with a simple way to set the CSRF token cookie. We can call it \\ndirectly when we load our frontend application.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 256, 'page_label': '238'}, page_content=\"238     Managing Authentication and Security in FastAPI\\nNow, let's try it out in our situation. As we did in the previous section, we'll run the \\nFastAPI application and the simple HTML application on two different ports. To do this, \\njust run the following commands:\\n$ uvicorn chapter7.csrf.app:app\\nThis will run the FastAPI application on port 8000. Now, run the following command:\\n$ python -m http.server --directory chapter7/csrf 9000\\nThe frontend application is now live on http://localhost:9000. Open it in your \\nbrowser. Y ou should see an interface similar to the following:\\nFigure 7.8 – Simple application to try out the CSRF protected API\\nHere, we've added forms to interact with our API endpoints: register, login, get \\nauthenticated user, and update them. If you try them out, they should work without \\nany issue. If you have a look at the requests that were sent in the network tab of the \\ndevelopment tools section, you'll see that the CSRF token is present in the cookies  \\nand in a header called x-csrftoken.\\nAt the top, there is a toggle to prevent the application from sending the CSRF token in the \\nheader. If you disable it, you'll see that all POST operations will result in an error.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 257, 'page_label': '239'}, page_content=\"Summary     239\\nGreat! We are now safe from CSRF attacks! Most of the work here is done by the \\nmiddleware, but it's interesting to understand how it works under the hood and how \\nit protects your application. Bear in mind, however, that it comes with a drawback: it \\nwill break the interactive documentation. Indeed, it's not designed to retrieve the CSRF \\ntoken from the cookie and put it in the headers in each request. Unless you plan of \\nauthenticating in another way (through a token in a header, for example), you won't be \\nable to directly call your endpoints in the documentation.\\nSummary\\nThat's all for this chapter, which covered authentication and security in FastAPI. We saw \\nthat implementing a basic authentication system is quite easy thanks to the tools provided \\nby FastAPI. We've shown you one way to do this, but there are plenty of other good \\npatterns out there to tackle this challenge. However, when working on this matter, always \\nkeep security in mind and be sure that you don't expose your application and your users' \\ndata to dangerous threats. In particular, you've seen that CSRF attacks have to be taken \\ncare of when designing a REST API that will be used in a browser application. A good \\nsource to understand all the security risks involved in a web application is the OW ASP \\nCheat Sheet Series: https://cheatsheetseries.owasp.org.\\nWith that, we've covered most of the important subjects concerning FastAPI application \\ndevelopment. In the next chapter, we'll learn how to work with a recent technology that's \\nintegrated with FastAPI that allows to have real time two-way communication between \\nthe client and the server: websockets.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 258, 'page_label': '240'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 259, 'page_label': '241'}, page_content=\"8\\nDefining \\nWebSockets for  \\nTwo-Way Interactive \\nCommunication in \\nFastAPI\\nThe HyperText Transfer Protocol (HTTP) is a simple yet powerful technique to send or \\nreceive data to and from a server. As we've seen, the principles of request and response \\nare at the core of this protocol: when developing our application programming interface \\n(API), our goal is to process the incoming request and build a response for the client. \\nThus, in order to get data from the server, the client always has to initiate a request first. \\nIn some contexts, however, this may not be very convenient. Imagine a typical chat \\napplication: when a user receives a new message, we would like them to be notified \\nimmediately by the server. Working only with HTTP , we would have to make requests \\nevery second to check if new messages have arrived, which would be a massive waste of \\nresources. This is why a new protocol has emerged: WebSocket. The goal of this protocol \\nis to open a communication channel between a client and a server so that they can \\nexchange data in real time, in both directions.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 260, 'page_label': '242'}, page_content=\"242     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nIn this chapter, we're going to cover the following main topics:\\n• Understanding the principles of two-way communication with WebSockets\\n• Creating a WebSocket with FastAPI\\n• Handling multiple WebSocket connections and broadcasting messages\\nTechnical requirements\\nY ou'll need a Python virtual environment, as we set up in Chapter 1, Python Development \\nEnvironment Setup.\\nFor the Handling multiple WebSocket connections and broadcasting messages section, you'll \\nneed a running Redis server on your local computer. The easiest way is to run it as a Docker \\ncontainer. If you've never used Docker before, we recommend you read the Getting started \\ntutorial in the official documentation at https://docs.docker.com/get-started/. \\nOnce done, you'll be able to run a Redis server with this simple command:\\n$ docker run -d --name fastapi-redis -p 6379:6379 redis\\nThis will make it available on your local computer on port 6379.\\nY ou'll find all the code examples of this chapter in the dedicated GitHub repository \\nat https://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter8.\\nUnderstanding the principles of two-way \\ncommunication with WebSockets\\nY ou have probably noticed that the name WebSockets is a direct reference to the \\ntraditional concept of sockets in Unix systems. While technically unrelated, they achieve \\nthe same goal: to open a communication channel between two applications. As we said in \\nthe introduction, HTTP works only on a request-response principle, which makes the \\nimplementation of applications that need real-time communication between the client \\nand the server difficult and inefficient.\\nWebSockets try to solve that by opening a full-duplex communication channel, meaning \\nthat messages can be sent in both directions and possibly at the same time. Once the \\nchannel is opened, the server can send messages to the client without having to wait for  \\na request from the client.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 261, 'page_label': '243'}, page_content='Creating a WebSocket with FastAPI     243\\nEven if HTTP and WebSocket are different protocols, WebSockets have been designed \\nto work with HTTP . Indeed, when opening a WebSocket, the connection is first \\ninitiated using an HTTP request and then upgraded to a WebSocket tunnel. This makes \\nit compatible out of the box with traditional 80 and 443 ports, which is extremely \\nconvenient because we can easily add this feature over existing web servers without the \\nneed for an extra process.\\nWebSockets also share another similarity with HTTP: Uniform Resource Identifiers \\n(URIs). As with HTTP , WebSockets are identified through classic URIs, with a host,  \\na path, and query parameters. Furthermore, we also have two schemes: ws (WebSocket) \\nfor unsecure connections and wss (WebSocket Secure) for Secure Sockets Layer/\\nTransport Layer Security (SSL/TLS)-encrypted connections.\\nFinally, this protocol is nowadays well supported in browsers, and opening a connection \\nwith a server involves just a few lines of JavaScript, as we\\'ll see in this chapter.\\nHowever, handling this two-way communication channel is quite different from handling \\ntraditional HTTP requests. Since things happen in real time and in both directions, \\nwe\\'ll see that we have to think differently from what we are used to. In FastAPI, the \\nasynchronous nature of the WebSocket implementation will greatly help us in finding our \\nway through that.\\nCreating a WebSocket with FastAPI\\nThanks to Starlette, FastAPI has built-in support to serve WebSockets. As we\\'ll see, \\ndefining a WebSocket endpoint is quick and easy, and we\\'ll be able to get started in \\nminutes. However, things will get more complex as we try to add more features to our \\nendpoint logic. Let\\'s start simple, with a WebSocket that waits for messages and simply \\nechoes them back.\\nIn the following example, you\\'ll see the implementation of such a simple case:\\napp.py\\nfrom fastapi import FastAPI, WebSocket\\nfrom starlette.websockets import WebSocketDisconnect\\n \\napp = FastAPI()\\n \\n \\n@app.websocket(\"/ws\")'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 262, 'page_label': '244'}, page_content='244     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nasync def websocket_endpoint(websocket: WebSocket):\\n\\xa0\\xa0\\xa0\\xa0await websocket.accept()\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while True:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = await websocket.receive_text()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.send_text(f\"Message text was: \\n{data}\")\\n\\xa0\\xa0\\xa0\\xa0except WebSocketDisconnect:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/echo/app.py\\nThe code is quite understandable by itself, but let\\'s focus on the important parts that differ \\nfrom classic HTTP endpoints.\\nFirst of all, you see that FastAPI provides a special websocket decorator to create a \\nWebSocket endpoint. As for regular endpoints, it takes as an argument the path at which \\nit\\'ll be available. However, other arguments not making sense in this context, such as the \\nstatus code or response model, are not available.\\nThen, in the path operation function, we can inject a WebSocket object, which will \\nprovide us all the methods to work with the WebSocket, as we\\'ll see.\\nThe first method we are calling in the implementation is accept. This method should be \\ncalled first as it tells the client that we agree to open the tunnel.\\nAfter that, you see that we start an infinite loop. That\\'s the main difference with an HTTP \\nendpoint: since we are opening a communication channel, it\\'ll remain open until the client \\nor the server decides to close it. While it\\'s open, they can exchange as many messages as \\nthey need, hence the infinite loop is here to keep it open and repeat the logic until the \\ntunnel is closed.\\nInside the loop, we make a first call to the receive_text method. As you may have \\nguessed, this returns us the data sent by the client in plain text format. It\\'s important here \\nto understand that this method will block until data is received from the client. Until that \\nevent, we won\\'t proceed with the rest of the logic.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 263, 'page_label': '245'}, page_content=\"Creating a WebSocket with FastAPI     245\\nWe see here the importance of asynchronous input/output, as we presented in Chapter 2, \\nPython Programming Specificities. By creating an infinite loop waiting for incoming data, \\nwe could have blocked the whole server process in a traditional blocking paradigm. Here, \\nthanks to the event loop, the process is able to answer other requests made by other clients \\nwhile we are waiting for this one.\\nWhen data is received, the method returns the text data and we can proceed with the next \\nline. Here, we simply send back the message to the client thanks to the send_text method. \\nOnce done, we are going back to the beginning of the loop to wait for another message.\\nY ou probably noticed that the whole loop is wrapped inside a try..except statement. \\nThis is necessary to handle client disconnection. Indeed, our server will most of the time \\nbe blocked at the receive_text line, waiting for client data. If the client decides \\nto disconnect, the tunnel will be closed and the receive_text call will fail, with a \\nWebSocketDisconnect exception. That's why it's important to catch it to break the \\nloop and properly call disconnect on the server side.\\nLet's try it! Y ou can run the FastAPI application, as usual, thanks to the Uvicorn server. \\nHere's the command you'll need:\\n$ uvicorn chapter8.echo.app:app\\nOur client will be a simple HyperText Markup Language (HTML) page with some \\nJavaScript code to interact with the WebSocket. We'll quickly go through this code after \\nthe demonstration. To run it, we can simply serve it with the built-in Python server,  \\nas follows:\\n$ python -m http.server --directory chapter8/echo 9000\\nStarting several terminals\\nOn Linux and macOS, you should be able to simply start a new terminal by \\ncreating a new window or tab. On Windows and Windows Subsystem for \\nLinux (WSL), you can also have several tabs if you use the Windows terminal \\napplication (see https://www.microsoft.com/en-us/p/\\nwindows-terminal/9n0dx20hk701?activetab=pivot:ove\\nrviewtab for more information). Otherwise, you can simply click again on \\nthe Ubuntu shortcut in your Start menu to start another terminal.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 264, 'page_label': '246'}, page_content=\"246     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nThis will serve our HTML page on port 9000 of your local machine. If you open  \\nthe http://localhost:9000 address, you'll see a simple interface like the one  \\nshown here: \\nFigure 8.1 – Simple application to try WebSocket\\nY ou have a simple input form, allowing you to send messages to the server through the \\nWebSocket. They appear in green in the list below. The server echoes back your messages, \\nwhich then appear in yellow in the list.\\nY ou can see what's happening under the hood by opening the Network tab in the \\ndeveloper tools of your browser. Reload the page to force the WebSocket to reconnect. \\nY ou should then see a row for the WebSocket connection. If you click on it, you'll see a \\nMessages tab where you can see all the messages passing through the WebSocket.\\nIn the following example, you'll see the JavaScript code used to open the WebSocket \\nconnection and to send and receive messages:\\nscript.js\\nconst socket = new WebSocket('ws://localhost:8000/ws');\\n// Connection opened\\nsocket.addEventListener('open', function (event) {\\n\\xa0\\xa0// Send message on form submission\\n\\xa0\\xa0document.getElementById('form').addEventListener('submit',\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 265, 'page_label': '247'}, page_content=\"Creating a WebSocket with FastAPI     247\\n(event) => {\\n\\xa0\\xa0\\xa0\\xa0event.preventDefault();\\n\\xa0\\xa0\\xa0\\xa0const message = document.getElementById('message').value;\\n\\xa0\\xa0\\xa0\\xa0addMessage(message, 'client');\\n\\xa0\\xa0\\xa0\\xa0socket.send(message);\\n\\xa0\\xa0\\xa0\\xa0event.target.reset();\\n\\xa0\\xa0});\\n});\\n// Listen for messages\\nsocket.addEventListener('message', function (event) {\\n\\xa0\\xa0addMessage(event.data, 'server');\\n});\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/echo/script.js\\nAs you can see, modern browsers provide a very simple API to interact with WebSockets. \\nY ou just have to instantiate a new WebSocket object with the Uniform Resource \\nLocator (URL) of your endpoint and wire some event listeners: open when the \\nconnection is ready and message when data is received from the server. Finally, the \\nsend method allows you to send data to the server. Y ou can view more details on the \\nWebSocket API in the Mozilla Developer Network (MDN) documentation at https://\\ndeveloper.mozilla.org/en-US/docs/Web/API/WebSockets_API.\\nHandling concurrency\\nIn the previous example, we've assumed that the client was always sending a message first: \\nwe wait for its message before sending it back. Once again, it's the client that takes the \\ninitiative in the conversation.\\nHowever, in usual scenarios, the server can have data to send to the client without being \\nat the initiative. In a chat application, another user can typically send one or several \\nmessages that we want to forward to the first user immediately. In this context, the \\nblocking call to receive_text we showed in the previous example is a problem:  \\nwhile we are waiting, the server could have messages to forward to the client.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 266, 'page_label': '248'}, page_content='248     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nTo solve this, we\\'ll rely on more advanced tools of the asyncio module. Indeed, it \\nprovides functions that allow us to schedule several coroutines concurrently and wait \\nuntil one of them is complete. In our context, we can have a coroutine that waits for client \\nmessages and another one that sends data to it when it arrives. The first one being fulfilled \\nwins and we can start again with another loop iteration.\\nTo make this clearer, let\\'s build another example, in which the server will once again echo \\nback the message of the client. Besides that, it\\'ll regularly send the current time to the \\nclient. Y ou can see the implementation in the following code snippet:\\napp.py\\nasync def echo_message(websocket: WebSocket):\\n\\xa0\\xa0\\xa0\\xa0data = await websocket.receive_text()\\n\\xa0\\xa0\\xa0\\xa0await websocket.send_text(f\"Message text was: {data}\")\\nasync def send_time(websocket: WebSocket):\\n\\xa0\\xa0\\xa0\\xa0await asyncio.sleep(10)\\n\\xa0\\xa0\\xa0\\xa0await websocket.send_text(f\"It is: {datetime.utcnow().\\nisoformat()}\")\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n\\xa0\\xa0\\xa0\\xa0await websocket.accept()\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while True:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0echo_message_task = asyncio.create_task(echo_\\nmessage(websocket))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0send_time_task = asyncio.create_task(send_\\ntime(websocket))\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0done, pending = await asyncio.wait(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0{echo_message_task, send_time_task},\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return_when=asyncio.FIRST_COMPLETED,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for task in pending:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0task.cancel()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for task in done:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 267, 'page_label': '249'}, page_content=\"Creating a WebSocket with FastAPI     249\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0task.result()\\n\\xa0\\xa0\\xa0\\xa0except WebSocketDisconnect:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/concurrency/app.py\\nAs you can see, we defined two coroutines: the first one, echo_message, waits for \\ntext messages from the client and sends them back, while the second one, send_time, \\nwaits for 10 seconds before sending the current time to the client. Both of them expect a \\nWebSocket instance in the argument.\\nThe most interesting part lives under the infinite loop: as you can see, we call our two \\nfunctions, wrapped by the create_task function of asyncio. This transforms the \\ncoroutine into a Task object. Under the hood, a task is how the event loop manages the \\nexecution of the coroutine. Put more simply, it gives us full control over the execution of \\nthe coroutine, to retrieve its result or even cancel it.\\nThose task objects are necessary to work with asyncio.wait. This function is \\nespecially useful to run tasks concurrently. It expects in the first argument a set of tasks to \\nrun. By default, this function will block until all given tasks are completed. However, we \\ncan control that thanks to the return_when argument: in our case, we want it to block \\nuntil one of the tasks is completed, which corresponds to the FIRST_COMPLETED value. \\nThe effect is the following: our server will launch the coroutines concurrently. The first \\none will block waiting for a client message, while the other one will block for 10 seconds. \\nIf the client sends a message before 10 seconds, it'll send the message back and complete. \\nOtherwise, the send_time coroutine will send the current time and complete.\\nAt that point, asyncio.wait will return us two sets: the first one, done, contains a set of \\ncompleted tasks, while the other one, pending, contains a set of tasks not yet completed.\\nWe want to now go back to the start of the loop to start again. However, we need to first \\ncancel all the tasks that have not been completed; otherwise, they would pile up at each \\niteration, hence the iteration over the pending set to cancel those tasks.\\nFinally, we also make an iteration over the done tasks and call the result method on \\nthem. This method returns the result of the coroutine but also re-raises an exception \\nthat could have been raised inside. This is especially useful to handle once again the \\ndisconnection of the client: when waiting for client data, if the tunnel is closed, an \\nexception is raised. Thus, our try..except statement can catch it to properly close  \\nthe WebSocket.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 268, 'page_label': '250'}, page_content=\"250     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nIf you try this example as we did previously, you'll see that the server will regularly send \\nyou the current time but is also able to echo the messages you send.\\nThis send_time example shows you how you can implement a process to send data to \\nthe client when an event happens on the server: new data is available in the database, an \\nexternal process has finished a long computation, and so on. In the next section, we'll see \\nhow we can properly handle the case of multiple clients sending messages to the server, \\nwhich then broadcasts them to all the clients.\\nThat's basically how you can handle concurrency with asyncio tools. So far, everyone  \\nis able to connect to those WebSocket endpoints without any restriction. Of course,  \\nas with classic HTTP endpoints, you'll likely need to authenticate a user before opening  \\nthe connection.\\nUsing dependencies\\nJust as with regular endpoints, you can use dependencies in WebSocket endpoints. \\nHowever, since they are designed with HTTP in mind, this comes with a few drawbacks.\\nFirst of all, you can't use security dependencies, as we showed in Chapter 7, Managing \\nAuthentication and Security in FastAPI. Indeed, under the hood, most of them work \\nby injecting the Request object, which only works for HTTP requests (we saw that \\nWebSockets are injected in a WebSocket object instead). Trying to inject those \\ndependencies in a WebSocket context will result in an error.\\nSimilarly, basic dependencies such as Query, Header, or Cookie have their quirks. \\nIndeed, FastAPI is perfectly able to solve them in a WebSocket context. However, if they \\nare required, FastAPI will throw an error when they are missing. Contrary to the HTTP \\nvalidation error that is handled globally to render a proper 422 error, there is no handler \\nfor this WebSocket equivalent at the time of writing. This comes from a limitation of \\nStarlette, the underlying server layer, that may be solved in future releases. Y ou can follow \\nthe work on this subject at the following GitHub pull request: https://github.com/\\nencode/starlette/pull/527.\\nMeanwhile, it's recommended to make all your WebSocket dependencies optional and \\nhandle missing values yourself.\\nThat's what we'll see in our next example. In this one, we'll inject two dependencies,  \\nas follows:\\n• A username query parameter, which we'll use to greet the user on connection.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 269, 'page_label': '251'}, page_content='Creating a WebSocket with FastAPI     251\\n• A token cookie, which we\\'ll compare with a static value, to keep the example \\nsimple. Of course, a proper strategy would be to have a proper user lookup, as we \\nimplemented in Chapter 7, Managing Authentication and Security in FastAPI. If \\nthis cookie is missing or doesn\\'t have the required value, we\\'ll close the WebSocket \\nimmediately with an error code.\\nLet\\'s see the implementation in the following sample:\\ndependencies.py\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(\\n\\xa0\\xa0\\xa0\\xa0websocket: WebSocket,\\n\\xa0\\xa0\\xa0\\xa0username: str = \"Anonymous\",\\n\\xa0\\xa0\\xa0\\xa0token: Optional[str] = Cookie(None),\\n):\\n\\xa0\\xa0\\xa0\\xa0if token != API_TOKEN:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close(code=status.WS_1008_POLICY_\\nVIOLATION)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return\\n\\xa0\\xa0\\xa0\\xa0await websocket.accept()\\n\\xa0\\xa0\\xa0\\xa0await websocket.send_text(f\"Hello, {username}!\")\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while True:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = await websocket.receive_text()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.send_text(f\"Message text was: \\n{data}\")\\n\\xa0\\xa0\\xa0\\xa0except WebSocketDisconnect:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/dependencies/app.py\\nAs you can see, injecting dependencies is no different from standard HTTP endpoints. \\nNotice that we take care of providing a default value or making them optional,  \\nas we said before.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 270, 'page_label': '252'}, page_content=\"252     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nThen, we can have our dummy authentication logic. If it fails, we immediately close the \\nsocket with a status code. WebSockets have their own set of status codes. Y ou can view \\na complete list of these on this MDN documentation page: https://developer.\\nmozilla.org/fr/docs/Web/API/CloseEvent. The most generic one when an \\nerror occurs is 1008.\\nIf it passes, we can start our classic echo server. Notice that we can use the username value \\nas we wish in our logic. Here, we send a first message to greet the user on connection. \\nIf you try this with the HTML application, you'll see this message first, as shown in the \\nfollowing screenshot:\\n Figure 8.2 – Greeting message on connection\\nWith the browser WebSocket API, query parameters can be passed into the URL and \\nthe browser automatically forwards the cookies. However, there is no way to pass custom \\nheaders. This means that if you rely on headers for authentication, you'll have to either add \\none using cookies or implement an authentication message mechanism in the WebSocket \\nlogic itself. However, if you don't plan to use your WebSocket with a browser, you can still \\nrely on headers since most WebSocket clients support them.\\nY ou now have a good overview of how to add WebSockets to your FastAPI application. As \\nwe said, they are generally useful when several users are involved in real time and we need \\nto broadcast messages to all of them. We'll see in the next section how to implement this \\npattern reliably.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 271, 'page_label': '253'}, page_content='Handling multiple WebSocket connections and broadcasting messages     253\\nHandling multiple WebSocket connections and \\nbroadcasting messages\\nAs we said in the introduction to this chapter, a typical use case for WebSockets is to \\nimplement real-time communication across multiple clients, such as a chat application.  \\nIn this configuration, several clients have an open WebSocket tunnel with the server. Thus, \\nthe role of the server is to manage all the client connections and broadcast messages to all \\nof them: when a user sends a message, the server has to send it to all other clients in their \\nWebSockets. We show you a schema of this principle here: \\nFigure 8.3 – Multiple clients connected through WebSocket to a server \\nA first approach could be simply to keep a list of all WebSocket connections and iterate \\nthrough them to broadcast messages. This would work but would quickly become \\nproblematic in a production environment. Indeed, most of the time, server processes \\nrun multiple workers when deployed. This means that instead of having only one \\nprocess serving requests, we can have several ones so that we can answer more requests \\nconcurrently. We could also think of deployments on multiple servers spread in several \\ndata centers.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 272, 'page_label': '254'}, page_content='254     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nHence, nothing guarantees you that two clients opening a WebSocket are served by the \\nsame process. Our simple approach would fail in this configuration: since connections \\nare kept in the process memory, the process receiving the message would not be able to \\nbroadcast the message to clients served by another process. We schematize this problem  \\nin the following diagram:\\nFigure 8.4 – Multiple server workers without a message broker\\nTo solve this, we generally rely on message brokers. Message brokers are pieces of \\nsoftware whose role is to receive messages published by a first program and broadcast \\nthem to programs that are subscribed to it. Usually, this publish-subscribe (pub-sub) \\npattern is organized into different channels so that messages are clearly organized \\nfollowing their topic or usage. Some of the best-known message broker software includes \\nApache Kafka, RabbitMQ, or cloud-based implementations from Amazon Web Services \\n(AWS), Google Cloud Platform (GCP) and Microsoft Azure: Amazon MQ, Cloud Pub/\\nSub and Service Bus, respectively.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 273, 'page_label': '255'}, page_content='Handling multiple WebSocket connections and broadcasting messages     255\\nHence, our message broker will be unique in our architecture, and several server  \\nprocesses will connect to it to either publish or subscribe to messages. This architecture  \\nis schematized in the following diagram: \\nFigure 8.5 – Multiple server workers with a message broker\\nIn this chapter, we\\'ll see how to set up a simple system using the broadcaster library \\nfrom Encode (the creators of Starlette) and Redis, which will act as a message broker.\\nA word on Redis\\nAs its core, Redis is a data store designed to achieve maximum performance. \\nIt\\'s widely used in the industry for storing temporary data that we want to \\naccess very quickly, such as cache or distributed locks. It also supports a \\nbasic pub/sub paradigm, which makes it a good candidate to be used as a \\nmessage broker. Y ou can learn more about this technology at its official website \\nhttps://redis.io.\\nFirst of all, let\\'s install the library with the following command:\\n$ pip install \"broadcaster[redis]\"\\nThis library will abstract away all the complexities of publishing and subscribing with \\nRedis for us.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 274, 'page_label': '256'}, page_content='256     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nLet\\'s see the details of the implementation. In the following example, you\\'ll see the \\ninstantiation of the Broadcaster object:\\napp.py\\nbroadcast = Broadcast(\"redis://localhost:6379\")\\nCHANNEL = \"CHAT\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/broadcast/app.py\\nAs you can see, it only expects a URL to our Redis server. Notice also that we define a \\nCHANNEL constant. This will be the name of the channel to publish and subscribe to \\nmessages. We choose a static value here for the sake of the example, but you could have \\ndynamic channel names in a real-world application—to support several chat rooms,  \\nfor example.\\nThen, we define two functions: one to subscribe to new messages and send them to the \\nclient and another one to publish messages received in the WebSocket. Y ou can see these \\nfunctions in the following sample:\\napp.py\\nclass MessageEvent(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0username: str\\n\\xa0\\xa0\\xa0\\xa0message: str\\nasync def receive_message(websocket: WebSocket, username: str):\\n\\xa0\\xa0\\xa0\\xa0async with broadcast.subscribe(channel=CHANNEL) as \\nsubscriber:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0async for event in subscriber:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0message_event = MessageEvent.parse_raw(event.\\nmessage)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0# Discard user\\'s own messages\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if message_event.username != username:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.send_json(message_event.dict())\\nasync def send_message(websocket: WebSocket, username: str):'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 275, 'page_label': '257'}, page_content='Handling multiple WebSocket connections and broadcasting messages     257\\n\\xa0\\xa0\\xa0\\xa0data = await websocket.receive_text()\\n\\xa0\\xa0\\xa0\\xa0event = MessageEvent(username=username, message=data)\\n\\xa0\\xa0\\xa0\\xa0await broadcast.publish(channel=CHANNEL, message=event.\\njson())\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/broadcast/app.py\\nFirst of all, notice that we defined a Pydantic model, MessageEvent, to help us structure \\nthe data contained in a message. Instead of just passing raw strings as we\\'ve been doing up \\nto now, we have an object bearing both the message and the username.\\nThe first function, receive_message, subscribes to the broadcast channel and waits \\nfor messages called event. The data of the message contains serialized JavaScript Object \\nNotation (JSON) that we deserialize to instantiate a MessageEvent object. Notice that \\nwe use the parse_raw method of the Pydantic model, allowing us to parse the JSON \\nstring into an object in one operation.\\nThen, we check if the message username is different from the current username. Indeed, \\nsince all users are subscribed to the channel, they will also receive the messages they sent \\nthemselves. That\\'s why we discard them based on the username to avoid this. Of course, in \\na real-world application, you\\'ll likely want to rely on a unique user identifier (UID) rather \\nthan a simple username.\\nFinally, we can send the message through the WebSocket thanks to the send_json \\nmethod, which takes care of serializing the dictionary automatically.\\nThe second function, send_message, is there to publish a message to the broker. Quite \\nsimply, it waits for new data in the socket, structures it into a MessageEvent object, and \\nthen publishes it.\\nThat\\'s about it for the broadcaster part. We then have the WebSocket implementation in \\nitself, which is very similar to what we saw in the previous sections. Y ou can see it in the \\nfollowing sample:\\napp.py\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket, username: \\nstr = \"Anonymous\"):\\n\\xa0\\xa0\\xa0\\xa0await websocket.accept()\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while True:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 276, 'page_label': '258'}, page_content='258     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0receive_message_task = asyncio.create_task(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0receive_message(websocket, username)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0send_message_task = asyncio.create_task(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0send_message(websocket, username)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0done, pending = await asyncio.wait(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0{receive_message_task, send_message_task},\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return_when=asyncio.FIRST_COMPLETED,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for task in pending:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0task.cancel()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for task in done:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0task.result()\\n\\xa0\\xa0\\xa0\\xa0except WebSocketDisconnect:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/broadcast/app.py\\nNotice that username is retrieved from the query parameters.\\nFinally, we need to tell FastAPI to open the connection with the broker when it starts the \\napplication and to close it when exiting, as you can see in the following extract:\\napp.py\\n@app.on_event(\"startup\")\\nasync def startup():\\n\\xa0\\xa0\\xa0\\xa0await broadcast.connect()\\n@app.on_event(\"shutdown\")\\nasync def shutdown():\\n\\xa0\\xa0\\xa0\\xa0await broadcast.disconnect()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter8/broadcast/app.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 277, 'page_label': '259'}, page_content=\"Handling multiple WebSocket connections and broadcasting messages     259\\nThe on_event decorators allow us to trigger some useful logic when FastAPI starts  \\nor stops.\\nLet's now try this application! First, we'll run the Uvicorn server. Be sure that your Redis \\ncontainer is running before starting, as we explained in the Technical requirements section. \\nHere's the code you'll need:\\n$ uvicorn chapter8.broadcast.app:app\\nWe also provided a simple HTML client in the examples. To run it, we can simply serve it \\nwith the built-in Python server, as follows:\\n$ python -m http.server --directory chapter8/broadcast 9000\\nY ou can now access it through http://localhost:9000. If you open it twice in your \\nbrowser, in two different windows, you can see whether the broadcasting is working. Input \\na username in the first window and click on Connect. Do the same in the second window \\nwith a different username. Y ou can now send messages and see that they are broadcasted \\nto the other client, as depicted in the following screenshot: \\nFigure 8.6 – Multiple WebSocket clients broadcasting messages\\nThat was a very quick overview of how you can implement broadcasting systems involving \\nmessage brokers. Of course, we only covered the basics here, and much more complex \\nthings can be done with those powerful technologies. Once again, we see that FastAPI \\ngives us access to powerful building bricks without locking us inside specific technologies \\nor patterns: it's very easy to include new libraries to expand our possibilities.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 278, 'page_label': '260'}, page_content=\"260     Defining WebSockets for Two-Way Interactive Communication in FastAPI \\nSummary\\nIn this chapter, you've learned how to work with one of the latest web technologies \\navailable: WebSocket. Y ou are now able to open a two-way communication channel \\nbetween a client and a server, allowing you to implement applications with real-time \\nconstraints. As you've seen, FastAPI makes it very easy to add such endpoints. Still, \\nthe way of thinking inside a WebSocket logic is quite different from traditional HTTP \\nendpoints: managing an infinite loop and handling several tasks at a time are completely \\nnew challenges. Fortunately, the asynchronous nature of the framework makes our life \\neasier in this matter and helps us write concurrent code that is easily understandable.\\nFinally, we also had a quick overview of the challenges to solve when handling multiple \\nclients that share messages between them. Y ou saw that message broker software such \\nas Apache Kafka or RabbitMQ is necessary to make this use case reliable across several \\nserver processes.\\nY ou are now acquainted with all the features of FastAPI. Up to now, we've shown very \\nsimple examples focused on a specific point. In the real world, however, you'll likely \\ndevelop big applications that can do a lot of things and grow larger over time. To make \\nthem reliable, maintainable, and keep high-quality code, it's necessary to test them to \\nmake sure they behave as intended and that you don't introduce bugs when adding  \\nnew things. \\nIn the next chapter, you'll see how to set up an efficient test environment for FastAPI.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 279, 'page_label': '261'}, page_content='9\\nTesting an API \\nAsynchronously \\nwith pytest and \\nHTTPX\\nIn software development, a significant part of the developer\\'s work should be dedicated to \\nwriting tests. At first, you may be tempted to manually test your application by running \\nit, making a few requests, and arbitrarily deciding that \"everything works\". However, this \\napproach is flawed and can\\'t guarantee that your program works in every circumstance \\nand that you didn\\'t break things along the way.\\nThat\\'s why several disciplines have emerged regarding software testing: unit tests, \\nintegration tests, E2E tests, acceptance tests, and more. These techniques aim to validate \\nthe functionality of the software from a micro level, where we test single functions  \\n(unit tests), to a macro level, where we test a global feature that delivers value to the  \\nuser (acceptance tests). In this chapter, we\\'ll focus on the first level: unit testing.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 280, 'page_label': '262'}, page_content=\"262     Testing an API Asynchronously with pytest and HTTPX\\nUnit tests are short programs designed to verify that our code behaves the way it should \\nin every circumstance. Y ou may think that tests are time-consuming to write and that \\nthey don't add value to your software, but this will save you time in the long run: first of \\nall, tests can be run automatically in a few seconds, ensuring that all your software works, \\nwithout you needing to manually go over every feature. Secondly, when you introduce \\nnew features or refactor the code, you're ensuring that you don't introduce bugs to existing \\nparts of the software. In conclusion, tests are just as important as the program itself, and \\nthey help you deliver reliable and high-quality software.\\nIn this chapter, you'll learn how to write tests for your FastAPI application, both for  \\nHTTP endpoints and WebSockets. To help with this, you'll learn how to configure pytest, \\na well-known Python test framework, and HTTPX, an asynchronous HTTP client  \\nfor Python.\\nIn this chapter, we're going to cover the following main topics:\\n• Introduction to unit testing with pytest\\n• Setting up the testing tools for FastAPI with HTTPX\\xa0\\n• Writing tests for REST API endpoints\\n• Writing tests for WebSocket endpoints\\nTechnical requirements\\nFor this chapter, you'll need a Python virtual environment, similar to the one we set up in \\nChapter 1, Python Development Environment Setup.\\nFor the Testing with a database section, you'll need a running MongoDB server on your \\nlocal computer. The easiest way to do this is to run it as a Docker container. If you've never \\nused Docker before, we recommend that you read the Get Started tutorial in the official \\ndocumentation: https://docs.docker.com/get-started/. Once done, you'll be \\nable to run a MongoDB server with this simple command:\\n$ docker run -d --name fastapi-mongo -p 27017:27017 mongo:4.4\\nThe MongoDB server instance will then be available on your local computer on  \\nport 27017.\\nY ou can find all the code examples for this chapter in its dedicated GitHub repository: \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter9.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 281, 'page_label': '263'}, page_content=\"Introduction to unit testing with pytest     263\\nIntroduction to unit testing with pytest\\nAs we mentioned in the introduction, writing unit tests is an essential task in software \\ndevelopment to deliver high-quality software. To help us be productive and efficient, lot of \\nlibraries exist that provide tools and shortcuts dedicated to testing. In the Python standard \\nlibrary, a module exists for unit testing called unittest. Even though it's quite common \\nin Python code bases, many Python developers tend to prefer pytest, which provides a \\nmore lightweight syntax and powerful tools for advanced use cases. \\nIn the following examples, we'll write a unit test for a function called add, both with \\nunittest and pytest, so that you can see how they compare on a basic use case. First, \\nwe'll install pytest:\\n$ pip install pytest\\nNow, let's see our simple add function, which simply performs an addition:\\nchapter9_introduction.py\\ndef add(a: int, b: int) -> int:\\n\\xa0\\xa0\\xa0\\xa0return a + b\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nintroduction.py\\nNow, let's implement a test that checks that 2 + 3 is indeed equal to 5 with unittest:\\nchapter9_introduction_unittest.py\\nimport unittest\\nfrom chapter9.chapter9_introduction import add\\nclass TestChapter9Introduction(unittest.TestCase):\\n\\xa0\\xa0\\xa0\\xa0def test_add(self):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.assertEqual(add(2, 3), 5)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nintroduction_unittest.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 282, 'page_label': '264'}, page_content=\"264     Testing an API Asynchronously with pytest and HTTPX\\nAs you can see, unittest expects us to define a class inheriting from TestCase.  \\nThen, each test lives in its own method. To assert that two values are equal, we must use \\nthe assertEqual method.\\nTo run this test, we can call the unittest module from the command line and pass it \\nthrough the dotted path to our test module:\\n$ python -m unittest chapter9.chapter9_introduction_unittest\\n.\\n--------------------------------------------------------------\\n--------\\nRan 1 test in 0.000s\\nOK\\nIn the output, each successful test is represented by a dot. If one or several tests are not \\nsuccessful, you will get a detailed error report for each, highlighting the failing assertion. \\nY ou can try it by changing the assertion in the test.\\nNow, let's write the same test with pytest:\\nchapter9_introduction_unittest.py\\nfrom chapter9.chapter9_introduction import add\\ndef test_add():\\n\\xa0\\xa0\\xa0\\xa0assert add(2, 3) == 5\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nintroduction_unittest.py\\nAs you can see, it's much shorter! Indeed, with pytest, you don't necessarily have to \\ndefine a class: a simple function is enough. The only constraint to making it work is that \\nthe function name has to start with test_. This way, pytest can automatically discover \\nthe test functions. Secondly, it relies on the built-in assert statement instead of specific \\nmethods, allowing you to write comparisons more naturally.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 283, 'page_label': '265'}, page_content=\"Introduction to unit testing with pytest     265\\nTo run this test, we must simply call the pytest executable with the path to our test file:\\n$ pytest chapter9/chapter9_introduction_pytest.py\\n================== test session starts ==================\\nplatform darwin -- Python 3.7.10, pytest-6.2.4, py-1.10.0, \\npluggy-0.13.1\\nrootdir: /Users/fvoron/Google Drive/Livre FastAPI/Building-\\nData-Science-Applications-with-FastAPI, configfile: setup.cfg\\nplugins: asyncio-0.15.1, cov-2.12.0, mock-3.6.1, repeat-0.9.1\\ncollected 1 item\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nchapter9/chapter9_introduction_pytest.py .\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[100%]\\n=================== 1 passed in 0.01s ===================\\nOnce again, the output represents each successful test with a dot. Of course, if you change \\nthe test to make it fail, you'll get a detailed error for the failing assertion.\\nIt's worth noting that if you run pytest without any arguments, it'll automatically \\ndiscover all the test files living in your folder, as long as their name starts with test_.\\nHere, we made a small comparison between unittest and pytest. For the rest of this \\nchapter, we'll stick with pytest, which should give you a more productive experience \\nwhile writing tests.\\nAt the beginning of this section, we said that pytest provides powerful tools to help  \\nus write tests. Before focusing on FastAPI testing, we'll review two of them: \\nparametrize and fixtures. \\nGenerating tests with parametrize\\nIn our previous example with the add function, we only tested one addition test, 2 + 3. \\nMost of the time, we'll want to check for more cases to ensure our function works in every \\ncircumstance. Our first approach could be to add more assertions to our test, like so:\\ndef test_add():\\n\\xa0\\xa0\\xa0\\xa0assert add(2, 3) == 5\\n\\xa0\\xa0\\xa0\\xa0assert add(0, 0) == 0\\n\\xa0\\xa0\\xa0\\xa0assert add(100, 0) == 100\\n\\xa0\\xa0\\xa0\\xa0assert add(1, 1) == 2\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 284, 'page_label': '266'}, page_content='266     Testing an API Asynchronously with pytest and HTTPX\\nWhile working, this method has two drawbacks: first, it may be a bit cumbersome to  \\nwrite the same assertion several times with only some parameters changing. In this \\nexample, it\\'s not too bad, but tests can be way more complex, as we\\'ll see with FastAPI. \\nSecondly, we still only have one test: the first failing assertion will stop the test and the \\nfollowing ones won\\'t be executed. Thus, we\\'ll only know the result if we fix the failing \\nassertion first and run the test again.\\nTo help with this specific task, pytest provides the parametrize marker. In pytest, \\na marker is a special decorator that\\'s used to easily pass metadata to the test. Special \\nbehaviors can then be implemented, depending on the markers used by the test.\\nHere, parametrize allows us to pass several sets of variables that will be passed as \\narguments to the test function. At runtime, each set will generate a new and independent \\ntest. To understand this better, let\\'s look at how to use this marker to generate several tests \\nfor our add function:\\nchapter9_introduction_pytest_parametrize.py\\nimport pytest\\nfrom chapter9.chapter9_introduction import add\\n@pytest.mark.parametrize(\"a,b,result\", [(2, 3, 5), (0, 0, 0), \\n(100, 0, 100), (1, 1, 2)])\\ndef test_add(a, b, result):\\n\\xa0\\xa0\\xa0\\xa0assert add(a, b) == result\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nintroduction_pytest_parametrize.py\\nHere, you can see that we simply decorated our test function with the parametrize \\nmarker. The basic usage is as follows: the first argument is a string with the name of each \\nparameter separated by a comma. Then, the second argument is a list of tuples. Each tuple \\ncontains the values of the parameters in order.\\nOur test function receives those parameters in arguments, each one named the way  \\nyou specified previously. Thus, you can use them at will in the test logic. As you can see, \\nthe great benefit here is that we only have to write the assert statement once. Besides,  \\nit\\'s very quick to add a new test case: we just have to add another tuple to the \\nparametrize marker.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 285, 'page_label': '267'}, page_content=\"Introduction to unit testing with pytest     267\\nNow, let's run this test to see what happens by using the following command:\\n$ pytest chapter9/chapter9_introduction_pytest_parametrize.py\\n==================== test session starts ====================\\nplatform darwin -- Python 3.7.10, pytest-6.2.4, py-1.10.0, \\npluggy-0.13.1\\nrootdir: /Users/fvoron/Google Drive/Livre FastAPI/Building-\\nData-Science-Applications-with-FastAPI, configfile: setup.cfg\\nplugins: asyncio-0.15.1, cov-2.12.0, mock-3.6.1, repeat-0.9.1\\ncollected 4 items\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nchapter9/chapter9_introduction_pytest_parametrize.py .... \\n[100%]\\n===================== 4 passed in 0.01s =====================\\nAs you can see, pytest executed four tests instead of one! This means that it generated \\nfour independent tests, along with their own sets of parameters. If several tests are failing, \\nwe'll be informed, and the output will tell us which set of parameters caused the error.\\nTo conclude, parametrize is a very convenient way to test different outcomes when  \\nit's given a different set of parameters.\\nWhile writing unit tests, you'll often need variables and objects several times across  \\nyour tests, such as in an app instance, as some fake data, and so on. To avoid having to \\nrepeat the same things over and over across your tests, pytest proposes an interesting \\nfeature: fixtures.\\nReusing test logic by creating fixtures\\nWhen testing a large application, tests tend to become quite repetitive: lots of them will \\nshare the same boilerplate code before their actual assertion. Let's consider using Pydantic \\nmodels to represent a person and their postal address:\\nchapter9_introduction_fixtures.py\\nfrom datetime import date\\nfrom enum import Enum\\nfrom typing import List\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 286, 'page_label': '268'}, page_content='268     Testing an API Asynchronously with pytest and HTTPX\\nfrom pydantic import BaseModel\\nclass Gender(str, Enum):\\n\\xa0\\xa0\\xa0\\xa0MALE = \"MALE\"\\n\\xa0\\xa0\\xa0\\xa0FEMALE = \"FEMALE\"\\n\\xa0\\xa0\\xa0\\xa0NON_BINARY = \"NON_BINARY\"\\nclass Address(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0street_address: str\\n\\xa0\\xa0\\xa0\\xa0postal_code: str\\n\\xa0\\xa0\\xa0\\xa0city: str\\n\\xa0\\xa0\\xa0\\xa0country: str\\nclass Person(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0first_name: str\\n\\xa0\\xa0\\xa0\\xa0last_name: str\\n\\xa0\\xa0\\xa0\\xa0gender: Gender\\n\\xa0\\xa0\\xa0\\xa0birthdate: date\\n\\xa0\\xa0\\xa0\\xa0interests: List[str]\\n\\xa0\\xa0\\xa0\\xa0address: Address\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nintroduction_fixtures.py\\nThis example may look familiar: it was taken from Chapter 4, Managing pydantic Data \\nModels in FastAPI. Now, let\\'s say that we want to write tests with some instances of those \\nmodels. Obviously, it would be a bit annoying to instantiate them in each test, filling them \\nwith fake data.\\nFortunately, fixtures allow us to write them in one go. The following example shows how \\nto use them:\\nchapter9_introduction_fixtures_test.py\\nimport pytest\\nfrom chapter9.chapter9_introduction_fixtures import Address, \\nGender, Person'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 287, 'page_label': '269'}, page_content='Introduction to unit testing with pytest     269\\n@pytest.fixture\\ndef address():\\n\\xa0\\xa0\\xa0\\xa0return Address(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0street_address=\"12 Squirell Street\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0postal_code=\"424242\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0city=\"Woodtown\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0country=\"US\",\\n\\xa0\\xa0\\xa0\\xa0)\\n@pytest.fixture\\ndef person(address):\\n\\xa0\\xa0\\xa0\\xa0return Person(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0first_name=\"John\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0last_name=\"Doe\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0gender=Gender.MALE,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0birthdate=\"1991-01-01\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0interests=[\"travel\", \"sports\"],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0address=address,\\n\\xa0\\xa0\\xa0\\xa0)\\ndef test_address_country(address):\\n\\xa0\\xa0\\xa0\\xa0assert address.country == \"US\"\\ndef test_person_first_name(person):\\n\\xa0\\xa0\\xa0\\xa0assert person.first_name == \"John\"\\ndef test_person_address_city(person):\\n\\xa0\\xa0\\xa0\\xa0assert person.address.city == \"Woodtown\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nintroduction_fixtures_test.py\\nOnce again, pytest makes it very straightforward: fixtures are simple functions decorated with \\nthe fixture decorator. Inside, you can write any logic and return the data you\\'ll need in your \\ntests. Here, in address, we instantiate an Address object with fake data and return it.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 288, 'page_label': '270'}, page_content=\"270     Testing an API Asynchronously with pytest and HTTPX\\nNow, how can we use this fixture? If you look at the test_address_country test, \\nyou'll see some magic happening: by setting an address argument on the test function, \\npytest automatically detects that it corresponds to the address fixture, executes it, and \\npasses its return value. Inside the test, we have our Address object ready to use. pytest \\ncalls this requesting a fixture.\\nY ou may have noticed that we also defined another fixture, person. Once again,  \\nwe instantiate a Person model with dummy data. The interesting thing to note, however, \\nis that we actually requested the address fixture to use it inside! That's what makes this \\nsystem so powerful: fixtures can depend on other fixtures, which can also depend on \\nothers, and so on. In some way, it's quite similar to dependency injection, as we discussed \\nin Chapter 5, Dependency Injections in FastAPI.\\nWith that, our quick introduction to pytest has come to an end. Of course, there are so \\nmany more things to say, but this will be enough for you to get started. If you want to \\nexplore this topic further, you can read the official pytest documentation, which includes \\ntons of examples showing you how you can benefit from all its features: https://docs.\\npytest.org/en/latest/.\\nNow, let's focus on FastAPI. We'll start by setting up the tools for testing our applications.\\nSetting up testing tools for FastAPI with HTTPX\\nIf you look at the FastAPI documentation regarding testing, you'll see that it recommends \\nthat you use TestClient provided by Starlette. In this book, we'll show you a different \\napproach involving an HTTP client, called HTTPX.\\nWhy? The default TestClient is implemented in a way that makes it completely \\nsynchronous, meaning you can write tests without worrying about async and await. \\nThis might sound nice, but we found that it causes some problems in practice: since \\nyour FastAPI app is designed to work asynchronously, you'll likely have lots of services \\nworking asynchronously, such as the database drivers we saw in Chapter 6, Databases and \\nAsynchronous ORMs. Thus, in your tests, you'll probably need to perform some actions on \\nthose asynchronous services, such as filling a database with dummy data, which will make \\nyour tests asynchronous anyway. Melting the two approaches often leads to strange errors \\nthat are hard to debug.\\nFortunately, HTTPX, an HTTP client created by the same team as Starlette, allows us to \\nhave a pure asynchronous HTTP client able to make requests to our FastAPI app. To make \\nthis approach work, we'll need three libraries:\\n• HTTPX, the client that will perform HTTP requests\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 289, 'page_label': '271'}, page_content='Setting up testing tools for FastAPI with HTTPX     271\\n• asgi-lifepsan, a library for managing the startup and shutdown events  \\nof your FastAPI app programmatically\\n• pytest-asyncio, an extension of pytest that allows us to write  \\nasynchronous tests\\nLet\\'s install these libraries using the following command:\\n$ pip install httpx asgi-lifespan pytest-asyncio\\nGreat! Now, let\\'s write some fixtures so that we can easily get an HTTP test client for  \\na FastAPI application. This way, when writing a test, we\\'ll only have to request the fixture \\nand we\\'ll be able to make a request right away.\\nIn the following example, we are considering a simple FastAPI application that we want  \\nto test:\\nchapter9_app.py\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@app.get(\"/\")\\nasync def hello_world():\\n\\xa0\\xa0\\xa0\\xa0return {\"hello\": \"world\"}\\n@app.on_event(\"startup\")\\nasync def startup():\\n\\xa0\\xa0\\xa0\\xa0print(\"Startup\")\\n@app.on_event(\"shutdown\")\\nasync def shutdown():\\n\\xa0\\xa0\\xa0\\xa0print(\"Shutdown\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 290, 'page_label': '272'}, page_content='272     Testing an API Asynchronously with pytest and HTTPX\\nIn a separate test file, we\\'ll implement two fixtures.\\nThe first one, event_loop, will ensure that we always work with the same event \\nloop instance. It\\'s automatically requested by pytest-asyncio before executing \\nasynchronous tests. While not strictly required, experience has shown us that it greatly \\nhelps us avoid errors that may occur when several event loops are launched. Y ou can see \\nits implementation in the following example:\\nchapter9_app_test.py\\n@pytest.fixture(scope=\"session\")\\ndef event_loop():\\n\\xa0\\xa0\\xa0\\xa0loop = asyncio.get_event_loop()\\n\\xa0\\xa0\\xa0\\xa0yield loop\\n\\xa0\\xa0\\xa0\\xa0loop.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\ntest.py\\nHere, you can see that we simply get the current event loop before yielding it. As  \\nwe discussed in Chapter 2, Python Programming Specificities, using a generator allows  \\nus to \"pause\" the function\\'s execution and get back to the execution of its caller. This way, \\nwhen the caller is done, we can execute cleanup operations, such as closing the loop. pytest \\nis smart enough to handle this correctly in fixtures, so this is a very common pattern for \\nsetting up test data, using it, and destroying it after.\\nOf course, this function is decorated with the fixture decorator to make it a fixture \\nfor pytest. Y ou may have noticed that we added an argument called scope with a value \\nof session. This argument controls at which level the fixture should be instantiated. \\nBy default, it\\'s recreated at the beginning of each single test function. The session value \\nis the highest level, meaning that the fixture is only created once at the beginning of the \\nwhole test run, which is relevant for our event loop. Y ou can find out more about this \\nmore advanced feature in the official documentation: https://docs.pytest.org/\\nen/latest/how-to/fixtures.html#scope-sharing-fixtures-across-\\nclasses-modules-packages-or-session.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 291, 'page_label': '273'}, page_content='Setting up testing tools for FastAPI with HTTPX     273\\nNext, we\\'ll implement our test_client fixture, which will create an instance of \\nHTTPX for our FastAPI application. We must also remember to trigger the app events \\nwith asgi-lifespan. Y ou can see what it looks like in the following example:\\nchapter9_app_test.py\\n@pytest.fixture\\nasync def test_client():\\n\\xa0\\xa0\\xa0\\xa0async with LifespanManager(app):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0async with httpx.AsyncClient(app=app, base_url=\"http://\\napp.io\") as test_client:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0yield test_client\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\ntest.py\\nOnly three lines are needed. Notice that the app variable is our FastAPI application \\ninstance is the one we imported from its module, from chapter9.chapter9_app \\nimport app.\\nUp until now, we haven\\'t had the opportunity to talk about the with syntax. In Python, \\nthis is what\\'s called a context manager. Simply put, it\\'s a convenient syntax for objects \\nthat need to execute setup logic when they are used and teardown logic when they are not \\nneeded anymore. When you enter the with block, the object automatically executes the \\nsetup logic. When you exit the block, it executes its teardown logic. Y ou can read more \\nabout context managers in the Python documentation: https://docs.python.\\norg/3/reference/datamodel.html#with-statement-context-managers.\\nIn our case, both LifespanManager and httpx.AsyncClient work as context \\nmanagers, so we simply have to nest their blocks. The first one ensures startup and \\nshutdown events are executed, while the second one ensures that an HTTP session  \\nis ready.\\nNotice that we once again used a generator here, with yield. This is important because, \\neven if we don\\'t have any more code after, we have to give the context managers the \\nopportunity to exit: after the yield statement, we implicitly exit the with blocks.\\nThat\\'s it! We now have all the fixtures ready to write tests for our REST API endpoints. \\nThat\\'s what we\\'ll do in the next section.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 292, 'page_label': '274'}, page_content=\"274     Testing an API Asynchronously with pytest and HTTPX\\nOrganizing tests and global fixtures in projects\\nIn larger projects, you'll likely have several test files to keep your tests \\norganized. Usually, those files are placed in a tests folder at the root of your \\nproject. If your test files are prefixed with test_, they will be automatically \\ndiscovered by pytest. Figure 9.1 shows an example of this.\\nBesides this, you'll need the fixtures we defined in this section for all your tests. \\nRather than repeating them again and again in all your test files, pytest allows \\nyou to write global fixtures in a file named conftest.py. After putting \\nit in your tests folder, it will automatically be imported, allowing you to \\nrequest all the fixtures you define inside it. Y ou can read more about this in the \\nofficial documentation at https://docs.pytest.org/en/latest/\\nreference/fixtures.html#conftest-py-sharing-\\nfixtures-across-multiple-files:\\nFigure 9.1 – Structure of a project containing tests\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 293, 'page_label': '275'}, page_content='Writing tests for REST API endpoints     275\\nWriting tests for REST API endpoints\\nAll the tools we need to test our FastAPI application are now ready. All these tests boil \\ndown to performing an HTTP request and checking the response to see if it corresponds \\nto what we expect.\\nLet\\'s start simple with a test for our hello_world path operation function. Y ou can see \\nit in the following code:\\nchapter9_app_test.py\\n@pytest.mark.asyncio\\nasync def test_hello_world(test_client: httpx.AsyncClient):\\n\\xa0\\xa0\\xa0\\xa0response = await test_client.get(\"/\")\\n\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_200_OK\\n\\xa0\\xa0\\xa0\\xa0json = response.json()\\n\\xa0\\xa0\\xa0\\xa0assert json == {\"hello\": \"world\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\ntest.py\\nFirst of all, notice that the test function is defined as async. As we mentioned previously, \\nto make it work with pytest, we had to install pytest-asyncio. This extension provides \\nthe asyncio marker: each asynchronous test should be decorated with this marker to \\nmake it work properly.\\nNext, we requested our test_client fixture, which we defined earlier. It gives us an \\nHTTPX client instance ready to make requests to our FastAPI app. Note that we manually \\ntype hinted the fixture. While not strictly required, it\\'ll greatly help you if you use an IDE \\nsuch as Visual Studio Code, which uses type hints to provide you with convenient auto-\\ncompletion features.\\nThen, in the body of our test, we performed the request. Here, it\\'s a simple GET request \\nto the / path. It returns an HTTPX Response object (which is different from the \\nResponse class of FastAPI) containing all the data of the HTTP response: the status \\ncode, the headers, and the body.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 294, 'page_label': '276'}, page_content='276     Testing an API Asynchronously with pytest and HTTPX\\nFinally, we made assertions based on this data. As you can see, we verified that the status \\ncode was indeed 200. We also checked the content of the body, which is a simple JSON \\nobject. Notice that the Response object has a convenient method called json for \\nautomatically parsing JSON content.\\nGreat! We wrote our first FastAPI test! Of course, you\\'ll likely have more complex tests, \\ntypically ones for POST endpoints.\\nWriting tests for POST endpoints\\nTesting a POST endpoint is not very different from what we\\'ve seen earlier. The difference \\nis that we\\'ll likely have more cases to check if data validation is working. In the following \\nexample, we are implementing a POST endpoint that accepts a Person model in  \\nthe body:\\nchapter9_app_post.py\\nclass Person(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0first_name: str\\n\\xa0\\xa0\\xa0\\xa0last_name: str\\n\\xa0\\xa0\\xa0\\xa0age: int\\n@app.post(\"/persons\", status_code=status.HTTP_201_CREATED)\\nasync def create_person(person: Person):\\n\\xa0\\xa0\\xa0\\xa0return person\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\npost.py\\nAn interesting test could be to ensure that an error is raised if some fields are missing \\nin the request payload. In the following extract, we wrote two tests: one with an invalid \\npayload and another with a valid one:\\nchapter9_app_post_test.py\\n@pytest.mark.asyncio\\nclass TestCreatePerson:\\n\\xa0\\xa0\\xa0\\xa0async def test_invalid(self, test_client: httpx.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 295, 'page_label': '277'}, page_content='Writing tests for REST API endpoints     277\\nAsyncClient):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0payload = {\"first_name\": \"John\", \"last_name\": \"Doe\"}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await test_client.post(\"/persons\", \\njson=payload)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_422_\\nUNPROCESSABLE_ENTITY\\n\\xa0\\xa0\\xa0\\xa0async def test_valid(self, test_client: httpx.AsyncClient):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0payload = {\"first_name\": \"John\", \"last_name\": \"Doe\", \\n\"age\": 30}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await test_client.post(\"/persons\", \\njson=payload)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_201_CREATED\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0json = response.json()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert json == payload\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\npost_test.py\\nThe first thing you may have noticed is that we wrapped our two tests inside a class. While \\nnot required in pytest, it could help you organize your tests; for example, to regroup tests \\nthat concern a single endpoint. Notice that, in this case, we only have to decorate the class \\nwith the asyncio marker; it will be automatically applied on single tests. Also, ensure \\nthat you add the self argument to each test: since we are now inside a class, they  \\nbecome methods.\\nThese tests are not very different from our first example. As you can see, the HTTPX client \\nmakes it very easy to perform POST requests with a JSON payload: you just have to pass  \\na dictionary to the json argument.\\nOf course, HTTPX helps you build all kinds of HTTP requests with headers, query \\nparameters, and so on. Be sure to check its official documentation to learn more about  \\nits usage: https://www.python-httpx.org/quickstart/.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 296, 'page_label': '278'}, page_content='278     Testing an API Asynchronously with pytest and HTTPX\\nTesting with a database\\nY our application will likely have a database connection to read and store data. In this \\ncontext, you\\'ll need to work with a fresh test database at each run to have a clean and \\npredictable set of data to write your tests.\\nFor this, we\\'ll use two things. The first one, dependency_overrides, is a FastAPI \\nfeature that allows us to replace some dependencies at runtime. For example, we can \\nreplace the dependency that returns the database instance with another one that returns \\na test database instance. The second one is, once again, fixtures, which will help us create \\nfake data in the test database before we run the tests.\\nTo show you a working example, we\\'ll consider the same example we built in the \\nCommunicating with a MongoDB database with Motor section of Chapter 6, Databases \\nand Asynchronous ORMs. In this example, we built REST endpoints to manage blog posts. \\nAs you may recall, we had a get_database dependency that returned the database \\ninstance. As a reminder, we\\'ll show it again here:\\napp.py\\nmotor_client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\\ndatabase = motor_client[\"chapter6_mongo\"]\\ndef get_database() -> AsyncIOMotorDatabase:\\n\\xa0\\xa0\\xa0\\xa0return database\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter6/mongodb/app.py\\nPath operation functions and other dependencies would then use this dependency to \\nretrieve the database instance.\\nFor our tests, we\\'ll create a new instance of AsyncIOMotorDatabase that points to \\nanother database. Then, we\\'ll create a new dependency, directly in our test file, that returns \\nthis instance. Y ou can see this in the following example:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 297, 'page_label': '279'}, page_content='Writing tests for REST API endpoints     279\\nchapter9_db_test.py\\nmotor_client = AsyncIOMotorClient(\"mongodb://localhost:27017\")\\ndatabase_test = motor_client[\"chapter9_db_test\"]\\ndef get_test_database():\\n\\xa0\\xa0\\xa0\\xa0return database_test\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_db_test.py\\nThen, in our test_client fixture, we\\'ll override the default get_database \\ndependency by using our current get_test_database dependency. The following \\nexample shows how this is done:\\nchapter9_db_test.py\\n@pytest.fixture\\nasync def test_client():\\n\\xa0\\xa0\\xa0\\xa0app.dependency_overrides[get_database] = get_test_database\\n\\xa0\\xa0\\xa0\\xa0async with LifespanManager(app):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0async with httpx.AsyncClient(app=app, base_url=\"http://\\napp.io\") as test_client:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0yield test_client\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_db_\\ntest.py\\nFastAPI provides a property called dependency_overrides, which is a dictionary \\nthat maps original dependency functions with substitutes. Here, we directly used the \\nget_database function as a key. The rest of the fixture doesn\\'t have to change. Now, \\nwhenever the get_database dependency is injected into the application code, FastAPI \\nwill automatically replace it with get_test_database. As a result, our endpoints will \\nnow work with the test database instance.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 298, 'page_label': '280'}, page_content='280     Testing an API Asynchronously with pytest and HTTPX\\nTo test some behaviors, such as retrieving a single post, it\\'s usually convenient to have \\nsome base data in our test database. To allow this, we\\'ll create a new fixture that will \\ninstantiate dummy PostDB objects and insert them into the test database. Y ou can see \\nthis in the following example:\\nchapter9_db_test.py\\n@pytest.fixture(autouse=True, scope=\"module\")\\nasync def initial_posts():\\n\\xa0\\xa0\\xa0\\xa0initial_posts = [\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0PostDB(title=\"Post 1\", content=\"Content 1\"),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0PostDB(title=\"Post 2\", content=\"Content 2\"),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0PostDB(title=\"Post 3\", content=\"Content 3\"),\\n\\xa0\\xa0\\xa0\\xa0]\\n\\xa0\\xa0\\xa0\\xa0await database_test[\"posts\"].insert_many(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[post.dict(by_alias=True) for post in initial_posts]\\n\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0yield initial_posts\\n\\xa0\\xa0\\xa0\\xa0await motor_client.drop_database(\"chapter9_db_test\")\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_db_\\ntest.py\\nHere, you can see that we just had to make an insert_many request to the MongoDB \\ndatabase to create the posts.\\nNotice that we used the autouse and scope arguments of the fixture decorator. The  \\nfirst one tells pytest to automatically call this fixture even if it\\'s not requested in any test.  \\nIn this case, it\\'s convenient because we\\'ll always ensure that the data has been created in \\nthe database, without the risk of forgetting to request it in the tests. The other one, scope, \\nallows us, as we mentioned previously, to not run this fixture at the beginning of each test. \\nWith the module value, the fixture will create the objects only once, at the beginning of \\nthis particular test file. It helps us keep the test fast because in this case, it doesn\\'t make \\nsense to recreate the posts before each test.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 299, 'page_label': '281'}, page_content='Writing tests for REST API endpoints     281\\nOnce again, we yield the posts instead of returning them. This pattern allows us to delete \\nthe test database after the tests run. By doing this, we\\'re making sure that we always start \\nwith a fresh database when we\\'ve run the tests.\\nAnd we are done! We can now write tests while knowing exactly what we have in the \\ndatabase. In the following example, you can see tests that are used to verify the behavior  \\nof the endpoint retrieving a single post:\\nchapter9_db_test.py\\n@pytest.mark.asyncio\\nclass TestGetPost:\\n\\xa0\\xa0\\xa0\\xa0async def test_not_existing(self, test_client: httpx.\\nAsyncClient):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await test_client.get(\"/posts/abc\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_404_NOT_\\nFOUND\\n\\xa0\\xa0\\xa0\\xa0async def test_existing(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self, test_client: httpx.AsyncClient, initial_posts: \\nList[PostDB]\\n\\xa0\\xa0\\xa0\\xa0):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await test_client.get(f\"/posts/{initial_\\nposts[0].id}\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_200_OK\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0json = response.json()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert json[\"_id\"] == str(initial_posts[0].id)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_db_\\ntest.py\\nNotice that we requested the initial_posts fixture in the second test to retrieve the \\nidentifier of the truly existing post in our database.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 300, 'page_label': '282'}, page_content='282     Testing an API Asynchronously with pytest and HTTPX\\nOf course, we can also test our endpoints by creating data and checking if they correctly \\ninsert this data into the database. Y ou can see this in the following example:\\nchapter9_db_test.py\\n@pytest.mark.asyncio\\nclass TestCreatePost:\\n\\xa0\\xa0\\xa0\\xa0async def test_invalid_payload(self, test_client: httpx.\\nAsyncClient):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0payload = {\"title\": \"New post\"}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await test_client.post(\"/posts\", \\njson=payload)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_422_\\nUNPROCESSABLE_ENTITY\\n\\xa0\\xa0\\xa0\\xa0async def test_valid_payload(self, test_client: httpx.\\nAsyncClient):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0payload = {\"title\": \"New post\", \"content\": \"New post \\ncontent\"}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await test_client.post(\"/posts\", \\njson=payload)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_201_CREATED\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0json = response.json()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0post_id = ObjectId(json[\"_id\"])\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0post_db = await database_test[\"posts\"].find_one({\"_id\": \\npost_id})\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert post_db is not None\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_db_\\ntest.py\\nIn the second test, we used the database_test instance to perform a request and check \\nthat the object was inserted correctly. This shows the benefit of using asynchronous tests: \\nwe can use the same libraries and tools inside our tests. That\\'s all you need to know about \\ndependency_overrides.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 301, 'page_label': '283'}, page_content='Writing tests for REST API endpoints     283\\nThis feature is also very helpful when you need to write tests for logic involving external \\nservices, such as external APIs. Instead of making real requests to those external services \\nduring your tests, which could cause issues or incur costs, you\\'ll be able to replace them \\nwith another dependency that fakes the requests. To understand this, we\\'ve built another \\nexample application with an endpoint for retrieving data from an external API:\\nchapter9_app_external_api.py\\nfrom typing import Any, Dict\\n \\nimport httpx\\nfrom fastapi import FastAPI, Depends\\n \\napp = FastAPI()\\n \\n \\nclass ExternalAPI:\\n\\xa0\\xa0\\xa0\\xa0def __init__(self) -> None:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.client = httpx.AsyncClient(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0base_url=\"https://dummy.restapiexample.com/api/v1/\"\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n \\n\\xa0\\xa0\\xa0\\xa0async def __call__(self) -> Dict[str, Any]:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0async with self.client as client:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response = await client.get(\"employees\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return response.json()\\n \\n \\nexternal_api = ExternalAPI()\\n \\n \\n@app.get(\"/employees\")\\nasync def external_employees(employees: Dict[str, Any] = \\nDepends(external_api)):\\n\\xa0\\xa0\\xa0\\xa0return employees\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\nexternal_api.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 302, 'page_label': '284'}, page_content='284     Testing an API Asynchronously with pytest and HTTPX\\nTo call our external API, we\\'ve built a class dependency, as we saw in the Creating and \\nusing a parameterized dependency with a class section of Chapter 5, Dependency Injections \\nin FastAPI. We use HTTPX as an HTTP client to make a request to the external API and \\nretrieve the data. This external API is a dummy API containing fake data, very useful for \\nexperiments like this: https://dummy.restapiexample.com/.\\nThe /employees endpoint is simply injected with this dependency and directly returns \\nthe data provided by the external API.\\nOf course, to test this endpoint, we don\\'t want to make real requests to the external API: \\nit may take time and could be subject to rate limiting. Besides, you may want to test \\nbehavior that is not easy to reproduce in the real API, such as errors.\\nThanks to dependency_overrides, it\\'s very easy to replace our ExternalAPI \\ndependency class with another one that returns static data. In the following example,  \\nyou can see how we implemented such a test:\\nchapter9_app_external_api_test.py\\nclass MockExternalAPI:\\n\\xa0\\xa0\\xa0\\xa0mock_data = {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"data\": [\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0{\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"employee_age\": 61,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"employee_name\": \"Tiger Nixon\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"employee_salary\": 320800,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"id\": 1,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"profile_image\": \"\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"status\": \"success\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"message\": \"Success\",\\n\\xa0\\xa0\\xa0\\xa0}\\n \\n\\xa0\\xa0\\xa0\\xa0async def __call__(self) -> Dict[str, Any]:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return MockExternalAPI.mock_data\\n \\n \\n@pytest.fixture(scope=\"session\")\\ndef event_loop():'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 303, 'page_label': '285'}, page_content='Writing tests for REST API endpoints     285\\n\\xa0\\xa0\\xa0\\xa0loop = asyncio.get_event_loop()\\n\\xa0\\xa0\\xa0\\xa0yield loop\\n\\xa0\\xa0\\xa0\\xa0loop.close()\\n \\n \\n@pytest.fixture\\nasync def test_client():\\n\\xa0\\xa0\\xa0\\xa0app.dependency_overrides[external_api] = MockExternalAPI()\\n\\xa0\\xa0\\xa0\\xa0async with LifespanManager(app):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0async with httpx.AsyncClient(app=app, base_url=\"http://\\napp.io\") as test_client:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0yield test_client\\n \\n \\n@pytest.mark.asyncio\\nasync def test_get_employees(test_client: httpx.AsyncClient):\\n\\xa0\\xa0\\xa0\\xa0response = await test_client.get(\"/employees\")\\n \\n\\xa0\\xa0\\xa0\\xa0assert response.status_code == status.HTTP_200_OK\\n \\n\\xa0\\xa0\\xa0\\xa0json = response.json()\\n\\xa0\\xa0\\xa0\\xa0assert json == MockExternalAPI.mock_data\\n\\xa0\\xa0\\xa0\\xa0\\xa0return response.json() \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_app_\\nexternal_api_test.py\\nHere, you can see that we wrote a simple class called MockExternalAPI that returns \\nhardcoded data. All we have to do then is override the original dependency with this one: \\nduring the tests, the external API won\\'t be called; we\\'ll only work with the static data.\\nWith the guidelines we\\'ve seen so far, you can now write tests for any HTTP endpoints \\nin your FastAPI app. However, there is another kind of endpoint that behaves differently: \\nWebSockets. As we\\'ll see in the next section, unit testing WebSockets is also quite different \\nfrom what we described for REST endpoints.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 304, 'page_label': '286'}, page_content='286     Testing an API Asynchronously with pytest and HTTPX\\nWriting tests for WebSocket endpoints\\nIn Chapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI,  \\nwe explained how WebSockets work and how you can implement such endpoints in \\nFastAPI. As you may have guessed, writing unit tests for WebSockets endpoints is quite \\ndifferent from what we\\'ve seen so far.\\nUnfortunately, we won\\'t be able to reuse HTTPX since, at the time of writing, this client \\ncan\\'t communicate with WebSockets. For the time being, our best bet is to use the default \\nTestClient provided by Starlette.\\n To show you this, we\\'ll consider the following WebSocket example:\\nchapter9_websocket.py\\nfrom fastapi import FastAPI, WebSocket\\nfrom starlette.websockets import WebSocketDisconnect\\napp = FastAPI()\\n@app.websocket(\"/ws\")\\nasync def websocket_endpoint(websocket: WebSocket):\\n\\xa0\\xa0\\xa0\\xa0await websocket.accept()\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while True:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = await websocket.receive_text()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.send_text(f\"Message text was: \\n{data}\")\\n\\xa0\\xa0\\xa0\\xa0except WebSocketDisconnect:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nwebsocket.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 305, 'page_label': '287'}, page_content='Writing tests for WebSocket endpoints     287\\nY ou may have recognized this \"echo\" example from Chapter 8, Defining WebSockets  \\nfor Two-Way Interactive Communication in FastAPI. To test this endpoint, we\\'ll create  \\na new fixture that will instantiate a test client for this application. Y ou can review  \\nits implementation in the following example:\\nchapter9_websocket_test.py\\nimport asyncio\\nimport pytest\\nfrom fastapi.testclient import TestClient\\nfrom chapter9.chapter9_websocket import app\\n@pytest.fixture(scope=\"session\")\\ndef event_loop():\\n\\xa0\\xa0\\xa0\\xa0loop = asyncio.get_event_loop()\\n\\xa0\\xa0\\xa0\\xa0yield loop\\n\\xa0\\xa0\\xa0\\xa0loop.close()\\n@pytest.fixture\\ndef websocket_client():\\n\\xa0\\xa0\\xa0\\xa0with TestClient(app) as websocket_client:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0yield websocket_client\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nwebsocket_test.py\\nAs you can see, we once again took care of defining the event_loop fixture,  \\nas we explained in the Setting up testing tools for FastAPI with HTTPX section.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 306, 'page_label': '288'}, page_content='288     Testing an API Asynchronously with pytest and HTTPX\\nThen, we implemented the websocket_client fixture. The TestClient class \\nbehaves as a context manager and simply expects the FastAPI application to test the \\nargument. Since we opened a context manager, we once again yielded the value to ensure \\nthe exit logic is executed after the test. Notice that we don\\'t have to manually take care  \\nof the lifespan events, contrary to what we did in the previous sections: TestClient  \\nis designed to trigger them on its own.\\nNow, let\\'s write a test for our WebSocket using this fixture:\\nchapter9_websocket_test.py\\n@pytest.mark.asyncio\\nasync def test_websocket_echo(websocket_client: TestClient):\\n\\xa0\\xa0\\xa0\\xa0with websocket_client.websocket_connect(\"/ws\") as \\nwebsocket:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0websocket.send_text(\"Hello\")\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0message = websocket.receive_text()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0assert message == \"Message text was: Hello\"\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter9/chapter9_\\nwebsocket_test.py\\nThe first thing to notice is that we still define our test as async, with the associated \\nasyncio marker, even if TestClient works synchronously. Once again, this is useful \\nif you need to call asynchronous services during your tests and limit the issues you may \\nencounter with event loops.\\nAs you can see, the test client exposes a websocket_connect method to open  \\na connection to a WebSocket endpoint. It also works as a context manager, giving you the \\nwebsocket variable. It\\'s an object that exposes several methods to either send or receive \\ndata. Each of those methods will block until a message has been sent or received.\\nHere, to test our \"echo\" server, we send a message thanks to the send_text method. \\nThen, we retrieve a message with receive_text and assert that it corresponds to what \\nwe expect. Equivalent methods also exist for sending and receiving JSON data directly: \\nsend_json and receive_json.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 307, 'page_label': '289'}, page_content=\"Summary     289\\nThis is what makes WebSocket testing a bit special: you have to think about the sequence \\nof sent and received messages and implement them programmatically to test the behavior \\nof your WebSocket.\\nOther than that, all the things we've seen so far regarding testing are applicable, especially \\ndependency_overrides, when you'll need to use a test database.\\nSummary\\nCongratulations! Y ou are now ready to build high-quality FastAPI applications that have \\nbeen well tested. In this chapter, you learned how to use pytest, a powerful and efficient \\ntesting framework for Python. Thanks to pytest fixtures, you saw how to create a reusable \\ntest client for your FastAPI application that can work asynchronously. Using this client, \\nyou learned how to make HTTP requests to assert the behavior of your REST API. Finally, \\nwe reviewed how to test WebSocket endpoints, which involves a fairly different way  \\nof thinking.\\nNow that you can build a reliable and efficient FastAPI application, it's time to  \\nbring it to the whole world! In the next chapter, we'll review the best practices and  \\npatterns for preparing a FastAPI application for the world before studying several \\ndeployment methods.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 308, 'page_label': '290'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 309, 'page_label': '291'}, page_content=\"10\\nDeploying a  \\nFastAPI Project\\nBuilding a good application is great, but it's even better if customers can enjoy it. In this \\nchapter, you'll look at different techniques and the best practices for deploying your \\nFastAPI application to make it available on the web. First, you'll learn how to structure \\nyour project to make it ready for deployment by using environment variables to set the \\nconfiguration options you need, as well as by managing your dependencies properly with \\npip. Once done, we'll show you three ways to deploy your application: with a serverless \\ncloud platform, with a Docker container, and with a traditional Linux server.\\nIn this chapter, we're going to cover the following main topics:\\n• Setting and using environment variables\\n• Managing Python dependencies\\n• Deploying a FastAPI application on a serverless platform\\n• Deploying a FastAPI application with Docker\\n• Deploying a FastAPI application on a traditional server\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 310, 'page_label': '292'}, page_content='292     Deploying a FastAPI Project \\nTechnical requirements\\nFor this chapter, you\\'ll need a Python virtual environment, similar to the one we set up in \\nChapter 1, Python Development Environment Setup.\\nY ou can find all the code examples for this chapter in its dedicated GitHub repository: \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter10.\\nSetting and using environment variables\\nBefore deep diving into the different deployment techniques, we need to structure our \\napplication to enable reliable, fast, and secure deployments. One of the key things in \\nthis process is handling configuration variables: a database URL, an external API token, \\na debug flag, and so on. When handling those variables, it\\'s necessary to handle them \\ndynamically instead of hardcoding them in your source code. Why?\\nFirst of all, those variables will likely be different in your local environment and \\nin production. Typically, your database URL will point to a local database on your \\ncomputer while developing but will point to a proper production database in production. \\nThis is even more true if you want to have other environments such as a staging or \\npre-production environment. Furthermore, if we need to change one of the values, we\\'ll \\nhave to change the code, commit it, and deploy it again. Thus, we need a convenient \\nmechanism to set those values.\\nSecondly, it\\'s unsafe to write those values in your code. Values such as database \\nconnection strings or API tokens are extremely sensitive. If they appear in your code, \\nthey\\'ll likely be committed into your repository: they can be read by anyone who has \\naccess to your repository, which causes obvious security issues.\\nTo solve this, we usually use environment variables. Environment variables are values \\nthat aren\\'t set in the program itself but on the whole system. Most programming \\nlanguages have the required functions to read those variables from the system. Y ou can try \\nthis very easily in a Unix command line:\\n$ export MY_ENVIRONMENT_VARIABLE=\"Hello\" # Set a temporary \\nvariable on the system\\n$ python\\n>>> import os\\n>>> os.getenv(\"MY_ENVIRONMENT_VARIABLE\")\\xa0\\xa0# Get it in Python\\n\\'Hello\\''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 311, 'page_label': '293'}, page_content=\"Setting and using environment variables     293\\nIn the Python source code, we can get the value dynamically from the system. During \\ndeployment, we'll only have to make sure that we set the correct environment variables \\non the server. This way, we can easily change a value without redeploying the code and \\nhave several deployments of our application containing different configurations sharing \\nthe same source code. However, bear in mind that sensitive values that have been set in \\nenvironment variables could still leak if you don't pay attention; for example, in log files  \\nor error stack traces.\\nTo help us with this task, we'll use a very convenient feature of Pydantic: settings \\nmanagement. This allows us to structure and use our configuration variables as we do \\nfor any other data model. It even takes care of automatically retrieving the values from \\nenvironment variables!\\nFor the rest of this chapter, we'll work with an application you can find in chapter10/\\nproject of our example repository. It's a simple FastAPI application that uses Tortoise \\nORM, very similar to the one we reviewed in the Communicating with a SQL database \\nwith the Tortoise ORM section of Chapter 6, Databases and Asynchronous ORMs.\\nRunning the commands from the project directory\\nIf you cloned the examples repository, be sure to run the commands shown in \\nthis chapter from the project directory. In a command line, simply type cd \\nchapter10/project.\\nTo structure a settings model, all you need to do is create a class that inherits from \\npydantic.BaseSettings. The following example shows a configuration class  \\nwith a debug flag, an environment name, and a database URL:\\nsettings.py\\nfrom pydantic import BaseSettings\\nclass Settings(BaseSettings):\\n\\xa0\\xa0\\xa0\\xa0debug: bool = False\\n\\xa0\\xa0\\xa0\\xa0environment: str\\n\\xa0\\xa0\\xa0\\xa0database_url: str\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter10/project/app/\\nsettings.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 312, 'page_label': '294'}, page_content='294     Deploying a FastAPI Project \\nAs you can see, creating this class is very similar to creating a standard Pydantic model. We \\ncan even define default values, as we did for debug here. The good thing with this model is \\nthat it works just like any other Pydantic model: it automatically parses the values it finds in \\nenvironment variables and raises an error if one value is missing in your environment. This \\nway, you can ensure you don\\'t forget any values directly when the app starts.\\nTo use it, we only have to create an instance of this class, as shown in the following  \\ncode extract:\\napp.py\\nfrom app.settings import Settings\\nsettings = Settings()\\napp = FastAPI()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter10/project/app/\\napp.py\\nThen, you can use it whenever you need one of the variables. In this application, we added \\na startup event handler that prints all the settings when debug is True. Besides this, the \\nTortoise database URL has been set thanks to the settings object. Y ou can see this in \\nthe following example:\\napp.py\\n@app.on_event(\"startup\")\\nasync def startup():\\n\\xa0\\xa0\\xa0\\xa0if settings.debug:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0print(settings)\\nTORTOISE_ORM = {\\n\\xa0\\xa0\\xa0\\xa0\"connections\": {\"default\": settings.database_url},\\n\\xa0\\xa0\\xa0\\xa0\"apps\": {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"models\": {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"models\": [\"chapter10.project.models\"],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"default_connection\": \"default\",\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0},\\n\\xa0\\xa0\\xa0\\xa0},'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 313, 'page_label': '295'}, page_content='Setting and using environment variables     295\\n}\\nregister_tortoise(\\n\\xa0\\xa0\\xa0\\xa0app,\\n\\xa0\\xa0\\xa0\\xa0config=TORTOISE_ORM,\\n\\xa0\\xa0\\xa0\\xa0generate_schemas=True,\\n\\xa0\\xa0\\xa0\\xa0add_exception_handlers=True,\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter10/project/app/\\napp.py\\nY ou can use settings like any other object in your Python code. If you run this \\napplication, you\\'ll likely get the following kind of output:\\n$ uvicorn app.app:app\\npydantic.error_wrappers.ValidationError: 2 validation errors \\nfor Settings\\nenvironment\\n\\xa0\\xa0field required (type=value_error.missing)\\ndatabase_url\\n\\xa0\\xa0field required (type=value_error.missing)\\nAs we mentioned previously, if one value is missing in your environment, Pydantic will \\nraise an error and the application won\\'t start. Let\\'s set those variables and try again:\\n$ export DEBUG=\"true\" ENVIRONMENT=\"development\" DATABASE_\\nURL=\"sqlite://chapter10_project.db\"\\n$ uvicorn app.app:app\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\nINFO:\\xa0\\xa0\\xa0\\xa0\\xa0Started server process [1572]\\nINFO:\\xa0\\xa0\\xa0\\xa0\\xa0Waiting for application startup.\\ndebug=True environment=\\'development\\' database_url=\\'sqlite://\\nchapter10_project.db\\'\\nINFO:\\xa0\\xa0\\xa0\\xa0\\xa0Application startup complete.\\nINFO:uvicorn.error:Application startup complete.\\nINFO:\\xa0\\xa0\\xa0\\xa0\\xa0Uvicorn running on http://127.0.0.1:8000 (Press \\nCTRL+C to quit)\\nINFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8000 \\n(Press CTRL+C to quit)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 314, 'page_label': '296'}, page_content='296     Deploying a FastAPI Project \\nThe application started! Y ou can even see that our startup event handler printed our \\nsettings values. Notice that Pydantic is case-insensitive (by default) when retrieving \\nenvironment variables. By convention, environment variables are usually set in all caps  \\non the system.\\nUsing a .env file\\nIn local development, it\\'s a bit annoying to set environment variables by hand, especially \\nif you\\'re working on several projects at the same time on your machine. To solve this, \\nPydantic allows you to read the values from a .env file. This file contains a simple \\nlist of environment variables and their associated values. It\\'s usually easier to edit and \\nmanipulate during development.\\nTo make this work, we\\'ll need a new library, python-dotenv, whose task is to parse \\nthose .env files. Y ou can install it as usual with the following command:\\n$ pip install python-dotenv\\nThen, you can edit your Settings class, like this:\\nclass Settings(BaseSettings):\\n\\xa0\\xa0\\xa0\\xa0debug: bool = False\\n\\xa0\\xa0\\xa0\\xa0environment: str\\n\\xa0\\xa0\\xa0\\xa0database_url: str\\n\\xa0\\xa0\\xa0\\xa0class Config:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0env_file = \".env\"\\nY ou simply have to add a Config class and set the env_file property to the path of \\nyour .env file.\\nFinally, you can create your .env file at the root of the project with the following content:\\nDEBUG=true\\nENVIRONMENT=development\\nDATABASE_URL=sqlite://chapter10_project.db\\nAnd that\\'s it! settings will now be read from this .env file. If the file is missing, \\nSettings will try to read them from the environment variables as usual. Of course, this \\nis only for convenience while developing: this file shouldn\\'t be committed and you should \\nrely on properly set environment variables in production. To ensure you don\\'t commit this \\nfile by accident, it\\'s usually recommended that you add it to your .gitignore file.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 315, 'page_label': '297'}, page_content=\"Managing Python dependencies     297\\nCreating hidden files such as .env files\\nIn Unix systems, files starting with a dot, such as .env, are considered hidden \\nfiles. If you try to create them from the operating system's file explorer, it might \\nshow you warnings or even prevent you from doing so. Thus, it's usually more \\nconvenient to create them from your IDE, such as Visual Studio Code, or from \\nthe command line by using the touch.env command, for example.\\nGreat! Our application now supports dynamic configuration variables, which are now easy \\nto set and change on our deployment platforms. Another important thing to take care of is \\ndependencies: we've installed quite a lot of them at this point, but we must make sure they \\nare installed properly during deployments!\\nManaging Python dependencies\\nThroughout this book, we've installed libraries using pip to add some useful features to \\nour application: FastAPI, of course, but also SQLAlchemy, Tortoise ORM, Pytest, and so \\non. When deploying a project to a new environment, such as a production server, we have \\nto make sure all those dependencies are installed for our application to work properly. \\nThis is also true if you have colleagues that also need to work on the project: they need to \\nknow the dependencies they must install on their machines.\\nFortunately, pip comes with a solution for this so that we don't have to remember all this \\nin our heads. Indeed, most Python projects define a requirements.txt file, which \\ncontains a list of all Python dependencies. It usually lives at the root of your project. pip \\nhas a special option for reading this file and installing all the needed dependencies.\\nWhen you already have a working environment, such as the one we've used since the \\nbeginning of this book, people usually recommend that you run the following command:\\n$ pip freeze\\naerich==0.5.3\\naiofiles==0.7.0\\naiosqlite==0.16.1\\nalembic==1.6.3\\nappdirs==1.4.4\\nasgi-lifespan==1.0.1\\nasgiref==3.3.4\\nasync-asgi-testclient==1.4.6\\nasyncio-redis==0.16.0\\n...\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 316, 'page_label': '298'}, page_content=\"298     Deploying a FastAPI Project \\nThe result of pip freeze is a list of every Python package currently installed in your \\nenvironment, along with their corresponding versions. This list can be directly used in the \\nrequirements.txt file.\\nThe problem with this approach is that it lists every package, including the sub-dependencies \\nof the libraries you install. Said another way, in this list, you'll see packages that you don't \\ndirectly use but that are needed by the ones you installed. If, for some reason, you decide to \\nnot use a library anymore, you'll be able to remove it, but it'll be very hard to guess which \\nsub-dependencies it has installed. In the long term, your requirements.txt file will \\ngrow larger and larger, with lots of dependencies that are useless in your project.\\nTo solve this, some people recommend that you manually maintain your \\nrequirements.txt file. With this approach, you have to list yourself all the libraries \\nyou use, along with their respective versions. During installation, pip will take care of \\ninstalling the sub-dependencies, but they'll never appear in requirements.txt. This \\nway, when you remove one of your dependencies, you make sure any useless packages are \\nnot kept.\\nIn the following example, you can see the requirements.txt file for the project we are \\nworking on in this chapter:\\nrequirements.txt\\nfastapi==0.65.2\\ntortoise-orm[asyncpg]==0.17.4\\nuvicorn[standard]==0.14.0\\ngunicorn==20.1.0\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter10/project/\\nrequirements.txt\\nAs you can see, the list is much shorter! Now, whenever we install a new dependency,  \\nour responsibility is to add it manually to requirements.txt.\\nA word on alternate package managers such as Poetry, Pipenv and Conda\\nWhile exploring the Python community, you may hear about alternate package \\nmanagers such as Poetry, Pipenv, and Conda. These managers were created \\nto solve some issues posed by pip, especially around sub-dependencies \\nmanagement. While they are very good tools, lots of cloud platforms expect \\na traditional requirements.txt file to specify the dependencies, rather \\nthan those more modern tools. Therefore, they may not be the best choice for a \\nFastAPI application.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 317, 'page_label': '299'}, page_content='Managing Python dependencies     299\\nThe requirements.txt file should be committed along with your source code. When \\nyou need to install the dependencies on a new computer or server, you\\'ll simply need to \\nrun this command:\\n$ pip install -r requirements.txt \\nOf course, make sure that you\\'re working on proper virtual environments when doing \\nthis, as we described in Chapter 1, Python Development Environment Setup.\\nY ou have probably noticed the gunicorn dependency in requirements.txt.  \\nLet\\'s look at what it is and why it\\'s needed.\\nAdding Gunicorn as a server process for deployment\\nIn Chapter 2, Python Programming Specificities, we briefly introduced WSGI and ASGI \\nprotocols. They define the norm and data structure for building web servers in Python. \\nTraditional Python web frameworks, such as Django and Flask, rely on the WSGI \\nprotocol. ASGI appeared recently and is presented as the \"spiritual successor\" of WSGI, \\nproviding a protocol for developing web servers running asynchronously. This protocol is \\nat the heart of FastAPI and Starlette.\\nAs we mentioned in Chapter 3, Developing RESTful APIs with FastAPI, we use Uvicorn \\nto run our FastAPI applications: its role is to accept HTTP requests, transform them \\naccording to the ASGI protocol, and pass them to the FastAPI application, which returns \\nan ASGI-compliant response object. Then, Uvicorn can form a proper HTTP response \\nfrom this object.\\nIn the WSGI world, the most widely used server is Gunicorn. It has the same role in the \\ncontext of a Django or Flask application. Why are we talking about it, then? Gunicorn  \\nhas lots of refinements and features that make it more robust and reliable in production \\nthan Uvicorn. However, Gunicorn is designed to work for WSGI applications. So, what \\ncan we do?\\nActually, we can use both: Gunicorn will be used as a robust process manager for our \\nproduction server. However, we\\'ll specify a special worker class provided by Uvicorn, \\nwhich will allow us to run ASGI applications such as FastAPI. This is the recommended \\nway of doing deployments in the official Uvicorn documentation: https://www.\\nuvicorn.org/deployment/#using-a-process-manager.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 318, 'page_label': '300'}, page_content=\"300     Deploying a FastAPI Project \\nSo, let's install Gunicorn to our dependencies by using the following command \\n(remember to add it to your requirements.txt file):\\n$ pip install gunicorn\\nIf you wish, you can try to run our FastAPI project using Gunicorn by using the  \\nfollowing command:\\n$ gunicorn -w 4 -k uvicorn.workers.UvicornWorker app.app:app\\nIts usage is quite similar to Uvicorn, except that we tell it to use a Uvicorn worker.  \\nOnce again, this is necessary to make it work with an ASGI application. Also, notice the \\n-w option. It allows us to set the number of workers to launch for our server. Here, we \\nlaunch four instances of our application. Then, Gunicorn takes care of load balancing the \\nincoming requests between each worker. This is what makes Gunicorn more robust: if, for \\nany reason, your application blocks the event loop with a synchronous operation, other \\nworkers will be able to process other requests while this is happening.\\nNow, we are ready to deploy our FastAPI application! In the next section, you'll learn how \\nto deploy one on a serverless platform.\\nDeploying a FastAPI application on a serverless \\nplatform\\nIn recent years, serverless platforms have gained a lot of popularity and have become \\na very common way to deploy web applications. Those platforms completely hide the \\ncomplexity of setting up and managing a server, giving you the tools to automatically \\nbuild and deploy your application in minutes. Google App Engine, Heroku, and Azure \\nApp Service are among the most popular. Even though they have their own specificities, \\nall these serverless platforms work on the same principles. This is why, in this section, we'll \\noutline the common steps you should follow.\\nUsually, serverless platforms expect you to provide the source code in the form of a \\nGitHub repository, which you push directly to their servers or that they pull automatically \\nfrom GitHub. Here, we'll assume that you have a GitHub repository with the source code \\nstructured like so:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 319, 'page_label': '301'}, page_content=\"Deploying a FastAPI application on a serverless platform     301\\nFigure 10.1 – Project structure for serverless deployment\\n1. Create an account on a cloud platform of your choice. Y ou must do this before you \\ncan start any work. It's worth noting that most cloud platforms offer free credits \\nwhen you are getting started so that you can try their services for free.\\n2. Install the necessary command-line tools. Most cloud providers supply a complete \\nCLI for managing their services. Typically this is required for deploying your \\napplication. Here are the relevant documentation pages for the most popular  \\ncloud providers:\\n \\x8f Google Cloud: https://cloud.google.com/sdk/gcloud\\n \\x8f Microsoft Azure: https://docs.microsoft.com/en-us/cli/azure/\\ninstall-azure-cli\\n \\x8f Heroku: https://devcenter.heroku.com/articles/heroku-cli\\n3. Set up the application configuration. Depending on the platform, you'll either have \\nto create a configuration file or use the CLI or the web interface to do this. Here are \\nthe relevant documentation pages for the most popular cloud providers:\\n \\x8f Google App Engine (configuration file): https://cloud.google.com/\\nappengine/docs/standard/python3/configuring-your-app-\\nwith-app-yaml\\n \\x8f Azure App Service (web interface and CLI): https://docs.microsoft.\\ncom/en-us/azure/app-service/quickstart-python and \\nhttps://docs.microsoft.com/en-us/azure/app-service/\\nconfigure-language-python\\n \\x8f Heroku (configuration file): https://devcenter.heroku.com/\\narticles/getting-started-with-python#define-a-procfile\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 320, 'page_label': '302'}, page_content=\"302     Deploying a FastAPI Project \\nThe key point in this step is to correctly set the startup command. As we saw in the \\nprevious section, it's essential to set the Uvicorn worker class using the Gunicorn \\ncommand, as well as set the correct path to your application.\\n4. Set the environment variables. Depending on the cloud provider, you should be \\nable to do so during configuration or deployment. Remember that they are key for \\nyour application to work. Here are the relevant documentation pages for the most \\npopular cloud providers:\\n \\x8f Google App Engine (configuration file): https://cloud.google.com/\\nappengine/docs/standard/python/config/appref\\n \\x8f Azure App Service (web interface): https://docs.microsoft.com/\\nen-us/azure/app-service/configure-common#configure-app-\\nsettings\\n \\x8f Heroku (CLI or web interface): https://devcenter.heroku.com/\\narticles/config-vars\\n5. Deploy the application. Some platforms can automatically deploy when they detect \\nchanges on a hosted repository, such as GitHub. Others require that you start a \\ndeployment from the command-line tools. Here are the relevant documentation \\npages for the most popular cloud providers:\\n \\x8f Google App Engine (CLI): https://cloud.google.com/appengine/\\ndocs/standard/python3/testing-and-deploying-your-\\napp#deploying_your_application\\n \\x8f Azure App Service (continuous deployment or manual Git deployment): \\nhttps://docs.microsoft.com/en-us/azure/app-service/\\ndeploy-continuous-deployment?tabs=github and https://\\ndocs.microsoft.com/en-us/azure/app-service/deploy-\\nlocal-git?tabs=cli\\n \\x8f Heroku (CLI): https://devcenter.heroku.com/articles/\\ngetting-started-with-python#deploy-the-app\\nY our application should now be live on the platform. Most cloud platforms actually \\nautomatically build and deploy Docker containers while following the configuration  \\nyou provide.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 321, 'page_label': '303'}, page_content=\"Deploying a FastAPI application on a serverless platform     303\\nThey will make your application available on generic subdomain such as \\nmyapplication.herokuapp.com. Of course, they also provide mechanisms for \\nbinding it to your own domain or subdomain. Here are the relevant documentation pages \\nfor the most popular cloud providers:\\n• Google App Engine: https://cloud.google.com/appengine/docs/\\nstandard/python3/mapping-custom-domains\\n• Azure App Service: https://docs.microsoft.com/en-us/azure/\\napp-service/manage-custom-dns-migrate-domain\\n• Heroku: https://devcenter.heroku.com/articles/custom-domains\\nAdding database servers\\nMost of the time, your application will be backed by a database engine, such as \\nPostgreSQL. Fortunately, cloud providers propose fully managed databases, billed \\naccording to the computing power, memory, and storage you need. Once created, you'll \\nhave access to a connection string to connect to the database instance. All you have to \\ndo then is set it in the environment variables of your application. Here are the relevant \\ndocumentation pages for getting started with managed databases with the most popular \\ncloud providers:\\n• Google Cloud SQL: https://cloud.google.com/sql/docs/postgres/\\ncreate-instance\\n• Azure Database for PostgreSQL: https://docs.microsoft.com/en-us/\\nazure/postgresql/quickstart-create-server-database-portal\\n• Amazon RDS: https://docs.aws.amazon.com/AmazonRDS/latest/\\nUserGuide/CHAP_GettingStarted.html\\n• Heroku Postgres: https://devcenter.heroku.com/articles/heroku-\\npostgresql\\nManaging Database Migrations\\nIn Chapter 6, Databases and Asynchronous ORMs, we presented you with \\nsome tools you can use to manage database migrations: Alembic and Aerich. \\nWhen working with a cloud database, we advise you to still run them from \\nyour local machine so that you have full control of how they are executed, as \\nwell as to check if everything goes well. Just be sure to set the correct database \\nconnection string.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 322, 'page_label': '304'}, page_content=\"304     Deploying a FastAPI Project \\nAs we've seen, serverless platforms are the quickest and easiest way to deploy a FastAPI \\napplication. However, in some situations, you may wish to have more control of how \\nthings are deployed, or you may need system packages that are not available on serverless \\nplatforms. In those cases, it may be worthwhile using a Docker container.\\nDeploying a FastAPI application with Docker\\nDocker is a widely used technology for containerization. Containers are small,  \\nself-contained systems running on a computer. Each container contains all the files \\nand configurations necessary for running a single application: a web server, a database \\nengine, a data processing application, and so on. The main goal is to be able to run those \\napplications without worrying about dependency and version conflicts that often happen \\nwhen trying to install and configure them on the system.\\nBesides, Docker containers are designed to be portable and reproducible: to create a \\nDocker container, you simply have to write a Dockerfile containing all the necessary \\ninstructions to build the small system, along with all the files and configuration you need. \\nThose instructions are executed during a build, which results in a Docker image. This \\nimage is a package containing your small system, ready to use, that you can easily share on \\nthe internet through registries. Any developer who has a working Docker installation can \\nthen download this image and run it on their system in a container.\\nDocker has been quickly adopted by developers as it greatly eases the setup of complex \\ndevelopment environments, allowing them to have several projects with different system \\npackage versions, all without worrying about their installation on their local machine.\\nHowever, Docker is not only for local development: it's also widely used for deploying \\napplications to production. Since the builds are reproducible, we can ensure that the \\nenvironments in local and in production remain the same; which limits issues when \\npassing to production.\\nIn this section, we'll learn how to write a Dockerfile for a FastAPI application, how to \\nbuild an image, and how to deploy it on a cloud platform.\\nWriting a Dockerfile\\nAs we mentioned in the introduction to this section a Dockerfile is a set of instructions for \\nbuilding your Docker image, a self-contained system containing all the required components \\nto run your applications. To begin with, all Dockerfiles derive from a base image; usually, \\nthis is a standard Linux installation, such as Debian or Ubuntu. From this base, we can copy \\nfiles from our local machine into the image (usually, the source code of our application) and \\nexecute Unix commands; for example, to install packages or execute scripts.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 323, 'page_label': '305'}, page_content=\"Deploying a FastAPI application with Docker     305\\nIn our case, the creator of FastAPI has created a base Docker image that contains all the \\nnecessary tools to run a FastAPI app! All we have to do is start from this image, copy our \\nsource files, and install our dependencies! Let's learn how to do that!\\nFirst of all, you'll need a working Docker installation on your machine. Follow the official \\ngetting started tutorial, which should guide you in this process: https://docs.\\ndocker.com/get-started/.\\nTo create a Docker image, we simply have to create a file named Dockerfile at the root \\nof our project. The following example shows the content of this file for our current project:\\nDockerfile\\nFROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\\nENV APP_MODULE app.app:app\\nCOPY requirements.txt /app\\nRUN pip install --upgrade pip && \\\\\\n\\xa0\\xa0\\xa0\\xa0pip install -r /app/requirements.txt\\nCOPY ./ /app\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter10/project/\\nDockerfile\\nLet's go through each instruction. The first instruction, FROM, is the base image we derive \\nfrom. Here, we took the uvicorn-gunicorn-fastapi image, which was created \\nby the creator of FastAPI. Docker images have tags, which can be used to pick a specific \\nversion of the image. Here, we chose Python version 3.7. Lots of variations exist for this \\nimage, including ones with newer versions of Python. Y ou can check them out in the \\nofficial README: https://github.com/tiangolo/uvicorn-gunicorn-\\nfastapi-docker.\\nThen, we set the APP_MODULE environment variable thanks to the ENV instruction. \\nIn a Docker image, environment variables can be set at build time, as we did here, or at \\nruntime. APP_MODULE is an environment variable defined by the base image. It should \\npoint to the path of your FastAPI application: it's the same argument that we set at the end \\nof Uvicorn and Gunicorn commands to launch the application. Y ou can find the list of all \\nthe accepted environment variables for the base image in the official README.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 324, 'page_label': '306'}, page_content=\"306     Deploying a FastAPI Project \\nNext, we have our first COPY statement. As you may have guessed, this instruction \\nwill copy a file from your local system to the image. Here, we only copied our \\nrequirements.txt file. We'll explain why shortly. Notice that we copied the file into the \\n/app directory of the image; it's the main working directory defined by the base image.\\nWe then have a RUN statement. This instruction is used to execute Unix commands. In our \\ncase, we ran pip to install our dependencies, following the requirements.txt file we \\njust copied. This is essential to make sure all our Python dependencies are present.\\nFinally, we copied the rest of our source code files into the /app directory. Now, let's \\nexplain why we separately copied requirements.txt. The important thing to \\nunderstand is that Docker images are built using layers: each instruction will create a new \\nlayer in the build system. To improve performance, Docker does its best to reuse layers it \\nhas already built. Therefore, if it detects no changes from the previous build, it'll reuse the \\nones it has in memory without rebuilding them.\\nBy copying the requirements.txt file alone and installing the Python dependencies \\nbefore the rest of the source code, we allow Docker to reuse the layer where the \\ndependencies have been installed. If we edit our source code but not requirements.\\ntxt, the Docker build will only execute the last COPY instruction, reusing all the previous \\nlayers. Thus, the image is built in a few seconds instead of minutes.\\nMost of the time, Dockerfiles end with a CMD instruction, which should be the command \\nto execute when the container is started. In our case, we would have used the Gunicorn \\ncommand we saw in the Adding Gunicorn as a server section. However, in our case, the \\nbase image is already handling this for us.\\nBuilding a Docker image\\nWe can now build our Docker image! From the root of your project, just run the  \\nfollowing command:\\n$ docker build -t fastapi-app\\xa0\\xa0.\\nThe dot (.) denotes the path of the root context to build your image – in this case, the \\ncurrent directory. The -t option is here to tag the image and give it a practical name.\\nDocker will then perform the build. Y ou'll see that it'll download the base image and \\nsequentially run your instructions. This should take a few minutes. If you run the \\ncommand again, you'll experience what we explained earlier about layers: if there is no \\nchange, layers are reused and the build takes only a few seconds.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 325, 'page_label': '307'}, page_content='Deploying a FastAPI application with Docker     307\\nRunning a Docker image locally\\nBefore deploying it to production, you can try to run your image locally. To do this, run \\nthe following command:\\n$ docker run -p 8000:80 -e ENVIRONMENT=production -e DATABASE_\\nURL=sqlite://./app.db fastapi-app\\nHere, we used the run command with the name of the image we just built. There are, of \\ncourse, a few options here:\\n• -p allows you to publish ports on your local machine. By default, Docker containers \\nare not accessible on your local machine. If you publish ports, they will be available \\nthrough localhost. On the container side, the FastAPI application is executed on \\nport 80. We publish it on port 8000 on our local machine; that is, 8000:80.\\n• -e is used to set environment variables. As we mentioned in the Setting and using \\nenvironment variables section, we need those variables to configure our application. \\nDocker allows us to set them easily and dynamically at runtime. Notice that we set \\na simple SQLite database for testing purposes. However, in production, it should \\npoint to a proper database.\\n• Y ou can review the numerous options of this command in the official Docker \\ndocumentation: https://docs.docker.com/engine/reference/\\ncommandline/run/#options.\\nThis command will run your application, which will be accessible through  \\nhttp://localhost:8000. Docker will show you the logs in the terminal.\\nDeploying a Docker image\\nNow that you have a working Docker image, you can deploy it on virtually any machine \\nthat runs Docker. This can be your own server or a dedicated platform. Lots of serverless \\nplatforms have emerged to help you deploy container images automatically: Google Cloud \\nRun, Amazon Elastic Container Service, and Microsoft Azure Container Instances are just \\na few.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 326, 'page_label': '308'}, page_content='308     Deploying a FastAPI Project \\nUsually, what you have to do is upload (push, in Docker jargon) your image to a registry. \\nBy default, Docker pulls and pushes images from Docker Hub, the official Docker registry, \\nbut lots of services and platforms propose their own registries. Usually, using the private \\ncloud registry proposed by the cloud platform is necessary to deploy it on this platform. \\nHere are the relevant documentation pages for getting started with private registries with \\nthe most popular cloud providers:\\n• Google Artifact Registry: https://cloud.google.com/artifact-\\nregistry/docs/docker/quickstart\\n• Amazon ECR: https://docs.aws.amazon.com/AmazonECR/latest/\\nuserguide/getting-started-console.html\\n• Microsoft Azure Container Registry: https://docs.microsoft.com/\\nen-us/azure/container-registry/container-registry-get-\\nstarted-docker-cli?tabs=azure-cli\\nIf you followed the relevant instructions, you should have a private registry for storing \\nDocker images. The instructions probably showed you how to authenticate your local \\nDocker command line with it and how to push your first image. Basically, all you have to \\ndo is tag the image you built with the path to your private registry:\\n$ docker tag fastapi-app aws_account_id.dkr.ecr.region.\\namazonaws.com/fastapi-app\\nThen, you need to push it to the registry:\\n$ docker push fastapi-app aws_account_id.dkr.ecr.region.\\namazonaws.com/fastapi-app\\nY our image is now safely stored in the cloud platform registry. Y ou can now use their \\nserverless container platform to deploy it automatically. Here are the relevant documentation \\npages for getting started with private registries with the most popular cloud providers:\\n• Google Cloud Run: https://cloud.google.com/run/docs/\\nquickstarts/build-and-deploy/python\\n• Amazon Elastic Container Service: https://docs.aws.amazon.com/\\nAmazonECS/latest/developerguide/getting-started-ecs-ec2.\\nhtml\\n• Microsoft Azure Container Instances: https://docs.microsoft.com/\\nen-us/azure/container-instances/container-instances-\\ntutorial-deploy-app'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 327, 'page_label': '309'}, page_content=\"Deploying a FastAPI application on a traditional server     309\\nOf course, you'll be able to set the environment variables just like you can for fully \\nmanaged apps. Those environments also provide lots of options for tuning the scalability \\nof your containers, both vertically (using more powerful instances) and horizontally \\n(spawn more instances).\\nOnce done, your application should be live on the web! The great thing about deploying \\nDocker images compared to automated serverless platforms is that you are not limited \\nto the features supported by the platform: you can deploy anything, even complex \\napplications that require a lot of exotic packages, without worrying about compatibility.\\nAt this point, we've seen the easiest and most efficient ways to deploy a FastAPI \\napplication. However, you may wish to deploy one the old-fashioned way and manually \\nset up your server. In the next section, we'll provide some guidelines to do so.\\nDeploying a FastAPI application on a \\ntraditional server\\nIn some situations, you may not have the chance to use a serverless platform to deploy \\nyour application. Some security or regulatory policies may force you to deploy on physical \\nservers with specific configurations. In this case, it's worth knowing some basic things so \\nthat you can deploy your application on traditional servers.\\nIn this section, we'll consider you are working on a Linux server:\\n1. First of all, make sure a recent version of Python has been installed on your server, \\nideally with the version matching the one you used in development. The easiest \\nway to do this is to set up pyenv, as we saw in Chapter 1, Python Development \\nEnvironment Setup.\\n2. To retrieve your source code and keep it in sync with your latest developments, \\nyou can clone your Git repository on your server. This way, you only have to pull the \\nchanges and restart the server process to deploy a new version.\\n3. Set up a Python virtual environment, as we explained in Chapter 1, Python \\nDevelopment Environment Setup. Y ou can install the dependencies with pip thanks \\nto your requirements.txt file.\\n4. At that point, you should be able to run Gunicorn and start serving your FastAPI \\napplication. However, some improvements are strongly recommended.\\n5. Use a process manager to ensure your Gunicorn process is always running and \\nrestarted when the server is restarted. A good option for this is Supervisor. The \\nGunicorn documentation provides good guidelines for this: https://docs.\\ngunicorn.org/en/stable/deploy.html#supervisor.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 328, 'page_label': '310'}, page_content=\"310     Deploying a FastAPI Project \\n6. It's also recommended to put Gunicorn behind an HTTP proxy instead of directly \\nputting it on the front line. Its role is to handle SSL connections, perform load \\nbalancing, and serve static files such as images or documents. The Gunicorn \\ndocumentation recommends using Nginx for this task and provides a basic \\nconfiguration: https://docs.gunicorn.org/en/stable/deploy.\\nhtml#nginx-configuration.\\nAs you can see, in this context, there's quite a lot of configurations and decisions to  \\nmake regarding your server configuration. Of course, you should also pay attention to \\nsecurity and make sure your server is well-protected against the usual attacks. In the \\nfollowing DigitalOcean tutorial, you'll find some guidelines for securing your server: \\nhttps://www.digitalocean.com/community/tutorials/recommended-\\nsecurity-measures-to-protect-your-servers.\\nIf you're not an experienced system administrator, we recommend that you favor serverless \\nplatforms: professional teams handle security, system updates, and server scalability for \\nyou, letting you focus on what matters most for you: developing a great application!\\nSummary\\nY our application is now live on the web! In this chapter, we covered the best practices to \\napply before deploying your application to production: use environment variables to set \\nconfiguration options, such as database URLs, and manage your Python dependencies \\nwith a requirements.txt file. Then, we showed you how to deploy your application \\nto a serverless platform, which handles everything for you by retrieving your source code, \\npackaging it with its dependencies, and serving it on the web. Next, you learned how to \\nbuild a Docker image for FastAPI using the base image created by the creator of FastAPI. \\nAs you've seen, it allows you to be flexible while configuring the system, but you can still \\ndeploy it in a few minutes with a serverless platform that's compatible with containers. \\nFinally, we provided you with some guidelines for manual deployment on a traditional \\nLinux server.\\nThis marks the end of the second part of this book. Y ou should now be confident in \\nwriting efficient, reliable FastAPI applications and be able to deploy them on the web.\\nIn the next chapter, we will begin some data science tasks and integrate them efficiently in \\na FastAPI project.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 329, 'page_label': '311'}, page_content=\"Section 3:  \\nBuild a Data Science \\nAPI with Python and \\nFastAPI\\nThis section will introduce the most common libraries used in Python to perform data \\nscience-related tasks. We'll see how to integrate those tools in a FastAPI backend with \\nperformance and maintainability in mind.\\n\\xa0This section comprises the following chapters:\\n• Chapter 11, Introduction to NumPy and pandas \\n• Chapter12, Train Machine Learning Models with scikit-learn \\n• Chapter13, Create an Efficient Prediction API Endpoint with FastAPI \\n• Chapter 14, Implement a Real-Time Face Detection System Using WebSockets with \\nFastAPI and OpenCV\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 330, 'page_label': '312'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 331, 'page_label': '313'}, page_content=\"11\\nIntroduction to \\nNumPy and pandas \\nIn recent years, Python has gained a lot of popularity in the data science field. Its very \\nefficient and readable syntax makes the language a very good choice for scientific research, \\nwhile still being suitable for production workloads: it's very easy to deploy research \\nprojects into real applications that will bring value to users. Thanks to this growing \\ninterest, a lot of specialized Python libraries have emerged. The most well known are \\nprobably NumPy and pandas. Their goal is to provide a set of tools to manipulate a big set \\nof data in an efficient way, much more than what we could actually achieve with standard \\nPython, and we'll show how and why in this chapter. NumPy and pandas are at the heart \\nof most data science applications in Python; knowing them is therefore the first step on \\nyour journey into Python for data science.\\nIn this chapter, we're going to cover the following main topics:\\n• Getting started with NumPy\\n• Manipulating arrays with NumPy: computation, aggregations, comparisons\\n• Getting started with pandas\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 332, 'page_label': '314'}, page_content='314     Introduction to NumPy and pandas \\nTechnical requirements\\nY ou\\'ll need a Python virtual environment, as we set up in Chapter 1, Python Development \\nEnvironment Setup.\\nY ou\\'ll find all the code examples of this chapter in the dedicated GitHub repository: \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter11.\\nGetting started with NumPy\\nIn Chapter 2, Python Programming Specificities, we stated that Python is a dynamically \\ntyped language. This means that the interpreter automatically detects the type of a variable \\nat runtime, and this type can even change throughout the program. For example, you can \\ndo something like this in Python:\\n$ python\\n>>> x = 1\\n>>> type(x)\\n<class \\'int\\'>\\n>>> x = \"hello\"\\n>>> type(x)\\n<class \\'str\\'>\\nThe interpreter was able to determine the type of x at each assignation.\\nUnder the hood, the standard implementation of Python, CPython, is written in C.  \\nThe C language is a compiled and statically typed language. This means that the nature \\nof the variables is fixed at compile time, and they can\\'t change during execution. Thus, \\nin the Python implementation, a variable doesn\\'t only consist in its value: it\\'s actually \\na structure containing information about the variable, including its type and size, in \\naddition to its value.\\nThanks to this, we can manipulate variables very dynamically in Python. However,  \\nit comes at a cost: each variable has a significantly higher memory footprint to store all its \\nmetadata than just the plain value.\\nThis is particularly true for data structures. Say we consider a simple list like this:\\n$ python\\n>>> l = [1, 2, 3, 4, 5]'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 333, 'page_label': '315'}, page_content=\"Getting started with NumPy     315\\nEach item of the list is a Python integer, with all the metadata associated. In a statically \\ntyped language such as C, the same list would only be a suite of values in memory sharing \\nthe same type.\\nLet's now imagine a big set of data, like the kind we usually encounter in data science: \\nthe cost of storing it in memory would be huge. That's exactly the purpose of NumPy: \\nprovide a powerful and efficient array structure to manipulate a big set of data. Under the \\nhood, it uses a fixed-type array, meaning all elements of the structure are of the same type, \\nwhich allows NumPy to get rid of the costly metadata of every single element. Moreover, \\ncommon arithmetic operations, such as additions or multiplications, are much faster.  \\nIn the Manipulating arrays with NumPy – computation, aggregations, comparisons section \\nof this chapter, we'll make a speed comparison to show you the difference with standard \\nPython lists.\\nTo get started, let's install NumPy using the following command:\\n$ pip install numpy\\nIn a Python interpreter, we can now import the library:\\n$ python\\n>>> import numpy as np\\nNotice that, by convention, NumPy is always imported with the alias np. Let's now discover \\nits basic features!\\nCreating arrays\\nTo create an array with NumPy, we can simply use the array function and pass it  \\na Python list:\\n>>> np.array([1, 2, 3, 4, 5])\\narray([1, 2, 3, 4, 5])\\nNumPy will detect the nature of the Python list. However, we can force the resulting type \\nby using the dtype argument:\\n>>> np.array([1, 2, 3, 4, 5], dtype=np.float64)\\narray([1., 2., 3., 4., 5.])\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 334, 'page_label': '316'}, page_content='316     Introduction to NumPy and pandas \\nAll elements were upcasted to the specified type. It is key to remember that a NumPy \\narray is of a fixed type. This means that every element will have the same type and \\nNumPy will silently cast a value to the array type. For example, let\\'s consider an integer  \\nlist in which we want to insert a floating-point value:\\n>>> l = np.array([1, 2, 3, 4, 5])\\n>>> l[0] = 13.37\\n>>> l\\narray([13,  2,  3,  4,  5])\\nThe value 13.37 has been truncated to fit into an integer.\\nIf the value cannot be cast to the type of array, an error is raised. For example, let\\'s try to \\nchange the first element by using a string:\\n>>> l[0] = \"a\"\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nValueError: invalid literal for int() with base 10: \\'a\\'\\nAs we said in the introduction to this section, Python lists are not very efficient for large \\ndatasets. This is why it\\'s generally more efficient to use NumPy functions to create arrays. \\nThe most commonly used ones are generally the following:\\n• np.zeros, to create an array filled with zeros\\n• np.ones, to create an array filled with ones\\n• np.empty, to create an empty array of the desired size in memory,  \\nwithout initializing the values\\n• np.arange, to create an array with a range of elements\\nLet\\'s see them in action:\\n>>> np.zeros(5)\\narray([0., 0., 0., 0., 0.])\\n>>> np.ones(5)\\narray([1., 1., 1., 1., 1.])\\n>>> np.empty(5)\\narray([1., 1., 1., 1., 1.])\\n>>> np.arange(5)\\narray([0, 1, 2, 3, 4])'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 335, 'page_label': '317'}, page_content=\"Getting started with NumPy     317\\nNotice that the result of np.empty can vary: since the values in the array are not \\ninitialized, they take whatever value there is currently in this memory block. The main \\nmotivation behind this function is speed, allowing you to quickly allocate memory; but \\ndon't forget to fill every element after.\\nBy default, NumPy create arrays with a floating-point type (float64). Once again,  \\nby using the dtype argument, you can force another type to be used:\\n>>> np.ones(5, dtype=np.int32)\\narray([1, 1, 1, 1, 1], dtype=int32)\\nNumPy provides a wide range of types, allowing you to finely optimize the memory \\nconsumption of your program by selecting the right type for your data. Y ou can find \\nthe whole list of types supported by NumPy in the official documentation: https://\\nnumpy.org/doc/stable/reference/arrays.scalars.html#sized-\\naliases.\\nNumPy also proposes a function to create an array with random values:\\n>>> np.random.seed(0)  # Set the random seed to make examples \\nreproducible\\n>>> np.random.randint(10, size=5)\\narray([5, 0, 3, 3, 7])\\nThe first argument is the maximum range of the random value, and the size argument \\nsets the number of values to generate.\\nUntil now, we showed how to create one-dimensional arrays. However, the great strength \\nof NumPy is that it natively handles multi-dimensional arrays! For example, let's create  \\na 3 x 4 matrix:\\n>>> m = np.ones((3,4))\\n>>> m\\narray([[1., 1., 1., 1.],\\n       [1., 1., 1., 1.],\\n       [1., 1., 1., 1.]])\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 336, 'page_label': '318'}, page_content='318     Introduction to NumPy and pandas \\nNumPy did create an array with three rows and four columns! All we had to do was to \\npass a tuple to the NumPy function to specify our dimensions. When having such an \\narray, NumPy gives us access to properties to know the number of dimensions, as well as \\nthe shape and size of it:\\n>>> m.ndim\\n2\\n>>> m.shape\\n(3, 4)\\n>>> m.size\\n12\\nAccessing elements and sub-arrays\\nNumPy arrays closely follow the standard Python syntax to manipulate lists. Therefore,  \\nto access an element in a one-dimensional array, just do the following:\\n>>> l = np.arange(5)\\n>>> l[2]\\n2\\nFor multi-dimensional arrays, we just have to add another index:\\n>>> np.random.seed(0)\\n>>> m = np.random.randint(10, size=(3,4))\\n>>> m\\narray([[5, 0, 3, 3],\\n       [7, 9, 3, 5],\\n       [2, 4, 7, 6]])\\n>>> m[1][2]\\n3\\nOf course, this can be used to re-assign elements:\\n>>> m[1][2] = 42\\n>>> m\\narray([[ 5,  0,  3,  3],\\n       [ 7,  9, 42,  5],\\n       [ 2,  4,  7,  6]])'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 337, 'page_label': '319'}, page_content=\"Getting started with NumPy     319\\nBut that's not all. Thanks to the slicing syntax, we can access sub-arrays with a start  \\nand end index and even a step. For example, on a one-dimensional array, we can do  \\nthe following:\\n>>> l = np.arange(5)\\n>>> l\\narray([0, 1, 2, 3, 4])\\n>>> l[1:4]  # From index 1 (inclusive) to 4 (exclusive)\\narray([1, 2, 3])\\n>>> l[::2]  # Every second element\\narray([0, 2, 4])\\nThis is exactly what we saw for standard Python lists in Chapter 2, Python Programming \\nSpecificities. Of course, it also works for multi-dimensional arrays, with one slice for  \\neach dimension:\\n>>> np.random.seed(0)\\n>>> m = np.random.randint(10, size=(3,4))\\n>>> m\\narray([[5, 0, 3, 3],\\n       [7, 9, 3, 5],\\n       [2, 4, 7, 6]])\\n>>> m[1:, 0:2]  # From row 1 to end and column 0 to 2\\narray([[7, 9],\\n       [2, 4]])\\n>>> m[::, 3:]  # Every row, only last column\\narray([[3],\\n       [5],\\n       [6]])\\nY ou can assign those sub-arrays to variables. However, for performance reasons, NumPy \\ndoesn't copy the values by default: it's only a view (or shallow copy), a representation of \\nthe existing data. This is important to bear in mind because if you change a value on the \\nview, it will also change the value on the original array:\\n>>> v = m[::, 3:]\\n>>> v[0][0] = 42\\n>>> v\\narray([[42],\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 338, 'page_label': '320'}, page_content=\"320     Introduction to NumPy and pandas \\n       [ 5],\\n       [ 6]])\\n>>> m\\narray([[ 5,  0,  3, 42],\\n       [ 7,  9,  3,  5],\\n       [ 2,  4,  7,  6]])\\nIf you need to deep copy the values, you just have to use the copy method on the array:\\n>>> v = m[::, 3:].copy()\\nv is now a separate copy of m, and changes on its values won't change the values in m.\\nY ou now have the basics of handling arrays with NumPy. As we've seen, the syntax is very \\nsimilar to standard Python. The key points to remember when working with NumPy are \\nthe following:\\n• NumPy arrays are of fixed types, meaning every item in the array are of the  \\nsame type.\\n• NumPy natively handles multi-dimensional arrays and allows us to subset them \\nusing the standard slicing notation.\\nOf course, NumPy can do much more than that: actually, it can apply common \\ncomputations to those arrays in a very performant way.\\nManipulating arrays with NumPy – \\ncomputation, aggregations, comparisons\\nAs we said, NumPy is all about manipulating large arrays with great performance and \\ncontrolled memory consumption. Let's say, for example, that we want to compute \\nthe double of each element in a large array. In the following example, you can see an \\nimplementation of such a function with a standard Python loop:\\nchapter11_compare_operations.py\\nimport numpy as np\\nnp.random.seed(0)  # Set the random seed to make examples \\nreproducible\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 339, 'page_label': '321'}, page_content='Manipulating arrays with NumPy – computation, aggregations, comparisons     321\\nm = np.random.randint(10, size=1000000)  # An array with a \\nmillion of elements\\ndef standard_double(array):\\n    output = np.empty(array.size)\\n    for i in range(array.size):\\n        output[i] = array[i] * 2\\n    return output\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter11/chapter11_\\ncompare_operations.py\\nWe instantiate an array with a million random integers. Then, we have our function \\nbuilding an array with the double of each element. Basically, we first instantiate an empty \\narray of the same size before looping over each element to set the double.\\nLet\\'s measure the performance of this function. In Python, there is a standard module, \\ntimeit, dedicated to this purpose. We can use it directly from the command line and \\npass in argument-valid Python statements that we want to measure performance. The \\nfollowing command will measure the performance of standard_double with our  \\nbig array:\\n$ python -m timeit \"from chapter11.chapter11_compare_operations \\nimport m, standard_double; standard_double(m)\"\\n1 loop, best of 5: 315 msec per loop\\nThe results will vary depending on your machine, but the magnitude should be equivalent. \\nWhat timeit does is to repeat your code a certain number of times and measure its \\nexecution time. Here, our function took around 300 milliseconds to compute the double \\nof each element in our array. For such simple computations on a modern computer,  \\nthat\\'s not very impressive.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 340, 'page_label': '322'}, page_content='322     Introduction to NumPy and pandas \\nLet\\'s compare this with the equivalent operation using NumPy syntax. Y ou can see it in \\nthe next sample:\\nchapter11_compare_operations.py\\ndef numpy_double(array):\\n    return array * 2\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter11/chapter11_\\ncompare_operations.py\\nThe code is much shorter! NumPy implements the basic arithmetic operations and can \\napply them to each element of the array. By multiplying the array by a value directly, \\nwe implicitly tell NumPy to multiply each element by this value. Let\\'s measure the \\nperformance with timeit:\\n$ python -m timeit \"from chapter11.chapter11_compare_operations \\nimport m, numpy_double; numpy_double(m)\"      \\n500 loops, best of 5: 667 usec per loop\\nHere, the best loop achieved the computation in 600 microseconds! That\\'s almost a \\nthousand times faster than the previous function! How can we explain such a variation?  \\nIn a standard loop, Python, because of its dynamic nature, has to check for the type of \\nvalue at each iteration to apply the right function for this type, which adds significant \\noverhead. With NumPy, the operation is deferred to an optimized and compiled loop \\nwhere types are known ahead of time, which saves a lot of useless checks.\\nWe once again see here the benefits of NumPy arrays over standard lists when working on \\na large dataset: it implements operations natively to help you make computations very fast.\\nAdding and multiplicating arrays \\nAs you saw in the previous example, NumPy supports the arithmetic operators to make \\noperations over arrays. \\nThis means that you can operate directly over two arrays of the same dimensions:\\n>>> np.array([1, 2, 3]) + np.array([4, 5, 6])\\narray([5, 7, 9])'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 341, 'page_label': '323'}, page_content='Manipulating arrays with NumPy – computation, aggregations, comparisons     323\\nIn this case, NumPy applies the operation element-wise. But it also works in certain \\nsituations if one of the operands is not of the same shape:\\n>>> np.array([1, 2, 3]) * 2\\narray([2, 4, 6])\\nNumPy automatically understands that it should multiply each element by two. This is \\ncalled broadcasting: NumPy \"expands\" the smaller array to match the shape of the larger \\narray. The previous example is equivalent to this one:\\n>>> np.array([1, 2, 3]) * np.array([2, 2, 2])\\narray([2, 4, 6])\\nNote that even if those two examples are conceptually equivalent, the first one is more \\nmemory-efficient and computationally efficient: NumPy is smart enough to use only one \\nvalue, \"two\", without having to create a full array of \"two\".\\nMore generally, broadcasting works if the rightmost dimensions of the arrays are of the \\nsame size or if one of them is one. For example, we can add an array of dimensions 4 x 3 to \\nan array of dimensions 1 x 3:\\n>>> a1 = np.ones((4, 3))\\n>>> a1\\narray([[1., 1., 1.],\\n       [1., 1., 1.],\\n       [1., 1., 1.],\\n       [1., 1., 1.]])\\n>>> a2 = np.ones((1, 3)) \\n>>> a2\\narray([[1., 1., 1.]])\\n>>> a1 + a2\\narray([[2., 2., 2.],\\n       [2., 2., 2.],\\n       [2., 2., 2.],\\n       [2., 2., 2.]])'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 342, 'page_label': '324'}, page_content='324     Introduction to NumPy and pandas \\nHowever, adding an array of dimensions 4 x 3 to an array of dimensions 1 x 4 is  \\nnot possible:\\n>>> a3 = np.ones((1, 4))\\n>>> a3\\narray([[1., 1., 1., 1.]])\\n>>> a1 + a3\\nTraceback (most recent call last):\\n  File \"<stdin>\", line 1, in <module>\\nValueError: operands could not be broadcast together with \\nshapes (4,3) (1,4)\\nIf this sounds complicated or confusing, that\\'s normal; it takes time to understand it \\nconceptually, especially in three or more dimensions. For a more detailed explanation  \\nof the concept, take time to read the related article in the official documentation: \\nhttps://numpy.org/doc/stable/user/theory.broadcasting.html.\\nAggregating arrays – sum, min, max, mean…\\nWhen working with arrays, we often need to summarize the data to extract some \\nmeaningful statistics: the mean, the minimum, the maximum... Fortunately, NumPy also \\nprovides those operations natively. Quite simply, they are provided as methods that you \\ncan call directly from an array:\\n>>> np.arange(10).mean()\\n4.5\\n>>> np.ones((4,4)).sum()\\n16.0\\nY ou can find the whole list of aggregating operations in the official documentation: \\nhttps://numpy.org/doc/stable/reference/arrays.ndarray.\\nhtml#calculation.\\nBy default, those operations will aggregate every value in the array. However, you can \\napply them per axis for multi-dimensional arrays:\\n>>> m = np.array(\\n    [[6, 5, 1, 1],\\n    [8, 9, 3, 2],\\n    [9, 3, 8, 5],\\n    [1, 0, 1, 9]]'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 343, 'page_label': '325'}, page_content=\"Manipulating arrays with NumPy – computation, aggregations, comparisons     325\\n)\\n>>> m.sum(axis=0)  # Sum on the rows axis (the first dimension)\\narray([24, 17, 13, 17])\\n>>> m.sum(axis=1)  # Sum on the columns axis (the second \\ndimension)\\narray([13, 22, 25, 11])\\nComparing arrays\\nNumPy also implements the standard comparison operators to compare arrays. As with \\narithmetic operators, which we saw in the Adding and multiplicating arrays section, \\nbroadcasting rules apply. This means that you can compare an array with a single value:\\n>>> l = np.array([1, 2, 3, 4])\\n>>> l < 3\\narray([ True,  True, False, False])\\nAnd you can also compare arrays with arrays, given that they are compatible on the basis \\nof the broadcasting rules:\\n>>> m = np.array(\\n    [[1., 5., 9., 13.], \\n    [2., 6., 10., 14.], \\n    [3., 7., 11., 15.], \\n    [4., 8., 12., 16.]]\\n)\\n>>> m <= np.array([1, 5, 9, 13])\\narray([[ True,  True,  True,  True],\\n       [False, False, False, False],\\n       [False, False, False, False],\\n       [False, False, False, False]])\\nThe resulting array is filled with the Boolean result of the comparison for each element.\\nThat's it for this very quick introduction to NumPy. There is a lot more to know and \\ndiscover with this library, so we strongly encourage you to read the official user guide: \\nhttps://numpy.org/doc/stable/user/index.html.\\nFor the rest of this book, this should be enough for you to understand the future examples. \\nLet's now have a look at a library often cited and used alongside NumPy: pandas.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 344, 'page_label': '326'}, page_content=\"326     Introduction to NumPy and pandas \\nGetting started with pandas\\nIn the previous section, we introduced NumPy and its ability to efficiently store and work \\nwith a large array of data. We'll now introduce another widely used library in data science: \\npandas. This library is built on top of NumPy to provide convenient data structures able to \\nefficiently store large datasets with labeled rows and columns. This is, of course, especially \\nhandy when working with most datasets representing real-world data that we want to \\nanalyze and use in data science projects.\\nTo get started, we will, of course, install the library with the usual command:\\n$ pip install pandas\\nOnce done, we can start to use it in a Python interpreter:\\n$ python\\n>>> import pandas as pd\\nJust like we alias numpy as np, the convention is to alias pandas as pd when  \\nimporting it.\\nUsing pandas Series for one-dimensional data\\nThe first pandas data structure we'll introduce is Series. This data structure behaves very \\nsimilarly to a one-dimensional array in NumPy. To create one, we can simply initialize  \\nit with a list of values:\\n>>> s = pd.Series([1, 2, 3, 4, 5])\\n>>> s\\n0    1\\n1    2\\n2    3\\n3    4\\n4    5\\ndtype: int64\\nUnder the hood, pandas create a NumPy array. As such, it uses the same data types to \\nstore the data. Y ou can verify this by accessing the values property of the Series \\nobject and check its type:\\n>>> type(s.values)\\n<class 'numpy.ndarray'>\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 345, 'page_label': '327'}, page_content='Getting started with pandas     327\\nIndexing and slicing work exactly the same way as in NumPy:\\n>>> s[0]\\n1\\n>>> s[1:3]\\n1    2\\n2    3\\ndtype: int64\\nSo far, this is not very different from a regular NumPy array. As we said, the main purpose \\nof pandas is to label the data. To allow this, pandas data structures maintain an index to \\nallow this data labeling. It is accessible through the index property:\\n>>> s.index\\nRangeIndex(start=0, stop=5, step=1)\\nHere, we have a simple range integer index, but we can actually have any arbitrary index. \\nIn the next example, we create the same series, labeling each value with a letter:\\n>>> s = pd.Series([1, 2, 3, 4, 5], index=[\"a\", \"b\", \"c\", \"d\", \\n\"e\"])\\n>>> s\\na    1\\nb    2\\nc    3\\nd    4\\ne    5\\nThe index argument on the Series initializer allows us to set the list of labels.  \\nWe can now access values with those labels instead:\\n>>> s[\"c\"]\\n3\\nSurprisingly, even slicing notation works with those kinds of labels:\\n>>> s[\"b\":\"d\"]\\nb    2\\nc    3\\nd    4\\ndtype: int64'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 346, 'page_label': '328'}, page_content='328     Introduction to NumPy and pandas \\nUnder the hood, pandas keep the order of the index to allow such useful notations. \\nNotice, however, that with this notation, the last index is inclusive (d is included in the \\nresult), unlike standard index notation, where the last index is exclusive:\\n>>> s[1:3]\\nb    2\\nc    3\\ndtype: int64\\nTo avoid confusion between those two styles, pandas exposes two special notations to \\nexplicitly indicate which indexing style you wish to use: loc (label notation with the last \\nindex being inclusive) and iloc (standard index notation). Y ou can read more about \\nthis in the official documentation: https://pandas.pydata.org/docs/user_\\nguide/indexing.html#different-choices-for-indexing.\\nSeries can also be instantiated directly from dictionaries:\\n>>> s = pd.Series({\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5})\\n>>> s\\na    1\\nb    2\\nc    3\\nd    4\\ne    5\\ndtype: int64\\nIn this case, the keys of the dictionaries are used as labels.\\nOf course, in the real world, you\\'ll more likely have to work with two-dimensional  \\n(or more!) datasets. This is exactly what DataFrames are for!\\nUsing pandas DataFrames for multi-dimensional data\\nMost of the time, datasets consist of two-dimensional data, where you have several \\ncolumns for each row, as in a classic spreadsheet application. In pandas, DataFrames are \\ndesigned to work this kind of data. As for Series, it can work with a large set of data that  \\nis labeled both by rows and columns.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 347, 'page_label': '329'}, page_content='Getting started with pandas     329\\nThe following examples will use a tiny dataset representing the number of tickets  \\n(paid and free) delivered in French museums in 2018. Let\\'s consider we have this data  \\nin the form of two dictionaries:\\n>>> paid = {\"Louvre Museum\": 5988065, \"Orsay Museum\": 1850092, \\n\"Pompidou Centre\": 2620481, \"National Natural History Museum\": \\n404497}\\n>>> free = {\"Louvre Museum\": 4117897, \"Orsay Museum\": 1436132, \\n\"Pompidou Centre\": 1070337, \"National Natural History Museum\": \\n344572}\\nEach key in those dictionaries is a label for a row. We can build a DataFrame directly \\nfrom those two dictionaries like this:\\n>>> museums = pd.DataFrame({\"paid\": paid, \"free\": free})\\n>>> museums\\n                                    paid     free\\nLouvre Museum                    5988065  4117897\\nOrsay Museum                     1850092  1436132\\nPompidou Centre                  2620481  1070337\\nNational Natural History Museum   404497   344572\\nThe DataFrame initializer accepts a dictionary of dictionaries, where keys represent the \\nlabel for the columns. \\nWe can have a look at the index property, storing the rows index, and the columns \\nproperty, storing the columns index:\\n>>> museums.index\\nIndex([\\'Louvre Museum\\', \\'Orsay Museum\\', \\'Pompidou Centre\\',\\n       \\'National Natural History Museum\\'],\\n      dtype=\\'object\\')\\n>>> museums.columns\\nIndex([\\'paid\\', \\'free\\'], dtype=\\'object\\')'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 348, 'page_label': '330'}, page_content='330     Introduction to NumPy and pandas \\nOnce again, we can now use indexing and slicing notation to get subsets of columns  \\nor rows:\\n>>> museums[\"free\"]\\nLouvre Museum                      4117897\\nOrsay Museum                       1436132\\nPompidou Centre                    1070337\\nNational Natural History Museum     344572\\nName: free, dtype: int64\\n>>> museums[\"Louvre Museum\":\"Orsay Museum\"]\\n                  paid     free\\nLouvre Museum  5988065  4117897\\nOrsay Museum   1850092  1436132\\n>>> museums[\"Louvre Museum\":\"Orsay Museum\"][\"paid\"]\\nLouvre Museum    5988065\\nOrsay Museum     1850092\\nName: paid, dtype: int64\\nSomething that is even more powerful, you can write a Boolean condition inside the \\nbrackets to match some data. This operation is called masking:\\n>>> museums[museums[\"paid\"] > 2000000]\\n                    paid     free\\nLouvre Museum    5988065  4117897\\nPompidou Centre  2620481  1070337\\nFinally, you can easily set new columns with this very same indexing notation:\\n>>> museums[\"total\"] = museums[\"paid\"] + museums[\"free\"]\\n>>> museums\\n                                    paid     free     total\\nLouvre Museum                    5988065  4117897  10105962\\nOrsay Museum                     1850092  1436132   3286224\\nPompidou Centre                  2620481  1070337   3690818\\nNational Natural History Museum   404497   344572    749069\\nAs you can see, just like NumPy arrays, pandas fully supports arithmetic operations over \\ntwo series or DataFrames.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 349, 'page_label': '331'}, page_content='Getting started with pandas     331\\nOf course, all the basic aggregation operations are supported, including mean and sum:\\n>>> museums[\"total\"].sum()\\n17832073\\n>>> museums[\"total\"].mean()\\n4458018.25\\nY ou can find the whole list of operations available in the official documentation: \\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/\\nbasics.html#descriptive-statistics.\\nImporting and exporting CSV data\\nOne very common way of sharing datasets is through CSV files. This format is very \\nconvenient because it only consists of a simple text file, each line representing a row of \\ndata, with each column separated by a comma. Our simple museums dataset is available \\nin the examples repository as a CSV file, which you can see in the next sample:\\nmuseums.csv\\nname,paid,free\\nLouvre Museum,5988065,4117897\\nOrsay Museum,1850092,1436132\\nPompidou Centre,2620481,1070337\\nNational Natural History Museum,404497,344572\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter11/museums.csv\\nImporting CSV files is so common that pandas provides a function to load a CSV file into \\na DataFrame directly:\\n>>> museums = pd.read_csv(\"./chapter11/museums.csv\", index_\\ncol=0)\\n>>> museums\\n                                    paid     free\\nname                                             \\nLouvre Museum                    5988065  4117897\\nOrsay Museum                     1850092  1436132\\nPompidou Centre                  2620481  1070337'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 350, 'page_label': '332'}, page_content='332     Introduction to NumPy and pandas \\nNational Natural History Museum   404497   344572\\nThe function simply expects the path to the CSV file. Several arguments are available \\nto finely control the operation: here, we used index_col to specify the index of the \\ncolumn that should be used as row labels. Y ou can find the whole list of arguments in the \\nofficial documentation: https://pandas.pydata.org/pandas-docs/stable/\\nreference/api/pandas.read_csv.html.\\nOf course, the opposite operation exists to export a DataFrame to a CSV file:\\n>>> museums[\"total\"] = museums[\"paid\"] + museums[\"free\"]\\n>>> museums.to_csv(\"museums_with_total.csv\")\\nWe will conclude this very quick introduction to pandas here. Of course, we\\'ve only \\ncovered the tip of the iceberg here and we recommend that you go through the official \\nuser guide to know more: https://pandas.pydata.org/pandas-docs/\\nstable/user_guide/index.html.\\nStill, you should now be able to perform basic operations and operate efficiently on  \\nlarge datasets.\\nSummary\\nGreat! Y ou now have a grasp of the ins and outs of NumPy and pandas. Basically, those \\nlibraries are the essential tool for data scientists in Python. By relying on optimized and \\ncompiled code, they allow you to load and manipulate large set of data in Python, without \\nsacrificing performance. To allow this, they define fixed-type data structures, meaning \\neach value in the dataset should be of the same type. This is what enables efficient memory \\nconsumption and fast computations.\\nEven though those basics should be enough for you to get started, we recommend that \\nyou spend some time on the official user guides and tinker with those a bit to discover  \\nall their aspects.\\nAs we said in the introduction, NumPy and pandas are at the heart of most data science \\napplications in Python. In the next chapter, we\\'ll see how they will help us in machine \\nlearning tasks, along with the well-known machine learning library scikit-learn.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 351, 'page_label': '333'}, page_content=\"12\\nTraining Machine \\nLearning Models \\nwith scikit-learn\\nAs we mentioned in the introduction of the previous chapter, Python has gained a  \\nlot of popularity in the data science field. We've seen that libraries such as NumPy and \\npandas have emerged to handle big datasets efficiently in Python. Those libraries are  \\nthe foundation for libraries dedicated to machine learning (ML), such as the famous  \\nscikit-learn library, a complete toolset for implementing most of the algorithms and \\ntechniques that are used daily by data scientists. In this chapter, we'll provide a quick \\nintroduction to ML, what it is about, what it tries to solve, and how. Then, we'll learn \\nhow to use scikit-learn to train and test ML models. We'll also have a deeper look at two \\nclassical ML models, Naive Bayes models and support vector machines, both of which can \\nperform surprisingly well if used correctly.\\nIn this chapter, we're going to cover the following main topics:\\n• What is machine learning?\\n• Basics of scikit-learn\\n• Classifying data with Naive Bayes models\\n• Classifying data with support vector machines\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 352, 'page_label': '334'}, page_content='334     Training Machine Learning Models with scikit-learn\\nTechnical requirements\\nY ou\\'ll need a Python virtual environment, similar to the one we set up in Chapter 1, \\nPython Development Environment Setup.\\nY ou can find all the code examples for this chapter in this book\\'s dedicated GitHub \\nrepository: https://github.com/PacktPublishing/Building-Data-\\nScience-Applications-with-FastAPI/tree/main/chapter12.\\nWhat is machine learning?\\nML is often seen as a sub-field of artificial intelligence. While this categorization is a subject \\nof debate, ML has had lot of exposure in recent years due to its vast and visible field of \\napplications, such as spam filters, natural language processing, and autonomous driving.\\nML is a field where we build mathematical models from existing data so that the machine \\ncan understand this data by itself. The machine is \"learning\" in the sense that the \\ndeveloper doesn\\'t have to program a step-by-step algorithm to solve the problem, which \\nwould be impossible for complex tasks. Once a model has been \"trained\" on existing data, \\nit can be used to predict new data or understand new observations.\\nConsider the spam filter example: if we have a sufficiently large collection of emails \\nmanually labeled \"spam\" or \"not spam,\" we can use ML techniques to build a model that \\ncan tell us if a new incoming email is spam or not.\\nBefore we look at this with scikit-learn, we\\'ll review the most fundamental concepts of ML.\\nSupervised versus unsupervised learning\\nML techniques can be divided into two main categories: supervised learning and \\nunsupervised learning.\\nWith supervised learning, the existing dataset is already labeled, which means we have \\nboth the inputs (the characteristics of an observation), known as features, and the \\noutputs. If we consider the spam filter example here, the features could be the frequencies \\nof each word and the label could be the category; that is, \"spam\" or \"not-spam\". \\nSupervised learning is subdivided into two groups:\\n• Classification problems, to classify data with a finite set of categories; for example, \\nthe spam filter\\n• Regression problems, to predict continuous numerical values; for example,  \\nthe number of rented electric scooters, given the day of the week, the weather,  \\nand the location'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 353, 'page_label': '335'}, page_content=\"What is machine learning?     335\\nUnsupervised learning, on the other hand, operates on data without any reference to  \\na label. The goal here is to discover interesting patterns from the features themselves.  \\nThe two main problems that unsupervised learning tries to solve are as follows:\\n• Clustering, where we want to find groups of similar data points; for example,  \\na recommender system to suggest products that you might like, given what other \\npeople similar to you like.\\n• Dimensionality reduction, where the goal is to find a more compact representation \\nof datasets that contain a lot of different features. Doing this will allow us to keep \\nonly the most meaningful and discriminant features while working with smaller \\ndataset dimensions.\\nModel validation\\nOne of the key aspects of ML is evaluating whether your model is performing well or \\nnot. How can you say that your model will perform well on newly observed data? When \\nbuilding your model, how can you tell if one algorithm performs better than another?  \\nAll of these questions can and should be answered with model validation techniques.\\nAs we mentioned previously, ML methods start with an existing set of data that we'll use \\nto train a model.\\nIntuitively, we may want to use all the data we have to train our model. Once done, what \\ncan we do to test it? We could apply our model to the same data and see if the output is \\ncorrect... and we would get a surprisingly good result! Here, we are testing the model \\nwith the same data we used to train it. Obviously, the model will overperform on this \\ndata because it has already seen it. As you may have guessed, this is not a reliable way to \\nmeasure the accuracy of our model.\\nThe right way to validate a model is to split the data into two: we keep one part for \\ntraining the data and another for testing it. This is known as the holdout set. This way, \\nwe'll test the model on data that it has never seen before and compare the result that's \\npredicted by the model with the real value. Hence, the accuracy we are measuring is much \\nmore sensible.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 354, 'page_label': '336'}, page_content=\"336     Training Machine Learning Models with scikit-learn\\nThis technique works well; however, it poses a problem: by retaining some data, we \\nare losing precious information that could have helped us build a better model. This is \\nespecially true if our initial dataset is small. To solve this, we can use cross-validation. \\nWith this method, we once again split the data into two sets. This time, we are training \\nthe model twice, using each set as training and testing sets. Y ou can see a schematic \\nrepresentation of this operation in the following diagram:\\nFigure 12.1 – Two-fold cross-validation \\nAt the end of the operation, we obtain two accuracies, which will give us a better overview \\nof how our model performs on the whole dataset. This technique can be applied to help us \\nperform more trials with a smaller testing set, as shown in the following diagram:\\nFigure 12.2 – Five-fold cross-validation \\nWe'll stop here regarding this very quick introduction to ML. We've barely scratched \\nthe surface: ML is a vast and complex field, and there are lots of books dedicated to this \\nsubject. Still, this information should be sufficient to help you understand the basic \\nconcepts of scikit-learn, which we'll show throughout the rest of this chapter.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 355, 'page_label': '337'}, page_content=\"Basics of scikit-learn     337\\nBasics of scikit-learn\\nNow, let's focus on scikit-learn, an essential ML library for Python. It implements dozens \\nof classic ML models, but also numerous tools to help you while training them, such as \\npre-processing methods and cross-validation.\\nThe first thing you must do to get started is install it in your Python environment:\\n$ pip install scikit-learn\\nWe can now start our scikit-learn journey!\\nTraining models and predicting\\nIn scikit-learn, ML models and algorithms are called estimators. Each is a Python class \\nthat implements the same methods. In particular, we have fit, which is used to train a \\nmodel, and predict, which is used to run the trained model on new data.\\nTo try this, we'll load a sample dataset. scikit-learn comes with a few toy datasets that are \\nvery useful for performing experiments. Y ou can find out more about them in the official \\ndocumentation: https://scikit-learn.org/stable/datasets.html.\\nHere, we'll use the digits dataset, a collection of pixels matrices representing handwritten \\ndigits. As you may have guessed, the goal of this dataset is to train a model to automatically \\nrecognize handwritten digits. The following example shows how to load this dataset:\\nchapter12_load_digits.py\\nfrom sklearn.datasets import load_digits\\ndigits = load_digits()\\ndata = digits.data\\ntargets = digits.target\\nprint(data[0].reshape((8, 8)))  # First handwritten digit 8 x 8 \\nmatrix\\nprint(targets[0])  # Label of first handwritten digit\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_load_\\ndigits.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 356, 'page_label': '338'}, page_content=\"338     Training Machine Learning Models with scikit-learn\\nNotice that the toy dataset's functions are imported from the datasets package of \\nscikit-learn. The load_digits function returns an object that contains the data and \\nsome metadata.\\nThe most interesting parts of this object are data, which contains the handwritten digits \\npixels matrices, and targets, which contains the corresponding label for those digits. \\nBoth are NumPy arrays.\\nTo get a grasp of what this looks like, we will take the first digit in the data and reshape it \\ninto an 8 x 8 matrix; this is the size of the source images. Each value represents a pixel on  \\na grayscale, from 0 to 16.\\nThen, we print the label of this first digit, which is a 0. If you run this code, you'll get the \\nfollowing output:\\n$ python chapter12/chapter12_load_digits.py     \\n[[ 0.  0.  5. 13.  9.  1.  0.  0.]\\n [ 0.  0. 13. 15. 10. 15.  5.  0.]\\n [ 0.  3. 15.  2.  0. 11.  8.  0.]\\n [ 0.  4. 12.  0.  0.  8.  8.  0.]\\n [ 0.  5.  8.  0.  0.  9.  8.  0.]\\n [ 0.  4. 11.  0.  1. 12.  7.  0.]\\n [ 0.  2. 14.  5. 10. 12.  0.  0.]\\n [ 0.  0.  6. 13. 10.  0.  0.  0.]]\\n0\\nSomehow, we can guess the shape of the zero from the matrix.\\nNow, let's try to build a model that recognizes handwritten digits. To start simple, we'll \\nuse a Gaussian Naive Bayes model, which we'll cover in more detail in the Classifying data \\nwith Naive Bayes models section. The following example shows the entire process:\\nchapter12_fit_predict.py\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import GaussianNB\\ndigits = load_digits()\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 357, 'page_label': '339'}, page_content=\"Basics of scikit-learn     339\\ndata = digits.data\\ntargets = digits.target\\n# Split into training and testing sets\\ntraining_data, testing_data, training_targets, testing_targets \\n= train_test_split(\\n    data, targets, random_state=0\\n)\\n# Train the model\\nmodel = GaussianNB()\\nmodel.fit(training_data, training_targets)\\n# Run prediction with the testing set\\npredicted_targets = model.predict(testing_data)\\n# Compute the accuracy\\naccuracy = accuracy_score(testing_targets, predicted_targets)\\nprint(accuracy) \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_fit_\\npredict.py\\nNow that we've loaded the dataset, we can see that it takes care of splitting it into a \\ntraining and a testing set. As we mentioned in the Model validation section, this is \\nessential for computing meaningful accuracy scores to check how our model performs.\\nTo do this, we can rely on the train_test_split function, which is provided in \\nthe model_selection package. It selects random instances from our dataset to form \\nthe two sets. By default, it keeps 25% percent of the data to create a testing set, but this \\ncan be customized. The random_state argument allows us to set the random seed to \\nmake the example reproducible. Y ou can find out more about this function in the official \\ndocumentation: https://scikit-learn.org/stable/modules/generated/\\nsklearn.model_selection.train_test_split.html#sklearn-model-\\nselection-train-test-split.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 358, 'page_label': '340'}, page_content=\"340     Training Machine Learning Models with scikit-learn\\nThen, we must instantiate the GaussianNB class. This class is one of the numerous \\nML estimators that's implemented in scikit-learn. Each has its own set of parameters, to \\nfinely tune the behavior of the algorithm. However, scikit-learn is designed to provide \\nsensible defaults for all the estimators, so it's usually good to start with the defaults before \\ntinkering with them.\\nAfter that, we must call the fit method to train our model. It expects an argument and \\ntwo arrays: the first one is the actual data, with all its features, while the second one is the \\ncorresponding labels. And that's it! Y ou've trained your first ML model!\\nNow, let's see how it behaves: we'll call predict on our model with the testing set so that \\nit automatically classifies the digits of the testing set. The result of this is a new array with \\nthe predicted labels.\\nAll we have to do now is compare it with the actual labels of our testing set. Once again, \\nscikit-learn helps by providing the accuracy_score function in the metrics package. \\nThe first argument is the true labels, while the second is the predicted labels.\\nIf you run this code, you'll get an accuracy score of around 83%. That isn't too bad for \\na first approach! As you have seen, training and running prediction on an ML model is \\nstraightforward with scikit-learn.\\nIn practice, we often need to perform pre-processing steps on the data before feeding \\nit to an estimator. Rather than doing this sequentially by hand, scikit-learn proposes a \\nconvenient feature that can automate this process: pipelines.\\nChaining pre-processors and estimators with pipelines\\nQuite often, you'll need to pre-process your data so that it can be used by the estimator \\nyou wish to use. Typically, you'll want to transform an image into an array of pixel values \\nor, as we'll see in the following example, transform raw text into numerical values so that \\nwe can apply some math to them.\\nRather than writing those steps by hand, scikit-learn proposes a feature that can \\nautomatically chain pre-processors and estimators: pipelines. Once created, they \\nexpose the very same interfaces as any other estimator, allowing you to run training and \\nprediction in one operation.\\nTo show you what this looks like, we'll look at an example of another classic dataset; that \\nis, the 20 newsgroups text dataset. It consists of 18,000 newsgroup articles categorized into \\n20 topics. The goal of this dataset is to build a model that will automatically categorize an \\narticle in one of those topics.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 359, 'page_label': '341'}, page_content='Basics of scikit-learn     341\\nThe following example shows how we can load this data thanks to the \\nfetch_20newsgroups function:\\nchapter12_pipelines.py\\nimport pandas as pd\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics import accuracy_score, confusion_matrix\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.pipeline import make_pipeline\\n# Load some categories of newsgroups dataset\\ncategories = [\\n    \"soc.religion.christian\",\\n    \"talk.religion.misc\",\\n    \"comp.sys.mac.hardware\",\\n    \"sci.crypt\",\\n]\\nnewsgroups_training = fetch_20newsgroups(\\n    subset=\"train\", categories=categories, random_state=0\\n)\\nnewsgroups_testing = fetch_20newsgroups(\\n    subset=\"test\", categories=categories, random_state=0\\n)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_\\npipelines.py\\nSince the dataset is rather large, we can only load some of the categories. Here, we\\'ll only \\nuse four categories. Also, notice that it\\'s already been split into training and testing sets, \\nso we only have to load them with the corresponding argument. Y ou can find out more \\nabout the functionality of this dataset in the official documentation: https://scikit-\\nlearn.org/stable/datasets/real_world.html#the-20-newsgroups-\\ntext-dataset.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 360, 'page_label': '342'}, page_content='342     Training Machine Learning Models with scikit-learn\\nBefore moving on, it\\'s important to understand what the underlying data is. Actually, this \\nis the raw text of an article. Y ou can check this by printing one of the samples in the data:\\n>>> newsgroups_training.data[0]\\n\"From: sandvik@newton.apple.com (Kent Sandvik)\\\\nSubject: \\nRe: Ignorance is BLISS, was Is it good that Jesus died?\\\\\\nnOrganization: Cookamunga Tourist Bureau\\\\nLines: 17\\\\n\\\\\\nnIn article <f1682Ap@quack.kfu.com>, pharvey@quack.kfu.com \\n(Paul Harvey)\\\\nwrote:\\\\n> In article <sandvik-170493104859@\\nsandvik-kent.apple.com> \\\\n> sandvik@newton.apple.com (Kent \\nSandvik) writes:\\\\n> >Ignorance is not bliss!\\\\n \\\\n> Ignorance \\nis STRENGTH!\\\\n> Help spread the TRUTH of IGNORANCE!\\\\n\\\\nHuh, \\nif ignorance is strength, then I won\\'t distribute this piece\\\\\\nnof information if I want to follow your advice (contradiction \\nabove).\\\\n\\\\n\\\\nCheers,\\\\nKent\\\\n---\\\\nsandvik@newton.apple.com. \\nALink: KSAND -- Private activities on the net.\\\\n\"\\nSo, we need to extract some features from this text before feeding it to an estimator. A \\ncommon approach for this when working with textual data is to use the Term Frequency-\\nInverse Document Frequency (TF-IDF). Without going into too much detail, this \\ntechnique will count the occurrences of each word in all the documents (term frequency), \\nweighted by the importance of this word in every document (inverse document \\nfrequency). The idea is to give more weight to rarer words, which should convey more \\nsense than frequent words such as \"the.\" Y ou can find out more about this in the scikit-\\nlearn documentation: https://scikit-learn.org/dev/modules/feature_\\nextraction.html#tfidf-term-weighting.\\nThis operation consists of splitting each word in the text samples and counting them. \\nUsually, we apply a lot of techniques to refine this, such as removing stop words; common \\nwords such as \"and\" or \"is\" that don\\'t bring much information. Fortunately, scikit-learn \\nprovides an all-in-one tool for this: TfidfVectorizer.\\nThis pre-processor can take an array of text, tokenize each word, and compute the TF-IDF \\nfor each of them. A lot of options are available for finely tuning its behavior, but the \\ndefaults are a good start for English text. The following example shows how to use it with \\nan estimator in a pipeline:\\nchapter12_pipelines.py\\n# Make the pipeline\\nmodel = make_pipeline(\\n    TfidfVectorizer(),\\n    MultinomialNB(),\\n)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 361, 'page_label': '343'}, page_content='Basics of scikit-learn     343\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_\\npipelines.py\\nThe make_pipeline function accepts any number of pre-processors and an estimator \\nin its argument. Here, we\\'re using the Multinomial Naive Bayes classifier, which is suitable \\nfor features representing frequency.\\nThen, we can simply train our model and run prediction to check its accuracy, as we did \\npreviously. Y ou can see this in the following example:\\nchapter12_pipelines.py\\n# Train the model\\nmodel.fit(newsgroups_training.data, newsgroups_training.target)\\n# Run prediction with the testing set\\npredicted_targets = model.predict(newsgroups_testing.data)\\n# Compute the accuracy\\naccuracy = accuracy_score(newsgroups_testing.target, predicted_\\ntargets)\\nprint(accuracy)\\n# Show the confusion matrix\\nconfusion = confusion_matrix(newsgroups_testing.target, \\npredicted_targets)\\nconfusion_df = pd.DataFrame(\\n    confusion,\\n    index=pd.Index(newsgroups_testing.target_names, \\nname=\"True\"),\\n    columns=pd.Index(newsgroups_testing.target_names, \\nname=\"Predicted\"),\\n)\\nprint(confusion_df)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_\\npipelines.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 362, 'page_label': '344'}, page_content=\"344     Training Machine Learning Models with scikit-learn\\nNotice that we also printed a confusion matrix, which is a very convenient representation \\nof the global results. scikit-learn has a dedicated function for this called confusion_\\nmatrix. Then, we wrap the result in a pandas DataFrame so that we can set the axis \\nlabels to improve readability. If you run this example, you'll get an output similar to what's \\nshown in the following screenshot. Depending on your machine and system, it could take \\na couple of minutes to run:\\nFigure 12.3 – Using a confusion matrix on 20 newsgroups dataset\\nHere, you can see that our results weren't too bad for our first try. Notice that there is one \\nbig area of confusion between the soc.religion.christian and talk.religion.misc categories, \\nwhich is not very surprising, given their similarity.\\nAs you've seen, building a pipeline with a pre-processor is very straightforward. The nice \\nthing about this is that it automatically applies it to the training data, but also when you're \\npredicting the results. \\nBefore moving on, let's look at one more important feature of scikit-learn:  \\ncross-validation.\\nValidating the model with cross-validation\\nIn the Model validation section, we introduced the cross-validation technique, which \\nallows us to use data in training or testing sets. As you may have guessed, this technique  \\nis so common that it's implemented natively in scikit-learn!\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 363, 'page_label': '345'}, page_content=\"Basics of scikit-learn     345\\nLet's take another look at the handwritten digit example and apply cross-validation:\\nchapter12_cross_validation.py\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.naive_bayes import GaussianNB\\ndigits = load_digits()\\ndata = digits.data\\ntargets = digits.target\\n# Create the model\\nmodel = GaussianNB()\\n# Run cross-validation\\nscore = cross_val_score(model, data, targets)\\nprint(score)\\nprint(score.mean())\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_cross_\\nvalidation.py\\nThis time, we don't have to split the data ourselves: the cross_val_score function \\nperforms the folds automatically. In argument, it expects the estimator, data, which \\ncontains the handwritten digits' pixels matrices, and targets, which contains the \\ncorresponding label for those digits. By default, it performs five folds.\\nThe result of this operation is an array that provides the accuracy score of the five folds. \\nTo get a global overview of this result, we can take, for example, the mean. If you run this \\nexample, you'll get the following output:\\n$ python chapter12/chapter12_cross_validation.py \\n[0.78055556 0.78333333 0.79387187 0.8718663  0.80501393]\\n0.8069281956050759\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 364, 'page_label': '346'}, page_content='346     Training Machine Learning Models with scikit-learn\\nAs you can see, our mean accuracy is around 80%, which is a bit lower than the 83% we \\nobtained with single training and testing sets. That\\'s the main benefit of cross-validation: \\nwe obtain a more statistically accurate metric regarding the performance of our model.\\nWith that, we have learned the basics of working with scikit-learn. Before going back \\nto FastAPI, we\\'ll review two categories of ML models: Naive Bayes models and support \\nvector machines.\\nClassifying data with Naive Bayes models\\nEven though you probably hear a lot about super-advanced ML methods such as deep \\nlearning, it\\'s important to say that simpler methods have existed for years and have  \\nproven to be very efficient in many situations. Generally, it\\'s always a good idea when you \\nstart with a data science problem to try out simpler models that have fewer parameters \\nand are easier to tune. This will quickly give you a baseline to compare with more \\nadvanced techniques.\\nIn this section, we\\'ll review Naive Bayes models, a group of fast and simple classification \\nalgorithms.\\nIntuition\\nNaive Bayes models rely on Bayes\\' theorem, which defines an equation to describe the \\nprobability of an event, given the probability of related events. In the context of \\nclassification, it gives us an equation to describe the probability of a label, 𝐿𝐿 , given a set of \\nfeatures. In our handwritten digit recognition problem, this would translate to \"the \\nprobability of this observation being the digit zero, given the pixel\\'s matrix values.\"  \\nThis equation looks like this:\\nThe notation 𝑃𝑃(𝐿𝐿 | features)  means \"the probability of 𝐿𝐿 , given features .\"\\nIn practice, our classifier will have to decide if an observation has a higher probability  \\nof being 𝐿𝐿1  or 𝐿𝐿2 : \"does it look more like a zero or an eight?\" To do this, we can compute \\nthe ratio of the two probabilities, which, thanks to the previous equation, gives us  \\nthe following:\\n(  | features) = \\n(features\\n \\n|\\n \\n)\\n \\n×\\n \\n( )\\nfeatures( )\\n \\n(  | features)\\n(  | features) =\\n(features\\n \\n|\\n \\n)\\n \\n×\\n ( )\\n(features |\\n))\\n) ))×'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 365, 'page_label': '347'}, page_content='Classifying data with Naive Bayes models     347\\nThe raw probability of   and  , ( ) and ( ) , is the relative frequency of   and   \\nin the training set. If our training set contains 100 samples and we have 15 samples of zero, \\nthe probability of the label being zero is 0.15.\\nNow, we have to find a way to compute the probability of the features given a label, \\n(features | )  and (features | ) . What we\\'ll do here is make assumptions about the \\ndistribution of the data by finding simple statistical rules. This is why those models are \\ncalled \"naïve.\"\\nOne of the first classical assumptions regarding those models is Gaussian distribution.\\nClassifying data with Gaussian Naive Bayes\\nAs we mentioned previously, Naive Bayes models work by making \"naive\" assumptions \\nabout the distribution of the underlying data. In the case of Gaussian Naive Bayes,  \\nwe assume that the data is drawn from a Gaussian distribution (or normal distribution).  \\nThe following is a graphical representation of such a distribution:\\nFigure 12.4 – Curve of a Gaussian distribution\\nThe intuition behind this is that, for data following a Gaussian distribution, the probability \\nis high around the mean, μ, and the standard deviation, σ. It then decreases rapidly when \\nit moves away from the mean. This is computed using the following formula:\\n1\\nσ√2π\\nexp −1\\n2\\n−μ\\nσ'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 366, 'page_label': '348'}, page_content='348     Training Machine Learning Models with scikit-learn\\nThen, all we need to do to train our model is compute the mean and standard deviation for \\neach feature in each label. This will give us, for each label, a simple formula to compute the \\nprobability of having features,  given  . Once we have them, all we need to do is apply the \\npreceding formula to get the probability of this observation, given  .\\nThis is exactly what happens when we train the GaussianNB estimator in scikit-learn.  \\nIf we consider the same example we showed in the Training models and predicting section, \\nwe can retrieve the mean and standard deviation that was computed for each pixel for \\neach possible digit. In the following example, you can see that we are training a Gaussian \\nNaive Bayes model with the handwritten digits set, before printing the mean and standard \\ndeviation for the digit zero:\\nchapter12_gaussian_naive_bayes.py\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import GaussianNB\\ndigits = load_digits()\\ndata = digits.data\\ntargets = digits.target\\n# Split into training and testing sets\\ntraining_data, testing_data, training_targets, testing_targets \\n= train_test_split(\\n    data, targets, random_state=0\\n)\\n# Train the model\\nmodel = GaussianNB()\\nmodel.fit(training_data, training_targets)\\n# Print mean and standard deviation of digit zero\\nprint(\"Mean of each pixel for digit zero\")\\nprint(model.theta_[0])'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 367, 'page_label': '349'}, page_content='Classifying data with Naive Bayes models     349\\nprint(\"Standard deviation of each pixel for digit zero\")\\nprint(model.sigma_[0])\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_\\ngaussian_naive_bayes.py\\nIf you run this example, you\\'ll get the following output:\\n$ python chapter12/chapter12_gaussian_naive_bayes.py\\nMean of each pixel for digit zero\\n[0.00000000e+00 2.83687943e-02 4.12765957e+00 1.29716312e+01\\n 1.13049645e+01 2.96453901e+00 3.54609929e-02 0.00000000e+00\\n 0.00000000e+00 9.50354610e-01 1.25035461e+01 1.37021277e+01\\n 1.16453901e+01 1.12765957e+01 9.00709220e-01 0.00000000e+00\\n 0.00000000e+00 3.79432624e+00 1.43758865e+01 5.57446809e+00\\n 2.13475177e+00 1.23049645e+01 3.43971631e+00 0.00000000e+00\\n 0.00000000e+00 5.31205674e+00 1.27517730e+01 2.06382979e+00\\n 1.34751773e-01 9.26241135e+00 6.45390071e+00 0.00000000e+00\\n 0.00000000e+00 5.78723404e+00 1.16737589e+01 1.00000000e+00\\n 5.67375887e-02 8.89361702e+00 7.10638298e+00 0.00000000e+00\\n 0.00000000e+00 3.41843972e+00 1.33687943e+01 1.82269504e+00\\n 1.69503546e+00 1.12127660e+01 5.90070922e+00 0.00000000e+00\\n 0.00000000e+00 7.80141844e-01 1.29787234e+01 1.02056738e+01\\n 1.06382979e+01 1.32340426e+01 2.53191489e+00 0.00000000e+00\\n 0.00000000e+00 7.09219858e-03 4.15602837e+00 1.35602837e+01\\n 1.33049645e+01 5.46099291e+00 2.83687943e-01 0.00000000e+00]\\nStandard deviation of each pixel for digit zero\\n[4.30146180e-08 5.59328432e-02 9.13263925e+00 5.40345057e+00\\n 1.19566421e+01 1.10838489e+01 3.42035539e-02 4.30146180e-08\\n 4.30146180e-08 3.62164885e+00 1.24060158e+01 8.98928630e+00\\n 1.66827625e+01 1.22284594e+01 3.08233997e+00 4.30146180e-08\\n 4.30146180e-08 7.09954232e+00 5.32679447e+00 2.42870077e+01\\n 1.03435441e+01 1.03112520e+01 7.16835174e+00 4.30146180e-08\\n 4.30146180e-08 6.08701780e+00 1.01298728e+01 1.13505357e+01\\n 3.57728527e-01 1.27609276e+01 5.38262667e+00 4.30146180e-08\\n 4.30146180e-08 5.03274487e+00 1.11843469e+01 5.54609933e+00'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 368, 'page_label': '350'}, page_content=\"350     Training Machine Learning Models with scikit-learn\\n 1.38624861e-01 1.46624416e+01 7.28655505e+00 4.30146180e-08\\n 4.30146180e-08 5.17951818e+00 5.96328157e+00 9.69196725e+00\\n 8.97791866e+00 1.45362910e+01 1.38482974e+01 4.30146180e-08\\n 4.30146180e-08 1.80272626e+00 7.62366082e+00 1.54257835e+01\\n 1.74365475e+01 1.00516071e+01 1.00503999e+01 4.30146180e-08\\n 4.30146180e-08 7.04194232e-03 7.77707363e+00 4.30310351e+00\\n 7.87153568e+00 1.51846487e+01 9.83350981e-01 4.30146180e-08]\\nAll those numbers represent the means and standard deviations of the 64 pixels of the 8x8 \\npixel matrix, for the digit zero.\\nIf you want to learn more about the mathematics behind this, you can read a very detailed \\nintroduction in the following PennState online course: https://online.stat.psu.\\nedu/stat414/lesson/16.\\nThis is why training and running prediction on a Gaussian Naive Bayes model is so fast:  \\nit only involves simple mathematical computations. Of course, its accuracy only depends on \\nthe correctness of the assumption: if our data doesn't conform to a Gaussian distribution, \\nthe model won't perform very well. Still, its simplicity and efficiency always make it a good \\nbasis before we consider more complex algorithms.\\nClassifying data with Multinomial Naive Bayes\\nAnother assumption we can make about the data is that it follows a multinomial \\ndistribution. This is particularly suited for datasets with features representing counts,  \\nsuch as the number of times they appear in the dataset, such as word frequencies. \\nIf we consider some text and we compute the frequency of each word (or the TF-IDF ,  \\nas we saw in the Chaining pre-processors and estimators with pipelines section), how do  \\nwe compute its probability of being in the   category; that is, our famous (features | ) ? \\nThe multinomial law says that it can be computed using this formula:\\nHere,   is the total number of occurrences,  ,  , ...   is the number of occurrences of \\nthe word 1, 2...  , and  ,  , ...   is the probability of the word 1, 2...  .\\n!\\n! ! … ! …\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 369, 'page_label': '351'}, page_content=\"Classifying data with support vector machines     351\\nAll we need to do now is find the probability of each word in each category:  \\nthis is the purpose of the training phase. It's computed as follows:\\nHere,  is the frequency of the word   in category  ,  is the total number of \\noccurrences of every word in category  , and   the number of different words. α  is a \\nsmoothing parameter to prevent some probabilities from being equal to zero, which \\nwould then propagate in the multinomial formula. It's usually set to 1 by default, but this \\ncan be tuned.\\nIf you want to learn more about the mathematics behind this, you can read a very detailed \\nintroduction to it in the following PennState online course: https://online.stat.\\npsu.edu/stat504/lesson/1/1.7.\\nWhen training a MultinomialNB estimator with scikit-learn, this is exactly what the \\nalgorithm does: it computes the probability of each word in each category.\\nWhen predicting the category of a new piece of text, it simply has to count the  \\nfrequency of each word and apply the first formula with the probabilities it computed \\nduring training.\\nThat's it for the theory behind Naive Bayes models. The key thing to remember is that \\nthey are very fast to train and generally provide quite a good basis when starting with a \\nclassification problem. Besides, they tend to work quite well if the number of features  \\nis large.\\nIn the next section, we'll review another type of model that's quite powerful both for \\nclassification and regression: support vector machines.\\nClassifying data with support vector machines\\nSupport Vector Machines (SVM) are another group of classification and regression \\nmodels that have proven to be quite powerful in many situations. The intuition behind \\nthem is quite straightforward to understand, but we'll see that their power comes mostly \\nfrom a mathematical technique that's used in many other ML algorithms, called the \\nkernel trick.\\n= + α\\n+ α\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 370, 'page_label': '352'}, page_content=\"352     Training Machine Learning Models with scikit-learn\\nIntuition\\nLet's consider a simple classification problem where we want to classify samples into  \\ntwo categories. The following is a graph containing some randomly generated data for  \\nthis problem:\\nFigure 12.5 – Simple classification problem data\\nIntuitively, with such data, finding a straight line to cleanly separate the two categories \\nseems simple. However, we quickly see that there are a lot of different solutions, as shown \\nin the following graph:\\nFigure 12.6 – Three possible linear classifiers\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 371, 'page_label': '353'}, page_content=\"Classifying data with support vector machines     353\\nSo, how do we find the one that will yield the best results to predict the category of a  \\nnew point?\\nWhat SVM does is draw a margin around each of those possible classifiers, up to the \\nnearest point. The classifier that maximizes the margin is the one that'll be selected for our \\nmodel. If we train an SVM on our sample dataset, we'll obtain the classifier shown in the \\nfollowing graph. This graph also shows the margin for better visualization:\\nFigure 12.7 – Three possible linear classifiers\\nThe two samples that are touching the margin are the support vectors.\\nOf course, in the real world, having such nicely separated data is very rare, and a linear \\nclassifier may not exist. The following graph shows some randomly generated data that is \\nnot linearly separable:\\nFigure 12.8 – Non-linearly separable data\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 372, 'page_label': '354'}, page_content=\"354     Training Machine Learning Models with scikit-learn\\nTo solve this, SVM projects the dataset to a higher dimension by applying a kernel \\nfunction to the data. We won't go into the mathematical details of this, but kernel \\nfunctions can compute the similarity between each pair of points: in the new dimension, \\nsimilar points are close, while dissimilar points are distant.\\nMetaphorically, imagine that we draw the data shown in the preceding graph on a sheet of \\npaper. The goal of the kernel is to find a way to fold or bend this paper so that the yellow \\nand purple dots can be linearly separated by a plane.\\nSeveral kernel functions exist, such as the Radial Basis Function (RBF), which is applied \\nby default when using SVM with scikit-learn.\\nThe following graph shows the result of performing such an operation on our sample data:\\nFigure 12.9 – Data projected in a third dimension that's now linearly separable\\nHere, we can see that there is a clear linear classifier in three dimensions that can separate \\nthe data.\\nY ou can read more about the mathematics behind this in the following scikit-learn \\ndocumentation: https://scikit-learn.org/stable/modules/svm.\\nhtml#mathematical-formulation.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 373, 'page_label': '355'}, page_content=\"Classifying data with support vector machines     355\\nUsing SVM in scikit-learn\\nNow that we have a good grasp of the functionality of SVM, we can try using it in  \\nscikit-learn. As you'll see, it's not very different from what we've seen so far with  \\nNaive Bayes models.\\nIt comes in different flavors, with slight adaptations depending on your use case.  \\nTypically, the SVC estimator is suitable for classification problems, while SVR is usually \\nadapted to regression.\\nIn the following example, once again, we're taking our handwritten digit recognition \\nexample and applying the SVC estimator. We will evaluate it using the cross-validation \\nmethod:\\nchapter12_svm.py\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.svm import SVC\\ndigits = load_digits()\\ndata = digits.data\\ntargets = digits.target\\n# Create the model\\nmodel = SVC()\\n# Run cross-validation\\nscore = cross_val_score(model, data, targets)\\nprint(score)\\nprint(score.mean())\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_svm.py\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 374, 'page_label': '356'}, page_content='356     Training Machine Learning Models with scikit-learn\\nAs you can see, we simply instantiate the SVC class and keep the default parameters. If you \\nrun this example, you\\'ll get the following output:\\n$ python chapter12/chapter12_svm.py\\n[0.96111111 0.94444444 0.98328691 0.98885794 0.93871866]\\n0.9632838130609718\\nThe mean accuracy of our model is 96%! That\\'s quite impressive, given that we didn\\'t even \\nhave to tune the parameters.\\nFinding the best parameters\\nWith Naive Bayes models, we almost had no parameters to tune. In the case of SVM, \\nhowever, there are quite a few of them – most notably, there\\'s the kernel function, which \\nis RBF by default, and the C parameter. C defines the \"hardness\" of the margin around the \\nlinear classifier: if C is high, no point can creep inside the margin. A lower C will relax this \\nconstraint and, in some cases, allow a better fit for the data.\\nHowever, finding the best set of parameters is not always intuitive and it would be quite \\ntime-consuming to do so by hand. What can we do, then? scikit-learn can help us with this!\\nThe model_selection package provides a useful class called GridSearchCV that \\nallows us to automatically search for the best parameters for our estimator. Here, we \\nset the different parameters we want to try and it trains the model with every possible \\ncombination. At the end of this process, it returns the parameters that achieved the  \\nbest accuracy.\\nIn the following example, we implemented a grid search to find the best parameters for C \\nand the kernel function for our handwritten digit recognition problem:\\nchapter12_finding_parameters.py\\nfrom sklearn.datasets import load_digits\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import SVC\\ndigits = load_digits()\\ndata = digits.data\\ntargets = digits.target'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 375, 'page_label': '357'}, page_content='Classifying data with support vector machines     357\\n# Create the grid of parameters\\nparam_grid = {\\n    \"C\": [1, 10, 100, 1000],\\n    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\\n}\\ngrid = GridSearchCV(SVC(), param_grid)\\ngrid.fit(data, targets)\\nprint(\"Best params\", grid.best_params_)\\nprint(\"Best score\", grid.best_score_)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter12/chapter12_\\nfinding_parameters.py\\nAs you can see, we only have to create a dictionary for mapping the name of the parameter \\nto the list of values we want to try for this parameter. GridSearchCV is then initialized \\nwith the estimator instance and this parameter grid.\\nCalling the fit method with the dataset will run the search. Once done, you\\'ll have \\naccess to the best_params_ and best_score_ properties, which will give you the \\nbest results.\\nIf you run this example, you\\'ll get the following result:\\n$ python chapter12/chapter12_finding_parameters.py\\nBest params {\\'C\\': 10, \\'kernel\\': \\'rbf\\'}\\nBest score 0.9738502011761063\\nHere, we achieved 97% accuracy with the C parameter set to 10 and the  \\nRBF kernel function.\\nOf course, the larger your grid is, the more time you\\'ll need to compute all the \\npossibilities. If you have a very large set of parameters to try, have a look at \\nRandomizedSearchCV, which works similarly but only tests a few combinations by \\npicking some randomly. Y ou can learn more about this in the scikit-learn documentation: \\nhttps://scikit-learn.org/stable/modules/generated/sklearn.\\nmodel_selection.RandomizedSearchCV.html.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 376, 'page_label': '358'}, page_content=\"358     Training Machine Learning Models with scikit-learn\\nSummary\\nCongratulations! Y ou've discovered the basics concepts of ML and scikit-learn. Now, you \\nshould be able to explore your first data science problems in Python. Of course, this was \\nby no means a complete lesson on ML: the field is vast and there are tons of algorithms \\nand techniques to explore. However, I hope that this has sparked your curiosity and that \\nyou'll deepen your knowledge of this subject.\\nNow, it's time to get back to FastAPI! With our new ML tools at hand, we'll be able to \\nleverage the power of FastAPI to serve our estimators and propose a reliable and efficient \\nprediction API for our users.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 377, 'page_label': '359'}, page_content=\"13\\nCreating an Efficient \\nPrediction API \\nEndpoint with \\nFastAPI\\nIn the previous chapters, we introduced the most common data science techniques and \\nlibraries largely used in the Python community. Thanks to those tools, we can now build \\nmachine learning models that can make efficient predictions and classify data. Of course, \\nwe now have to think about a convenient interface so that we can take advantage of their \\nintelligence. This way, microservices or frontend applications can ask our model to make \\npredictions to improve the user experience or business operations.\\nIn this chapter, we'll learn how to do that with FastAPI. As we've seen throughout this \\nbook, FastAPI allows us to implement very efficient REST APIs with clear and lightweight \\nsyntax. In this chapter, you'll learn how to do this as efficiently as possible so that it can serve \\nthousands of prediction requests. To help us with this task, we'll introduce another library, \\nJoblib, that provides tools to help us serialize a trained model and cache predicted results.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 378, 'page_label': '360'}, page_content=\"360     Creating an Efficient Prediction API Endpoint with FastAPI\\nIn this chapter, we're going to cover the following main topics:\\n• Persisting a trained model with Joblib\\n• Implementing an efficient prediction endpoint\\n• Caching results with Joblib\\nTechnical requirements\\nY ou'll need a Python virtual environment, similar to the one we set up in Chapter 1, \\nPython Development Environment Setup.\\nY ou can find all the code examples for this chapter in this book's dedicated GitHub \\nrepository: https://github.com/PacktPublishing/Building-Data-\\nScience-Applications-with-FastAPI/tree/main/chapter13.\\nPersisting a trained model with Joblib\\nIn the previous chapter, you learned how to train an estimator with scikit-learn. When \\nbuilding such models, you'll likely obtain a rather complex Python script to load your \\ntraining data, pre-process it, and train your model with the best set of parameters. However, \\nwhen deploying your model in a web application, such as FastAPI, you don't want to repeat \\nthis script and run all those operations when the server is starting. Instead, you need a \\nready-to-use representation of your trained model that you can just load and use.\\nThis is what Joblib does. This library aims to provide tools for efficiently saving Python \\nobjects to disk, such as large arrays of data or function results: this operation is generally \\ncalled dumping. Joblib is already a dependency of scikit-learn, so we don't even need to \\ninstall it. scikit-learn uses it internally to load the bundled toy datasets.\\nAs we'll see, dumping a trained model involves just one line of code with Joblib.\\nDumping a trained model\\nIn this example, we're using the newsgroups example we saw in the Chaining \\npre-processors and estimators with pipelines section of Chapter 12, Training Machine \\nLearning Models with scikit-learn. As a reminder, we load four categories of the 20 \\nnewsgroups dataset and build a model to automatically categorize news articles into those \\ncategories. Once we've done this, we dump the model into a file called newsgroups_\\nmodel.joblib:\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 379, 'page_label': '361'}, page_content='Persisting a trained model with Joblib     361\\nchapter13_dump_joblib.py\\n# Make the pipeline\\nmodel = make_pipeline(\\n    TfidfVectorizer(),\\n    MultinomialNB(),\\n)\\n# Train the model\\nmodel.fit(newsgroups_training.data, newsgroups_training.target)\\n# Serialize the model and the target names\\nmodel_file = \"newsgroups_model.joblib\"\\nmodel_targets_tuple = (model, newsgroups_training.target_names)\\njoblib.dump(model_targets_tuple, model_file)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_dump_\\njoblib.py\\nAs you can see, Joblib exposes a function called dump, which simply expects two \\narguments: the Python object to save and the path of the file.\\nNotice that we don\\'t dump the model variable alone: instead, we wrap it in a tuple, along \\nwith the name of the categories, target_names. This allows us to retrieve the actual \\nname of the category after the prediction has been made, without us having to reload the \\ntraining dataset.\\nIf you run this script, you\\'ll see that the newsgroups_model.joblib file was created:\\n$ python chapter13/chapter13_dump_joblib.py\\n$ ls -lh *.joblib\\n-rw-r--r--  1 fvoron  staff   3,2M 10 jul 10:41 newsgroups_\\nmodel.joblib\\nNotice that this file is rather large: it\\'s more than 3 MB! It stores all the probabilities of \\neach word in each category, as computed by the Multinomial Naive Bayes model.  \\nThat\\'s all we need to do. This file now contains a static representation of our Python \\nmodel, which will be easy to store, share, and load. Now, let\\'s learn how to load it and \\ncheck that we can run predictions on it.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 380, 'page_label': '362'}, page_content='362     Creating an Efficient Prediction API Endpoint with FastAPI\\nLoading a dumped model\\nNow that we have our dumped model file, let\\'s learn how to load it again using Joblib  \\nand check that everything is working. In the following example, we\\'re loading the Joblib \\ndump present in the chapter13 directory of the examples repository and running  \\na prediction:\\nchapter13_load_joblib.py\\nimport os\\nfrom typing import List, Tuple\\nimport joblib\\nfrom sklearn.pipeline import Pipeline\\n# Load the model\\nmodel_file = os.path.join(os.path.dirname(__file__), \\n\"newsgroups_model.joblib\")\\nloaded_model: Tuple[Pipeline, List[str]] = joblib.load(model_\\nfile)\\nmodel, targets = loaded_model\\n# Run a prediction\\np = model.predict([\"computer cpu memory ram\"])\\nprint(targets[p[0]])\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_load_\\njoblib.py\\nAll we need to do here is call the load function from Joblib and pass it a valid path to a \\ndump file. The result of this function is the very same Python object we dumped. Here,  \\nit\\'s a tuple composed of the scikit-learn estimator and a list of categories.\\nNotice that we added some type hints: while not necessary, it helps mypy or your IDE \\nidentify the nature of the objects you loaded and benefit from type-checking and  \\nauto-completion.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 381, 'page_label': '363'}, page_content='Implementing an efficient prediction endpoint     363\\nFinally, we ran a prediction on the model: it\\'s a true scikit-learn estimator, with all the \\nnecessary training parameters.\\nThat\\'s it! As you\\'ve seen, Joblib is straightforward to use. Nevertheless, it\\'s an essential  \\ntool for exporting your scikit-learn models and being able to use them in external  \\nservices without repeating the training phase. Now, we can use those dump files in \\nFastAPI projects.\\nImplementing an efficient prediction endpoint\\nNow that we have a way to save and load our machine learning models, it\\'s time to use \\nthem in a FastAPI project. As you\\'ll see, the implementation shouldn\\'t be too much of \\na surprise if you\\'ve followed this book. The main part of the implementation is the class \\ndependency, which will take care of loading the model and making predictions. If you \\nneed a refresher on class dependencies, check out Chapter 5, Dependency Injections  \\nin FastAPI.\\nLet\\'s go! Our example will be based on the newgroups model we dumped in the previous \\nsection. We\\'ll start by showing you how to implement the class dependency, which will \\ntake care of loading and making predictions:\\nchapter13_prediction_endpoint.py\\nclass PredictionInput(BaseModel):\\n    text: str\\nclass PredictionOutput(BaseModel):\\n    category: str\\nclass NewsgroupsModel:\\n    model: Optional[Pipeline]\\n    targets: Optional[List[str]]\\n    def load_model(self):\\n        \"\"\"Loads the model\"\"\"\\n        model_file = os.path.join(os.path.dirname(__file__), \\n\"newsgroups_model.joblib\")\\n        loaded_model: Tuple[Pipeline, List[str]] = joblib.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 382, 'page_label': '364'}, page_content='364     Creating an Efficient Prediction API Endpoint with FastAPI\\nload(model_file)\\n        model, targets = loaded_model\\n        self.model = model\\n        self.targets = targets\\n    async def predict(self, input: PredictionInput) -> \\nPredictionOutput:\\n        \"\"\"Runs a prediction\"\"\"\\n        if not self.model or not self.targets:\\n            raise RuntimeError(\"Model is not loaded\")\\n        prediction = self.model.predict([input.text])\\n        category = self.targets[prediction[0]]\\n        return PredictionOutput(category=category)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_\\nprediction_endpoint.py\\nFirst, we start by defining two Pydantic models: PredictionInput and \\nPredictionOutput. In a pure FastAPI philosophy, they will help us validate the \\nrequest payload and return a structured JSON response. Here, as input, we simply expect \\na text property containing the text we want to classify; in terms of the output, we expect \\na category property containing the predicted category.\\nThe most interesting part of this extract is the NewsgroupsModel class. It implements \\ntwo methods: load_model and predict.\\nThe load_model method loads the model using Joblib, as we saw in the previous \\nsection, and stores the model and the targets in class properties. Hence, they will be \\navailable for use by the predict method.\\nOn the other hand, the predict method will be injected into the path operation \\nfunction. As you can see, it directly accepts a PredictionInput that will be injected  \\nby FastAPI. Inside this method, we are making a prediction, as we usually do with  \\nscikit-learn. We return a PredictionOutput object with the category we predicted.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 383, 'page_label': '365'}, page_content='Implementing an efficient prediction endpoint     365\\nY ou may have noticed that, first, we check if the model and its targets have been \\nassigned in the class properties before performing the prediction. Of course, we need to \\nensure load_model was called at some point before making a prediction. Y ou may be \\nwondering why we are not putting this logic in an initializer, __init__, so that we can \\nensure the model is loaded at class instantiation. This would work perfectly fine; however, \\nit would cause some issues. As we\\'ll see, we are instantiating a NewsgroupsModel \\ninstance right after FastAPI so that we can use it in our routes. If the loading logic was \\nin __init__, the model would be loaded whenever we import some variables (such \\nas the app instance) from this file, such as in unit tests. In most cases, this would incur \\nunnecessary I/O operations and memory consumption. As we\\'ll see, it\\'s better to use the \\nstartup event of FastAPI to load the model when the app is run.\\nThe following extract shows the rest of the implementation, along with the actual FastAPI \\nroute for handling predictions:\\nchapter13_prediction_endpoint.py\\napp = FastAPI()\\nnewgroups_model = NewsgroupsModel()\\n@app.post(\"/prediction\")\\nasync def prediction(\\n    output: PredictionOutput = Depends(newgroups_model.\\npredict),\\n) -> PredictionOutput:\\n    return output\\n@app.on_event(\"startup\")\\nasync def startup():\\n    newgroups_model.load_model()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_\\nprediction_endpoint.py'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 384, 'page_label': '366'}, page_content='366     Creating an Efficient Prediction API Endpoint with FastAPI\\nAs we mentioned previously, we are creating an instance of NewsgroupsModel so \\nthat we can inject it into our path operation function. Moreover, we are implementing a \\nstartup event handler to call load_model. This way, we are making sure that the model \\nis loaded during application startup and is ready to use.\\nThe prediction endpoint is quite straightforward: as you can see, we directly depend on \\nthe predict method, which will take care of injecting the payload and validating it.  \\nWe only have to return the output.\\nThat\\'s it! Once again, FastAPI makes our life very easy by allowing us to write very simple \\nand readable code, even for complex tasks. We can run this application using Uvicorn,  \\nas usual:\\n$ uvicorn chapter13.chapter13_prediction_endpoint:app\\nNow, we can try to run some predictions with HTTPie:\\n$ http POST http://localhost:8000/prediction text=\"computer cpu \\nmemory ram\"      \\nHTTP/1.1 200 OK\\ncontent-length: 36\\ncontent-type: application/json\\ndate: Tue, 13 Jul 2021 06:34:58 GMT\\nserver: uvicorn\\n{\\n    \"category\": \"comp.sys.mac.hardware\"\\n}\\nOur machine learning classifier is alive! To push this further, let\\'s see how we can \\nimplement a simple caching mechanism using Joblib.\\nCaching results with Joblib\\nIf your model takes time to make predictions, it may be interesting to cache the results:  \\nif the prediction for a particular input has already been done, it makes sense to return the \\nsame result we saved on disk, rather than running the computations again. In this section, \\nwe\\'ll learn how to do this with the help of Joblib.\\nJoblib provides us with a very convenient and easy-to-use tool to do this, so the \\nimplementation is quite straightforward. The main concern will be about whether we \\nshould choose standard or async functions to implement the endpoints and dependencies. \\nThis will allow us to explain some of the technical details of FastAPI in more detail.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 385, 'page_label': '367'}, page_content='Caching results with Joblib     367\\nWe\\'ll build upon the example we provided in the previous section. The first thing we must \\ndo is initialize a Joblib Memory class, which is the helper for caching functions results. \\nThen, we can add a decorator to the functions we want to cache. Y ou can see this in the \\nfollowing example: \\nchapter13_caching.py\\nmemory = joblib.Memory(location=\"cache.joblib\")\\n@memory.cache(ignore=[\"model\"])\\ndef predict(model: Pipeline, text: str) -> int:\\n    prediction = model.predict([text])\\n    return prediction[0]\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_\\ncaching.py\\nWhen initializing memory, the main argument is location, which is the directory path \\nwhere Joblib will store the results. Joblib automatically saves cached results on the hard disk.\\nThen, you can see that we implemented a predict function that accepts our scikit-\\nlearn model, some text input, and then returns the predicted category index. This \\nis the same prediction operation we\\'ve seen so far. Here, we extracted it from the \\nNewsgroupsModel dependency class because Joblib caching is primarily designed to \\nwork with regular functions. Caching class methods is not recommended. As you can see, \\nwe simply have to add a @memory.cache decorator on top of this function to enable \\nJoblib caching.\\nWhenever this function is called, Joblib will check if it has the result on disk for the  \\nsame arguments. If it does, it returns it directly. Otherwise, it proceeds with the regular \\nfunction call.\\nAs you can see, we added an ignore argument to the decorator, which allows us to tell \\nJoblib to not take into account some arguments in the caching mechanism. Here, we \\nexcluded the model argument. Joblib cannot dump complex objects, such scikit-learn \\nestimators. This isn\\'t a problem, though: the model is not changing between several \\npredictions, so we don\\'t care about having it cached. If we make improvements to our \\nmodel and deploy a new one, all we have to do is clear the whole cache so that older \\npredictions are made again with the new model.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 386, 'page_label': '368'}, page_content='368     Creating an Efficient Prediction API Endpoint with FastAPI\\nNow, we can tweak the NewsgroupsModel dependency class so that it works with this \\nnew predict function. Y ou can see this in the following example:\\nchapter13_caching.py\\nclass NewsgroupsModel:\\n    model: Optional[Pipeline]\\n    targets: Optional[List[str]]\\n    def load_model(self):\\n        \"\"\"Loads the model\"\"\"\\n        model_file = os.path.join(os.path.dirname(__file__), \\n\"newsgroups_model.joblib\")\\n        loaded_model: Tuple[Pipeline, List[str]] = joblib.\\nload(model_file)\\n        model, targets = loaded_model\\n        self.model = model\\n        self.targets = targets\\n    def predict(self, input: PredictionInput) -> \\nPredictionOutput:\\n        \"\"\"Runs a prediction\"\"\"\\n        if not self.model or not self.targets:\\n            raise RuntimeError(\"Model is not loaded\")\\n        prediction = predict(self.model, input.text)\\n        category = self.targets[prediction]\\n        return PredictionOutput(category=category)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_\\ncaching.py\\nIn the predict method, we are calling the external predict function instead of \\ndoing so directly inside the method, taking care to pass the model and the input text as \\narguments. All we have to do after is retrieve the corresponding category name and build \\na PredictionOutput object.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 387, 'page_label': '369'}, page_content='Caching results with Joblib     369\\nFinally, we have the REST API endpoints. Here, we added a DELETE/cache route so \\nthat we can clear the whole Joblib cache with an HTTP request. This can be seen in the \\nfollowing example:\\nchapter13_caching.py\\n@app.post(\"/prediction\")\\ndef prediction(\\n    output: PredictionOutput = Depends(newgroups_model.\\npredict),\\n) -> PredictionOutput:\\n    return output\\n@app.delete(\"/cache\", status_code=status.HTTP_204_NO_CONTENT)\\ndef delete_cache():\\n    memory.clear()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_\\ncaching.py\\nThe clear method of the memory object removes all the Joblib cache files on the disk.\\nOur FastAPI application is now caching prediction results. If you make a request with the \\nsame input twice, the second response will show you the cached result. In this example, \\nour model is fast, so you won\\'t notice a difference in terms of execution time; however, \\nthis could be interesting with more complex models.\\nChoosing between standard or async functions\\nY ou may have noticed that we changed the predict method and the prediction and \\ndelete_cache path operation functions so that they\\'re standard, non-async functions.\\nSince the beginning of this book, we\\'ve shown you how FastAPI completely embraces \\nasynchronous I/O and why it\\'s good for the performance of your applications. We\\'ve  \\nalso recommended libraries that also work asynchronously, such as database drivers,  \\nto leverage that power.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 388, 'page_label': '370'}, page_content='370     Creating an Efficient Prediction API Endpoint with FastAPI\\nIn some cases, however, that\\'s not always possible. In this case, Joblib is implemented to \\nwork synchronously. Nevertheless, it\\'s performing long I/O operations: it reads and writes \\ncache files on the hard disk. Hence, it will block the process and won\\'t be able to answer \\nother requests while this is happening, as we\\'ve explained in the Asynchronous I/O\\xa0section \\nof Chapter 2, Python Programming Specificities.\\nTo solve this, FastAPI implements a neat mechanism: if you define a path operation \\nfunction or a dependency as a standard, non-async function, it\\'ll run it in a separate thread. \\nThis means that blocking operations, such as synchronous file reading, won\\'t block the \\nmain process. In a sense, we could say that it mimics an asynchronous operation.\\nTo understand this, we\\'ll perform a simple experiment. In the following example, we are \\nbuilding a dummy FastAPI application with three endpoints:\\n• /fast, which directly returns a response.\\n• /slow-async, a path operation defined as async, which makes a synchronous \\nblocking operation that takes 10 seconds to run.\\n• /slow-sync, a path operation that\\'s defined as a standard method, which makes a \\nsynchronous blocking operation that takes 10 seconds to run:\\nchapter13_async_not_async.py\\nimport time\\nfrom fastapi import FastAPI\\napp = FastAPI()\\n@app.get(\"/fast\")\\nasync def fast():\\n    return {\"endpoint\": \"fast\"}\\n@app.get(\"/slow-async\")\\nasync def slow_async():\\n    \"\"\"Runs in the main process\"\"\"\\n    time.sleep(10)  # Blocking sync operation\\n    return {\"endpoint\": \"slow-async\"}'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 389, 'page_label': '371'}, page_content='Caching results with Joblib     371\\n@app.get(\"/slow-sync\")\\ndef slow_sync():\\n    \"\"\"Runs in a thread\"\"\"\\n    time.sleep(10)  # Blocking sync operation\\n    return {\"endpoint\": \"slow-sync\"}\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter13/chapter13_async_\\nnot_async.py\\nWith this simple application, the goal is to see how those blocking operations block the \\nmain process. Let\\'s run this application with Uvicorn:\\n$ uvicorn chapter13.chapter13_async_not_async:app\\nNext, open two new terminals. In the first one, make a request to the / \\nslow-async endpoint:\\n$ http GET http://localhost:8000/slow-async\\nWithout waiting for the response, in the second terminal, make a request to the / \\nfast endpoint:\\n$ http GET http://localhost:8000/fast\\nY ou\\'ll see that you have to wait 10 seconds before you get the response for the /fast \\nendpoint. This means that /slow-async blocked the process and prevented the server \\nfor answering the other request while this was happening.\\nNow, let\\'s perform the same experiment with the /slow-sync endpoint:\\n$ http GET http://localhost:8000/slow-sync\\nAnd again, run the following command:\\n$ http GET http://localhost:8000/fast\\nY ou\\'ll immediately get /fast as a response, without having to wait for /slow-sync to \\nfinish. Since it\\'s defined as a standard, non-async function, FastAPI will run it in a thread \\nto prevent blocking. However, bear in mind that sending the task to a separate thread \\nimplies a small overhead, so it\\'s important to think about the best approach for your \\ncurrent problem.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 390, 'page_label': '372'}, page_content=\"372     Creating an Efficient Prediction API Endpoint with FastAPI\\nSo, when developing with FastAPI, how can you choose between standard or async \\nfunctions for path operations and dependencies? The rules of thumb for this are  \\nas follows:\\n• If it's not making long I/O operations (file reading, network requests, and so on), \\ndefine them as async.\\n• If you are making I/O operations, do the following:\\na. Try to choose libraries that are compatible with asynchronous I/O, as we saw for \\ndatabases or HTTP clients. In this case, your functions will be async.\\nb. If it's not possible, which is the case for Joblib caching, define them as standard \\nfunctions. FastAPI will run them in a separate thread.\\nSince Joblib is completely synchronous at making I/O operations, we switched the path \\noperations and the dependency method so that they're synchronous, standard methods.\\nIn this example, the difference is not very noticeable because the I/O operations are \\nsmall and fast. However, it's good to keep this in mind if you have to implement slower \\noperations, such as for performing file uploads to cloud storage.\\nSummary\\nCongratulations! Y ou're now able to build a fast and efficient REST API to serve your \\nmachine learning models. Thanks to Joblib, you've learned how to dump a trained  \\nscikit-learn estimator into a file that's easy to load and use inside your application. We've \\nalso seen an approach to caching prediction results using Joblib. Finally, we discussed how \\nFastAPI handles synchronous operations by sending them to a separate thread to prevent \\nblocking. While this was a bit technical, it's important to bear this aspect in mind when \\ndealing with blocking I/O operations.\\nWe're near the end of our FastAPI journey. Before letting you build awesome data science \\napplications by yourself, we have provided one last chapter to push this a bit further: \\nusing WebSockets and a library dedicated to computer vision, OpenCV , we'll learn how to \\nimplement an application that can perform real-time face detection.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 391, 'page_label': '373'}, page_content=\"14\\nImplement a Real-\\nTime Face Detection \\nSystem Using \\nWebSockets with \\nFastAPI and OpenCV\\nIn the previous chapter, you learned how to create efficient REST API endpoints to make \\npredictions with trained machine learning models. This approach covers a lot of use cases, \\ngiven that we have a single observation we want to work on. In some cases, however, \\nwe may need to continuously perform predictions on a stream of input, for instance, a \\nface detection system that works in real time with video input. This is exactly what we'll \\nbuild in this chapter. How? If you remember, besides HTTP endpoints, FastAPI also has \\nthe ability to handle WebSockets endpoints, which allow us to send and receive streams \\nof data. In this case, the browser will send into the WebSocket a stream of images from \\nthe webcam, and our application will run a face detection algorithm and send back the \\ncoordinates of the detected face in the image. For this face detection task, we'll rely on \\nOpenCV , which is a library dedicated to computer vision.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 392, 'page_label': '374'}, page_content=\"374     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\nIn this chapter, we're going to cover the following main topics:\\n• Getting started with OpenCV\\n• Implementing an HTTP endpoint to perform face detection on a single image\\n• Implementing a WebSocket to perform face detection on a stream of images\\n• Sending a stream of images from the browser in a WebSocket\\n• Showing the face detection results in a browser\\nTechnical requirements\\nY ou'll need a Python virtual environment, as we set up in Chapter 1, Python Development \\nEnvironment Setup.\\nY ou'll also need a webcam on your computer to be able to run the examples.\\nY ou'll find all the code examples of this chapter in the dedicated GitHub repository: \\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/tree/main/chapter14.\\nGetting started with OpenCV\\nComputer vision is a field related to machine learning that aims at developing algorithms \\nand systems to analyze images and videos automatically. A typical example of computer \\nvision's application is face detection: a system automatically detecting human faces in an \\nimage. This is the kind of system we'll build in this chapter. \\nTo help us in this task, we'll use OpenCV , which is one of the most popular computer \\nvision libraries. It's written in C and C++ but has bindings to make it usable in many other \\nprogramming languages, including Python. We could have used scikit-learn to develop a \\nface detection model, but we'll see that OpenCV already includes all the necessary tools to \\nperform this task without having to manually train and tune machine learning estimators.\\nTo begin with OpenCV , we'll implement a simple Python script to perform face detection \\nlocally using a computer webcam:\\n1. The first step is, of course, to install the OpenCV library for Python:\\n$ pip install opencv-python\\nNow, all we need to do is to use the tools provided by OpenCV to implement a \\nsimple face detection program. As you'll see, everything is bundled in the library.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 393, 'page_label': '375'}, page_content='Getting started with OpenCV     375\\n2. In the following example, you can see the whole implementation:\\nchapter14_opencv.py\\nimport cv2\\n# Load the trained model\\nface_cascade = cv2.CascadeClassifier(\\n\\xa0\\xa0\\xa0\\xa0cv2.data.haarcascades + \"haarcascade_frontalface_default.\\nxml\"\\n)\\n# You may need to change the index depending on your computer \\nand camera\\nvideo_capture = cv2.VideoCapture(0)\\nwhile True:\\n\\xa0\\xa0\\xa0\\xa0# Get an image frame\\n\\xa0\\xa0\\xa0\\xa0ret, frame = video_capture.read()\\n\\xa0\\xa0\\xa0\\xa0# Convert it to grayscale and run detection\\n\\xa0\\xa0\\xa0\\xa0gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\xa0\\xa0\\xa0\\xa0faces = face_cascade.detectMultiScale(gray)\\n\\xa0\\xa0\\xa0\\xa0# Draw a rectangle around the faces\\n\\xa0\\xa0\\xa0\\xa0for (x, y, w, h) in faces:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0cv2.rectangle(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0img=frame,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pt1=(x, y),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pt2=(x + w, y + h),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0color=(0, 255, 0),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0thickness=2,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)\\n\\xa0\\xa0\\xa0\\xa0# Display the resulting frame\\n\\xa0\\xa0\\xa0\\xa0cv2.imshow(\"Chapter 14 – OpenCV\", frame)'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 394, 'page_label': '376'}, page_content='376     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\n\\xa0\\xa0\\xa0\\xa0# Break when key \"q\" is pressed\\n\\xa0\\xa0\\xa0\\xa0if cv2.waitKey(1) == ord(\"q\"):\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0break\\nvideo_capture.release()\\ncv2.destroyAllWindows()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/chapter14_\\nopencv.py\\nY ou can simply run this script by invoking it with Python:\\n$ python chapter14/chapter14_opencv.py\\nA window similar to the one shown in Figure 14.1 will open and start streaming \\nimages from your webcam. \\n3. When the algorithm detects a face, it draws a green rectangle around it. Press the q \\nkey on your keyboard to stop the script:\\nFigure 14.1 – Face detection script with OpenCV'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 395, 'page_label': '377'}, page_content=\"Getting started with OpenCV     377\\n4. Let's go through the implementation. The first thing we do is instantiate a \\nCascadeClassifier class with an XML file bundled with the library. This class \\nis actually a machine learning algorithm using the Haar cascade principle. Y ou can \\nread more about the theory behind this algorithm in the OpenCV documentation: \\nhttps://docs.opencv.org/master/db/d28/tutorial_cascade_\\nclassifier.html.\\nThe nice thing here is that OpenCV comes with pre-trained models, provided in the \\nform of XML files, including ones for face detection. Hence, we only have to load \\nthem to start working on images.\\n5. Then, we instantiate a VideoCapture class. It'll allow us to stream images from a \\nwebcam. The integer argument in the initializer is the index of the camera you want \\nto use. If you have several cameras, you may need to adjust this argument.\\n6. After that, we start an infinite loop so that we can continuously run detections on \\nthe stream of images. Inside it, we start by retrieving an image, frame, from the \\nvideo_capture instance. This image is then fed to the classifier thanks to the \\ndetectMultiScale method. Notice that we first convert it to grayscale, which is \\na requirement for Haar cascade classifiers.\\nThe result of this operation is a list of tuples containing the characteristics of the \\nrectangles around the detected faces: x and y are the coordinates of the starting \\npoint; w and h are the width and height of this rectangle. All we have to do is draw \\neach rectangle on the image using the rectangle function.\\n7. Finally, we can display the image in a window. Notice that before ending the loop, \\nwe give it a chance to break by listening for a keypress on the keyboard: if the q key \\nis pressed, we break the loop.\\nAnd that's it! Fewer than 40 lines of code to have a working face detection system! \\nAs you can see, OpenCV makes our life very easy by providing trained classifiers. \\nBesides, it comes with all the tools to capture and work on images.\\nOf course, our goal in this chapter is to put all this intelligence on a remote server so \\nthat we can serve this experience to thousands of users. Once again, FastAPI will be \\nour ally here.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 396, 'page_label': '378'}, page_content='378     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\nImplementing an HTTP endpoint to perform \\nface detection on a single image\\nBefore working with WebSockets, we\\'ll start simple and implement, using FastAPI, a \\nclassic HTTP endpoint for accepting image uploads and performing face detection on \\nthem. As you\\'ll see, the main difference from the previous example is in how we acquire \\nthe image: instead of streaming it from the webcam, we get it from a file upload that we \\nhave to convert into an OpenCV image object.\\nY ou can see the whole implementation in the following code:\\nchapter14_api.py\\nfrom typing import List, Tuple\\nimport cv2\\nimport numpy as np\\nfrom fastapi import FastAPI, File, UploadFile\\nfrom pydantic import BaseModel\\napp = FastAPI()\\ncascade_classifier = cv2.CascadeClassifier()\\nclass Faces(BaseModel):\\n\\xa0\\xa0\\xa0\\xa0faces: List[Tuple[int, int, int, int]]\\n@app.post(\"/face-detection\", response_model=Faces)\\nasync def face_detection(image: UploadFile = File(...)) -> \\nFaces:\\n\\xa0\\xa0\\xa0\\xa0data = np.fromfile(image.file, dtype=np.uint8)\\n\\xa0\\xa0\\xa0\\xa0image = cv2.imdecode(data, cv2.IMREAD_UNCHANGED)\\n\\xa0\\xa0\\xa0\\xa0gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\\n\\xa0\\xa0\\xa0\\xa0faces = cascade_classifier.detectMultiScale(gray)\\n\\xa0\\xa0\\xa0\\xa0if len(faces) > 0:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0faces_output = Faces(faces=faces.tolist())'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 397, 'page_label': '379'}, page_content='Implementing an HTTP endpoint to perform face detection on a single image     379\\n\\xa0\\xa0\\xa0\\xa0else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0faces_output = Faces(faces=[])\\n\\xa0\\xa0\\xa0\\xa0return faces_output\\n@app.on_event(\"startup\")\\nasync def startup():\\n\\xa0\\xa0\\xa0\\xa0cascade_classifier.load(\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0cv2.data.haarcascades + \"haarcascade_frontalface_\\ndefault.xml\"\\n\\xa0\\xa0\\xa0\\xa0)\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/chapter14_api.py\\nAs you can see, we start with a rather simple FastAPI application. At the top of the file, we \\ninstantiate a CascadeClassifier class. Notice, however, that contrary to the previous \\nexample, we load the trained model inside the startup event instead of doing it right away. \\nThis is for the same reason we explained in Chapter 13, Creating an Efficient Prediction \\nAPI Endpoint with FastAPI, when we loaded our dumped Joblib model: we want to load it \\nonly when the application is actually starting, not when we are importing the module.\\nThen, we define a face_detection endpoint that expects FileUpload. If you need \\na refresher on file uploads, you can check out Chapter 3, Developing a RESTful API with \\nFastAPI. Once we have the file, you can see that we are performing two operations using \\nNumPy and OpenCV . Indeed, the images need to be loaded into a NumPy matrix that is \\nusable by OpenCV .\\nIf we had a file path, we could have directly used the imread function of OpenCV to \\nload it. Here, we have an UploadFile object that has a file property pointing to a file \\ndescriptor. Using NumPy, we can load the binary data into an array of pixels, data. This \\ncan be used afterward by the imdecode function to create a proper OpenCV matrix.\\nFinally, we can run the prediction using the classifier, as we saw in the previous section. \\nNotice that we structure the result into a structured Pydantic model. When OpenCV \\ndetects faces, it returns the result as a nested NumPy array. The goal of the tolist \\nmethod is just to transform it into a standard list of lists.\\nY ou can run this example using the usual Uvicorn command:\\n$ uvicorn chapter14.chapter14_api:app'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 398, 'page_label': '380'}, page_content='380     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\nIn the code example repository, you\\'ll find a picture of a group of people: https://\\ngithub.com/PacktPublishing/Building-Data-Science-Applications-\\nwith-FastAPI/blob/main/assets/people.jpg.\\nLet\\'s upload it on our endpoint with HTTPie:\\n$ http --form POST http://localhost:8000/face-detection \\nimage@./assets/people.jpg\\nHTTP/1.1 200 OK\\ncontent-length: 43\\ncontent-type: application/json\\ndate: Wed, 21 Jul 2021 07:58:17 GMT\\nserver: uvicorn\\n{\\n\\xa0\\xa0\\xa0\\xa0\"faces\": [\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0237,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa092,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa080,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa080\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0],\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0426,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa075,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0115,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0115\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0]\\n\\xa0\\xa0\\xa0\\xa0]\\n}\\nThe classifier was able to detect two faces in the image.\\nGreat! Our face detection system is now available as a web server. However, our goal is \\nstill to make a real-time system: thanks to WebSockets, we\\'ll be able to handle a stream  \\nof images.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 399, 'page_label': '381'}, page_content=\"Implementing a WebSocket to perform face detection on a stream of images     381\\nImplementing a WebSocket to perform face \\ndetection on a stream of images\\nOne of the main benefits of WebSockets, as we saw in Chapter 8, Defining WebSockets \\nfor Two-Way Interactive Communication in FastAPI, is that it opens a full-duplex \\ncommunication channel between the client and the server. Once the connection is \\nestablished, messages can be passed quickly without having to go through all the steps of the \\nHTTP protocol. Therefore, it's much more suited to sending lots of messages in real time.\\nThe point here will be to implement a WebSocket endpoint that is able to both accept \\nimage data and run OpenCV detection on it. The main challenge here will be to handle \\na phenomenon known as backpressure. Put simply, we'll receive more images from the \\nbrowser than the server is able to handle, because of the time needed to run the detection \\nalgorithm. Thus, we'll have to work with a queue (or buffer) of limited size and drop some \\nimages along the way to handle the stream in near real time. \\nY ou can read the implementation in the following sample:\\napp.py\\nasync def receive(websocket: WebSocket, queue: asyncio.Queue):\\n\\xa0\\xa0\\xa0\\xa0bytes = await websocket.receive_bytes()\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0queue.put_nowait(bytes)\\n\\xa0\\xa0\\xa0\\xa0except asyncio.QueueFull:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pass\\nasync def detect(websocket: WebSocket, queue: asyncio.Queue):\\n\\xa0\\xa0\\xa0\\xa0while True:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0bytes = await queue.get()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0data = np.frombuffer(bytes, dtype=np.uint8)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0img = cv2.imdecode(data, 1)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0faces = cascade_classifier.detectMultiScale(gray)\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if len(faces) > 0:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0faces_output = Faces(faces=faces.tolist())\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0else:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0faces_output = Faces(faces=[])\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 400, 'page_label': '382'}, page_content='382     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.send_json(faces_output.dict())\\n@app.websocket(\"/face-detection\")\\nasync def face_detection(websocket: WebSocket):\\n\\xa0\\xa0\\xa0\\xa0await websocket.accept()\\n\\xa0\\xa0\\xa0\\xa0queue: asyncio.Queue = asyncio.Queue(maxsize=10)\\n\\xa0\\xa0\\xa0\\xa0detect_task = asyncio.create_task(detect(websocket, queue))\\n\\xa0\\xa0\\xa0\\xa0try:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0while True:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await receive(websocket, queue)\\n\\xa0\\xa0\\xa0\\xa0except WebSocketDisconnect:\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0detect_task.cancel()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0await websocket.close()\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/websocket_face_\\ndetection/app.py\\nAs we said, we have two tasks: receive and detect. The first one is for reading raw \\nbytes from the WebSocket, while the second one is for performing the detection and \\nsending the result, exactly as we saw in the last section.\\nThe key here is to use the asyncio.Queue object. This is a convenient structure \\nallowing us to queue some data in memory and retrieve it in a first in, first out (FIFO) \\nstrategy. We are able to set a limit on the number of elements we store in the queue: this is \\nhow we\\'ll be able to limit the number of images we handle.\\nThe receive function is receiving data and putting it at the end of the queue. When \\nworking with Queue, we have two methods to put a new element in the queue: put and \\nput_nowait. If the queue is full, the first one will wait until there is room in the queue. \\nThis is not what we want here: we want to drop images that we won\\'t be able to handle in \\ntime. With put_nowait, the QueueFull exception is raised if the queue is full. In this \\ncase, we just pass and drop the data.\\nOn the other hand, the detect function is pulling the first message from the queue \\nand runs its detection before sending the result. In the previous section, we used the \\nfromfile function to read the image data. Here, we directly have bytes data, so \\nfrombuffer is more appropriate.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 401, 'page_label': '383'}, page_content='Sending a stream of images from the browser in a WebSocket     383\\nThe implementation of the WebSocket itself is a bit different from what we saw in  \\nChapter 8, Defining WebSockets for Two-Way Interactive Communication in FastAPI. \\nIndeed, we don\\'t want the two tasks to be concurrent here: we want to accept new images \\nand continuously run detections on images as they come in.\\nThis is why the detect function has its own infinite loop. By using create_task on this \\nfunction, we schedule it in the event loop so that it starts to handle the images in the queue. \\nThen, we have the regular WebSocket loop, which calls the receive function. In a sense, \\nwe could say that that detect runs \"in the background.\" Notice that we ensure that this \\ntask is canceled when the WebSocket is closed so that the infinite loop is correctly stopped.\\nThe rest of the implementation is similar to what we saw in the previous section. Our \\nbackend is now ready! Let\\'s now see how to use its power from a browser.\\nSending a stream of images from the browser \\nin a WebSocket\\nIn this section, we\\'ll see how you can capture images from the webcam in the browser and \\nsend it through a WebSocket. Since it mainly involves JavaScript code, it\\'s admittedly a bit \\nbeyond the scope of this book, but it\\'s necessary to make the application work fully:\\n1. The first step is to enable a camera input in the browser, open the WebSocket \\nconnection, pick a camera image, and send it through the WebSocket. Basically, \\nit\\'ll work like this: thanks to the MediaDevices browser API, we\\'ll be able to \\nlist all the camera inputs available on the device. With this, we\\'ll build a selection \\nform using which the user can select the camera they want to use. Y ou can see the \\nconcrete JavaScript implementation in the following code:\\nscript.js\\nwindow.addEventListener(\\'DOMContentLoaded\\', (event) => {\\n\\xa0\\xa0const video = document.getElementById(\\'video\\');\\n\\xa0\\xa0const canvas = document.getElementById(\\'canvas\\');\\n\\xa0\\xa0const cameraSelect = document.getElementById(\\'camera-\\nselect\\');\\n\\xa0\\xa0let socket;\\n\\xa0\\xa0// List available cameras and fill select\\n\\xa0\\xa0navigator.mediaDevices.enumerateDevices().then((devices) => {\\n\\xa0\\xa0\\xa0\\xa0for (const device of devices) {'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 402, 'page_label': '384'}, page_content=\"384     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if (device.kind === 'videoinput' && device.deviceId) {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0const deviceOption = document.createElement('option');\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0deviceOption.value = device.deviceId;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0deviceOption.innerText = device.label;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0cameraSelect.appendChild(deviceOption);\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0});\\n\\xa0\\xa0// Start face detection on the selected camera on submit\\n\\xa0\\xa0document.getElementById('form-connect').\\naddEventListener('submit', (event) => {\\n\\xa0\\xa0\\xa0\\xa0event.preventDefault();\\n\\xa0\\xa0\\xa0\\xa0// Close previous socket is there is one\\n\\xa0\\xa0\\xa0\\xa0if (socket) {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0socket.close();\\n\\xa0\\xa0\\xa0\\xa0}\\n\\xa0\\xa0\\xa0\\xa0const deviceId = cameraSelect.selectedOptions[0].value;\\n\\xa0\\xa0\\xa0\\xa0socket = startFaceDetection(video, canvas, deviceId);\\n\\xa0\\xa0});\\n});\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/websocket_face_\\ndetection/script.js\\n2. Once the user submits the form, the MediaDevices API will allow us to start \\ncapturing video and display the output in an HTML <video> element. Y ou can \\nread all the details about the MediaDevices API in the MDN documentation: \\nhttps://developer.mozilla.org/en-US/docs/Web/API/\\nMediaDevices.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 403, 'page_label': '385'}, page_content=\"Sending a stream of images from the browser in a WebSocket     385\\n3. In parallel, we'll also establish a connection with the WebSocket. Once it's \\nestablished, we'll launch a repetitive task that captures an image from the video \\ninput and sends it to the server. To do this, we have to use a <canvas> element, \\nan HTML tag dedicated to graphics drawing. It comes with a complete JavaScript \\nAPI so that we can programmatically draw images in it. There, we'll be able to draw \\nthe current video image and convert it to valid JPEG bytes. If you want to know \\nmore about this, MDN gives a very detailed tutorial on <canvas>: https://\\ndeveloper.mozilla.org/en-US/docs/Web/API/Canvas_API/\\nTutorial.\\nThe concrete JavaScript implementation of this is as follows:\\nscript.js\\nconst startFaceDetection = (video, canvas, deviceId) => {\\n\\xa0\\xa0const socket = new WebSocket('ws://localhost:8000/face-\\ndetection');\\n\\xa0\\xa0let intervalId;\\n\\xa0\\xa0// Connection opened\\n\\xa0\\xa0socket.addEventListener('open', function () {\\n\\xa0\\xa0\\xa0\\xa0// Start reading video from device\\n\\xa0\\xa0\\xa0\\xa0navigator.mediaDevices.getUserMedia({\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0audio: false,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0video: {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0deviceId,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0width: { max: 640 },\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0height: { max: 480 },\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0},\\n\\xa0\\xa0\\xa0\\xa0}).then(function (stream) {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0video.srcObject = stream;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0video.play().then(() => {\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0// Adapt overlay canvas size to the video size\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0canvas.width = video.videoWidth;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0canvas.height = video.videoHeight;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0// Send an image in the WebSocket every 160 ms\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0intervalId = setInterval(() => {\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 404, 'page_label': '386'}, page_content=\"386     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0// Create a virtual canvas to draw current video \\nimage\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0const canvas = document.createElement('canvas');\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0const ctx = canvas.getContext('2d');\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0canvas.width = video.videoWidth;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0canvas.height = video.videoHeight;\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ctx.drawImage(video, 0, 0);\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0// Convert it to JPEG and send it to the WebSocket\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0canvas.toBlob((blob) => socket.send(blob), 'image/\\njpeg');\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0}, IMAGE_INTERVAL_MS);\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0});\\n\\xa0\\xa0\\xa0\\xa0});\\n\\xa0\\xa0});\\n\\xa0\\xa0// Listen for messages\\n\\xa0\\xa0socket.addEventListener('message', function (event) {\\n\\xa0\\xa0\\xa0\\xa0drawFaceRectangles(video, canvas, JSON.parse(event.data));\\n\\xa0\\xa0});\\n\\xa0\\xa0// Stop the interval and video reading on close\\n\\xa0\\xa0socket.addEventListener('close', function () {\\n\\xa0\\xa0\\xa0\\xa0window.clearInterval(intervalId);\\n\\xa0\\xa0\\xa0\\xa0video.pause();\\n\\xa0\\xa0});\\n\\xa0\\xa0return socket;\\n};\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/websocket_face_\\ndetection/script.js\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 405, 'page_label': '387'}, page_content='Showing the face detection results in the browser     387\\nNotice that we limit the size of the video input to 640 by 480 pixels, so that we don\\'t blow \\nup the server with too big images. Besides, we set the interval to run every 42 milliseconds \\n(the value is set in the IMAGE_INTERVAL_MS constant), which is roughly equivalent to \\n24 images per second.\\nAs you can see, we also wire the event listener to handle the messages received from  \\nthe WebSocket. It calls the drawFaceRectangles function, which we\\'ll detail in the \\nnext section.\\nShowing the face detection results in the \\nbrowser\\nNow that we are able to send input images to the server, we have to show the result of the \\ndetection in the browser. In a similar way to what we showed in the Getting started with \\nOpenCV section, we\\'ll draw a green rectangle around the detected faces. Thus, we have to \\nfind a way to take the rectangle coordinates sent by the server and draw them in the browser:\\n1. To do this, we\\'ll once again use a <canvas> element. This time, it\\'ll be visible to the \\nuser and we\\'ll draw the rectangles using it. The trick here is to use CSS positioning \\nso that this element overlays the video: this way, the rectangles will be shown right \\non top of the video and the corresponding faces. Y ou can see the HTML code here:\\nindex.html\\n<body>\\n\\xa0\\xa0<div class=\"container\">\\n\\xa0\\xa0\\xa0\\xa0<h1 class=\"my-3\">Chapter 14 – Real time face detection</h1>\\n\\xa0\\xa0\\xa0\\xa0<form id=\"form-connect\">\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<div class=\"input-group mb-3\">\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<select id=\"camera-select\"></select>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<button class=\"btn btn-success\" type=\"submit\" \\nid=\"button-start\">Start</button>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0</div>\\n\\xa0\\xa0\\xa0\\xa0</form>\\n\\xa0\\xa0\\xa0\\xa0<div class=\"position-relative\">\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<video id=\"video\"></video>\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0<canvas id=\"canvas\" class=\"position-absolute top-0 \\nstart-0\"></canvas>\\n\\xa0\\xa0\\xa0\\xa0</div>'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 406, 'page_label': '388'}, page_content='388     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\n\\xa0\\xa0</div>\\n\\xa0\\xa0<script src=\"script.js\"></script>\\n</body>\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/websocket_face_\\ndetection/index.html\\nThe CSS class we are using is utilities, provided by Bootstrap, a very common CSS \\nlibrary. Basically, we set the canvas with absolute positioning and put it at the top \\nleft so that it covers the video element.\\n2. The key now is to use the Canvas API to draw the rectangles according to the \\nreceived coordinates. This is the purpose of the drawFaceRectangles function, \\nwhich is shown in the next sample code block:\\nscript.js\\nconst drawFaceRectangles = (video, canvas, faces) => {\\n\\xa0\\xa0const ctx = canvas.getContext(\\'2d\\');\\n\\xa0\\xa0ctx.width = video.videoWidth;\\n\\xa0\\xa0ctx.height = video.videoHeight;\\n\\xa0\\xa0ctx.beginPath();\\n\\xa0\\xa0ctx.clearRect(0, 0, ctx.width, ctx.height);\\n\\xa0\\xa0for (const [x, y, width, height] of faces.faces) {\\n\\xa0\\xa0\\xa0\\xa0ctx.strokeStyle = \"#49fb35\";\\n\\xa0\\xa0\\xa0\\xa0ctx.beginPath();\\n\\xa0\\xa0\\xa0\\xa0ctx.rect(x, y, width, height);\\n\\xa0\\xa0\\xa0\\xa0ctx.stroke();\\n\\xa0\\xa0}\\n};\\nhttps://github.com/PacktPublishing/Building-Data-Science-\\nApplications-with-FastAPI/blob/main/chapter14/websocket_face_\\ndetection/script.js'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 407, 'page_label': '389'}, page_content=\"Showing the face detection results in the browser     389\\nWith the canvas element, we can use a 2D context to draw things in the object. \\nNotice that we first clean everything to remove the rectangles from the previous \\ndetection. Then, we simply have to loop through all the detected faces and draw a \\nrectangle with the given x, y, width, and height values.\\n3. Our system is now ready and it's time to give it a try! As in Chapter 8, Defining \\nWebSockets for Two-Way Interactive Communication in FastAPI, we'll start two \\nservers: one with Uvicorn to serve the FastAPI application and another that uses  \\nthe built-in Python server to serve the HTML and JavaScript files.\\n• In one terminal, launch the FastAPI application:\\n$ uvicorn chapter14.websocket_face_detection.app:app\\n• In another terminal, serve the HTML application with the built-in Python server:\\n$ python -m http.server --directory chapter14/websocket_\\nface_detection 9000\\nThe HTML application is now ready on port 9000. Y ou can access it in your \\nbrowser with the address http://localhost:9000. Y ou'll see an interface \\ninviting you to choose the camera you want to use, as shown in Figure 14.2:\\nFigure 14.2 – Webcam selection for the face detection web application\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 408, 'page_label': '390'}, page_content=\"390     Implement a Real-Time Face Detection System Using WebSockets with FastAPI and OpenCV\\n4. Select the webcam you wish to use and click on Start. The video output will show \\nup, face detection will start via the WebSocket and green rectangles will be drawn \\naround the detected faces. We show this in Figure 14.3:\\nFigure 14.3 – Running the face detection web application\\nIt works! We brought the intelligence of our Python system right into the user's web \\nbrowser. This is just an example of what you could achieve using WebSockets and machine \\nlearning algorithms, but this definitely enables you to create near real-time experiences for \\nyour users.\\nSummary\\nIn this chapter, we showed how WebSockets can help us bring a more interactive \\nexperience to users. Thanks to OpenCV , we were able to quickly implement a face \\ndetection system. Then, we integrated it into a WebSocket endpoint with the help of \\nFastAPI. Finally, by using a modern JavaScript API, we sent video input and displayed \\nalgorithm results directly in the browser. All in all, a project like this might sound complex \\nto make at first, but we saw that powerful tools such as FastAPI enable us to get results in a \\nvery short time and with very comprehensible source code.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 409, 'page_label': '391'}, page_content=\"Summary     391\\nThis is the end of the book and our FastAPI journey together. We sincerely hope that you \\nliked it and that you learned a lot along the way. We covered many subjects, sometimes \\njust by scratching the surface, but you should now be ready to build your own projects \\nwith FastAPI and serve up smart data science algorithms. Be sure to check out all the \\nexternal resources we mentioned along the way, as they will give you all the insights you \\nneed for mastery.\\nIn recent years, Python has gained a lot of popularity, especially in the data science \\ncommunity, and FastAPI, even though it's still very young, is already a game-changer and \\nhas seen an unprecedented adoption rate. It'll likely be at the heart of many data science \\nsystems in the coming years... And if you've read this book, you'll probably be one of the \\ndevelopers behind them. Cheers!\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 410, 'page_label': '392'}, page_content=''),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 411, 'page_label': '393'}, page_content='Packt.com\\nSubscribe to our online digital library for full access to over 7,000 books and videos, as \\nwell as industry leading tools to help you plan your personal development and advance \\nyour career. For more information, please visit our website.\\nWhy subscribe?\\n• Spend less time learning and more time coding with practical eBooks and Videos \\nfrom over 4,000 industry professionals\\n• Improve your learning with Skill Plans built especially for you\\n• Get a free eBook or video every month\\n• Fully searchable for easy access to vital information\\n• Copy and paste, print, and bookmark content\\nDid you know that Packt offers eBook versions of every book published, with PDF and \\nePub files available? Y ou can upgrade to the eBook version at packt.com and as a print \\nbook customer, you are entitled to a discount on the eBook copy. Get in touch with us at \\ncustomercare@packtpub.com for more details.\\nAt www.packt.com, you can also read a collection of free technical articles, sign up  \\nfor a range of free newsletters, and receive exclusive discounts and offers on Packt books \\nand eBooks.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 412, 'page_label': '394'}, page_content=\"394     Other Books Y ou May Enjoy\\nOther Books You \\nMay Enjoy\\nIf you enjoyed this book, you may be interested in these other books by Packt:\\nHands-On Machine Learning with scikit-learn and Scientific Python Toolkits\\nTarek Amr\\nISBN: 9781838826048\\n• Understand when to use supervised, unsupervised, or reinforcement learning \\nalgorithms \\n• Find out how to collect and prepare your data for machine learning tasks \\n• Tackle imbalanced data and optimize your algorithm for a bias or variance tradeoff \\n• Apply supervised and unsupervised algorithms to overcome various machine \\nlearning challenges \\n• Employ best practices for tuning your algorithm's hyper parameters \\n• Discover how to use neural networks for classification and regression \\n• Build, evaluate, and deploy your machine learning solutions to production\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 413, 'page_label': '395'}, page_content='Other Books Y ou May Enjoy     395\\nAutomated Machine Learning with AutoKeras\\nLuis Sobrecueva\\nISBN: 9781800567641\\n• Set up a deep learning workstation with TensorFlow and AutoKeras \\n• Automate a machine learning pipeline with AutoKeras \\n• Create and implement image and text classifiers and regressors using AutoKeras \\n• Use AutoKeras to perform sentiment analysis of a text, classifying it as negative  \\nor positive \\n• Leverage AutoKeras to classify documents by topics \\n• Make the most of AutoKeras by using its most powerful extensions'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 414, 'page_label': '396'}, page_content=\"396     \\nPackt is searching for authors like you\\nIf you're interested in becoming an author for Packt, please visit authors.\\npacktpub.com and apply today. We have worked with thousands of developers and \\ntech professionals, just like you, to help them share their insight with the global tech \\ncommunity. Y ou can make a general application, apply for a specific hot topic that we are \\nrecruiting an author for, or submit your own idea.\\nShare Your Thoughts\\nNow you’ve finished Building Data Science Applications with FastAPI, we’ d love to hear \\nyour thoughts! If you purchased the book from Amazon, please click here to go \\nstraight to the Amazon review page for this book and share your feedback \\nor leave a review on the site that you purchased it from.\\nY our review is important to us and the tech community and will help us make sure we’re \\ndelivering excellent quality content.\"),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 415, 'page_label': '397'}, page_content='Index\\nSymbols\\n.env file\\ncreating  297\\nusing  296\\nA\\nAerich\\ndatabase migration system, \\nsetting up  198-200\\naggregating operations\\nreference link  324\\nAlembic\\ndatabase migration system, \\nsetting up  180-185\\nAmazon ECR\\nreference link  308\\nAmazon Elastic Container Service\\nreference link  308\\nAmazon RDS\\nreference link  303\\nAmazon Web Services (AWS)  254\\nAny, typing module  54, 55\\nAPI endpoint\\ncreating  62-65\\nrunning, locally  62-65\\napplication programming \\ninterface (API)  132\\narguments\\naccepting, with *args  30, 31\\naccepting, with **kwargs  30, 31\\narray broadcasting\\nreference link  324\\narrays\\nadding  322-324\\naggregating  324\\ncomparing  325\\ncreating, with NumPy  315-317\\nmanipulating, with NumPy  320-322\\nmultiplying  323, 324\\nasync functions\\nversus standard functions  369-372\\nasynchronous generator  205\\nasynchronous I/O  56-59\\nAsynchronous Server Gateway \\nInterface (ASGI)  57\\nAzure App Service, CLI\\nreference link  301\\nAzure App Service, continuous \\ndeployment/manual Git deployment\\nreference link  302\\nAzure App Service, web interface\\nreference link  301, 302'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 416, 'page_label': '398'}, page_content='398     Index\\nAzure CLI\\ninstallation link  301\\nAzure Database for PostgreSQL\\nreference link  303\\nB\\nbackpressure  381\\nbest parameters\\nfinding, with SVM  356, 357\\nBoolean logic\\nperforming  23\\nbreak statement\\nin Python  29\\nbroadcasting  323\\nbrowser\\nface detection results, \\ndisplaying  387-390\\nbuilt-in types, Python \\nprogramming  17, 18\\nC\\nCallable class\\ntype function signatures with  53, 54\\nCallable object  44\\ncamel case  39\\ncanvas tutorial\\nreference link  385\\ncast, typing module  54, 55\\nC language  314\\nclass\\ndefining  39, 40\\nclassification problems  334\\nclass inheritance\\nused, for creating model \\nvariations  126-128\\nclass methods\\nusing, as dependencies  150-152\\nclustering  335\\ncomputer vision  374\\nconcurrency\\nhandling  247-250\\nConda, package managers  298\\nconditional statements, in Python\\nelif statement  26\\nelse statement  26\\nexecuting  25-27\\nif statement  26\\ncontainers  304\\ncontext manager  273\\ncontinue statement\\nin Python  29\\ncontrol flow statements\\nin Python  25\\ncoroutines  57\\nCORS\\nabout  228-233\\nconfiguring, in FastAPI  228-233\\nprotecting, against CSRF \\nattacks  227, 228\\nCPython  314\\ncross-origin HTTP requests  228\\nCross-Site Request Forgery (CSRF)  228\\ncross-validation  336\\ncryptographic hash functions  216\\nCSRF attacks\\ndouble-submit cookies, implementing \\nto prevent  233-239\\nCSV data\\nexporting  331, 332\\nimporting  331, 332\\ncustom data validation\\nadding, with pydantic  129'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 417, 'page_label': '399'}, page_content='Index   399\\ncustom response\\nabout  106, 107\\nbuilding  102\\nfile, serving  104, 105\\nredirection, making  104\\nresponse_class argument, using  102, 103\\nD\\ndata\\nclassifying, with Gaussian \\nNaive Bayes  347-350\\nclassifying, with multinomial \\nNaive Bayes  350, 351\\nclassifying, with Naive Bayes models  346\\nclassifying, with Support Vector \\nMachines (SVM)  351\\ndatabase\\nmodels and tables, creating  217, 218\\npasswords, hashing  218, 219\\nregistration routes, \\nimplementing  219, 220\\nuser entity, storing  216\\nusing, for testing  278-285\\ndatabase access token\\nendpoints, securing with  225-227\\nimplementing  220, 221\\nlogin endpoint, implementing  222-225\\ndatabase migrations\\nmanaging   303\\ndatabase migration system\\nsetting up, with Aerich  198-200\\nsetting up, with Alembic  180-185\\ndatabase servers\\nadding  303\\ndataset, loading utilities\\nreference link  337\\ndata structures, Python programming\\ndictionaries  21\\nlists  18, 19\\nsets  22\\ntuples  19-21\\nworking with  18\\ndecorator  62\\ndeep copy  320\\ndependencies\\n404 error, raising  147, 148\\nclass methods, using as  150-152\\nobject, retrieving  147, 148\\nusing, in WebSocket endpoints  250-252\\nusing, on path decorator  153\\nusing, on whole application  156, 157\\nusing, on whole router  154, 155\\ndependency injection\\nabout  55, 142\\nadvantages  143\\ndependency return value  145\\ndictionary\\nabout  21\\nobject, converting into  132-134\\nDigitalOcean tutorial\\nreference link  310\\ndimensionality reduction  335\\nDocker\\ndownload and installation link  305\\nFastAPI application, deploying  304\\nDockerfile\\nabout  304\\nwriting, for FastAPI application  304-306\\nDocker image\\nabout  304\\nbuilding  306\\ndeploying  307-309\\nlocally, running  307'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 418, 'page_label': '400'}, page_content='400     Index\\nDocker, run\\nreference link  307\\ndouble submit  236\\ndouble-submit cookies\\nimplementing, to prevent \\nCSRF attacks  233-239\\ndumping  360\\nE\\nemail addresses\\nvalidating, with pydantic types  124-126\\nendpoints\\nsecuring, with access tokens  225-227\\nenvironment variables\\nsetting  292-296\\nusing  292-296\\nestimators\\nabout  337\\nchaining, with pipelines  340-344\\nevent loop  57\\nF\\nface detection\\nresults, displaying in browser  387-390\\nface detection, on single image\\nimplementing, with HTTP \\nendpoint  378-380\\nface detection, on stream of images\\nimplementing, with WebSocket  381-383\\nFastAPI\\nabout  7, 142\\nCORS, configuring in  228-233\\nsecurity dependencies  212-216\\nused, for creating WebSocket  243-247\\nFastAPI application\\ndeploying, on serverless \\nplatform  300-302\\ndeploying, on traditional server  309\\ndeploying, with Docker  304\\nfeatures  334\\nfield level\\nvalidation, applying at  129, 130\\nfilter modifier  226\\nfirst in, first out (FIFO)  382\\nforeign key  163\\nfour-space indentation  17\\nf-strings  16\\nfunction dependency\\ncreating  143-146\\nusing  144-146\\nfunctions\\narguments, accepting with *args  30\\narguments, accepting with **kwargs  30\\ndefining  29, 30\\nG\\nGaussian Naive Bayes\\ndata, classifying with  347-350\\ngcloud CLI\\nreference link  301\\ngenerator\\nfunctions  37\\nin Python  36-39\\ngenerics  50\\nGoogle App Engine, CLI\\nreference link  302\\nGoogle App Engine, configuration file\\nreference link  301, 302\\nGoogle Artifact Registry\\nreference link  308'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 419, 'page_label': '401'}, page_content='Index   401\\nGoogle Cloud Platform (GCP)  254\\nGoogle Cloud Run\\nreference link  308\\nGoogle Cloud SQL\\nreference link  303\\nGunicorn\\nadding, as server process for \\ndeployment  299, 300\\nreference link  309\\nH\\nHeroku\\ninstallation link  301\\nHeroku, CLI\\nreference link  302\\nHeroku, CLI/web interface\\nreference link  302\\nHeroku, configuration file\\nreference link  301\\nHeroku Postgres\\nreference link  303\\nhidden files\\ncreating  297\\nholdout set  335\\nHTTP endpoint\\nimplementing, to perform face \\ndetection on single image  378-380\\nHTTP errors\\nraising  99-102\\nHTTPie command-line utility\\ninstalling  9-11\\nHTTPX\\ntesting tools, setting up for \\nFastAPI  270-273\\nHyperText Markup Language \\n(HTML)  245\\nHyperText Transfer Protocol (HTTP)  124\\nI\\nindexing data\\nreference link  328\\ninheritance, object-oriented programming\\nlogic, reusing with  45, 46\\nmultiple inheritance  46-48\\nrepetition, avoiding with  45, 46\\ninput/output (I/O)  245\\ninstance\\ncreating, from sub-class object  135, 136\\nupdating, with partial one  137, 138\\nInternational Organization for \\nStandardization (ISO)  116\\niterator  26\\nJ\\nJavaScript Object Notation \\n(JSON)  136, 257\\nJoblib\\nresults, caching with  366-369\\ntrained model, persisting with  360\\njoin query  164\\nJSONResponse  102\\nK\\nkernel trick  351\\nkeyword arguments  30\\nL\\nlabel  334\\nLinux server\\nFastAPI application, deploying on  309\\nlist comprehensions\\nin Python  34-36'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 420, 'page_label': '402'}, page_content='402     Index\\nlists  18, 19\\nlogin endpoint\\nimplementing  222-225\\nM\\nmachine learning (ML)\\nabout  334\\nmodel validation  335, 336\\nsupervised, versus unsupervised \\nlearning  334, 335\\nmagic methods, object-oriented \\nprogramming\\n__add__ operator  43\\n__call__ method  44, 45\\n__eq__ method  42, 43\\n__gt__ method  42, 43\\n__lt__ method  42, 43\\n__mul__ operator  43\\n__repr__ method  41, 42\\n__str__ method  41, 42\\n__sub__ operator  43\\nimplementing  41\\nmarker  266\\nmasking  330\\nMediaDevices\\nreference link  384\\nmessage brokers  254\\nmessages\\nbroadcasting  253-259\\nMethod Resolution Order (MRO)  47\\nMicrosoft Azure Container Instances\\nreference link  308\\nMicrosoft Azure Container Registry\\nreference link  308\\nmixins  47\\nmodel\\ncreating, compatible with \\nMongoDB ID  201\\ndefining, with pydantic  114\\ntraining  337-340\\nvalidating, with cross-\\nvalidation  344, 345\\nvalidating, with ML  335, 336\\nmodels, field types\\ndefining, with pydantic  114\\nmodel variations\\ncreating, with class inheritance  126-128\\nMotor, used for communicating \\nwith MongoDB database\\nabout  200\\ndatabase, connecting to  202\\ndocuments, deleting  207, 208\\ndocuments, inserting  203, 204\\ndocuments, nesting  208-210\\ndocuments, retrieving  204-207\\ndocuments, updating  207, 208\\nmodels, creating compatible \\nwith MongoDB ID  201\\nMozilla Developer Network (MDN)\\nreference link  247\\nmulti-dimensional data\\npandas DataFrames, using for  329, 330\\nmultinomial Naive Bayes\\ndata, classifying with  350, 351\\nmultiple inheritance  46-48\\nmultiple WebSocket connections\\nhandling  253-259\\nmypy\\ntype checking  48\\ntype hinting  48'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 421, 'page_label': '403'}, page_content='Index   403\\nN\\nnaïve  347\\nNaive Bayes models\\ndata, classifying with  346\\nintuition  346\\nname collision  109\\nnamespace package  33\\nnewsgroups text dataset\\nreference link  341\\nNoSQL databases\\nabout  162, 164, 165\\nselecting  165\\nNumPy\\nabout  315\\narrays, manipulating with  320-322\\ninstalling  315\\nusing, to create arrays  315-317\\nNumPy arrays\\nconsiderations  320\\nelements, accessing  318\\nNumPy documentation\\nreference link  317\\nNumPy user guide\\nreference link  325\\nO\\nobject\\nconverting, into dictionary  132-134\\nobject level\\nvalidation, applying at  130, 131\\nobject-oriented programming\\nclass, defining  39, 40\\nlogic, reusing with inheritance  45, 46\\nmagic methods, implementing  41\\nrepetition, avoiding with \\ninheritance  45, 46\\nwriting  39\\none-dimensional data\\npandas Series, using for  326-328\\nOpenCV\\nreference link  377\\nusing  374-377\\noperators, Python programming\\nabout  23\\nBoolean logic, performing  23\\nvalue, checking in data structure  24, 25\\nvariables, checking  23, 24\\nP\\npandas  326\\npandas DataFrames\\nusing, for multi-dimensional \\ndata  328-330\\npandas Series\\nusing, for one-dimensional \\ndata  326, 327\\npandas user guide\\nreference link  332\\nparameterized dependency\\ncreating  148\\nusing  149, 150\\nparametrize marker\\nused, for generating tests  265-267\\npath decorator\\ndependencies, using on  153\\npath operation function  62\\npath operation parameters\\nused, for customizing response  88\\npath parameters\\nabout  65-67\\nadvanced validation rules  70-72\\nvalues, limiting  68-70'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 422, 'page_label': '404'}, page_content='404     Index\\nPennState online course\\nreference link  350, 351\\npipelines\\nabout  340\\nestimators, chaining with  340-344\\nPipenv, package managers  298\\nPoetry, package managers  298\\npointer  24\\nPOST endpoints\\ntests, writing for  276, 277\\nprediction\\nrunning, on ML model  337-340\\nprediction endpoint\\nimplementing  363-366\\npreflight requests  231\\npre-processors\\nchaining  340-344\\nprimary key  163\\nprojects\\nstructuring, with multiple \\nrouters  107-111\\npublish-subscribe (pub-sub)  254\\npush\\nin Docker jargon  308\\npydantic\\ncustom data validation, adding with  129\\nmodels, defining with  114\\npydantic data models\\ndefault values  120-122\\ndynamic default values  123\\nfield validation  122\\noptional fields  120-122\\nstandard field types  114-119\\npydantic objects\\nworking with  132\\npydantic parsing\\nvalidation, applying before  131, 132\\npydantic types\\nused, for validating email \\naddresses  124-126\\nused, for validating URLs  124-126\\npyenv\\nreference link  4\\nused, for installing Python \\ndistribution  4-7\\nPyPi\\nURL  7\\npytest\\nunit testing  263-265\\nPython  314\\nPython dependencies\\nmanaging  297-299\\nPython distribution\\ninstalling, with pyenv  4-7\\nPython modules\\nusing  31-34\\nwriting  31-34\\nPython packages\\ninstalling, with pip  8\\nusing  31-34\\nwriting  31-34\\nPython programming\\nabout  14\\nbreak statement  29\\nbuilt-in types  17, 18\\ncontinue statement  29\\ncontrol flow statements  25\\ndata structures  18\\ngenerator  36-39\\nindentation  16, 17\\nlist comprehensions  34-36\\nscripts, running  14, 15\\nwhile loop statement  28\\nPython virtual environment\\ncreating  7, 8'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 423, 'page_label': '405'}, page_content='Index   405\\nQ\\nquery parameters  72-74\\nR\\nRadial Basis Function (RBF)  354\\nRandomizedSearchCV\\nreference link  357\\nRedis\\nabout  255\\nreference link  255\\nregistries  304\\nregression problems  334\\nregular expression (regex)  122\\nrelational databases\\nabout  162-164\\nselecting  165\\nrequest body\\nabout  74-77\\nmultiple objects  77-79\\nrequest parameters\\ncookies  85-87\\nfile uploads  79-84\\nform data  79-81\\nhandling  65\\nheaders  85-87\\npath parameters  65-67\\nquery parameters  72-74\\nrequest body  74-77\\nrequest object  87\\nresponse\\ncustomizing, with path \\noperation parameters  88\\nresponse_class argument\\nusing  102, 103\\nresponse, customizing with path \\noperation parameters\\nresponse model  90-94\\nstatus code  88-90\\nresponse model  90-94\\nresponse parameter\\nabout  95\\ncookies, setting  96, 97\\nheaders, setting  95, 96\\nstatus code, setting dynamically  97-99\\nREST API endpoints\\ntests, writing for  275\\nresults\\ncaching, with Joblib  366-369\\nrouters\\nabout  107\\nproject, structuring with  107-109, 111\\nS\\nsame-origin policy  228\\nscikit-learn\\nbasics  337\\nestimators, chaining with \\npipelines  340-344\\nmodels, training  337-340\\nmodel, validating with cross-\\nvalidation  344, 345\\nprediction, running  337-340\\npre-processors, chaining  340-344\\nSupport Vector Machines \\n(SVM), using  355, 356\\nSecure Sockets Layer/Transport Layer \\nSecurity (SSL/TLS)  243\\nsecurity dependencies\\nin FastAPI  212-216'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 424, 'page_label': '406'}, page_content='406     Index\\nserverless platform\\nFastAPI application, deploying  300-302\\nsets  22\\nsingular body values  78\\nsnake case  30\\nsockets  242\\nSQLAlchemy, used for communicating \\nwith SQL database\\nabout  166, 167\\ndatabase, connecting to  169-171\\ndatabase migration system, setting \\nup with Alembic  180-185\\ndelete queries, making  175-177\\ninsert queries, making  171, 172\\nrelationships, adding  177-180\\nselect queries, making  173-175\\ntable schema, creating  168, 169\\nupdate queries, making  175-177\\nstandard functions\\nversus async functions  369-372\\nStarlette\\nURL  65\\nstatic-type checkers  48\\nstatus code  88-90\\nstop words  342\\nstream of images\\nsending, from browser in \\nWebSocket  383-387\\nsub-arrays\\naccessing  319\\nsub-class object\\ninstance, creating from  135, 136\\nsupervised learning  334\\nSupport Vector Machines (SVM)\\nabout  351\\nbest parameters, finding  356, 357\\ndata, classifying with  351\\nintuition  352-354\\nusing, in scikit-learn  355, 356\\nSupport Vector Machines (SVM), \\nmathematical formulation\\nreference link  354\\nT\\nTerm Frequency-Inverse Document \\nFrequency (TF-IDF)  342\\ntesting\\nwith database  278-285\\ntesting tools\\nsetting up, for FastAPI with \\nHTTPX  270-273\\ntest logic\\nreusing, by creating fixtures  267-270\\ntests\\ngenerating, with parametrize  265-267\\nwriting, for POST endpoints  276, 277\\nwriting, for REST API endpoints  275\\nwriting, for WebSocket \\nendpoints  286-289\\nTF-IDF term weighting\\nreference link  342\\nTortoise ORM, used for communicating \\nwith SQL database \\nabout  186\\ndatabase migration system, setting \\nup with Aerich  198-200\\ndatabase models, creating  186-188\\nobjects, creating  190\\nobjects, deleting  193, 194\\nobjects, filtering  191, 192\\nobjects, retrieving  191, 192\\nobjects, updating  193, 194\\nrelationships, adding  194-197\\nTortoise engine, setting up  188, 189'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.3 (Windows)', 'creationdate': '2021-08-25T21:11:02+05:30', 'author': 'Voron, Francois;', 'ebx_publisher': 'Packt Publishing, Limited', 'moddate': '2021-09-21T08:43:28+03:00', 'title': 'Building Data Science Applications with FastAPI', 'trapped': '/False', 'source': 'data/François Voron - Building Data Science Applications with FastAPI_ Develop, manage, and deploy efficient machine learning applications with Python (2021, Packt Publishing) - libgen.li.pdf', 'total_pages': 426, 'page': 425, 'page_label': '407'}, page_content='Index   407\\ntrained model\\ndumping  360, 361\\nloading  362, 363\\npersisting, with Joblib  360\\ntrain_test_split function\\nreference link  339\\ntuples  19-21\\ntwo-way communication\\nprinciples, with WebSockets  242\\ntype annotations  48\\ntype checking\\nwith mypy  48\\ntype function signatures\\nwith Callable  53, 54\\ntype hinting\\nwith mypy  48\\ntype hinting, in Python\\nworking with  48-50\\ntyping module\\nabout  50-53\\nAny  54, 55\\ncast  54, 55\\nU\\nUniform Resource Identifiers (URIs)  243\\nUniform Resource Locators (URLs)\\nabout  247\\nvalidating, with pydantic types  124-126\\nunit testing\\nwith pytest  263-265\\nunsupervised learning  334\\nuser identifier (UID)  257\\nUvicorn  8\\nUvicorn documentation\\nreference link  299\\nV\\nvalidation\\napplying, at field level  129, 130\\napplying, at object level  130, 131\\napplying, before pydantic \\nparsing  131, 132\\nvalidators  129\\nW\\nWeb Server Gateway Interface (WSGI)  56\\nWebSocket\\nabout  242\\ncreating, with FastAPI  243-247\\nimplementing, to perform face detection \\non stream of images  381-383\\nstream of images, sending \\nfrom browser  383-387\\ntwo-way communication, principles  242\\nWebSocket endpoints\\ntests, writing for  286-289\\nwhile loop statement\\nin Python  28\\nwhitespace indentation  16\\nWindows Subsystem for Linux (WSL)  245\\nWindows terminal application\\nreference link  245\\nwss (WebSocket Secure)  243'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 0, 'page_label': '1'}, page_content=''),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 1, 'page_label': '2'}, page_content='Generative AI Foundations in Python\\nCopyright © 2024 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval\\nsystem, or transmitted in any form or by any means, without the prior written\\npermission of the publisher, except in the case of brief quotations embedded in critical\\narticles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of\\nthe information presented. However, the information contained in this book is sold\\nwithout warranty, either express or implied. Neither the author, nor Packt Publishing or\\nits dealers and distributors, will be held liable for any damages caused or alleged to\\nhave been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the\\ncompanies and products mentioned in this book by the appropriate use of capitals.\\nHowever, Packt Publishing cannot guarantee the accuracy of this information.\\nGroup Product Manager: Niranjan Naikwadi\\nPublishing Product Manager: Tejashwini R\\nBook Project Manager: Hemangi Lotlikar\\nSenior Editor: Shrishti Pandey\\nTechnical Editor: Rahul Limbachiya\\nCopy Editor: Safis Editing\\nProofreader: Shrishti Pandey\\nIndexer: Manju Arasan\\nProduction Designer: Alishon Mendonca\\nSenior DevRel Marketing Coordinator: Vinishka Kalra\\nFirst published: July 2024\\nProduction reference: 1020724\\nPublished by Packt Publishing Ltd.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 2, 'page_label': '3'}, page_content='Grosvenor House\\n11 St Paul’s Square\\nBirmingham\\nB3 1RB, UK\\nISBN 978-1-83546-082-5\\nwww.packtpub.com'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 3, 'page_label': '4'}, page_content='To the memory of my late friend, Austin Tribble, for exemplifying resilience and\\ndetermination. To my wife, Jill Rodriguez, whose brilliance and intellectual\\ncuriosity have inspired me every day since the day we met.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 4, 'page_label': '5'}, page_content='– Carlos Rodriguez\\nForeword\\nLarge Language Models (LLMs) are poised to transform the way we interact with\\ntechnology, offering unprecedented capabilities in understanding and generating\\nhuman language. They have become essential tools in numerous applications, from\\nchatbots and virtual assistants to content creation and translation services. For a\\nsubject that is extremely dynamic and complex, Carlos has managed to distill years of\\nexpertise into a work that is both accessible and comprehensive. This book not only\\ndemystifies the complexities of LLMs but also provides a comprehensive guide for\\npractitioners and enthusiasts alike. So, it is with great pride and excitement that I pen\\nthis foreword for my good friend and esteemed colleague, Carlos Rodriguez, whose\\nwork on LLMs delves into the intricacies of model architecture, training\\nmethodologies, and practical implementations, all while maintaining a clarity that\\nensures readers, regardless of their background, can grasp the fundamental principles\\nand potential applications of LLMs. Our journey together began only two short years\\nago; however, we found ourselves to be kindred spirits in the ever-evolving world of\\nAI. From the outset, I was struck by Carlos’ insatiable curiosity and unyielding\\ndedication to the field of AI. Over numerous discussions and collaborative projects, I\\nhave witnessed firsthand the depth of his knowledge, the rigor of his research, and the\\npassion that fuels his relentless pursuit of innovation. What sets Generative AI\\nFoundations in Python apart is Carlos’ unique ability to blend technical depth with\\npractical insights. Each chapter is a testament to his meticulous approach and his\\ncommitment to bridging the gap between theoretical concepts and real-world\\nsolutions. Interweaving real-world examples, code snippets, and practical\\nconsiderations ensures that seasoned professionals or newcomers to the field will find\\nthis book to be an invaluable resource. In closing, I invite you to embark on this\\njourney with an open mind and a passion for learning. The landscape of LLMs is vast;\\nthere is no better guide than the one you hold in your hands. May this book inspire,\\neducate, and ignite a passion for learning and discovery in every reader. Enjoy the\\njourney.\\n– Samira Shaikh, PhD.\\nVP of Data Science, Artificial Intelligence, and Advanced Analytics, Popular Bank\\nAssociate Professor of Computer Science, UNC Charlotte\\nContributors'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 5, 'page_label': '6'}, page_content='About the author\\nCarlos Rodriguez is the Director of AI risk at a major financial institution, where he\\noversees the validation of cutting-edge AI and machine learning models, including\\ngenerative AI, to ensure that they remain trustworthy, unbiased, and compliant with\\nstringent regulatory standards. With a degree in data science, numerous professional\\ncertifications, and two decades of experience in emerging technology, Carlos is a\\nrecognized expert in natural language processing and machine learning. Throughout\\nhis career, he has fostered and led high-performing machine learning engineering and\\ndata science teams specializing in natural language processing and AI risk,\\nrespectively. Known for his human-centered approach to AI, Carlos is a passionate\\nautodidact who continuously expands his knowledge as a data scientist, machine\\nlearning practitioner, and risk executive. His current focus lies in developing a\\ncomprehensive framework for evaluating generative AI models within a regulatory\\nsetting, aiming to set new industry standards for responsible AI adoption and\\ndeployment.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 6, 'page_label': '7'}, page_content='I want to express my gratitude to everyone who supported me throughout this\\nprocess, with special thanks to my wife, Jill, for her unwavering support. I also\\nwant to extend a thank you to my parents, particularly my mother, a lifelong\\neducator who has always encouraged me to find every opportunity to teach\\nothers. Finally, a special thanks to Morgan, Eric, Jeremy, Rose, and Samira, who\\nso graciously took the time to review the manuscript at various stages.\\nAbout the reviewers\\nMorgan Boyce’s education includes bachelor’s degrees in economics and finance, a\\nmaster’s degree in mathematical finance, and a PhD in economics. He has worked in\\nthe financial services industry for nearly 20 years in various roles such as economic\\nand quantitative research, model development, analytics, and model validation.\\nOutside the financial services industry, Morgan’s research focuses on the economics of\\ntechnological innovation as well as public entrepreneurship. He also teaches various\\neconomics courses at university level.\\nEric Rui is a distinguished technology and data leader in the financial services\\nindustry, renowned for his expertise in Data and AI. With a career dedicated to driving\\ninnovation and efficiency, Eric leverages cutting-edge technologies and data-driven\\ninsights to transform organizational processes. His strategic vision and technical\\nacumen make him a key influencer, excelling in creating robust solutions that enhance\\ndata utilization and analytical capabilities. Additionally, Eric has deep knowledge of\\npractical generative AI, applying advanced machine learning techniques to drive\\nbusiness growth and optimize decision-making.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 7, 'page_label': '8'}, page_content='Table of Contents\\nPreface'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 8, 'page_label': '9'}, page_content='Part 1: Foundations of Generative AI and the\\nEvolution of Large Language Models\\n1\\nUnderstanding Generative AI: An Introduction\\nGenerative AI\\nDistinguishing generative AI from other AI models\\nBriefly surveying generative approaches\\nClarifying misconceptions between discriminative and\\ngenerative paradigms\\nChoosing the right paradigm\\nLooking back at the evolution of generative AI\\nOverview of traditional methods in NLP\\nArrival and evolution of transformer-based models\\nDevelopment and impact of GPT-4\\nLooking ahead at risks and implications\\nIntroducing use cases of generative AI\\nThe future of generative AI applications'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 9, 'page_label': '10'}, page_content='Summary\\nReferences\\n2\\nSurveying GenAI Types and Modes: An Overview of\\nGANs, Diffusers, and Transformers\\nUnderstanding General Artificial Intelligence (GAI)\\nTypes – distinguishing features of GANs, diffusers,\\nand transformers\\nDeconstructing GAI methods – exploring GANs,\\ndiffusers, and transformers\\nA closer look at GANs\\nA closer look at diffusion models\\nA closer look at generative transformers\\nApplying GAI models – image generation using GANs,\\ndiffusers, and transformers\\nWorking with Jupyter Notebook and Google Colab\\nStable diffusion transformer\\nScoring with the CLIP model\\nSummary'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 10, 'page_label': '11'}, page_content='References\\n3\\nTracing the Foundations of Natural Language\\nProcessing and the Impact of the Transformer\\nEarly approaches in NLP\\nAdvent of neural language models\\nDistributed representations\\nTransfer Learning\\nAdvent of NNs in NLP\\nThe emergence of the Transformer in advanced\\nlanguage models\\nComponents of the transformer architecture\\nSequence-to-sequence learning\\nEvolving language models – the AR Transformer and\\nits role in GenAI\\nImplementing the original Transformer\\nData loading and preparation\\nTokenization'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 11, 'page_label': '12'}, page_content='Data tensorization\\nDataset creation\\nEmbeddings layer\\nPositional encoding\\nMulti-head self-attention\\nFFN\\nEncoder layer\\nEncoder\\nDecoder layer\\nDecoder\\nComplete transformer\\nTraining function\\nTranslation function\\nMain execution\\nSummary\\nReferences\\n4'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 12, 'page_label': '13'}, page_content='Applying Pretrained Generative Models: From\\nPrototype to Production\\nPrototyping environments\\nTransitioning to production\\nMapping features to production setup\\nSetting up a production-ready environment\\nLocal development setup\\nVisual Studio Code\\nProject initialization\\nDocker setup\\nRequirements file\\nApplication code\\nCreating a code repository\\nCI/CD setup\\nModel selection – choosing the right pretrained\\ngenerative model\\nMeeting project objectives\\nModel size and computational complexity'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 13, 'page_label': '14'}, page_content='Benchmarking\\nUpdating the prototyping environment\\nGPU configuration\\nLoading pretrained models with LangChain\\nSetting up testing data\\nQuantitative metrics evaluation\\nAlignment with CLIP\\nInterpreting outcomes\\nResponsible AI considerations\\nAddressing and mitigating biases\\nTransparency and explainability\\nFinal deployment\\nTesting and monitoring\\nMaintenance and reliability\\nSummary'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 14, 'page_label': '15'}, page_content='Part 2: Practical Applications of Generative AI\\n5\\nFine-Tuning Generative Models for Specific Tasks\\nFoundation and relevance – an introduction to fine-\\ntuning\\nPEFT\\nLoRA\\nAdaLoRA\\nIn-context learning\\nFine-tuning versus in-context learning\\nPractice project: Fine-tuning for Q&A using PEFT\\nBackground regarding question-answering fine-tuning\\nImplementation in Python\\nEvaluation of results\\nSummary\\nReferences'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 15, 'page_label': '16'}, page_content='6\\nUnderstanding Domain Adaptation for Large\\nLanguage Models\\nDemystifying domain adaptation – understanding its\\nhistory and importance\\nPractice project: Transfer learning for the finance\\ndomain\\nTraining methodologies for financial domain\\nadaptation\\nEvaluation and outcome analysis – the ROUGE metric\\nSummary\\nReferences\\n7\\nMastering the Fundamentals of Prompt Engineering\\nThe shift to prompt-based approaches\\nBasic prompting – guiding principles, types, and\\nstructures\\nGuiding principles for model interaction\\nPrompt elements and structure'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 16, 'page_label': '17'}, page_content='Elevating prompts – iteration and influencing model\\nbehaviors\\nLLMs respond to emotional cues\\nEffect of personas\\nSituational prompting or role-play\\nAdvanced prompting in action – few-shot learning and\\nprompt chaining\\nPractice project: Implementing RAG with LlamaIndex\\nusing Python\\nSummary\\nReferences\\n8\\nAddressing Ethical Considerations and Charting a\\nPath Toward Trustworthy Generative AI\\nEthical norms and values in the context of generative\\nAI\\nInvestigating and minimizing bias in generative LLMs\\nand generative image models\\nConstrained generation and eliciting trustworthy\\noutcomes\\nConstrained generation with fine-tuning'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 17, 'page_label': '18'}, page_content='Constrained generation through prompt engineering\\nUnderstanding jailbreaking and harmful behaviors\\nPractice project: Minimizing harmful behaviors with\\nfiltering\\nSummary\\nReferences\\nIndex\\nOther Books You May Enjoy'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 18, 'page_label': '19'}, page_content='Preface\\nWelcome to Generative AI Foundations in Python: Discover key techniques and\\nnavigate modern challenges in LLMs. This book offers an accessible introduction to\\ngenerative AI and large language models (LLMs), guiding the reader from core\\nprinciples to practical applications. It aims to present a balanced approach, offering\\ntheory and hands-on examples, providing a strong foundation for those seeking to\\nunderstand and leverage generative AI in their respective disciplines and fields.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 19, 'page_label': '20'}, page_content='Who this book is for\\nWritten for data scientists, machine learning engineers, IT professionals, educators,\\nand students with a basic grasp of machine learning and Python, the book meets the\\nreaders where they are, enabling them to engage fully with the content and build their\\nfoundational knowledge of generative AI concepts.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 20, 'page_label': '21'}, page_content='What this book covers\\nChapter 1, Understanding Generative AI: An Introduction, lays the conceptual\\ngroundwork, broadening the reader’s fundamental understanding of what this\\ntechnology does, how it was derived, and how it can be used. It establishes how\\ngenerative models differ from classical machine learning paradigms and elucidates\\nhow they discern complex relationships and idiosyncrasies in data to synthesize\\nhuman-like text, audio, and video.\\nChapter 2, Surveying GenAI Types and Modes: An Overview of GANs, Diffusers, and\\nTransformers, explores the theoretical foundations and real-world applications of these\\ntechniques in greater depth. It dissects the architectural innovations and enhancements\\nthat improved training stability and output quality over time, bringing us to state-of-\\nthe-art LLMs.\\nChapter 3, Tracing the Foundations of Natural Language Processing and the Impact\\nof the Transformer, covers the evolution of natural language processing (NLP) that\\nultimately led to the advent of the Transformer architecture. It introduces the\\nTransformer—its basis in deep learning, its self-attention architecture, and its rapid\\nevolution, which has led to the generative AI phenomenon.\\nChapter 4, Applying Pretrained Generative Models: From Prototype to Production,\\noutlines the process of transitioning a generative AI prototype to a production-ready\\ndeployment. It walks through setting up a robust Python environment using Docker,\\nGitHub, and CI/CD pipelines, then presents considerations for selecting and deploying\\na suitable pre-trained model for the project at hand, emphasizing computational\\nconsiderations, proper evaluation, monitoring, and responsible AI practices.\\nChapter 5, Fine-Tuning Generative Models for Specific Tasks, examines how\\nParameter-Efficient Fine-Tuning (PEFT) facilitates approachable continued training\\nfor specific tasks such as question-answering. It explores and defines a range of\\nscalable fine-tuning techniques, comparing them with other approaches such as in-\\ncontext learning.\\nChapter 6, Understanding Domain Adaptation for Large Language Models, introduces\\ndomain adaptation, a unique fine-tuning approach that equips models to interpret\\nlanguage unique to specific industries or domains, addressing the gap in LLMs’\\nunderstanding of specialized language.\\nChapter 7, Mastering the Fundamentals of Prompt Engineering, explores prompting\\ntechniques to examine how to adapt a general-purpose LLM without fine-tuning. It\\nexplores various prompting strategies that leverage the model’s inherent capabilities to'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 21, 'page_label': '22'}, page_content='produce targeted and contextually relevant outputs. It explores a simple approach to\\nRAG and provides techniques to understand and measure performance.\\nChapter 8, Addressing Ethical Considerations and Charting a Path Toward\\nTrustworthy Generative AI, recognizes the increasing prominence of generative AI and\\nexplores the ethical considerations that should guide its progress. It outlines key\\nconcepts such as transparency, fairness, accountability, respect for privacy, informed\\nconsent, security, and inclusivity, which are essential for the responsible development\\nand use of these technologies.\\nTo get the most out of this book\\nReaders should have a foundational understanding of Python programming and a basic\\ngrasp of machine learning concepts. Familiarity with deep learning frameworks such\\nas TensorFlow or PyTorch will be beneficial but not essential. The book assumes an\\nintermediate level of Python proficiency, enabling readers to focus on the generative\\nAI concepts and applications covered throughout the chapters.\\nSoftware/hardware covered in the\\nbook\\nOperating system requirements\\nPython 3 GPU-enabled Windows, macOS, or\\nLinux\\nThe book’s coding examples are designed to be compatible with Python 3 and run on\\nWindows, macOS, or Linux operating systems. To fully engage with the hands-on\\ntutorials and examples, access to a GPU is recommended, as many generative AI\\nmodels are computationally intensive. The book provides guidance on setting up a\\nsuitable development environment, including instructions for installing necessary\\nlibraries and dependencies.\\nIf you are using the digital version of this book, we advise you to type the code\\nyourself or access the code from the book’s GitHub repository (a link is available\\nin the next section). Doing so will help you avoid any potential errors related to\\nthe copying and pasting of code.\\nThroughout the book, readers are encouraged to actively experiment with the code\\nsamples provided and adapt them to their own projects. The companion GitHub\\nrepository serves as a valuable resource, offering more complete and modular versions\\nof the code examples presented in the chapters. Accessing and working with this code'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 22, 'page_label': '23'}, page_content='will enhance the reader’s learning experience and help solidify their understanding of\\nthe concepts covered.\\nDownload the example code files\\nYou can download the example code files for this book from GitHub at\\nhttps://github.com/PacktPublishing/Generative-AI-Foundations-in-Python. Any code\\nupdates will be provided in the GitHub repository. Please feel free to open issues on\\nthis repository should any arise.\\nWe also have other code bundles from our rich catalog of books and videos available\\nat https://github.com/PacktPublishing/. Check them out!\\nConventions used\\nThere are a number of text conventions used throughout this book.\\nCode in text: Indicates code words in text, database table names, folder names,\\nfilenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.\\nHere is an example: “Each entry in the dataset needs to be tokenized and structured\\nwith the necessary fields such as input_ids and attention_mask.”\\nA block of code is set as follows:\\n \\n# Get the start and end positions \\nanswer_start_scores = outputs.start_logits \\nanswer_end_scores = outputs.end_logits\\nBold: Indicates a new term, an important word, or words that you see onscreen. For\\ninstance, words in menus or dialog boxes appear in bold. Here is an example: “Click\\nthe + icon in the top-right corner of the GitHub home page and select New\\nrepository.”\\nTIPS OR IMPORTANT NOTES\\nAppear like this.\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback: If you have questions about any aspect of this book, email us at\\ncustomercare@packtpub.com and mention the book title in the subject of your\\nmessage.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 23, 'page_label': '24'}, page_content='Errata: Although we have taken every care to ensure the accuracy of our content,\\nmistakes do happen. If you have found a mistake in this book, we would be grateful if\\nyou would report this to us. Please visit www.packtpub.com/support/errata and fill in\\nthe form.\\nPiracy: If you come across any illegal copies of our works in any form on the internet,\\nwe would be grateful if you would provide us with the location address or website\\nname. Please contact us at copyright@packt.com with a link to the material.\\nIf you are interested in becoming an author: If there is a topic that you have\\nexpertise in and you are interested in either writing or contributing to a book, please\\nvisit authors.packtpub.com.\\nShare Your Thoughts\\nOnce you’ve read Generative AI Foundations in Python, we’d love to hear your\\nthoughts! Please click here to go straight to the Amazon review page for this book and\\nshare your feedback.\\nYour review is important to us and the tech community and will help us make sure\\nwe’re delivering excellent quality content.\\nDownload a free PDF copy of this book\\nThanks for purchasing this book!\\nDo you like to read on the go but are unable to carry your print books everywhere?\\nIs your eBook purchase not compatible with the device of your choice?\\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that\\nbook at no cost.\\nRead anywhere, any place, on any device. Search, copy, and paste code from your\\nfavorite technical books directly into your application.\\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and\\ngreat free content in your inbox daily\\nFollow these simple steps to get the benefits:\\n1. Scan the QR code or visit the link below'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 24, 'page_label': '25'}, page_content='https://packt.link/free-ebook/9781835460825\\n2. Submit your proof of purchase\\n3. That’s it! We’ll send your free PDF and other benefits to your email directly'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 25, 'page_label': '26'}, page_content='Part 1: Foundations of Generative AI and the\\nEvolution of Large Language Models\\nThis part provides an overview of generative AI and the role of large language models.\\nIt covers the basics of generative AI, different types of generative models, including\\nGANs, diffusers, and transformers, and the foundational aspects of natural language\\nprocessing. Additionally, it explores how pretrained generative models can be applied\\nfrom prototype to production, setting the stage for more advanced topics.\\nThis part contains the following chapters:\\nChapter 1, Understanding Generative AI: An Introduction\\nChapter 2, Surveying GenAI Types and Modes: An Overview of GANs, Diffusers, and\\nTransformers\\nChapter 3, Tracing the Foundations of Natural Language Processing and the Impact of the\\nTransformer\\nChapter 4, Applying Pretrained Generative Models: From Prototype to Production'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 26, 'page_label': '27'}, page_content='1 \\nUnderstanding Generative AI: An Introduction\\nIn his influential book The Singularity Is Near (2005), renowned inventor and futurist\\nRay Kurzweil asserted that we were on the precipice of an exponential acceleration in\\ntechnological advancements. He envisioned a future where technological innovation\\nwould continue to accelerate, eventually leading to a singularity—a point where\\nartificial intelligence (AI) could transcend human intelligence, blurring the lines\\nbetween humans and machines. Fast-forward to today and we find ourselves\\nadvancing along the trajectory Kurzweil outlined, with generative AI marking a\\nsignificant stride along this path. Today, we are experiencing state-of-the-art\\ngenerative models can behave as collaborators capable of synthetic understanding and\\ngenerating sophisticated responses that mirror human intelligence.. The rapid and\\nexponential growth of generative approaches is propelling Kurzweil’s vision forward,\\nfundamentally reshaping how we interact with technology.\\nIn this chapter, we lay the conceptual groundwork for anyone hoping to apply\\ngenerative AI to their work, research, or field of study, broadening a fundamental\\nunderstanding of what this technology does, how it was derived, and how it can be\\nused. It establishes how generative models differ from classical machine learning\\n(ML) paradigms and elucidates how they discern complex relationships and\\nidiosyncrasies in data to synthesize human-like text, audio, and video. We will explore\\ncritical foundational generative methods, such as generative adversarial networks\\n(GANs), diffusion models, and transformers, with a particular emphasis on their real-\\nworld applications.\\nAdditionally, this chapter hopes to dispel some common misunderstandings\\nsurrounding generative AI and provides guidelines to adopt this emerging technology\\nethically, considering its environmental footprint and advocating for responsible\\ndevelopment and adoption. We will also highlight scenarios where generative models\\nare apt for addressing business challenges. By the conclusion of this chapter, we will\\nbetter understand the potential of generative AI and its applications across a wide array\\nof sectors and have critically assessed the risks, limitations, and long-term\\nconsiderations.\\nWhether your interest is casual, you are a professional transitioning from a different\\nfield, or you are an established practitioner in the fields of data science or ML, this\\nchapter offers a contextual understanding to make informed decisions regarding the\\nresponsible adoption of generative AI.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 27, 'page_label': '28'}, page_content='Ultimately, we aim to establish a foundation through an introductory exploration of\\ngenerative AI and large language models (LLMs), dissected into two parts.\\nThe beginning of the book will introduce the fundamentals and history of generative\\nAI, surveying various types, such as GANs, diffusers, and transformers, tracing the\\nfoundations of natural language generation (NLG), and demonstrating the basic\\nsteps to implement generative models from prototype to production. Moving forward,\\nwe will focus on slightly more advanced application fundamentals, including fine-\\ntuning generative models, prompt engineering, and addressing ethical considerations\\ntoward the responsible adoption of generative AI. Let’s get started.\\nGenerative AI\\nIn recent decades, AI has made incredible strides. The origins of the field stem from\\nclassical statistical models meticulously designed to help us analyze and make sense of\\ndata. As we developed more robust computational methods to process and store data,\\nthe field shifted—intersecting computer science and statistics and giving us ML. ML\\nsystems could learn complex relationships and surface latent insights from vast\\namounts of data, transforming our approach to statistical modeling.\\nThis shift laid the groundwork for the rise of deep learning, a substantial step forward\\nthat introduced multi-layered neural networks (i.e., a system of interconnected\\nfunctions) to model complex patterns. Deep learning enabled powerful discriminative\\nmodels that became pivotal for advancements in diverse fields of research, including\\nimage recognition, voice recognition, and natural language processing.\\nHowever, the journey continues with the emergence of generative AI. Generative AI\\nharnesses the power of deep learning to accomplish a broader objective. Instead of\\nclassifying and discriminating data, generative AI seeks to learn and replicate data\\ndistributions to “create” entirely new and seemingly original data, mirroring human-\\nlike output.\\nDistinguishing generative AI from other AI\\nmodels\\nAgain, the critical distinction between discriminative and generative models lies in\\ntheir objectives. Discriminative models aim to predict target outputs given input data.\\nClassification algorithms, such as logistic regression or support vector machines, find\\ndecision boundaries in data to categorize inputs as belonging to one or more class.\\nNeural networks learn input-output mappings by optimizing weights through\\nbackpropagation (or tracing back to resolve errors) to make accurate predictions.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 28, 'page_label': '29'}, page_content='Advanced gradient boosting models, such as XGBoost or LightGBM, further enhance\\nthese discriminative models by employing decision trees and incorporating the\\nprinciples of gradient boosting (or the strategic ensembling of models) to make highly\\naccurate predictions.\\nGenerative methods learn complex relationships through expansive training in order to\\ngenerate new data sequences enabling many downstream applications. Effectively,\\nthese models create synthetic outputs by replicating the statistical patterns and\\nproperties discovered in training data, capturing nuances and idiosyncrasies that\\nclosely reflect human behaviors.\\nIn practice, a discriminative image classifier labels images containing a cat or a dog. In\\ncontrast, a generative model can synthesize diverse, realistic cat or dog images by\\nlearning the distributions of pixels and implicit features from existing images.\\nMoreover, generative models can be trained across modalities to unlock new\\npossibilities in synthesis-focused applications to generate human-like photographs,\\nvideos, music, and text.\\nThere are several key methods that have formed the foundation for many of the recent\\nadvancements in Generative AI, each with unique approaches and strengths. In the\\nnext section, we survey generative advancements over time, including adversarial\\nnetworks, variational autoencoders, diffusion models, and autoregressive transformers,\\nto better understand their impact and influence.\\nBriefly surveying generative approaches\\nModern generative modeling encompasses diverse architectures suited to different data\\ntypes and distinct tasks. Here, we briefly introduce some of the key approaches that\\nhave emerged over the years, bringing us to the state-of-the-art models:\\nGenerative adversarial networks (GANs) involve two interconnected neural networks—one\\nacting as a generator to create realistic synthetic data and the other acting as a discriminator\\nthat distinguishes between real and synthetic (fake) data points. The generator and\\ndiscriminator are adversaries in a zero-sum game, each fighting to outperform the other. This\\nadversarial relationship gradually improves the generator’s capacity to produce vividly realistic\\nsynthetic data, making GANs adept at creating intricate image distributions and achieving\\nphoto-realistic image synthesis.\\nVariational autoencoders (VAEs) employ a unique learning method to compress data into a\\nsimpler form (or latent representation). This process involves an encoder and a decoder that\\nwork conjointly (Kingma & Welling, 2013). While VAEs may not be the top choice for image\\nquality, they are unmatched in efficiently separating and understanding complex data patterns.\\nDiffusion models continuously add Gaussian noise to data over multiple steps to corrupt it.\\nGaussian noise can be thought of as random variations applied to a signal to distort it, creating'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 29, 'page_label': '30'}, page_content='“noise”. Diffusion models are trained to eliminate the added noise to recover the original data\\ndistribution. This type of reverse engineering process equips diffusion models to generate\\ndiverse, high-quality samples that closely replicate the original data distribution, producing\\ndiverse high-fidelity images (Ho et al., 2020).\\nAutoregressive transformers leverage parallelizable self-attention to model complex\\nsequential dependencies, showing exceptional performance in language-related tasks (Vaswani\\net al., 2017). Pretrained models such as GPT-4 or Claude have demonstrated the capability for\\ngeneralizations in natural language tasks and impressive human-like text generation. Despite\\nethical issues and misuse concerns, transformers have emerged as the frontrunners in language\\nmodeling and multimodal generation.\\nCollectively, these methodologies paved the way for advanced generative modeling\\nacross a wide array of domains, including images, videos, audio, and text. While\\narchitectural and engineering innovations progress daily, generative methods showcase\\nunparalleled synthesis capabilities across diverse modalities. Throughout the book, we\\nwill explore and apply generative methods to simulate real-world scenarios. However,\\nbefore diving in, we further distinguish generative methods from traditional ML\\nmethods by addressing some common misconceptions.\\nClarifying misconceptions between discriminative\\nand generative paradigms\\nTo better understand the distinctive capabilities and applications of traditional ML\\nmodels (often referred to as discriminative) and generative methods, here, we clear up\\nsome common misconceptions and myths:\\nMyth 1: Generative models cannot recognize patterns as effectively as discriminative\\nmodels.\\nTruth: State-of-the-art generative models are well-known for their impressive abilities\\nto recognize and trace patterns, rivaling some discriminative models. Despite primarily\\nfocusing on creative synthesis, generative models display classification capabilities.\\nHowever, the classes output from a generative model can be difficult to explain as\\ngenerative models are not explicitly trained to learn decision boundaries or\\npredetermined relationships. Instead, they may only learn to simulate classification\\nbased on labels learned implicitly (or organically) during training. In short, in cases\\nwhere the explanation of model outcomes is important, classification using a\\ndiscriminative model may be the better choice.\\nExample: Consider GPT-4. In addition to synthesizing human-like text, it can\\nunderstand context, capture long-range dependencies, and detect patterns in texts.\\nGPT-4 uses these intrinsic language processing capabilities to discriminate between\\nclasses, such as traditional classifiers. However, because GPT learns semantic'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 30, 'page_label': '31'}, page_content='relationships through extensive training, explaining its decision-making cannot be\\naccomplished using any established methods.\\nMyth 2: Generative AI will eventually replace discriminative AI.\\nTruth: This is a common misunderstanding. Discriminative models have consistently\\nbeen the option for high-stakes prediction tasks because they focus directly on learning\\nthe decision boundary between classes, ensuring high precision and reliability. More\\nimportantly, discriminative models can be explained post-hoc, making them the\\nultimate choice for critical applications in sectors such as healthcare, finance, and\\nsecurity. However, generative models may increasingly become more popular for high-\\nstakes modeling as explainability techniques emerge.\\nExample: Consider a discriminative model trained specifically for disease prediction\\nin healthcare. A specialized model can classify data points (e.g., images of skin) as\\nhealthy or unhealthy, giving healthcare professionals a tool for early intervention and\\ntreatment plans. Post-hoc explanation methods, such as SHAP, can be employed to\\nidentify and analyze the key features that influence classification outcomes. This\\napproach offers clear insights into the specific results (i.e., feature attribution).\\nMyth 3: Generative models continuously learn from user input.\\nTruth: Not exactly. Generative LLMs are trained using a static approach. This means\\nthey learn from a vast training data corpora, and their knowledge is limited to the\\ninformation contained within that training window. While models can be augmented\\nwith additional data or in-context information to help them contextualize, giving the\\nimpression of real-time learning, the underlying model itself is essentially frozen and\\ndoes not learn in real time.\\nExample: GPT-3 was trained in 2020 and only contained information up to that date\\nuntil its successor GPT-3.5, released in March of 2023. Naturally, GPT-4 was trained\\non more recent data, but due to training limitations (including diminishing\\nperformance returns), it is reasonable to expect that subsequent training checkpoints\\nwill be released periodically and not continuously.\\nWhile generative and discriminative models have distinct strengths and limitations,\\nknowing when to apply each paradigm requires evaluating several key factors. As we\\nhave clarified some common myths about their capabilities, let’s turn our attention to\\nguidelines for selecting the right approach for a given task or problem.\\nChoosing the right paradigm\\nThe choice between generative and discriminative models depends on various factors,\\nsuch as the task or problem at hand, the quality and quantity of data available, the'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 31, 'page_label': '32'}, page_content='desired output, and the level of performance required. The following is a list of key\\nconsiderations:\\nTask specificity: Discriminative models are more suitable for high-stakes applications, such as\\ndisease diagnosis, fraud detection, or credit risk assessment, where precision is crucial.\\nHowever, generative models are more adept at creative tasks such as synthesizing images, text,\\nmusic, or video.\\nData availability: Discriminative models tend to overfit (or memorize examples) when trained\\non small datasets, which may lead to poor generalization. On the other hand, because\\ngenerative models are often pretrained on vast amounts of data, they can produce a diverse\\noutput even with minimal input, making them a viable choice when data are scarce.\\nModel performance: Discriminative models outperform generative models in tasks where it is\\ncrucial to learn and explain a decision boundary between classes or where expected\\nrelationships in the data are well understood. Generative models usually excel in less\\nconstrained tasks that require a measure of perceived creativity and flexibility.\\nModel explainability: While both paradigms can include models that are considered “black\\nboxes” or not intrinsically interpretable, generative models can be more difficult, or at times,\\nimpossible to explain, as they often involve complex data generation processes that rely on\\nunderstanding the underlying data distribution. Alternatively, discriminative models often\\nfocus on learning the boundary between classes. In use cases where model explainability is a\\nkey requirement, discriminative models may be more suitable. However, generative\\nexplainability research is gaining traction.\\nModel complexity: Generally, discriminative models require less computational power\\nbecause they learn to directly predict some output given a well-defined set of inputs.\\nAlternatively, generative models may consume more computational resources, as\\ntheir training objective is to jointly capture the intricate hidden relationships\\nbetween both inputs and presumed outputs. Accurately learning these intricacies\\nrequires vast amounts of data and large computations. Computational efficiency in\\ngenerative LLM training (e.g., quantization) is a vibrant area of research.\\nUltimately, the choice between generative and discriminative models should be made\\nby considering the trade-offs involved. Moreover, the adoption of these paradigms\\nrequires different levels of infrastructure, data curation, and other prerequisites.\\nOccasionally, a hybrid approach that combines the strengths of both models can serve\\nas an ideal solution. For example, a pretrained generative model can be fine-tuned as a\\nclassifier. We will learn about task-specific fine-tuning in Chapter 5.\\nNow that we have explored the key distinctions between traditional ML (i.e.,\\ndiscriminative) and generative paradigms, including their distinct risks, we can look\\nback at how we arrived at this paradigm shift. In the next section, we take a brief look\\nat the evolution of generative AI.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 32, 'page_label': '33'}, page_content='Looking back at the evolution of generative AI\\nThe field of generative AI has experienced an unprecedented acceleration, leading to a\\nsurge in the development and adoption of foundation models such as GPT. However,\\nthis momentum has been building for several decades, driven by continuous and\\nsignificant advancements in ML and natural language generation research. These\\ndevelopments have brought us to the current generation of state-of-the-art models.\\nTo fully appreciate the current state of generative AI, it is important to understand its\\nevolution, beginning with traditional language processing techniques and moving\\nthrough to more recent advancements.\\nOverview of traditional methods in NLP\\nNatural language processing (NLP) technology has enabled machines to understand,\\ninterpret, and generate human language. It emerged from traditional statistical\\ntechniques such as n-grams and hidden Markov models (HMMs), which converted\\nlinguistic structures into mathematical models that machines could understand.\\nInitially, n-grams and HMMs were the primary methods used in NLP. N-grams\\npredicted the next word in a sequence based on the last “n” words, while HMMs\\nmodeled sequences by considering every word as a state in a Markov process. These\\nearly methods were good at capturing local patterns and short-range dependencies in\\nlanguage.\\nAs computational power and data availability grew, more sophisticated techniques for\\nnatural language processing emerged. Among these was the recurrent neural\\nnetwork (RNN), which managed relationships across extended sequences and was\\nproven to be effective in tasks where prior context influenced future predictions.\\nSubsequently, long short-term memory networks (LSTMs) were developed.\\nUnlike traditional RNNs, LSTMs had a unique ability to retain relevant long-term\\ninformation while disregarding irrelevant data, maintaining semantic relationships\\nacross prolonged sequences.\\nFurther advancements led to the introduction of sequence-to-sequence models, often\\nutilizing LSTMs as their underlying structure. These models revolutionized fields such\\nas machine translation and text summarization by dramatically improving efficiency\\nand effectiveness.\\nOverall, NLP evolved from traditional statistical methods to advanced neural\\nnetworks, transforming how we interacted with machines and enabling countless\\napplications, such as machine translation and information retrieval (IR) (or finding'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 33, 'page_label': '34'}, page_content='relevant text based on a query). As the NLP field matured, incorporating the strengths\\nof traditional statistical methods and advanced neural networks, a renaissance was\\nforming. The next generation of NLP advancements would introduce transformer\\narchitectures, starting with the seminal paper Attention is All You Need and later the\\nrelease of models such as BERT and eventually GPT.\\nArrival and evolution of transformer-based models\\nThe release of the research paper titled Attention is All You Need in 2017 served as a\\nparadigm shift in natural language processing. This pivotal paper introduced the\\ntransformer model, an architectural innovation that provided an unprecedented\\napproach to sequential language tasks such as translation. The transformer model\\ncontrasted with prior models that processed sequences serially. Instead, it\\nsimultaneously processed different segments of an input sequence, determining its\\nrelevance based on the task. This innovative processing addressed the complexity of\\nlong-range dependencies in sequences, enabling the model to draw out the critical\\nsemantic information needed for a task. The transformer was such a critical\\nadvancement that nearly every state-of-the-art generative LLM applies some\\nderivation of the original architecture. Its importance and influence motivate our\\ndetailed exploration and implementation of the original transformer in Chapter 3.\\nWith the transformer came significant advancements in natural language processing,\\nincluding GPT-1 or Generative Pretrained Transformer 1 (Radford et al., 2018). GPT-1\\nintroduced a novel directional architecture to tackle diverse NLP tasks.\\nCoinciding with GPT-1 was BERT, or bidirectional encoder representations from\\ntransformers, a pioneering work in the family of transformer-based models. BERT\\nstood out among its predecessors, analyzing sentences forward and backward (or bi-\\ndirectionally). This bidirectional analysis allowed BERT to capture semantic and\\nsyntactic nuances more effectively. At the time, BERT achieved unprecedented results\\nwhen applied to complex natural language tasks such as named entity recognition,\\nquestion answering, and sentiment analysis (Devlin et al., 2018).\\nLater, GPT-2, the much larger successor to GPT-1, attracted immense attention, as it\\ngreatly outperformed any of its predecessors across various tasks. In fact, GPT-2 was\\nso unprecedented in its ability to generate human-like output that concerns about\\npotential implications led to a delay in its initial release (Hern, 2019).\\nAmid early concerns, OpenAI followed up with the development of GPT-3, signaling a\\nleap in the potential of LLMs. Developers demonstrated the potential of training at a\\nmassive scale, reaching 175 billion parameters (or adjustable variables learned during\\ntraining), surpassing its two predecessors. GPT-3 was a “general-purpose” learner,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 34, 'page_label': '35'}, page_content='capable of performing a wide range of natural language tasks learned implicitly from\\nits training corpus instead of through task-specific fine-tuning. This capability sparked\\nthe exploration of foundation model development for general use across various\\ndomains and tasks. GPT-3’s distinct design and unprecedented scale led to a generation\\nof generative models that could perform an indefinite number of increasingly complex\\ndownstream tasks learned implicitly through its extensive training.\\nDevelopment and impact of GPT-4\\nOpenAI’s development of GPT-4 marked a significant advance in the potential of\\nlarge-scale, multimodal models. GPT-4, capable of processing image and text inputs\\nand producing text outputs, represented yet another giant leap ahead of predecessors.\\nGPT-4 exhibited human-level performance on various professional and academic\\nbenchmarks. For instance, it passed a simulated bar exam with a score falling into the\\ntop 10% of test-takers (OpenAI, 2023).\\nA key distinction of GPT-4 is what happens after pretraining. Open AI applied\\nreinforcement learning with human feedback (RLHF)—a type of risk/reward\\ntraining derived from the same technique used to teach autonomous vehicles to make\\ndecisions based on the environment they encounter. In the case of GPT-4, the model\\nlearned to respond appropriately to a myriad of scenarios, incorporating human\\nfeedback along the way. This novel refinement strategy drastically improved the\\nmodel’s propensity for factuality and its adherence to desired behaviors. The\\nintegration of RLHF demonstrated how models could be better aligned with human\\njudgment toward the goal of responsible AI.\\nHowever, despite demonstrating groundbreaking abilities, GPT-4 had similar\\nlimitations to earlier GPT models. It was not entirely reliable and had a limited context\\nwindow (or input size). Meaning it could not receive large texts or documents as input.\\nIt was also prone to hallucination. As discussed, Hallucination is an\\nanthropomorphized way of describing the model’s tendency to generate content that is\\nnot grounded in fact or reality. A hallucination occurs because generative language\\nmodels (without augmentation) synthesize content purely based on semantic context\\nand don’t perform any logical processing to verify factuality. This weakness presented\\nmeaningful risks, particularly in contexts where fact-based outcomes are paramount.\\nDespite limitations, GPT-4 made significant strides in language model performance.\\nAs with prior models, GPT-4’s development and potential use underscored the\\nimportance of safety and ethical considerations for future AI applications. As a result,\\nthe rise of GPT-4 accentuated the ongoing discussions and research into the potential'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 35, 'page_label': '36'}, page_content='implications of deploying such powerful models. In the next section, we briefly survey\\nsome of the known risks that are unique to generative AI.\\nLooking ahead at risks and implications\\nBoth generative and discriminative AI introduce unique risks and benefits that must be\\nweighed carefully. However, generative methods can not only carry forward but also\\nexacerbate many risks associated with traditional ML while also introducing new risks.\\nConsequently, before we can adopt generative AI in the real world and at scale, it is\\nessential to understand the risks and establish responsible governance principles to\\nhelp mitigate them:\\nHallucination: This is a term widely used to describe when models generate factually\\ninaccurate information. Generative models are adept at producing plausible-sounding output\\nwithout basis in fact. As such, it is critical to ground generative models with factual\\ninformation. The term “grounding” refers to appending model inputs with additional\\ninformation that is known to be factual. We explore grounding techniques in Chapter 7.\\nAdditionally, it is essential to have a strategy for evaluating model outputs that includes human\\nreview.\\nPlagiarism: Since generative models are sometimes trained on uncrated datasets, some\\ntraining corpora may have included data without explicit permissions. Models may produce\\ninformation that is subject to copyright protections or can be claimed as intellectual property.\\nAccidental memorization: As with many ML models that train on immense corpora,\\ngenerative models tend to memorize parts of the training data. In particular, they are prone to\\nmemorizing sparse examples that do not fit neatly into a broader pattern. In some cases,\\nmodels could memorize sensitive information that can be extracted and exposed (Brundage et\\nal., 2020; Carlini et al., 2020). Consequently, whether consuming a pretrained model or fine-\\ntuning (i.e., continued model training), training data curation is essential.\\nToxicity and bias: Another byproduct of large-scale model training is that the model will\\ninevitably learn any societal biases embedded in the training data. Biases can manifest as\\ngender, racial, or socioeconomic biases in generated text or images, often replicating or\\namplifying stereotypes. We detail mitigations for this risk in Chapter 8.\\nWith an understanding of some of the risks, we turn our focus to the nuanced\\nimplications of adopting generative AI:\\nEthical: As discussed, these models inevitably learn and reproduce the biases inherent in the\\ntraining data, raising serious ethical questions. Similarly, concerns about data privacy and\\nsecurity have emerged due to the model’s susceptibility to memorizing and exposing its\\ntraining data. This has led to calls for robust ethical guidelines and data privacy regulations\\n(Gebru et al., 2018).\\nEnvironmental: LLMs are computational giants, demanding unprecedented resources for\\ntraining and implementation. Thus, they inevitably present environmental impacts. The energy'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 36, 'page_label': '37'}, page_content='consumption required to train an LLM produces substantial carbon dioxide emissions—\\nroughly the equivalent lifetime emissions of five vehicles. Consequently, multiple efforts are\\nunderway to increase model efficiency and reduce carbon footprints. For example, techniques\\nsuch as reduced bit precision training (or quantization) and parameter efficient fine-tuning\\n(discussed in Chapter 5) reduce overall training time, helping to shrink carbon footprints.\\nSocial: Along with environmental impacts, LLMs also have social implications. As these\\nmodels become proficient at generating text, simulating intelligent conversation, and\\nautomating fundamental tasks, they present an unparalleled opportunity for job automation.\\nDue to various complex factors, this potential for large-scale automation in the US may\\ndisproportionately affect marginalized or underrepresented communities. Thus, this amplifies\\nprior concerns regarding labor rights and the need for additional protections to minimize harm.\\nBusiness and labor: Along with broader socio-economic implications, we must examine more\\ndirect impacts on the business sector. While generative AI opens up new opportunities,\\nchanges in the labor market could bring about immense disruption if not addressed responsibly.\\nBeyond labor impacts, AI advancements also significantly affect various business sectors.\\nThey can result in the creation of new roles, business models, and opportunities, requiring\\nongoing governance strategy and explorative frameworks that center on inclusivity, ethics, and\\nresponsible adoption.\\nAddressing these challenges will require technical and scientific improvements, data-\\nspecific regulations and laws, ethical guidelines, and human-centered AI governance\\nstrategies. These are integral to building an equitable, secure, and inclusive AI-driven\\nfuture.\\nHaving discussed the history, risks, and limitations of generative AI, we are now better\\nequipped to explore the vast opportunities and applications of such transformative\\ntechnology.\\nIntroducing use cases of generative AI\\nGenerative AI has already begun to disrupt various sectors. The technology is making\\nwaves across many disciplines, from enhancing language-based tasks to reshaping\\ndigital art. The following section offers examples of real-world applications of\\ngenerative AI across different sectors:\\nTraditional natural language processing: LLMs, such as Open AI’s GPT series, have\\nelevated traditional NLP and NLG. As discussed, these models have a unique ability to\\ngenerate coherent, relevant, and human-like text. The potential of these models was\\ndemonstrated when GPT-3 outperformed classical and modern approaches in several language\\ntasks, displaying an unprecedented understanding of human language. The release of GPT-4\\nand Claude 3 marked another milestone, raising the standard even further for state-of-the-art\\nmodels.\\nDigital art creation: The advent of “generative art” is evidence of the radical impact of\\ngenerative AI in the field of digital art. For instance, artists can use AI generative models to'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 37, 'page_label': '38'}, page_content='create intricate designs, allowing them to focus on the conceptual aspect of art. It simplifies the\\nprocess, reducing the need for high-level technical acumen.\\nMusic creation: In the music industry, generative AI can enhance the composition process.\\nSeveral platforms offer high-quality AI-driven music creation tools that can generate long-\\nform musical compositions combining different music styles across various eras and genres.\\nStreamlining business processes: Several businesses have started employing generative AI to\\nenable faster and more efficient processes. Generative AI-enabled operational efficiencies\\nallow employees to focus on more strategic tasks. For example, fully integrated LLM email\\nclients can organize emails and (combined with other technologies) learn to prioritize critical\\nemails over time.\\nEntertainment: While still largely experimental, LLMs show promising potential to disrupt\\ncreative writing and storytelling, particularly in the gaming industry. For example, procedural\\ngames could apply LLMs to enhance dynamic storytelling and create more engaging,\\npersonalized user experiences. As technology advances, we may see more mainstream\\nadoption of LLMs in gaming, opening up new possibilities for interactive narratives.\\nFashion: In the fashion industry, generative models help designers innovate. By using a state-\\nof-the-art generative AI model, designers can create and visualize new clothing styles by\\nsimply tweaking a few configurations.\\nArchitecture and construction: In the architectural world, generative-enhanced tools can help\\narchitects and urban planners optimize and generate design solutions, leading to more efficient\\nand sustainable architectural designs.\\nFood industry: Emerging AI-driven cooking assistants can generate unique food\\ncombinations, novel recipes, and modified recipes for highly specific dietary needs.\\nEducation: Generative AI-enhanced educational platforms offer the automatic creation of\\nstudy aids that can facilitate personalized learning experiences and can automatically generate\\ntailored content to accommodate specific and diverse learning styles.\\nHowever, we must balance the breadth of opportunities with sophisticated guardrails\\nand the continued promotion of ethical use. As data scientists, policymakers, and\\nindustry leaders, we must continue to work towards fostering an environment\\nconducive to responsible AI deployment. That said, as generative AI continues to\\nevolve, it presents a future replete with novel innovations and applications.\\nThe future of generative AI applications\\nThe relentless advancement of generative AI presents a future filled with both\\npossibilities and complex challenges. Imagine a future where a generative model\\ntrained on the world’s leading climate change research can offer practical yet\\ngroundbreaking counteractive strategies with precise details about their application.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 38, 'page_label': '39'}, page_content='However, as we embrace an increasingly AI-centered future, we should not overlook\\nthe existing challenges. These involve the potential misuse of AI tools, unpredictable\\nimplications, and the profound ethical considerations underlying AI adoption.\\nAdditionally, sustainable and eco-conscious development is key, as training large-scale\\nmodels can be resource-intensive\\nIn an age of accelerated progress, collaboration across all stakeholders—from data\\nscientists, AI enthusiasts, and policymakers to industry leaders—is essential. By being\\nequipped with comprehensive oversight, robust guidelines, and strategic education\\ninitiatives, concerted efforts can safeguard a future where generative AI is ubiquitous.\\nDespite these hurdles, the transformative potential of generative AI remains\\nunquestionable. With its capacity to reshape industries, redefine societal\\ninfrastructures, and alter our ways of living, learning, and working, generative AI\\nserves as a reminder that we are experiencing a pivotal moment—one propelled by\\ndecades of scientific research and computational ingenuity that are coalescing to bring\\nus forward as a society.\\nSummary\\nIn this chapter, we traced the evolution of generative AI, distinguished it from\\ntraditional ML, explored its evolution, discussed its risks and implications, and,\\nhopefully, dispelled some common misconceptions. We contemplated some of the\\npossibilities anchored by consideration for its responsible adoption.\\nAs we move on to the next chapter, we will examine the fundamental architectures\\nbehind generative AI, giving us a foundational understanding of the key generative\\nmethods, including GANs, diffusion models, and transformers. These ML methods\\nform the backbone of generative AI and have been instrumental in bringing about the\\nremarkable advancements we see today.\\nReferences\\nThis reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nhttps://doi.org/10.1007/s11023-020-09526-\\n7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-\\nconvincing-news-fiction\\nBrundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P.,\\nZeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen, G. C., Steinhardt, J., Flynn, C., Ó'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 39, 'page_label': '40'}, page_content='hÉigeartaigh, S., Beard, S., Belfield, H., Farquhar, S., & Amodei, D. (2018). The malicious use\\nof artificial intelligence: Forecasting, prevention, and mitigation. arXiv [cs.AI].\\nhttp://arxiv.org/abs/1802.07228.\\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A.,\\nBrown, T., Song, D., Erlingsson, U., Oprea, A., & Raffel, C. (2020). Extracting training data\\nfrom large language models. arXiv [cs.CR]. http://arxiv.org/abs/2012.07805.\\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep\\nbidirectional transformers for language understanding. arXiv [cs.CL].\\nhttp://arxiv.org/abs/1810.04805.\\nHagendorff, T. (2020). Publisher correction to The ethics of AI ethics: An evaluation of\\nguidelines. Minds and Machines, 30(3), 457–461. https://doi.org/10.1007/s11023-020-09526-7.\\nHern, A. (2019, February 14). New AI fake text generator may be too dangerous to release, say\\ncreators. The Guardian. https://www.theguardian.com/technology/2019/feb/14/elon-musk-\\nbacked-ai-writes-convincing-news-fiction.\\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. arXiv [cs.LG].\\nhttp://arxiv.org/abs/2006.11239.\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S.,\\nKingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv [stat.ML].\\nhttp://arxiv.org/abs/1312.6114.\\nMuhammad, T., Aftab, A. B., Ahsan, M. M., Muhu, M. M., Ibrahim, M., Khan, S. I., & Alam,\\nM. S. (2022). Transformer-based deep learning model for stock price prediction: A case study\\non Bangladesh stock market. arXiv [q-fin.ST]. http://arxiv.org/abs/2208.08300.\\nOpenAI. (2023). GPT-4 technical report. arXiv [cs.CL]. http://arxiv.org/abs/2303.08774.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018). Language models\\nare unsupervised multitask learners.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &\\nPolosukhin, I. (2017). Attention Is All You Need. arXiv [cs.CL].\\nhttp://arxiv.org/abs/1706.03762.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 40, 'page_label': '41'}, page_content='2 \\nSurveying GenAI Types and Modes: An\\nOverview of GANs, Diffusers, and Transformers\\nIn the previous chapter, we established the key distinction between generative and\\ndiscriminative models. Discriminative models focus on predicting outputs by learning\\np(output∣ input), or the conditional probability of some expected output given an\\ninput or set of inputs. In contrast, generative models, such as Generative Pretrained\\nTransformer (GPT), generate text by predicting the next token (a partial word, whole\\nword, or punctuation) using p(next token∣ previous tokens), based on the\\nprobabilities of possible continuations given the current context. Tokens are\\nrepresented as vectors containing embeddings that capture latent features and rich\\nsemantic dependencies learned through extensive training.\\nWe briefly surveyed leading generative approaches, including Generative\\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion\\nmodels, and autoregressive transformers. Each methodology possesses unique\\nstrengths suitable for different data types and tasks. For example, GANs are adept at\\ngenerating high-fidelity photographic images through an adversarial process. Diffusion\\nmodels take a probabilistic approach, iteratively adding and removing noise from data\\nto learn robust generative representations. Autoregressive transformers leverage self-\\nattention and massive scale to achieve remarkable controlled text generation.\\nIn this chapter, we will explore the theoretical foundations and real-world applications\\nof these techniques in greater depth. We will make direct comparisons, elucidating\\narchitectural innovations and enhancements that improve training stability and output\\nquality over time. Through practical examples, we will see how researchers have\\nadapted these models to produce art, music, videos, stories, and so on.\\nTo enable an unbiased comparison, we will concentrate primarily on image synthesis\\ntasks. GANs and diffusion models are specifically architected for image data,\\nharnessing advances in convolutional processing and computer vision. Transformers,\\npowered by self-attention, excel at language modeling but can also generate images.\\nThis will allow us to benchmark performance on a common task.\\nBy the end of this chapter, we will have implemented state-of-the-art image generation\\nmodels and explored how these core methods enhance and complement each other.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 41, 'page_label': '42'}, page_content='Understanding General Artificial Intelligence\\n(GAI) Types – distinguishing features of GANs,\\ndiffusers, and transformers\\nThe often-stunning human-like quality we experience from GAI can be attributed to\\ndeep-generative machine learning advances. In particular, three fundamental methods\\nhave inspired many derivative innovations – GANs, diffusion models, and\\ntransformers. Each has its distinct strengths and is particularly well-suited for specific\\napplications.\\nWe briefly described GANs, a groundbreaking approach that exploits the adversarial\\ninterplay between two competing neural networks – a generator and a discriminator –\\nto generate hyper-realistic synthetic data. Over time, GANs have seen substantial\\nadvancements, achieving greater control in data generation, higher image fidelity, and\\nenhanced training stability. For instance, NVIDIA’s StyleGAN has created highly\\ndetailed and realistic human faces. The adversarial training process of GANs, where\\none network generates data and the other evaluates it, allows you to create highly\\nrefined and detailed synthetic images, enhancing realism with each training iteration.\\nThe synthetic images generated can be utilized in a plethora of domains. In the\\nentertainment industry, they can be used to create realistic characters for video games\\nor films. In research, they provide a means to augment datasets, especially in scenarios\\nwhere real data is scarce or sensitive. Moreover, in computer vision, these synthetic\\nimages aid in training and fine-tuning other machine-learning models, advancing\\napplications like facial recognition.\\nDiffusion models, an innovative generative modeling alternative, explicitly address\\nsome GAN limitations. As discussed briefly in Chapter 1, diffusion models adopt a\\nunique approach to introducing and systematically removing noise, enabling high-\\nquality image synthesis with less training complexity. In medical imaging, diffusion\\nmodels can significantly enhance image clarity by generating high-resolution synthetic\\nexamples to train other machine-learning models. Introducing and then iteratively\\nremoving noise can help reconstruct high-fidelity images from lower-quality inputs,\\nwhich is invaluable in scenarios where obtaining high-resolution medical images is\\nchallenging.\\nSimultaneously, generative transformers, initially designed for language modeling,\\nhave been adopted for multimodal synthesis. Today, transformers are not confined to\\nlanguage and have permeated into audio, images, and video applications. For instance,\\nOpenAI’s GPT-4 excels in processing and generating text, while DALL-E creates\\nimages from textual descriptions, a perfect example of the interplay between methods.\\nWhen integrated, GPT-4 and DALL-E form a robust multimodal system. GPT-4\\nprocesses and understands textual instructions, while DALL-E takes the interpreted'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 42, 'page_label': '43'}, page_content='instructions to generate corresponding visual representations. A practical application\\nof this combination could be automated digital advertisement creation. For example,\\ngiven textual descriptions of a product and the desired aesthetic, GPT-4 could interpret\\nthese instructions, and DALL-E could generate visually compelling advertisements\\naccordingly.\\nDeconstructing GAI methods – exploring GANs,\\ndiffusers, and transformers\\nLet’s deconstruct these core approaches to understand their distinct characteristics and\\nillustrate their transformative role in advancing generative machine learning. As GAI\\ncontinues to move forward, it’s crucial to understand how these approaches drive\\ninnovation.\\nA closer look at GANs\\nGANs, introduced by Goodfellow et al. in 2014, primarily consist of two neural\\nnetworks – the Generator (G) and the Discriminator (D). G aims to create synthetic\\ndata resembling real data, while D strives to distinguish real from synthetic data.\\nIn this setup, the following occurs:\\n1. G receives input from a “latent space,” a high-dimensional space representing structured\\nrandomness. This structured randomness serves as a seed to generate synthetic data,\\ntransforming it into meaningful information.\\n2. D evaluates the generated data, attempting to differentiate between real (or reference) and\\nsynthetic data.\\nIn short, the process begins with G deriving random noise from the latent space to\\ncreate data. This synthetic data, along with real data, is supplied to D, which then\\ntries to discern between the two. Feedback from D informs the parameters of G to\\nrefine its data generation process. The adversarial interaction continues until an\\nequilibrium is reached.\\n3. Equilibrium in GANs occurs when D can no longer differentiate between real and synthetic\\ndata, assigning an equal probability of 0.5 to both. Arriving at this state signals that the\\nsynthetic data produced by G is indistinguishable from real data, which is the core objective of\\nthe synthesis process.\\nUltimately, the success of GANs has had meaningful implications for various sectors.\\nIn the automotive industry, GANs have been used to simulate real-world scenarios for\\nautonomous vehicle testing. In the entertainment sector, GANs are deployed to'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 43, 'page_label': '44'}, page_content='generate digital characters and realistic environments for filmmaking and game design.\\nIn the art world, GANs can literally craft new words. Moreover, the development of\\nGANs has continued to move forward over the years with significant improvements in\\nquality, control, and overall performance.\\nAdvancement of GANs\\nSince its inception, GAN technology has evolved significantly with several notable\\nadvancements:\\nConditional GANs (cGANs): Introduced by Mirza and Osindero in 2014, conditional GANs\\nincorporated specific conditions during data generation, enabling more controlled outputs.\\ncGANs have been used in tasks such as image-to-image translation (e.g., converting photos\\ninto paintings).\\nDeep Convolutional GANs (DCGANs): In 2015, Radford et al. enhanced GANs by\\nintegrating convolutional layers, which help to analyze image data in small, overlapping\\nregions to capture fine granularity, substantially improving the visual quality of the synthetic\\noutput. DCGANs can generate realistic images for applications such as fashion design, where\\nthe model evolves new designs from existing trends.\\nWasserstein GANs (WGANs): Introduced by Arjovsky et al. in 2017, Wasserstein GANs\\napplied the Wasserstein distance metric to GANs’ objective function, facilitating a more\\naccurate measurement of differences between real and synthetic data. Specifically, the metric\\nhelps you find the most efficient way to make the generated data distribution resemble the real\\ndata distribution. This small adjustment leads to a more stable learning process, minimizing\\nvolatility during training. WGANs have helped generate realistic medical imagery to aid in\\ntraining diagnostic AI algorithms, improving a model’s ability to generalize from synthetic to\\nactual data.\\nFollowing the advent of Wasserstein GANs, the landscape experienced a surge of\\ninventive expansions, each tailor-made to address specific challenges or open new\\navenues in synthetic data generation:\\nProgressively growing GANs incrementally increase the resolution during training, starting\\nwith lower-resolution images and gradually moving to higher resolution. This approach allows\\nthe model to learn coarse-to-fine details effectively, making training more manageable and\\ngenerating high-quality images (Karras et al. 2017). These high-resolution images can enhance\\nthe realism and immersion of virtual reality environments.\\nCycleGANs facilitates image-to-image translations, bridging domain adaptation tasks (Zhu et\\nal., 2017). For example, a CycleGAN could transform a summer scene into a winter scene\\nwithout requiring example pairs (e.g., summer-winter) during training. CycleGANs have been\\nused to simulate weather conditions in autonomous vehicle testing, evaluating system\\nperformance under varying environmental conditions.\\nBigGANs push the boundaries in high-resolution image generation, showcasing the versatility\\nof GANs in complex generation tasks. They achieve this by scaling up the size of the model'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 44, 'page_label': '45'}, page_content='(more layers and units per layer) and the batch size during training, alongside other\\narchitectural and training innovations (Brock et al., 2018). BigGANs have been used to\\ngenerate realistic textures for video games, enhancing gaming environments’ realism.\\nThese developments significantly broadened what GANs could achieve, ranging from\\nhigh-resolution image synthesis to domain adaptation and cross-modal generation\\ntasks. However, despite these incredible advancements, GANs have suffered from\\nsome continual limitations, which inspired alternative approaches such as diffusion.\\nLimitations and challenges of GANs\\nThe training process of GANs requires a careful balance between the G and D\\nnetworks. It requires substantial computational resources, often demanding powerful\\nGPUs and enormous datasets to achieve desirable outcomes. Moreover, there are\\ncomplexities in training GANs that arise from challenges such as vanishing gradients\\nand mode collapse. While the vanishing gradient problem is a problem broadly\\naffecting deep neural networks, mode collapse is a challenge that is particularly unique\\nto the training of GANs. Let’s explore these a bit further:\\nVanishing gradients: This issue arises during the neural network training phase when the\\ngradient of the loss function diminishes to a point where the learning either drastically slows or\\nhalts. The crux of GANs lies in the delicate balance of learning between the G and D models.\\nDisproportionate learning can hinder the overall training process. In practical terms, the issue\\nof vanishing gradients can lead to longer training times and increased computational costs,\\nwhich might render GANs impractical for time-sensitive or resource-constrained applications.\\nMode collapse: Inherent to GANs, mode collapse occurs when the G starts producing a narrow\\nvariety of samples, thereby stifling output diversity and undermining a network’s effectiveness.\\nTechniques such as a gradient penalty and spectral normalization have alleviated these issues.\\nThis phenomenon can significantly degrade the quality of generated data, limiting the use of\\nGANs in applications that require diverse outputs, such as data augmentation for machine\\nlearning or generating diverse design alternatives in creative industries.\\nOf course, GANs carry the same ethical considerations as any state-of-the-art\\ngenerative synthesis. For instance, they can be used to create deepfakes or generate\\nbiased outputs that reinforce societal prejudices. For example, when GANs, often used\\nto generate synthetic data (e.g., faces), underrepresent certain groups, downstream\\napplications may exhibit gender or racial bias (Kenfack et al., 2021).\\nEven with the advent of other generative models such as diffusion models and\\nTransformer-based image generators, GANs have played a seminal role in shaping the\\ntrajectory of generative image synthesis, showcasing both the potential and some of\\nthe challenges inherent in this domain.\\nNow that we better understand GANs in the context of deep generative models, let’s\\nshift our focus to a successor in image generation, the diffusion model.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 45, 'page_label': '46'}, page_content='A closer look at diffusion models\\nHaving explored the dynamics of GANs, let’s transition our attention to a subsequent\\ninnovation in image generation – the diffusion model. Initially proposed by Sohl-\\nDickstein et al. in 2015, diffusion models present a novel approach, where a neural\\nnetwork iteratively introduces and subsequently removes noise from data to generate\\nhighly refined images. Unlike GANs, which leverage an adversarial mechanism\\ninvolving two contrasting models, diffusion models apply a more gradual, iterative\\nprocess of noise manipulation within the data.\\nIn practical terms, GANs have shown substantial merit in art and design, creating\\nrealistic faces or generating sharp, high-fidelity images from descriptions. They are\\nalso used in data augmentation, expanding datasets by generating realistic synthetic\\ndata to augment the training of machine learning models.\\nConversely, diffusion models excel in tasks requiring a structured approach to image\\ngeneration, such as in medical imaging. Their iterative process can enhance the quality\\nof medical images, such as MRI or CT scans, where noise reduction and clarity are\\nparamount. This makes diffusion models invaluable in clinical settings, aiding in better\\ndiagnostics and analysis. Moreover, their controlled and gradual process offers a more\\npredictable or stable training process compared to the adversarial and dynamic training\\nof GANs.\\nThe foundation of diffusion models is anchored in two primary processes:\\nA forward diffusion process: This process begins with clean data (x₀) and iteratively\\nintroduces Gaussian noise, akin to progressively applying a fog-like filter, transforming the\\ndata into indistinguishable noise (xₜ ).\\nA learned reverse model: Following the forward diffusion, the “reverse model” (pθ) attempts\\nto eliminate (or de-fog) the noise from the noisy data (xₜ ), aiming to revert to the original clean\\nstate (xₜ ₋ ₁). Specifically, this reversion is orchestrated by estimating the probability of\\ntransitioning from the noisy state back to the clear state, using a conditional distribution\\ndenoted as pθ(xₜ ₋ ₁|xₜ ). A conditional distribution tells us the likelihood of one event\\nhappening when we know another related event has occurred. In this case, the reversion\\nestimates the likelihood of reverting to the original state, given some amount of noise.\\nIn the pivotal work Score-Based Generative Modeling through Stochastic Differential\\nEquations, the authors propose a novel framework that unifies score-based generative\\nmodels and diffusion probabilistic modeling by employing Stochastic Differential\\nEquations (SDEs). This framework involves the transformation of data distributions\\nto a known prior distribution through the gradual addition and then removal of noise,\\nguided by SDEs. Optimizing the reverse-time SDE – dependent only on the score of\\nthe perturbed data distribution – allows you to generate new samples. Stochastic'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 46, 'page_label': '47'}, page_content='Gradient Descent (SGD) is then applied to fine-tune the model parameters until\\narriving at an improved pθ.\\nThe reverse model (pθ) was implemented using convolutional networks to predict\\nvariations in the Gaussian noise distribution – a critical component of the noise-\\nintroduction process within the forward diffusion. Initially, the efficacy of this\\napproach was validated on more straightforward datasets. However, the methodology’s\\napplicability was later significantly improved to handle more complex images (Ho et\\nal., 2020). This expansion demonstrated the practical potential of diffusion models in\\ngenerating highly refined images across a broader spectrum of complexities.\\nAdvancement of diffusion models\\nSince its inception, diffusion model technology has witnessed key advancements,\\npropelling its capabilities in image generation:\\nSimplified training objectives: Ho et al. proposed simplified training objectives that predict\\nGaussian noise directly, eliminating the need for conditional means and facilitating the\\napplication to more complex datasets (Ho et al., 2020). This advancement facilitated handling\\nmore complex datasets, potentially aiding in tasks such as anomaly detection or complex data\\nsynthesis, which could be resource-intensive with traditional models.\\nUNet modules with self-attention: Ho et al. also incorporated UNet modules with self-\\nattention into the diffusion model architecture, inspired by PixelCNN++ by Salimans et al.\\n(2017), enhancing a model’s performance on complex datasets (Ho et al., 2020). Again,\\nenhancing performance on complex datasets facilitates better image restoration, which is\\nparticularly beneficial in fields such as medical imaging or satellite imagery analysis, where\\nhigh-fidelity image reconstruction is crucial.\\nSynchronization with SDEs: Song et al. defined diffusion models as solutions to SDEs,\\nlinking score learning with denoising score-matching losses and expanding model usage for\\nimage generation, editing, in-painting, and colorization (Song et al., 2020).\\nFollowing these foundational advancements, diffusion models witnessed a wave of\\ninnovative enhancements as researchers introduced novel methodologies to address\\nexisting challenges and broaden a model’s applicability in generative modeling tasks.\\nThese advancements include the following:\\nNoise conditioning and annealing strategies: Song et al. improved score-based models by\\nincluding noise conditioning and annealing strategies, achieving performance comparable to\\nGANs on benchmark datasets like the Flickr-Faces-HQ dataset (Song et al., 2021), which is a\\nhigh-quality image dataset of human faces designed to measure GAN performance. Achieving\\nperformance comparable to GANs could make diffusion models a viable alternative for high-\\nfidelity image generation tasks in areas where GANs are traditionally used.\\nLatent Diffusion Models (LDMs): Rombach et al. addressed computational inefficiency by\\nproposing LDMs, which operate in a compressed latent space learned by autoencoders,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 47, 'page_label': '48'}, page_content='employing perceptual losses to create a visually equivalent, reduced latent space (Rombach et\\nal., 2021). By addressing computational inefficiency, LDMs could expedite the image\\ngeneration process, making them suitable for real-time applications or scenarios where\\ncomputational resources are limited.\\nClassifier-free guidance: Ho & Salimans introduced classifier-free guidance for controlled\\ngeneration without relying on pre-trained networks, marking a step toward more flexible\\ngeneration techniques (Ho & Salimans, 2022). This advancement led to more flexible\\ngeneration techniques, enabling more controlled and customized image generation in\\napplications such as design, advertising, or content creation without relying on pre-trained\\nnetworks.\\nSubsequent explorations in the diffusion model domain extended its applications,\\nshowcasing versatility:\\nVideo generation: Ho et al. adapted diffusion models for video generation, demonstrating\\ntheir utility beyond static image generation (Ho et al., 2022)\\n3D data processing: Luo & Hu extended the application to 3D data processing, showcasing\\nthe flexibility of diffusion models (Luo & Hu, 2021)\\nThe evolution of diffusion models has led to enhanced image generation and expanded\\napplications in video, 3D data processing, and rapid learning methodologies. However,\\nthe methodology does have its challenges and limitations, outlined in some detail in\\nthe section that follows..\\nLimitations and challenges of diffusion models\\nDespite their evident benefits and notable progress, diffusion models have some\\nunique limitations, such as the following:\\nSampling speed: A notable limitation of diffusion models is the slow sampling process,\\nparticularly when compared to GANs. Sampling, in this context, refers to the process of\\ngenerating new data points from the learned distribution of a model. The speed at which new\\nsamples can be generated is crucial for many real-time or near-real-time applications, and the\\nslower sampling speed of diffusion models can be a significant drawback.\\nStability during large-scale training: The stability of diffusion models during large-scale\\ntraining is another area requiring further exploration. Large-scale training refers to training a\\nmodel on a substantial amount of data, sometimes leading to instability in the model’s learning\\nprocess. Ensuring stability during this phase is crucial to achieve reliable and consistent\\nperformance from the model.\\nA close examination of the societal impact of the media generated by these models is\\ncrucial, especially given the level of fine control now possible over the generated\\ncontent. However, diffusion models’ inherent simplicity, versatility, and positive\\ninductive biases signify a bright future. These attributes suggest a trajectory of rapid'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 48, 'page_label': '49'}, page_content='development within generative modeling, potentially integrating diffusion models as\\npivotal components in various disciplines, such as computer vision and graphics.\\nA closer look at generative transformers\\nThe revolutionary advent of transformer models has significantly impacted the task of\\ngenerating high-fidelity images from text descriptions. Notable models such as CLIP\\n(Contrastive Language-Image Pretraining) and DALL-E utilized transformers in\\nunique ways to create images based on natural language captions. This section will\\ndiscuss the transformer-based approach for text-to-image generation, its foundations,\\nthe key techniques, the resulting benefits, and some challenges.\\nA brief overview of transformer architecture\\nThe original transformer architecture, introduced by Vaswani et al. in 2017, is a\\ncornerstone of many modern language-processing systems. In fact, the transformer\\nmay be considered the most important architecture in the area of GAI, as it is\\nfoundational to the GPT series of models and many other state-of-the-art generative\\nmethods. As such, we’ll cover the architecture briefly in our survey of generative\\napproaches but will have a dedicated chapter, where we will have the opportunity to\\ndeconstruct and implement the transformer from scratch.\\nAt the core of the transformer architecture lies the self-attention mechanism, a unique\\napproach that captures complex relationships among different elements within an\\nordered data sequence. These elements, known as tokens, represent words in a\\nsentence or characters in a word based on the level of granularity chosen for\\ntokenization.\\nThe principle of attention in this architecture enables a model to focus on certain\\npivotal aspects of the input data while potentially disregarding less significant parts.\\nThis mechanism augments the model’s understanding of the context and the relative\\nimportance of words in a sentence.\\nThe transformer bifurcates into two main segments, the encoder and the decoder,\\neach comprising multiple layers of self-attention mechanisms. While the encoder\\ndiscerns relationships between different positions in the input sequence, the decoder\\nfocuses on the outputs from the encoder, employing a variant of self-attention termed\\nmasked self-attention to prevent consideration of future outputs it hasn’t generated\\nyet.\\nThe calculation of attention weights through the scaled dot-product of query and key\\nvectors plays a crucial role in determining the level of focus on different parts of the'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 49, 'page_label': '50'}, page_content='input. Additionally, multi-head attention allows the model to channel attention\\ntoward multiple data points simultaneously.\\nLastly, to retain the sequence order of data, the model adopts a strategy known as\\npositional encoding. This mechanism is vital for tasks requiring an understanding of\\nsequence or temporal dynamics, ensuring the model preserves the initial order of data\\nthroughout its processing.\\nAgain, we will revisit the transformer architecture in Chapter 3 to further reinforce our\\nunderstanding, as it is foundational to the continued research and evolution of\\ngenerative AI. Nevertheless, with at least a fundamental grasp of the Transformer\\narchitecture, we are better positioned to dissect transformer-driven generative\\nmodeling paradigms across a spectrum of applications.\\nGenerative modeling paradigms with transformers\\nIn tackling various tasks, transformers adopt distinct training paradigms aligning with\\nthe task at hand. For example, discriminative tasks such as classification might use a\\nmasking paradigm:\\nMasked Language Modeling (MLM): MLM is a discriminative pretraining technique used\\nby models such as BERT (Bidirectional Encoder Representations from Transformers).\\nDuring training, some percentage of input tokens are randomly masked out. The model must\\nthen predict the original masked words based on the context of the surrounding unmasked\\nwords. This teaches the model to build robust context-based representations, facilitating many\\ndownstream natural language processing (NLP) tasks.\\nMLM, as utilized in BERT, has been instrumental in enhancing the performance of\\nNLP systems across various domains. For instance, it can power medical coding\\nsystems in healthcare by accurately identifying and categorizing medical terms within\\nclinical notes. This automatic coding can save significant time and reduce errors in\\nmedical documentation, thereby improving the efficiency and accuracy of healthcare\\ndata management.\\nFor generative tasks, the focus shifts to creating new data sequences, requiring\\ndifferent training paradigms:\\nSequence-to-sequence modeling: Sequence-to-sequence models employ both an encoder and\\na decoder. The encoder maps the input sequence to a latent representation. The decoder then\\ngenerates the target sequence token by token from that representation. This paradigm is useful\\nfor tasks such as translation, summarization, and question-answering.\\nAutoregressive modeling: Autoregressive modeling generates sequences by predicting the\\nnext token conditioned only on previous tokens. The model produces outputs one step at a\\ntime, with each new token depending on those preceding it. Autoregressive transformers such\\nas GPT leverage this technique for controlled text generation.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 50, 'page_label': '51'}, page_content='Transformers combine self-attention for long-range dependencies, pre-trained\\nrepresentations, and autoregressive decoding to adapt to discriminative and generative\\ntasks.\\nAdvanced generative synthesis can be achieved with different architectures that make\\ntrade-offs between complexity, scalability, and specialization. For example, instead of\\nusing both the encoder and decoder, many state-of-the-art generative models employ a\\ndecoder-only or encoder-only approach. The encoder-decoder framework is often the\\nmost computationally intensive learning to specialize in, as it increases model size.\\nDecoder-only architectures leverage powerful pre-trained language models such as\\nGPT as the decoder, reducing parameters through weight sharing. Encoder-only\\nmethods forego decoding, instead, they encode inputs and perform regression or\\nsearch on the resulting embeddings. Each approach has advantages that suit certain use\\ncases, datasets, and computational budgets. In the following sections, we explore\\nexamples of models that employ these derivative transformer architectures for creative\\napplications, such as image generation and captioning.\\nEncoder-only approach\\nIn certain models, only the encoder network maps the input to an embedding space.\\nThe output is then generated directly from this embedding, eliminating the need for a\\ndecoder. While this straightforward architecture has typically found its place in\\nclassification or regression tasks, recent advancements have broadened its application\\nto more complex tasks. In particular, models developed for tasks such as image\\nsynthesis leverage the encoder-only setup to process both text and visual inputs,\\ncreating a multimodal relationship that facilitates the generation of high-fidelity\\nimages from natural language instruction.\\nDecoder-only approach\\nSimilarly, some models operate using a decoder-only strategy, where a singular\\ndecoder network is tasked with both encoding the input and generating output. This\\nmechanism starts by joining the input and output sequences, which the decoder\\nprocesses. Despite its simplicity and the characteristic sharing of parameters between\\ninput and output stages, the effectiveness of this architecture relies heavily on the\\npretraining of robust decoders. Recently, even more complex tasks such as text-to-\\nimage synthesis have seen the successful deployment of the decoder-only architecture,\\nillustrating its versatility and adaptability to diverse applications.\\nAdvancement of transformers'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 51, 'page_label': '52'}, page_content='Transformer mechanisms with other novel techniques to tackle generative tasks. This\\nevolution led to distinct approaches to handling text and image generation. In this\\nsection, we will explore some of these innovative models and their unique\\nmethodologies in advancing GAI.\\nEncoder-decoder image generation with DALL-E\\nIntroduced by Ramesh et al. in 2021, DALL-E employs an encoder-decoder\\nframework to facilitate text-to-image generation. This model comprises two primary\\ncomponents:\\nText encoder: Applies the transformer’s encoder, processing plain text to derive a semantic\\nembedding that serves as the context for the image decoder.\\nImage decoder: Applies the transformer’s decoder to generate the image autoregressively,\\npredicting each pixel based on the text embedding and previously predicted pixels.\\nBy training on image-caption datasets, DALL-E refines the transition from text to\\ndetailed image renderings. This setup underscores the capability of dedicated encoder\\nand decoder modules for conditional image generation.\\nEncoder-only image captioning with CLIP\\nCLIP, conceptualized by Radford et al. in 2021, adopts an encoder-only approach for\\nimage-text tasks. Key components include a visual encoder and a text encoder.\\nVisual Encoder and Text Encoder process the image and candidate captions,\\nrespectively, determining the matching caption based on encoded representations.\\nPretraining on extensive image-text datasets enables CLIP to establish a shared\\nembedding space, facilitating efficient inference for retrieval-based captioning.\\nImproving image fidelity with scaled transformers\\n(DALL-E 2)\\nRamesh et al. in 2022 extended DALL-E to DALL-E 2, showcasing techniques to\\nenhance visual quality:\\nA scaled-up decoder: By expanding the decoder to 3.5 billion parameters and applying\\nclassifier-free guidance during sampling, visual quality in complex image distributions such as\\nhuman faces is significantly improved.\\nHierarchical decoding for high-resolution images (GLIDE): Proposed by Nichol et al. in\\n2021, GLIDE employs a hierarchical generation strategy.\\nA coarse-to-fine approach: This entails an initial low-resolution image prediction followed by\\nprogressive detailing through up-sampling and refining, capturing global structure and high-'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 52, 'page_label': '53'}, page_content='frequency textures.\\nMultimodal image generation with GPT-4\\nGPT-4 developed by OpenAI, is a powerful multimodal model based on the\\nTransformer architecture. GPT-4 demonstrates a capability for conditional image\\ngeneration without requiring continued training or fine-tuning:\\nPretraining and fine-tuning: The massive scale of GPT-4 and its pretraining on diverse\\ndatasets enable a robust understanding of relationships between textual and visual data.\\nMultimodal generation: GPT-4 can generate images based on text descriptions. The model\\nuses a deep neural network to encode the semantic meaning of the text into a visual\\nrepresentation. Given a text prompt, GPT-4 generates an image by predicting the visual content\\nconsistent with the provided text. This involves taking high-dimensional text embeddings and\\nprocessing them through successive neural network layers to generate a corresponding visual\\nrepresentation.\\nUsing a pretrained multimodal model eliminates the need for a separate encoder\\nmodule for image inputs, facilitating rapid adaptation for image generation tasks. This\\napproach underscores the versatility and power of Transformer architectures in\\ngenerative tasks, providing a streamlined methodology to translate text into high-\\nquality images.\\nTransformer architectures offer many benefits for controlled image generation when\\ncompared to GANs. Their autoregressive nature ensures precise control over image\\nconstruction while allowing you to adapt to varying computational needs and diverse\\ndownstream applications. However, transformers also introduce new challenges in this\\ndomain.\\nLimitations and challenges of transformer-based\\napproaches\\nSome early transformers-based approaches demonstrated slower sampling speed and\\nrestricted fidelity compared to GANs. Generating or manipulating images while\\nmaintaining precise control over specific attributes or characteristics of the objects\\nwithin those images remains challenging. Additionally, training large-scale\\ntransformers that can overcome these challenges demands extensive computing\\nresources. Notwithstanding, current multimodal results demonstrate a rapidly evolving\\nand promising landscape.\\nWe must also remember that alongside technical challenges there are broader\\nsociotechnical implications and considerations.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 53, 'page_label': '54'}, page_content='Bias and ethics in generative models\\nSignificant advancements in generative models such as GANs, diffusers, and\\ntransformers necessitate serious contemplation of potential bias and ethical\\nimplications.\\nWe need to remain alert to the risk of reinforcing prejudices and stereotypes that\\nreflect skewed training data. For instance, diffusion models trained on data that over-\\nrepresents specific demographics might propagate these biases in their output.\\nAnalogously, language models exposed to toxic or violent content during training\\nmight generate similar content.\\nThe directive nature of prompt-based generation also, unfortunately, opens doors to\\nmisuse if deployed carelessly. Transformers risk facilitating impersonation,\\nmisinformation, and the creation of deceptive content. Image synthesis models such as\\nGANs could potentially be exploited to generate non-consensual deepfakes or artificial\\nmedia.\\nAdditionally, the potential for ultra-realistic output prompts ethical dilemmas\\nregarding consent, privacy, identity, and copyright. The ability to create convincingly\\nreal yet fictional faces or voices complicates the distinction between real and synthetic,\\nnecessitating careful examination of training data sources and generative capabilities.\\nFurther, as these technologies become ubiquitous, their societal impact must be\\nconsidered. Defining clear policies will be crucial as the distinction between authentic\\nand AI-generated content becomes increasingly ambiguous. Upholding principles of\\nintegrity, attribution, and consent remains vital.\\nDespite these risks, the potential benefits of generative models are substantial.\\nAddressing bias proactively, advocating transparency, auditing data and models, and\\nimplementing safeguards become increasingly critical as technologies evolve.\\nUltimately, the responsibility to ensure fairness, accountability, and ethical practice\\nfalls on all developers and practitioners.\\nApplying GAI models – image generation using\\nGANs, diffusers, and transformers\\nIn this hands-on section, we’ll reinforce the concepts discussed throughout the chapter\\nby putting them into practice. You’ll get a first-hand experience and deep dive into the\\nactual implementation of generative models, specifically GANs, diffusion models, and\\ntransformers.\\nThe Python code provided will guide you through this process. Manipulating and\\nobserving the code in action will build your understanding of the intricate workings'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 54, 'page_label': '55'}, page_content='and potential applications of these models. This exercise will provide insight into\\nmodel capabilities for tasks like generating art from prompts and synthesizing hyper-\\nrealistic images.\\nWe’ll be utilizing the highly versatile PyTorch library, a popular choice among\\nmachine learning practitioners, to facilitate our operations. PyTorch provides a\\npowerful and dynamic toolset to define and compute gradients, which is central to\\ntraining these models.\\nIn addition, we’ll also use the diffusers library. It’s a specialized library that\\nprovides functionality to implement diffusion models. This library enables us to\\nreproduce state-of-the-art diffusion models directly from our workspace. It underpins\\nthe creation, training, and usage of denoising diffusion probabilistic models at an\\nunprecedented level of simplicity, without compromising the models’ complexity.\\nThrough this practical session, we’ll explore how to operate and integrate these\\nlibraries and implement and manipulate GANs, diffusers, and transformers using the\\nPython programming language. This hands-on experience will complement the\\ntheoretical knowledge we have gained in the chapter, enabling us to see these models\\nin action in the real world.\\nBy the end of this section, you will not only have a conceptual understanding of these\\ngenerative models but also understand how they are implemented, trained, and used\\nfor several innovative applications in data science and machine learning. You’ll have a\\nmuch deeper understanding of how these models work and the experience of\\nimplementing them yourself.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 55, 'page_label': '56'}, page_content='Working with Jupyter Notebook and Google Colab\\nJupyter notebooks enable live code execution, visualization, and explanatory text,\\nsuitable for prototyping and data analysis. Google Colab, conversely, is a cloud-based\\nversion of Jupyter Notebook, designed for machine learning prototyping. It provides\\nfree GPU resources and integrates with Google Drive for file storage and sharing.\\nWe’ll leverage Colab as our prototyping environment going forward.\\nStable diffusion transformer\\nWe begin with a pre-trained stable diffusion model, a text-to-image latent diffusion\\nmodel created by researchers and engineers from CompVis, Stability AI, and LAION\\n(Patil et al., 2022). The diffusion process is used to draw samples from complex, high-\\ndimensional distributions, and when it interacts with the text embeddings, it creates a\\npowerful conditional image synthesis model.\\nThe term “stable” in this context refers to the fact that during training, a model\\nmaintains certain properties that stabilize the learning process. Stable diffusion models\\noffer rich potential to create entirely new samples from a given data distribution, based\\non text prompts.\\nAgain, for our practical example, we will Google Colab to alleviate a lot of initial\\nsetups. Colab also provides all of the computational resources needed to begin\\nexperimenting right away. We start by installing some libraries, and with three simple\\nfunctions, we will build out a minimal StableDiffusionPipeline using a well-\\nestablished open-source implementation of the stable diffusion method.\\nFirst, let’s navigate to our pre-configured Python environment, Google Colab, and\\ninstall the diffusers open-source library, which will provide most of the key\\nunderlying components we need for our experiment.\\nIn the first cell, we install all dependencies using the following bash command. Note\\nthe exclamation point at the beginning of the line, which tells our environment to reach\\ndown to its underlying process and install the packages we need:\\n \\n!pip install pytorch-fid torch diffusers clip transformers \\naccelerate\\nNext, we import the libraries we’ve just installed to make them available to our Python\\nprogram:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 56, 'page_label': '57'}, page_content='from typing import List \\nimport torch \\nimport matplotlib.pyplot as plt \\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler\\nNow, we’re ready for our three functions, which will execute the three tasks – loading\\nthe pre-trained model, generating the images based on prompting, and rendering the\\nimages:\\n \\ndef load_model(model_id: str) -> StableDiffusionPipeline: \\n\\xa0\\xa0\\xa0\\xa0\"\"\"Load model with provided model_id.\"\"\" \\n\\xa0\\xa0\\xa0\\xa0return StableDiffusionPipeline.from_pretrained( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model_id,  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch_dtype=torch.float16,  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0revision=\"fp16\",  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0use_auth_token=False \\n\\xa0\\xa0\\xa0\\xa0).to(\"cuda\") \\ndef generate_images( \\n\\xa0\\xa0\\xa0\\xa0pipe: StableDiffusionPipeline,  \\n\\xa0\\xa0\\xa0\\xa0prompts: List[str] \\n) -> torch.Tensor: \\n\\xa0\\xa0\\xa0\\xa0\"\"\"Generate images based on provided prompts.\"\"\" \\n\\xa0\\xa0\\xa0\\xa0with torch.autocast(\"cuda\"): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0images = pipe(prompts).images \\n\\xa0\\xa0\\xa0\\xa0return images \\ndef render_images(images: torch.Tensor): \\n\\xa0\\xa0\\xa0\\xa0\"\"\"Plot the generated images.\"\"\" \\n\\xa0\\xa0\\xa0\\xa0plt.figure(figsize=(10, 5)) \\n\\xa0\\xa0\\xa0\\xa0for i, img in enumerate(images): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0plt.subplot(1, 2, i + 1) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0plt.imshow(img) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0plt.axis(\"off\") \\n\\xa0\\xa0\\xa0\\xa0plt.show()\\nIn summary, load_model loads a machine learning model identified by model_id\\nonto a GPU for faster processing. The generate_images function takes this model\\nand a list of prompts to create our images. Within this function, you will notice\\ntorch.autocast(\"cuda\"), which is a special command that allows PyTorch (our\\nunderlying machine learning library) to perform operations faster while maintaining\\naccuracy. Lastly, the render_images function displays these images in a simple grid\\nformat, making use of the matplotlib visualization library to render our output.\\nWith our functions defined, we select our model version, define our pipeline, and\\nexecute our image generation process:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 57, 'page_label': '58'}, page_content='# Execution \\nmodel_id = \"CompVis/stable-diffusion-v1-4\" \\nprompts = [ \\n\\xa0\\xa0\\xa0\\xa0\"A hyper-realistic photo of a friendly lion\", \\n\\xa0\\xa0\\xa0\\xa0\"A stylized oil painting of a NYC Brownstone\" \\n] \\npipe = load_model(model_id)\\nimages = generate_images(pipe, prompts) \\nrender_images(images)\\nThe output in Figure 2.1 is a vivid example of the imaginativeness and creativity we\\ntypically expect from human art, generated entirely by the diffusion process. Except,\\nhow do we measure whether the model was faithful to the text provided?\\nFigure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion” (left) and “A\\nstylized oil painting of a NYC Brownstone” (right)\\nThe next step is to evaluate the quality and relevance of our generated images in\\nrelation to the prompts. This is where CLIP comes into play. CLIP is designed to\\nmeasure the alignment between text and images by analyzing their semantic\\nsimilarities, giving us a true quantitative measure of the fidelity of our synthetic\\nimages to the prompts.\\nScoring with the CLIP model\\nCLIP is trained to understand the relationship between text and images by learning to\\nplace similar images and text near each other in a shared space. When evaluating a'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 58, 'page_label': '59'}, page_content='generated image, CLIP checks how closely the image aligns with the textual\\ndescription provided. A higher score indicates a better match, meaning the image\\naccurately represents the text. Conversely, a lower score suggests a deviation from the\\ntext, indicating a lesser quality or fidelity to the prompt, providing a quantitative\\nmeasure of how well the generated image adheres to the intended description.\\nAgain, we will import the necessary libraries:\\n \\nfrom typing import List, Tuple \\nfrom PIL import Image \\nimport requests \\nfrom transformers import CLIPProcessor, CLIPModel \\nimport torch\\nWe begin by loading the CLIP model, processor, and necessary parameters:\\n \\n# Constants \\nCLIP_REPO = \"openai/clip-vit-base-patch32\" \\ndef load_model_and_processor( \\n    model_name: str \\n) -> Tuple[CLIPModel, CLIPProcessor]: \\n    \"\"\" \\n    Loads the CLIP model and processor. \\n    \"\"\" \\n    model = CLIPModel.from_pretrained(model_name) \\n    processor = CLIPProcessor.from_pretrained(model_name) \\n    return model, processor\\nNext, we define a processing function to adjust the textual prompts and images,\\nensuring that they are in the correct format for CLIP inference:\\n \\ndef process_inputs( \\n\\xa0\\xa0\\xa0\\xa0processor: CLIPProcessor, prompts: List[str], \\n\\xa0\\xa0\\xa0\\xa0images: List[Image.Image]) -> dict: \\n\"\"\" \\nProcesses the inputs using the CLIP processor. \\n\"\"\" \\n\\xa0\\xa0\\xa0\\xa0return processor(text=prompts, images=images, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return_tensors=\"pt\", padding=True)\\nIn this step, we initiate the evaluation process by inputting the images and textual\\nprompts into the CLIP model. This is done in parallel across multiple devices to\\noptimize performance. The model then computes similarity scores, known as logits,\\nfor each image-text pair. These scores indicate how well each image corresponds to the\\ntext prompts. To interpret these scores more intuitively, we convert them into'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 59, 'page_label': '60'}, page_content='probabilities, which indicate the likelihood that an image aligns with any of the given\\nprompts:\\n \\ndef get_probabilities( \\n\\xa0\\xa0\\xa0\\xa0model: CLIPModel, inputs: dict) -> torch.Tensor: \\n\"\"\" \\nComputes the probabilities using the CLIP model. \\n\"\"\" \\n\\xa0\\xa0\\xa0\\xa0outputs = model(**inputs) \\n\\xa0\\xa0\\xa0\\xa0logits = outputs.logits_per_image \\n\\xa0\\xa0\\xa0\\xa0# Define temperature - higher temperature will make the \\ndistribution more uniform. \\n\\xa0\\xa0\\xa0\\xa0T = 10 \\n\\xa0\\xa0\\xa0\\xa0# Apply temperature to the logits \\n\\xa0\\xa0\\xa0\\xa0temp_adjusted_logits = logits / T \\n\\xa0\\xa0\\xa0\\xa0probs = torch.nn.functional.softmax( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0temp_adjusted_logits, dim=1) \\n\\xa0\\xa0\\xa0\\xa0return probs\\nLastly, we display the images along with their scores, visually representing how well\\neach image adheres to the provided prompts:\\n \\ndef display_images_with_scores( \\n\\xa0\\xa0\\xa0\\xa0images: List[Image.Image], scores: torch.Tensor) -> None: \\n\"\"\" \\nDisplays the images alongside their scores. \\n\"\"\" \\n\\xa0\\xa0\\xa0\\xa0# Set print options for readability \\n\\xa0\\xa0\\xa0\\xa0torch.set_printoptions(precision=2, sci_mode=False) \\n\\xa0\\xa0\\xa0\\xa0for i, image in enumerate(images): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0print(f\"Image {i + 1}:\") \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0display(image) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0print(f\"Scores: {scores[i, :]}\") \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0print()\\nWith everything detailed, let’s execute the pipeline as follows:\\n \\n# Load CLIP model \\nmodel, processor = load_model_and_processor(CLIP_REPO) \\n# Process image and text inputs together \\ninputs = process_inputs(processor, prompts, images) \\n# Extract the probabilities\\nprobs = get_probabilities(model, inputs) \\n# Display each image with corresponding scores \\ndisplay_images_with_scores(images, probs)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 60, 'page_label': '61'}, page_content='We now have scores for each of our synthetic images that quantify the fidelity of the\\nsynthetic image to the text provided, based on the CLIP model, which interprets both\\nimage and text data as one combined mathematical representation (or geometric space)\\nand can measure their similarity.\\nFigure 2.2: CLIP scores\\nFor our “friendly lion,” we computed scores of 83% and 17% for each prompt, which\\nwe can interpret as an 83% likelihood that the image aligns with the first prompt.\\nIn practical scenarios, this metric can be applied across various domains:\\nContent moderation: Automatically moderating or flagging inappropriate content by\\ncomparing images to a set of predefined descriptive prompts\\nImage retrieval: Facilitating refined image searches by matching textual queries to a vast\\ndatabase of images, hence narrowing down the search to the most relevant visuals\\nImage captioning: Assisting in generating accurate captions for images by identifying the\\nmost relevant descriptive prompts\\nAdvertising: Tailoring advertisements based on the content of images on a web page to\\nenhance user engagement\\nAccessibility: Enhancing accessibility features by providing accurate descriptions of images\\nfor individuals with visual impairments\\nThis evaluation method not only speeds up processes that would otherwise require\\nmanual inspection but also lends itself to many applications that could benefit from a\\ndeeper understanding and contextual analysis of visual data. We will revisit the CLIP\\nevaluation in Chapter 4, where we simulate a real-world scenario to determine the'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 61, 'page_label': '62'}, page_content='quality and appropriateness of automatically generated captions for a set of product\\nimages.\\nSummary\\nThis chapter explored the theoretical underpinnings and real-world applications of\\nleading GAI techniques, including GANs, diffusion models, and transformers. We\\nexamined their unique strengths, including GANs’ ability to synthesize highly realistic\\nimages, diffusion models’ elegant image generation process, and transformers’\\nexceptional language generation capabilities.\\nUsing a cloud-based Python environment, we implemented these models to generate\\ncompelling images and evaluated their output quality using CLIP. We analyzed how\\ntechniques such as progressive growing and classifier guidance enhanced output\\nfidelity over time. We also considered societal impacts, urging developers to address\\npotential harm through transparency and ethical practices.\\nGenerative methods have unlocked remarkable creative potential, but thoughtful\\noversight is critical as capabilities advance. We can guide these technologies toward\\nbroadly beneficial outcomes by grounding ourselves in core methodologies,\\nscrutinizing their limitations, and considering downstream uses. The path ahead will\\nrequire continued research and ethical reflection to unlock AI’s creative promise while\\nmitigating risks.\\nReferences\\nThis reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nKenfack, P. J., Arapov, D. D., Hussain, R., Ahsan Kazmi, S. M., & Khan, A. (2021). On the\\nfairness of generative adversarial networks (GANs). Arxiv.org.\\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,\\nA., & Bengio, Y. (2014). Generative adversarial nets. Advances in neural information\\nprocessing systems, 27.\\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., &\\nChen, M. (2021). GLIDE: Towards photorealistic image generation and editing with text-\\nguided diffusion models. arXiv preprint arXiv:2112.10741.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,\\nMishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual\\nModels From Natural Language Supervision. ArXiv. /abs/2103.00020.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 62, 'page_label': '63'}, page_content='Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I.\\n(2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint\\narXiv:2204.06125.\\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., & Sutskever, I.\\n(2021). Zero-shot text-to-image generation. In International Conference on Machine Learning\\n(pp. 8821–8831). PMLR.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &\\nPolosukhin, I. (2017). Attention is all you need. Advances in neural information processing\\nsystems, 30.\\nArjovsky, M., Chintala, S. & Bottou, L. (2017). Wasserstein GAN. In Proceedings of the 31st\\nInternational Conference on Neural Information Processing System (NIPS).\\nBrock, A., Donahue, J., & Simonyan, K. (2018). BigGANs: Large Scale GAN Training for\\nHigh Fidelity Natural Image Synthesis. https://arxiv.org/abs/1809.11096.\\nKarras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive Growing of GANs for\\nImproved Quality, Stability, and Variation. https://arxiv.org/abs/1710.10196.\\nMirza, M., & Osindero, S. (2014). Conditional Generative Adversarial Nets.\\nhttps://arxiv.org/abs/1411.1784.\\nRadford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep\\nconvolutional generative adversarial networks. 3rd International Conference for Learning\\nRepresentations.\\nZhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired Image-to-Image Translation\\nUsing Cycle-Consistent Adversarial Networks. In Proceedings of the IEEE International\\nConference on Computer Vision (ICCV).\\nHo, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. Advances in Neural\\nInformation Processing Systems, 34.\\nHo, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., & Fleet, D. J. (2022). Video\\ndiffusion models. arXiv preprint arXiv:2205.10477.\\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in\\nNeural Information Processing Systems, 33, 6840–6851.\\nLuo, S., & Hu, W. (2021). Diffusion probabilistic models for 3d point cloud generation.\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2837–\\n2845.\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-resolution\\nimage synthesis with latent diffusion models. Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, 10684–10695.\\nSalimans, T., Karpathy, A., Chen, X., & Kingma, D. P. (2017). PixelCNN++: Improving the\\npixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint\\narXiv:1701.05517.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 63, 'page_label': '64'}, page_content='Song, Y., Meng, C., & Ermon, S. (2021). Denoising diffusion implicit models. arXiv preprint\\narXiv:2010.02502.\\nSong, Y., & Ermon, S. (2021). Improved techniques for training score-based generative\\nmodels. Advances in Neural Information Processing Systems, 33, 12438–12448.\\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., & Ganguli, S. (2015). Deep\\nunsupervised learning using nonequilibrium thermodynamics. arXiv preprint\\narXiv:1503.03585.\\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in\\nNeural Information Processing Systems, 33, 6840–6851.\\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... & Sutskever, I. (2022).\\nZero-shot text-to-image generation. International Conference on Machine Learning, 8821-\\n8831.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D.\\n(2020). Language models are few-shot learners. Advances in neural information processing\\nsystems, 33, 1877–1901.\\nPatil, S., Cuenca, P., Lambert, N., & von Platen, P. (2022). Stable diffusion with diffusers.\\nHugging Face Blog. https://huggingface.co/blog/stable_diffusion.\\nBoris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê, Luke,\\nRitobrata Ghosh. (2022, June 4). DALL-E Mini Explained. W&B; Weights & Biases, Inc.\\nhttps://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--\\nVmlldzo4NjIxODA.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 64, 'page_label': '65'}, page_content='3 \\nTracing the Foundations of Natural Language\\nProcessing and the Impact of the Transformer\\nThe transformer architecture is a key advancement that underpins most modern\\ngenerative language models. Since its introduction in 2017, it has become a\\nfundamental part of natural language processing (NLP), enabling models such as\\nGenerative Pre-trained Transformer 4 (GPT-4) and Claude to advance text\\ngeneration capabilities significantly. A deep understanding of the transformer\\narchitecture is crucial for grasping the mechanics of modern large language models\\n(LLMs).\\nIn the previous chapter, we explored generative modeling techniques, including\\ngenerative adversarial networks (GANs), diffusion models, and autoregressive\\n(AR) transformers. We discussed how Transformers can be leveraged to generate\\nimages from text. However, transformers are more than just one generative approach\\namong many; they form the basis for nearly all state-of-the-art generative language\\nmodels.\\nIn this chapter, we’ll cover the evolution of NLP that ultimately led to the advent of\\nthe transformer architecture. We cannot cover all the critical steps forward, but we will\\nattempt to cover major milestones, starting with early linguistic analysis techniques\\nand statistical language modeling, followed by advancements in recurrent neural\\nnetworks (RNNs) and convolutional neural networks (CNNs) that highlight the\\npotential of deep learning (DL) for NLP. Our main objective will be to introduce the\\ntransformer—its basis in DL, its self-attention architecture, and its rapid evolution,\\nwhich has led to LLMs and this phenomenon we call generative AI (GenAI).\\nUnderstanding the origins and mechanics of the transformer architecture is important\\nfor recognizing its groundbreaking impact. The principles and modeling capabilities\\nintroduced by transformers are carried forward by all modern language models built\\nupon this framework. We will build our intuition for Transformers through historical\\ncontext and hands-on implementation, as this foundational understanding is key to\\nunderstanding the future of GenAI.\\nEarly approaches in NLP'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 65, 'page_label': '66'}, page_content='Before the widespread use of neural networks (NNs) in language processing, NLP\\nwas largely grounded in methods that counted words. Two particularly notable\\ntechniques were count vectors and Term Frequency-Inverse Document Frequency\\n(TF-IDF). In essence, count vectors tallied up how often each word appeared in a\\ndocument. Building on this, Dadgar et al. applied the TF-IDF algorithm (historically\\nused for information retrieval) to text classification in 2016. This method assigned\\nweights to words based on their significance in one document relative to their\\noccurrence across a collection of documents. These count-based methods were\\nsuccessful for tasks such as searching and categorizing. However, they presented a key\\nlimitation in that they could not capture the semantic relationships between words,\\nmeaning they could not interpret the nuanced meanings of words in context. This\\nchallenge paved the way for exploring NNs, offering a deeper and more nuanced way\\nto understand and represent text.\\nAdvent of neural language models\\nIn 2003, Yoshua Bengio’s team at the University of Montreal introduced the Neural\\nNetwork Language Model (NNLM), a novel approach to language technology. The\\nNNLM was designed to predict the next word in a sequence based on prior words\\nusing a particular type of neural network (NN). The design prominently featured\\nhidden layers that learned word embeddings, which are compact vector representations\\ncapturing the core semantic meanings of words. This aspect was absent in count-based\\napproaches. However, the NNLM was still limited in its ability to interpret longer\\nsequences and handle large vocabularies. Despite these limitations, the NNLM sparked\\nwidespread exploration of NNs in language modeling.\\nThe introduction of the NNLM highlighted the potential of NNs in language\\nprocessing, particularly using word embeddings. Yet, its limitations with long\\nsequences and large vocabulary signaled the need for further research.\\nDistributed representations\\nFollowing the inception of the NNLM, NLP research was propelled toward crafting\\nhigh-quality word vector representations. These representations could be initially\\nlearned from extensive sets of unlabeled text data and later applied to downstream\\nmodels for various tasks. The period saw the emergence of two prominent methods:\\nWord2Vec (introduced by Mikolov et al., 2013) and Global Vectors (GloVe,\\nintroduced by Pennington et al., 2014). These methods applied distributed\\nrepresentation to craft high-quality word vector representations. Distributed\\nrepresentation portrays items such as words not as unique identifiers but as sets of'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 66, 'page_label': '67'}, page_content='continuous values or vectors. In these vectors, each value corresponds to a specific\\nfeature or characteristic of the item. Unlike traditional representations, where each\\nitem has a unique symbol, distributed representations allow these items to share\\nfeatures with others, enabling a more intelligent capture of underlying patterns in the\\ndata.\\nLet us elucidate this concept a bit further. Suppose we represent words based on two\\nfeatures: Formality and Positivity. We might have vectors such as the following:\\nFormal: [1, 0]\\nHappy: [0, 1]\\nCheerful: [0, 1]\\nIn this example, each element in the vector corresponds to one of these features. In the\\nvector for Formal, the 1 element under Formality indicates that the word is formal,\\nwhile the 0 element under Positivity indicates neutrality in terms of positivity.\\nSimilarly, for Happy and Cheerful, the 1 element under Positivity indicates that these\\nwords have a positive connotation. This way, distributed representation captures the\\nessence of words through vectors, allowing for shared features among different words\\nto understand underlying patterns in data.\\nWord2Vec employs a relatively straightforward approach where NNs are used to\\npredict the surrounding words for each target word in a dataset. Through this process,\\nthe NN ascertains values or “weights” for each target word. These weights form a\\nvector for each word in a continuous vector space—a mathematical space wherein\\neach point represents a possible value a vector can take. In the context of NLP, each\\ndimension of this space corresponds to a feature, and the position of a word in this\\nspace captures its semantic or linguistic relationships to other words.\\nThese vectors form a feature-based representation—a type of representation where\\neach dimension represents a different feature that contributes to the word’s meaning.\\nUnlike a symbolic representation, where each word is represented as a unique symbol,\\na feature-based representation captures the semantic essence of words in terms of\\nshared features.\\nOn the other hand, GloVe adopts a different approach. It analyzes the global co-\\noccurrence statistics—a count of how often words appear together in a large text\\ncorpus. GloVe learns vector representations that capture the relationships between\\nwords by analyzing these counts across the entire corpus. This method also results in a\\ndistributed representation of words in a continuous vector space, capturing semantic\\nsimilarity—a measure of the degree to which two words are similar in meaning. In a\\ncontinuous vector space, we can think about semantic similarity as the simple\\ngeometric proximity of vectors representing words.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 67, 'page_label': '68'}, page_content='To further illustrate, suppose we have a tiny corpus of text containing the following\\nsentences:\\n“Coffee is hot.”\\n“Ice cream is cold.”\\nFrom this corpus, GloVe would notice that “coffee” co-occurs with “hot” and “ice\\ncream” co-occurs with “cold.” Through its optimization process, it would aim to create\\nvectors for these words in a way that reflects these relationships. In this oversimplified\\nexample, GloVe might produce a vector such as this:\\nCoffee: [1, 0]\\nHot: [0.9, 0]\\nIce Cream: [0, 1]\\nCold: [0, 0.9]\\nHere, the closeness of the vectors for “coffee” and “hot” (and, similarly, “ice cream”\\nand “cold”) in this space reflects the co-occurrence relationships observed in the\\ncorpus. The vector difference between “coffee” and “hot” might resemble the vector\\ndifference between “ice cream” and “cold,” capturing the contrasting temperature\\nrelationships in a geometric way within the vector space.\\nBoth Word2Vec and GloVe excel at encapsulating relevant semantic information about\\nwords to represent an efficient encoding—a compact way of representing information\\nthat captures the essential features necessary for a task while reducing the\\ndimensionality and complexity of the data.\\nThese methodologies in creating meaningful vector representations served as a step\\ntoward the adoption of transfer learning in NLP. The vectors provide a shared\\nsemantic foundation that facilitates the transfer of learned relationships across varying\\ntasks.\\nTransfer Learning\\nGloVe and other methods of deriving distributed representations paved the way for\\ntransfer learning in NLP. By creating rich vector representations of words that\\nencapsulate semantic relationships, these methods provided a foundational\\nunderstanding of text. The vectors serve as a shared base of knowledge that can be\\napplied to different tasks. When a model, initially trained on one task, is utilized for\\nanother, the pre-learned vector representations aid in preserving the semantic\\nunderstanding, thereby reducing the data or training needed for the new task. This'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 68, 'page_label': '69'}, page_content='practice of transferring acquired knowledge has become fundamental for efficiently\\naddressing a range of NLP tasks.\\nConsider a model trained to understand sentiments (positive or negative) in movie\\nreviews. Through training, this model has learned distributed representations of words,\\ncapturing sentiment-related nuances. Now, suppose there is a new task: understanding\\nsentiments in product reviews. Instead of training a new model from the beginning,\\ntransfer learning allows us to use the distributed representations from the movie review\\ntask to initiate training for the product review task. This could lead to quicker training\\nand better performance, especially with limited data for the product review task.\\nThe effectiveness of transfer learning, bolstered by distributed representations from\\nmethods such as GloVe, highlighted the potential of leveraging pre-existing knowledge\\nfor new tasks. It was a precursor to the integration of NNs in NLP, highlighting the\\nbenefits of utilizing learned representations across tasks. The advent of NNs in NLP\\nbrought about models capable of learning even richer representations, further\\namplifying the impact and scope of transfer learning.\\nAdvent of NNs in NLP\\nThe advent of NNs in NLP marked a monumental shift in the field’s capability to\\nunderstand and process language. Building upon the groundwork laid by\\nmethodologies such as Word2Vec, GloVe, and the practice of transfer learning, NNs\\nintroduced a higher level of abstraction and learning capacity. Unlike previous\\nmethods that often relied on hand-crafted features, NNs could automatically learn\\nintricate patterns and relationships from data. This ability to learn from data propelled\\nNLP into a new era where models could achieve unprecedented levels of performance\\nacross a myriad of language-related tasks. The emergence of architectures such as\\nCNNs and RNNs, followed by the revolutionary transformer architecture, showcased\\nthe remarkable versatility and efficacy of NNs in tackling complex NLP challenges.\\nThis transition not only accelerated the pace of innovation but also expanded the\\nhorizon of what could be achieved in understanding human language computationally.\\nLanguage modeling with RNNs\\nDespite how well these distributed word vectors excelled at encoding local semantic\\nrelationships, modeling long-range dependencies would require a more sophisticated\\nnetwork architecture. This led to the use of RNNs. RNNs (originally introduced by\\nElman in 1990) are a type of NN architecture that processes data sequences by\\niterating through each element of the sequence while maintaining a dynamic internal\\nstate that captures information about the previous elements. Unlike traditional'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 69, 'page_label': '70'}, page_content='feedforward networks (FNNs) that processed each input independently, RNNs\\nintroduced iterations that allowed information to be passed from one step in the\\nsequence to the next, enabling them to capture temporal dependencies in data. The\\niterative processing and dynamic updating in NNs enable them to learn and represent\\nrelationships within the text. These networks can capture contextual connections and\\ninterdependencies across sentences or even entire documents.\\nHowever, standard RNNs had technical limitations when dealing with long sequences.\\nThis led to the development of long short-term memory (LSTM) networks. LSTMs\\nwere first introduced by Hochreiter and Schmidhuber in 1997. They were a special\\nclass of RNNs designed to address the vanishing gradient problem, which is the\\nchallenge where the network cannot learn from earlier parts of a sequence as the\\nsequence gets longer. LSTMs applied a unique gating architecture to control the flow\\nof information within the network, enabling them to maintain and access information\\nover long sequences without suffering from the vanishing gradient problem.\\nThe name “long short-term memory” refers to the network’s ability to keep track of\\ninformation over both short and long sequences of data:\\nShort-term: LSTMs can remember recent information, which is useful for understanding the\\ncurrent context. For example, in language modeling, knowing the last few words can be crucial\\nfor predicting the next word. Consider a phrase such as, “The cat, which already ate a lot, was\\nnot hungry.” As the LSTM processes the text, when it reaches the word “not,” the recent\\ninformation that the cat “ate a lot” is crucial to predict the next word, “hungry,” accurately.\\nLong-term: Unlike standard RNNs, LSTMs are also capable of retaining information from\\nmany steps back in the sequence, which is particularly useful for long-range dependencies,\\nwhere a piece of information early in a sentence could be important for understanding a word\\nmuch later in the sequence. In the same phrase, the information that “The cat” is the subject of\\nthe sentence is introduced early on. This information is crucial later to understand who “was\\nnot hungry” as it processes the later part of the sentence.\\nThe M or memory in LSTMs is maintained through a unique architecture that\\nemploys three gating mechanisms—input, output, and forget gates. These gates control\\nthe flow of information within the network, deciding what information should be kept,\\ndiscarded, or used at each step in the sequence, enabling LSTMs to maintain and\\naccess information over long sequences. Effectively, these gates and the network state\\nallowed LTSMs to carry the “memory” across time steps, ensuring that valuable\\ninformation was retained throughout the processing of the sequence.\\nUltimately, LSTMs obtained state-of-the-art results on many language modeling and\\ntext classification benchmarks. They became the dominant NN architecture for NLP\\ntasks due to their ability to capture short- and long-range contextual relationships.\\nThe success of LSTMs demonstrated the potential of neural architectures in capturing\\nthe complex relationships inherent in language, significantly advancing the field of'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 70, 'page_label': '71'}, page_content='NLP. However, the continuous pursuit of more efficient and effective models led the\\ncommunity toward exploring other NN architectures.\\nRise of CNNs\\nAround 2014, the NLP domain witnessed a rise in the popularity of CNNs for tackling\\nNLP tasks, a notable shift led by Yoon Kim. CNNs (originally brought forward by\\nLeCun et al. for image recognition) operate based on convolutional layers that scan the\\ninput by moving a filter (or kernel) across the input data, at each position calculating\\nthe dot product of the filter’s weights and the input data. In NLP, these layers work\\nover local n-gram windows (consecutive sequences of n words) to identify patterns or\\nfeatures, such as specific sequences of words or characters in the text. Employing\\nconvolutional layers over local n-gram windows, CNNs scan and analyze the data to\\ndetect initial patterns or features. Following this, pooling layers are used to reduce the\\ndimensionality of the data, which helps in both reducing computational complexity\\nand focusing on the most salient features identified by the convolutional layers.\\nCombining convolutional and pooling layers, CNNs can extract hierarchical features.\\nThese features represent information at different levels of abstraction by combining\\nsimpler, lower-level features to form more complex, higher-level features. In NLP, this\\nprocess might start with detecting basic patterns such as common word pairs or\\nphrases in the initial layers, progressing to recognizing more abstract concepts such as\\nsemantic relationships in the higher layers.\\nFor comparison, we again consider a scenario where a CNN is employed to analyze\\nand categorize customer reviews into positive, negative, or neutral sentiments:\\nLower-level features (initial layers): The CNN might identify basic patterns such as common\\nword pairs or phrases in the initial layers. For instance, it might recognize phrases such as\\n“great service,” “terrible experience,” or “not happy.”\\nIntermediate-level features (middle layers): As data progresses through the network, middle\\nlayers might start recognizing more complex patterns, such as negations (“not good”) or\\ncontrasts (“good but expensive”).\\nHigher-level features (later layers): The CNN could identify abstract concepts such as\\noverall sentiment in the later layers. For instance, it might deduce a positive sentiment from\\nphrases such as “excellent service” or “loved the ambiance” and a negative sentiment from\\nphrases such as “worst experience” or “terrible food.”\\nIn this way, CNNs inherently learn higher-level abstract representations of text.\\nAlthough they lack the sequential processing characteristic of RNNs, they offer a\\ncomputational advantage due to their inherent parallelism or ability to process\\nmultiple parts of the data simultaneously. Unlike RNNs, which process sequences\\niteratively and require the previous step to be completed before proceeding to the next,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 71, 'page_label': '72'}, page_content='CNNs can process various parts of the input data in parallel, significantly speeding up\\ntraining times.\\nCNNs, while efficient, have a limitation in their convolution operation, which only\\nprocesses local data from smaller or nearby regions, thereby missing relationships\\nacross more significant portions of the entire input data, referred to as global\\ninformation. This gave rise to attention-augmented convolutional networks that\\nintegrate self-attention with convolutions to address this limitation. Self-attention,\\ninitially used in sequence and generative modeling, was adapted for visual tasks such\\nas image classification, enabling the network to process and capture relationships\\nacross the entire input data. However, attention augmentation, which combines\\nconvolutions and self-attention, yielded the best results. This method retained the\\ncomputational efficiency of CNNs and captured global information, marking an\\nadvancement in image classification and object detection tasks. We will discuss self-\\nattention in detail later as it became a critical component of the transformer.\\nThe ability of CNNs to process multiple parts of data simultaneously marked a\\nsignificant advancement in computational efficiency, paving the way for further\\ninnovations in NN architectures for NLP. As the field progressed, a pivotal shift\\noccurred with the advent of attention-augmented NNs, introducing a new paradigm in\\nhow models handle sequential data.\\nThe emergence of the Transformer in advanced\\nlanguage models\\nIn 2017, inspired by the capabilities of CNNs and the innovative application of\\nattention mechanisms, Vaswani et al. introduced the transformer architecture in the\\nseminal paper Attention is All You Need. The original transformer applied several\\nnovel methods, particularly emphasizing the instrumental impact of attention. It\\nemployed a self-attention mechanism, allowing each element in the input sequence to\\nfocus on distinct parts of the sequence, capturing dependencies regardless of their\\npositions in a structured manner. The term “self” in “self-attention” refers to how the\\nattention mechanism is applied to the input sequence itself, meaning each element in\\nthe sequence is compared to every other element to determine its attention scores.\\nTo truly appreciate how the transformer architecture works, we can describe how the\\ncomponents in its architecture play a role in handling a particular task. Suppose we\\nneed our transformer to translate the English sentence “Hello, how are you?” into\\nFrench: “Bonjour, comment ça va?” Let us walk through this step by step to examine\\nand elucidate how the transformer might accomplish this task. For now, we will\\ndescribe each step in detail and later implement the full architecture using Python.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 72, 'page_label': '73'}, page_content='Components of the transformer architecture\\nBefore diving into how the transformer model fulfills our translation task, we need to\\nunderstand the steps involved. The complete architecture is quite dense, so we will\\nbreak it down into small, logical, and digestible components.\\nFirst, we discuss the two components central to the architectural design of the\\ntransformer model: the encoder and decoder stacks. We will also explain how data\\nflows within these layer stacks, including the concept of tokens, and how relationships\\nbetween tokens are captured and refined using critical techniques such as self-attention\\nand FFNs.\\nThen, we transition into the training process of the transformer model. Here, we\\nreview fundamental concepts such as batches, masking, the training loop, data\\npreparation, optimizer selection, and strategies to improve performance. We will\\nexplain how the transformer optimizes performance using a loss function, which is\\ncrucial in shaping how the model learns to translate.\\nFollowing the training process, we discuss model inference, which is how our trained\\nmodel generates translations. This section points out the order in which individual\\nmodel components operate during translation and emphasizes the importance of each\\nstep.\\nAs discussed, central to the transformer are two vital components, often called the\\nencoder stack and the decoder stack.\\nEncoder and decoder stacks\\nIn the context of the transformer model, stacks reference a hierarchical arrangement of\\nlayers. Each layer in this context is, in fact, an NN layer like the layers we come\\nacross in classical DL models. While a layer is a level in the model where specific\\ncomputational operations occur, a stack refers to multiple such layers arranged\\nconsecutively.\\nEncoder stack\\nConsider our example sentence “Hello, how are you?”. We first convert it into tokens.\\nEach token typically represents a word. In the case of our example sentence,\\ntokenization would break it down into separate tokens, resulting in the following:\\n[“Hello”, “,”, “how”, “are”, “you”, “?”]\\nHere, each word or punctuation represents a distinct token. These tokens are then\\ntransformed into numerical representations, also known as embeddings. These\\nembedding vectors capture the semantic meaning and context of the words, enabling'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 73, 'page_label': '74'}, page_content='the model to understand and process the input data effectively. The embeddings aid in\\ncapturing complex relationships and contexts from the original English input sentence\\nthrough this series of transformations across layers.\\nThis stack comprises multiple layers, where each layer applies self-attention and FFN\\ncomputations on its input data (which we will describe in detail shortly). The\\nembeddings iteratively capture complex relationships and context from the original\\nEnglish input sentence through this series of transformations across layers.\\nDecoder stack\\nOnce the encoder completes its task, the output vectors—or the embeddings of the\\ninput sentence that hold its contextual information—are passed on to the decoder.\\nWithin the decoder stack, multiple layers work sequentially to generate a French\\ntranslation from the embeddings.\\nThe process begins by converting the first embedding into the French phrase\\n“Bonjour.” The subsequent layer uses the following embedding and context from the\\npreviously generated words to predict the next word in the French sentence. This\\nprocess is repeated through all the layers in the stack, each using input embeddings\\nand generated words to define and refine the translation.\\nThe decoder stack progressively builds (or decodes) the translated sentence through\\nthis iterative process, arriving at “Bonjour, comment ça va?”.\\nWith an overall understanding of the encoder-decoder structure, our next step is\\nunraveling the intricate operations within each stack. However, before delving into the\\nself-attention mechanism and FFNs, there is one vital component we need to\\nunderstand — positional encoding. Positional encoding is paramount to the\\ntransformer’s performance because it gives the transformer model a sense of the order\\nof words, something subsequent operations in the stack lack.\\nPositional encoding\\nEvery word in a sentence holds two types of information — its meaning and its role in\\nthe larger context of the sentence. The contextual role often stems from a word’s\\nposition in the arrangement of words. A sentence such as “Hello, how are you?”\\nmakes sense because the words are in a specific order. Change that to “Are you, how\\nhello?” and the meaning becomes unclear.\\nConsequently, Vaswani et al. introduced positional encoding to ensure that the\\ntransformer encodes each word with additional data about its position in the sentence.\\nPositional encodings are computed using a blend of sine and cosine functions across\\ndifferent frequencies, which generate a unique set of values for each position in a\\nsequence. These values are then added to the original embeddings of the tokens,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 74, 'page_label': '75'}, page_content='providing a way for the model to capture the order of words. These enriched\\nembeddings are then ready to be processed by the self-attention mechanism in the\\nsubsequent layers of the transformer model.\\nSelf-attention mechanism\\nAs each token of our input sentence “Hello, how are you?” passes through each layer\\nof the encoder stack, it undergoes a transformation via the self-attention mechanism.\\nAs the name suggests, the self-attention mechanism allows each token (word) to attend\\nto (or focus on) other vital tokens to understand the full context within the sentence.\\nBefore encoding a particular word, this attention mechanism interprets the relationship\\nbetween each word and the others in the sequence. It then assigns distinct attention\\nscores to different words based on their relevance to the current word being processed.\\nConsider again our input sentence “Hello, how are you?”. When the self-attention\\nmechanism is processing the last word, “you,” it does not just focus on “you.” Instead,\\nit takes into consideration the entire sentence: it looks at “Hello,” glances over “how,”\\nreflects on “are,” and, of course, focuses on “you.”\\nIn doing so, it assigns various levels of attention to each word. You can visualize\\nattention (Figure 3.1) as lines connecting “you” to every other word. The line to\\n“Hello” might be thick, indicating a lot of attention, representing the influence of\\n“Hello” on the encoding of “you.” The line connecting “you” and “how” might be\\nthinner, suggesting less attention given to “how.” The lines to “are” and “you” would\\nhave other thicknesses based on how they help in providing context to “you”:\\nFigure 3.1: Self-attention mechanism\\nThis way, when encoding “you,” a weighted mix of the entire sentence is considered,\\nnot just the single word. And these weights defining the mix are what we refer to as\\nattention scores.\\nThe self-attention mechanism is implemented through a few steps:\\n1. Initially, each input word is represented as a vector, which we obtain from the word\\nembedding.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 75, 'page_label': '76'}, page_content='2. These vectors are then mapped to new vectors called query, key, and value vectors through\\nlearned transformations.\\n3. An attention score for each word is then computed by taking the dot product of the query\\nvector of the word with the key vector of every other word, followed by a SoftMax operation\\n(which we will describe later).\\n4. These scores indicate how much focus to place on other parts of the input sentence for each\\nword as it is encoded.\\n5. Finally, a weighted sum of the value vectors is computed based on these scores to give us our\\nfinal output vectors, or the self-attention outputs.\\nIt is important to note that this computation is done for each word in the sentence. This\\nensures a comprehensive understanding of the context in the sentence, considering\\nmultiple parts of the sentence at once. This concept set the transformer apart from\\nnearly every model that came before it.\\nInstead of running the self-attention mechanism once (or “single-head” attention), the\\ntransformer replicates the self-attention mechanism multiple times in parallel. Each\\nreplica or head operates on the same input but has its own independent set of learned\\nparameters to compute the attention scores. This allows each head to learn different\\ncontextual relationships between words. This parallel process is known as multi-head\\nattention (MHA).\\nImagine our sentence “Hello, how are you?” again. One head might concentrate on\\nhow “Hello” relates to “you,” whereas another head might focus more on how “how”\\nrelates to “you.” Each head has its own set of query, key, and value weights, further\\nenabling them to specialize and learn different things. The outputs of these multiple\\nheads are then concatenated and transformed to produce final values passed onto the\\nnext layer in the stack.\\nThis multi-head approach allows the model to capture a wider range of information\\nfrom the same input words. It is like having several perspectives on the same sentence,\\neach providing unique insights.\\nSo far, for our input sentence “Hello, how are you?”, we have converted each word\\ninto token representations, which are then contextualized using the MHA mechanism.\\nThrough parallel self-attention, our transformer can consider the full range of\\ninteractions between each word and every other word in the sentence. We now have a\\nset of diverse and context-enriched word representations, each containing a textured\\nunderstanding of a word’s role in the sentence. However, this contextual understanding\\ncontained within the attention mechanism is just one component of the information\\nprocessing in our transformer model. Next comes another layer of interpretation\\nthrough position-wise FFNs. The FFN will add further nuances to these\\nrepresentations, making them more informative and valuable for our translation task.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 76, 'page_label': '77'}, page_content='In the next section, we discuss a vital aspect of the transformer’s training sequence:\\nmasking. Specifically, the transformer applies causal (or look-ahead) masking during\\nthe decoder self-attention to ensure that each output token prediction depends only on\\npreviously generated tokens, not future unknown tokens.\\nMasking\\nThe transformer applies two types of masking during training. The first is a\\npreprocessing step to ensure input sentences are of the same length, which enables\\nefficient batch computation. The second is look-ahead (or causal) masking, which\\nallows the model to selectively ignore future tokens in a sequence. This type of\\nmasking occurs in the self-attention mechanism in the decoder and prevents the model\\nfrom peeking ahead at future tokens in the sequence. For example, when translating\\nthe word “Hello” to French, look-ahead masking ensures that the model does not have\\naccess to the subsequent words “how,” “are,” or “you.” This way, the model learns to\\ngenerate translations based on the current and preceding words, adhering to a natural\\nprogression in translation tasks, mimicking that of human translation.\\nWith a clearer understanding of how data is prepared and masked for training, we now\\ntransition to another significant aspect of the training process: hyperparameters. Unlike\\nparameters learned from the data, hyperparameters are configurations set before\\ntraining to control the model optimization process and guide the learning journey. The\\nfollowing section will explore various hyperparameters and their roles during training.\\nSoftMax\\nTo understand the role of the FFN, we can describe its two primary components—\\nlinear transformations and an activation function:\\nLinear transformations are essentially matrix multiplications. Think of them as tools that\\nreshape or tweak the input data. In the FFN, these transformations occur twice, where two\\ndifferent weights (or matrices) are used.\\nA rectified linear unit (ReLU) function is applied between these two transformations. The\\nrole of the ReLU function is to introduce non-linearity in the model. Simply put, the ReLU\\nfunction allows the model to capture patterns within the input data that are not strictly\\nproportional, i.e., non-linear, which is typical of natural language (NL) data.\\nThe FFN is called position-wise because it treats each word in the sentence separately\\n(position by position), regardless of the sequence. This contrasts with the self-attention\\nmechanism, which considers the entire sequence at once.\\nSo, let us attempt to visualize the process: Imagine our word “Hello” arriving here\\nafter going through the self-attention mechanism. It carries with it information about'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 77, 'page_label': '78'}, page_content='its own identity mixed with contextual references to “how,” “are,” and “you.” This\\nintegrated information resides within a vector that characterizes “Hello.”\\nWhen “Hello” enters the FFN, picture it as a tunnel with two gates. At the first gate (or\\nlinear layer), “Hello” is transformed by a matrix multiplication operation, changing its\\nrepresentation. Afterward, it encounters the ReLU function—which makes the\\nrepresentation non-linear.\\nAfter this, “Hello” passes through a second gate (another linear layer), emerging on the\\nother side transformed yet again. The core identity of “Hello” remains but is now\\nimbued with even more context, carefully calibrated and adjusted by the FFN.\\nOnce the input passes through the gates, there is one additional step. The transformed\\nvector still must be converted into a form that can be interpreted as a prediction for our\\nfinal translation task.\\nThis brings us to using the SoftMax function, the final transformation within the\\ntransformer’s decoder. After the vectors pass through the FFN, they are further\\nprocessed through a final linear layer. The result is then fed into a SoftMax function.\\nSoftMax serves as a mechanism for converting the output of our model into a form\\nthat can be interpreted as probabilities. In essence, the SoftMax function will take the\\noutput from our final linear layer (which could be any set of real numbers) and\\ntransform it into a distribution of probabilities, representing the likelihood of each\\nword being the next word in our output sequence. For example, if our target\\nvocabulary includes “Bonjour,” “Hola,” “Hello,” and “Hallo,” the SoftMax function\\nwill assign each of these words a probability, and the word with the highest probability\\nwill be chosen as the output translation for the word “Hello.” We can illustrate with\\nthis oversimplified representation of the output probabilities:\\n[ Bonjour: 0.4, Hola: 0.3, Hello: 0.2, Hallo: 0.1 ]\\nFigure 3.2 shows a more complete (albeit oversimplified) view of the flow of\\ninformation through the architecture.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 78, 'page_label': '79'}, page_content='Figure 3.2: A simplified illustration of the transformer\\nNow that we’ve introduced the architectural components of the transformer, we are\\npoised to understand how its components work together.\\nSequence-to-sequence learning\\nThe components of a transformer come together to learn from data using a mechanism\\nknown as sequence-to-sequence (Seq2Seq) learning, a subset of supervised learning\\n(SL). Recall that SL is a technique that uses labeled data to train models to predict\\noutcomes accurately. In Seq2Seq learning, we provide the transformer with training\\ndata that comprises examples of input and corresponding correct output, which, in this\\ncase, are correct translations. Seq2Seq learning is particularly well suited for tasks\\nsuch as machine translation where both the input and output are sequences of words.\\nThe very first step in the learning process is to convert each word in the phrase into\\ntokens, which are then transformed into numerical embeddings. These embeddings\\ncarry the semantic essence of each word. Positional encodings are computed and\\nadded to these embeddings to imbue them with positional awareness.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 79, 'page_label': '80'}, page_content='As these enriched embeddings traverse through the encoder stack, within each layer,\\nthe self-attention mechanism refines the embeddings by aggregating contextual\\ninformation from the entire phrase. Following self-attention, each word’s embedding\\nundergoes further transformation in the position-wise FFNs, adjusting the embeddings\\nto capture even more complex relationships.\\nUpon exiting the encoder, the embeddings now hold a rich mixture of semantic and\\ncontextual information. They are passed onto the decoder stack, which aims to\\ntranslate the phrase into another language (that is, the target sequence). As with the\\nencoder, each layer in the decoder also employs self-attention and position-wise FFNs,\\nbut with an additional layer of cross-attention that interacts with the encoder’s outputs.\\nThis interaction helps align the input and output phrases, a crucial aspect of\\ntranslation.\\nAs the embeddings move through the decoder layers, they are progressively refined to\\nrepresent the translated phrase that the model will predict. The final layer of the\\ndecoder processes the embeddings through a linear transformation and SoftMax\\nfunction to produce a probability distribution over the target vocabulary. This\\ndistribution defines the model’s predicted likelihood for each potential next token at\\neach step. The decoder then samples from this distribution to select the token with the\\nhighest predicted probability as its next output. By iteratively sampling the most likely\\nnext tokens according to the predicted distributions, the decoder can autoregressively\\ngenerate the full translated output sequence token by token.\\nHowever, for the transformer to reliably sample from the predicted next-token\\ndistributions to generate high-quality translations, it must progressively learn by\\niterating over thousands of examples of input-output pairs. In the next section, we\\nexplore model training in further detail.\\nModel training\\nAs discussed, the primary goal of the training phase is to refine the model’s parameters\\nto facilitate accurate translation from one language to another. But what does the\\nrefinement of parameters entail, and why is it pivotal?\\nParameters are internal variables that the model utilizes to generate translations.\\nInitially, these parameters are assigned random values, which are adjusted with each\\ntraining iteration. Again, the model is provided with training data that comprises\\nthousands of examples of input data and corresponding correct output, which, in this\\ncase, is the correct translation. It then compares its predicted output tokens to the\\ncorrect (or actual) target sequences using an error (or loss) function.\\nBased on the loss, the model updates its parameters, gradually improving its ability to\\nchoose the correct item in the sequence at each step of decoding. This slowly refines'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 80, 'page_label': '81'}, page_content='the probability distributions.\\nOver thousands of training iterations, the model learns associations between source\\nand target languages. Eventually, it acquires enough knowledge to decode coherent,\\nhuman-like translations from unseen inputs by relying on patterns discovered during\\ntraining. Therefore, training drives the model’s ability to produce accurate target\\nsequences from the predicted vocabulary distributions.\\nAfter training on sufficient translation pairs, the transformer reaches reliable\\ntranslation performance. The trained model can then take in new input sequences and\\noutput translated sequences by generalizing to that new data.\\nFor instance, with our example sentence “Hello, how are you?” and its French\\ntranslation “Bonjour, comment ça va?”, the English sentence serves as the input, and\\nthe French sentence serves as the target output. The training data comprises many\\ntranslated pairs. Each time the model processes a batch of data, it generates predictions\\nfor the translation, compares them to the actual target translations, and then adjusts its\\nparameters to reduce the discrepancy (or minimize the loss) between the predicted and\\nactual translations. This is repeated with numerous batches of data until the model’s\\ntranslations are sufficiently accurate.\\nHyperparameters\\nAgain, unlike parameters, which the model learns from the training data,\\nhyperparameters are preset configurations that govern the training process and the\\nstructure of the model. They are a crucial part of setting up a successful training run.\\nSome key hyperparameters in the context of transformer models include the following:\\nLearning rate: This value determines the step size at which the optimizer updates the model\\nparameters. A higher learning rate could speed up the training but may overshoot the optimal\\nsolution. A lower learning rate may result in a more precise convergence to the optimal\\nsolution, albeit at the cost of longer training time. We will discuss optimizers in detail in the\\nnext section.\\nBatch size: The number of data examples processed in a single batch affects the computational\\naccuracy and the memory requirements during training.\\nModel dimensions: The model’s size (for example, the number of layers in the encoder and\\ndecoder, the dimensionality of the embeddings, and so on) is a crucial hyperparameter that\\naffects the model’s capacity to learn and generalize.\\nOptimizer settings: Choosing an optimizer and its settings, such as the initial learning rate,\\nbeta values in the Adam optimizer, and so on, are also considered hyperparameters. Again, we\\nwill explore optimizers further in the next section.\\nRegularization terms: Regularization terms such as dropout rate are hyperparameters that\\nhelp prevent overfitting by adding some form of randomness or constraint to the training'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 81, 'page_label': '82'}, page_content='process.\\nSelecting the proper values for hyperparameters is crucial for the training process as it\\nsignificantly impacts the model’s performance and efficiency. It often involves\\nhyperparameter tuning, which involves experimentation and refining to find values for\\nhyperparameters that yield reliable performance for a given task. Hyperparameter\\ntuning can be somewhat of an art and a science. We will touch on this more in later\\nchapters.\\nWith a high-level grasp of hyperparameters, we will move on to the choice of\\noptimizer, which is pivotal in controlling how efficiently the model learns from the\\ntraining data.\\nChoice of optimizer\\nThe optimizer is a fundamental component of the training process and is responsible\\nfor updating the model’s parameters to minimize error. Different optimizers have\\ndifferent strategies for navigating the parameter space to find a set of parameter values\\nthat yield low loss (or less error). The choice of optimizer can significantly impact the\\nspeed and quality of the training process.\\nIn the context of transformer models, the Adam optimizer is often the optimizer of\\nchoice due to its efficiency and empirical success in training deep networks. Adam\\nadapts learning rates during training. For simplicity, we will not explore all the\\npossible optimizers but instead describe their purpose.\\nThe optimizer’s primary task is to fine-tune the model’s parameters to reduce\\ntranslation errors, progressively guiding the model toward the desired level of\\nperformance. However, an over-zealous optimization could lead the model to\\nmemorize the training data, failing to generalize well to unseen data. To mitigate this,\\nwe employ regularization techniques.\\nIn the next section, we will explore regularization—a technique that works with\\noptimization to ensure that while the model learns to minimize translation errors, it\\nalso remains adaptable to new, unseen data.\\nRegularization\\nRegularization techniques are employed to deter the model from memorizing the\\ntraining data (a phenomenon known as overfitting) and to promote better performance\\non new, unseen data. Overfitting arises when the model, to minimize the error, learns\\nthe training data to such an extent that it captures useless patterns (or noise) along with\\nthe actual patterns. This over-precision in learning the training data leads to a decline\\nin performance when the model is exposed to new data.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 82, 'page_label': '83'}, page_content='Let us revisit our simple scenario where we train a model to translate English greetings\\nto French greetings using a dataset that includes the word “Hello” and its translation\\n“Bonjour.” If the model is overfitting, it may memorize the exact phrases from the\\ntraining data without understanding the broader translation pattern.\\nIn an overfit scenario, suppose the model learns to translate “Hello” to “Bonjour” with\\na probability of 1.0 because that is what it encountered most often in the training data.\\nWhen presented with new, unseen data, it may encounter variations it has not seen\\nbefore, such as “Hi,” which should also translate to “Bonjour.” However, due to\\noverfitting, the model might fail to generalize from “Hello” to “Hi” as it is overly\\nfocused on the exact mappings it saw during training.\\nSeveral regularization techniques can mitigate the overfitting problem. These\\ntechniques apply certain constraints on the model’s parameters during training,\\nencouraging the model to learn a more generalized representation of the data rather\\nthan memorizing the training dataset.\\nHere are some standard regularization techniques used in the context of transformer\\nmodels:\\nDropout: In the context of NN-based models such as the transformer, the term “neurons”\\nrefers to individual elements within the model that work together to learn from the data and\\nmake predictions. Each neuron learns specific aspects or features from the data, enabling the\\nmodel to understand and translate text. During training, dropout randomly deactivates or\\n“drops out” a fraction of these neurons, temporarily removing them from the network. This\\nrandom deactivation encourages the model to spread its learning across many neurons rather\\nthan relying too heavily on a few. By doing so, dropout helps the model to better generalize its\\nlearning to unseen data rather than merely memorizing the training data (that is, overfitting).\\nLayer normalization: Layer normalization is a technique that normalizes the activations of\\nneurons in a layer for each training example rather than across a batch of examples. This\\nnormalization helps stabilize the training process and acts as a form of regularization,\\npreventing overfitting.\\nL1 or L2 regularization: L1 regularization, also known as Lasso, adds a penalty equal to the\\nabsolute magnitude of coefficients, promoting parameter sparsity. L2 regularization, or Ridge,\\nadds a penalty based on the square of the coefficients, discouraging large values to prevent\\noverfitting. Although these techniques help in controlling model complexity and enhancing\\ngeneralization, they were not part of the transformer’s initial design.\\nBy employing these regularization techniques, the model is guided toward learning\\nmore generalized patterns in the data, which improves its ability to perform well on\\nunseen data, thus making the model more reliable and robust in translating new text\\ninputs.\\nThroughout the training process, we have mentioned the loss function and discussed\\nhow the optimizer leverages it to adjust the model’s parameters, aiming to minimize'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 83, 'page_label': '84'}, page_content='prediction error. The loss function quantifies the model’s performance. We discussed\\nhow regularization penalizes the loss function to prevent overfitting, encouraging the\\nmodel to learn simpler, more generalizable patterns. In the next section, we look closer\\nat the nuanced role of the loss function itself.\\nLoss function\\nThe loss function is vital in training the transformer model, quantifying the differences\\nbetween the model’s predictions and the actual data. In language translation, this error\\nis measured between generated and actual translations in the training dataset. A\\ncommon choice for this task is cross-entropy loss, which measures the difference\\nbetween the model’s predicted probability distribution across the target vocabulary and\\nthe actual distribution, where the truth has a probability of 1 for the correct word and 0\\nfor the rest.\\nThe transformer often employs a variant known as label-smoothed cross-entropy loss.\\nLabel smoothing adjusts the target probability distribution during training, slightly\\nlowering the probability for the correct class and increasing the probability for all\\nother classes, which helps prevent the model from becoming too confident in its\\npredictions. For instance, with a target vocabulary comprising “Bonjour,” “Hola,”\\n“Hello,” and “Hallo,” and assuming “Bonjour” is the correct translation, a standard\\ncross-entropy loss would aim for the probability distribution of Bonjour: 1.0, Hola:\\n0.0, Hello: 0.0, Hallo: 0.0. However, the label-smoothed cross-entropy loss\\nwould slightly adjust these probabilities, as follows:\\n[ “Bonjour”: 0.925, “Hola”: 0.025, “Hello”: 0.025, “Hallo”: 0.025\\n]\\nThe smoothing reduces the model’s confidence and promotes better generalization to\\nunseen data. With a clearer understanding of the loss function’s role, we can move on\\nto the inference phase, where the trained model generates translations for new, unseen\\ndata.\\nInference\\nHaving traversed the training landscape, our trained model is now adept with\\noptimized parameters to tackle the translation task. In the inference stage, these\\nlearned parameters are employed to translate new, unseen text. We will continue with\\nour example phrase “Hello, how are you?” to elucidate this process.\\nThe inference stage is the practical application of the trained model on new data. The\\ntrained parameters, refined after numerous iterations during training, are now used to'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 84, 'page_label': '85'}, page_content='translate text from one language to another. The inference steps can be described as\\nfollows:\\n1. Input preparation: Initially, our phrase “Hello, how are you?” is tokenized and encoded into a\\nformat that the model can process, akin to the preparation steps in the training phase.\\n2. Passing through the model: The encoded input is then propagated through the model. As it\\nnavigates through the encoder and decoder stacks, the trained parameters guide the\\ntransformation of the input data, inching closer to accurate translations at each step.\\n3. Output generation: At the culmination of the decoder stack, the model generates a probability\\ndistribution across the target vocabulary for each word in the input text. For the word “Hello,”\\na probability distribution is formed over the target vocabulary, which, in our case, comprises\\nFrench words. The word with the highest probability is selected as the translation. This process\\nis replicated for each word in the phrase, rendering the translated output “Bonjour, comment ça\\nva?”.\\nNow that we understand how the model produces the final output, we can implement a\\ntransformer model step by step to solidify the concepts we have discussed. However,\\nbefore we dive into the code, we can briefly give a synopsis of the end-to-end\\narchitecture flow:\\n1. Input tokenization: The initial English phrase “Hello, how are you?” is tokenized into smaller\\nunits such as “Hello,” “,,” “how,” and so on.\\n2. Embeddings: These tokens are then mapped to continuous vector representations through an\\nembedding layer.\\n3. Positional encoding: To preserve the order of the sequence, positional encodings are added to\\nthe embeddings.\\n4. Encoder self-attention: The embedded input sequence navigates through the encoder’s\\nsequence of self-attention layers. Here, each word gauges the relevance of every other word to\\ncomprehend the full context.\\n5. FFN: The representations are subsequently refined by position-wise FFNs within each encoder\\nlayer.\\n6. Encoder output: The encoder renders contextual representations capturing the essence of the\\ninput sequence.\\n7. Decoder attention: Incrementally, the decoder crafts the output sequence, employing self-\\nattention solely on preceding words to maintain the sequence order.\\n8. Encoder-decoder attention: The decoder evaluates the encoder’s output, centering on\\npertinent input context while generating each word in the output sequence.\\n9. Output layers: The decoder feeds its output to the linear and SoftMax layers to produce\\n“Bonjour, comment ça va?\\nAt the end of this chapter, we will adapt a best-in-class implementation of the original\\ntransformer (Huang et al., 2022) into a minimal example that could later be trained on'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 85, 'page_label': '86'}, page_content='various downstream tasks. This will serve as a theoretical exercise to further solidify\\nour understanding. In practice, we would rely on pre-trained or foundation models,\\nwhich we will learn to implement in later chapters.\\nHowever, before we begin our practice project, we can trace its impact on the current\\nlandscape of GenAI. We follow the trajectory of early applications of the architecture\\n(for example, Bidirectional Encoded Representations from Transformers (BERT))\\nthrough to the first GPT.\\nEvolving language models – the AR Transformer\\nand its role in GenAI\\nIn Chapter 2, we reviewed some of the generative paradigms that apply a transformer-\\nbased approach. Here, we trace the evolution of Transformers more closely, outlining\\nsome of the most impactful transformer-based language models from the initial\\ntransformer in 2017 to more recent state-of-the-art models that demonstrate the\\nscalability, versatility, and societal considerations involved in this fast-moving domain\\nof AI (as illustrated in Figure 3.3):\\nFigure 3.3: From the original transformer to GPT-4\\n2017 – Transformer: The transformer model, introduced by Vaswani et al., was a paradigm\\nshift in NLP, featuring self-attention layers that could process entire sequences of data in\\nparallel. This architecture enabled the model to evaluate the importance of each word in a\\nsentence relative to all other words, thereby enhancing the model’s ability to capture the\\ncontext.\\n2018 – BERT: Google’s BERT model innovated on the transformer architecture by utilizing a\\nbidirectional context in its encoder layers during pre-training. It was one of the first models to\\nunderstand the context of a word based on its entire sentence, both left and right, significantly\\nimproving performance on a wide range of NLP tasks, especially those requiring a deep\\nunderstanding of context.\\n2018 – GPT-1: OpenAI’s GPT-1 model was a milestone in NLP, adopting a generative pre-\\ntrained approach with a transformer’s decoder-only model. It was pre-trained on a diverse\\ncorpus of text data and fine-tuned for various tasks, using a unidirectional approach that'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 86, 'page_label': '87'}, page_content='generated text sequentially from left to right, which was particularly suited for generative text\\napplications.\\n2019 – GPT-2: GPT-2 built upon the foundation laid by GPT-1, maintaining its decoder-only\\narchitecture but significantly expanding its scale in terms of dataset and model size. This\\nallowed GPT-2 to generate text that was more coherent and contextually relevant across a\\nbroader range of topics, demonstrating the power of scaling up transformer models.\\n2020 – GPT-3: OpenAI’s GPT-3 pushed the boundaries of scale in transformer models to 175\\nbillion parameters, enabling a wide range of tasks to be performed with minimal input, often\\nwith zero-shot learning (ZSL) or few-shot learning (FSL). This showed that Transformers\\ncould generalize across tasks and data types, often without the need for extensive task-specific\\ndata or fine-tuning.\\n2021 – InstructGPT: An optimized variant of GPT-3, InstructGPT was fine-tuned specifically\\nto follow user instructions and generate aligned responses, incorporating feedback loops that\\nemphasized safety and relevance in its outputs. This represented a focus on creating AI models\\nthat could more accurately interpret and respond to human prompts.\\n2023 – GPT-4: GPT-4 was an evolution of OpenAI’s transformer models into the multimodal\\nspace, capable of understanding and generating content based on both text and images. This\\nmodel aimed to produce safer and more contextually nuanced responses, showcasing a\\nsignificant advancement in the model’s ability to handle complex tasks and generate creative\\ncontent.\\n2023 – LLaMA 2: Meta AI’s LLaMA 2 was part of a suite of models that focused on\\nefficiency and accessibility, allowing for high-performance language modeling while being\\nmore resource-efficient. This model was aimed at facilitating a broader range of research and\\napplication development within the AI community.\\n2023 – Claude 2: Anthropic’s Claude 2 was an advancement over Claude 1, increasing its\\ntoken context window and improving its reasoning and memory capabilities. It aimed to align\\nmore closely with human values, offering responsible and nuanced generative capabilities for\\nopen-domain question-answering and other conversational AI applications, marking progress\\nin ethical AI development.\\nThe timeline presented highlights the remarkable progress in transformer-based\\nlanguage models over the past several years. What originated as an architecture that\\nintroduced the concept of self-attention has rapidly evolved into models with billions\\nof parameters that can generate coherent text, answer questions, and perform a variety\\nof intellectual tasks at high levels of performance. The increase in scale and\\naccessibility of models such as GPT-4 has opened new possibilities for AI\\napplications. At the same time, recent models have illustrated a focus on safety and\\nethics and providing more nuanced, helpful responses to users.\\nIn the next section, we accomplish a rite of passage for practitioners with an interest in\\nthe NL field. We implement the key components of the original transformer\\narchitecture using Python to more fully understand the mechanics that started it all.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 87, 'page_label': '88'}, page_content='Implementing the original Transformer\\nThe following code demonstrates how to implement a minimal transformer model for\\na Seq2Seq translation task, mainly translating English text to French. The code is\\nstructured into multiple sections, handling various aspects from data loading to model\\ntraining and translation.\\nData loading and preparation\\nInitially, the code loads a dataset and prepares it for training. The data is loaded from a\\nCSV file, which is then split into English and French text. The text is limited to 100\\ncharacters for demonstration purposes to reduce training time. The CSV file includes a\\nfew thousand example data points and can be found in the book’s GitHub repository\\n(https://github.com/PacktPublishing/Python-Generative-AI) along with the complete\\ncode:\\n \\nimport pandas as pd \\nimport numpy as np \\n# Load demo data \\ndata = pd.read_csv(\"./Chapter_3/data/en-fr_mini.csv\") \\n# Separate English and French lexicons \\nEN_TEXT = data.en.to_numpy().tolist() \\nFR_TEXT = data.fr.to_numpy().tolist() \\n# Arbitrarily cap at 100 characters for demonstration to avoid \\nlong training times \\ndef demo_limit(vocab, limit=100): \\n\\xa0\\xa0\\xa0\\xa0return [i[:limit] for i in vocab] \\nEN_TEXT = demo_limit(EN_TEXT) \\nFR_TEXT = demo_limit(FR_TEXT) \\n# Establish the maximum length of a given sequence \\nMAX_LEN = 100\\nTokenization\\nNext, a tokenizer is trained on the text data. The tokenizer is essential for converting\\ntext data into numerical data that can be fed into the model:\\n \\nfrom tokenizers import Tokenizer \\nfrom tokenizers.models import WordPiece \\nfrom tokenizers.trainers import WordPieceTrainer \\nfrom tokenizers.pre_tokenizers import Whitespace \\ndef train_tokenizer(texts):'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 88, 'page_label': '89'}, page_content='tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\")) \\n\\xa0\\xa0\\xa0\\xa0tokenizer.pre_tokenizer = Whitespace() \\n\\xa0\\xa0\\xa0\\xa0trainer = WordPieceTrainer( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0vocab_size=5000, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"\\n[MASK]\",  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"<sos>\", \"<eos>\"], \\n\\xa0\\xa0\\xa0\\xa0) \\n\\xa0\\xa0\\xa0\\xa0tokenizer.train_from_iterator(texts, trainer) \\n\\xa0\\xa0\\xa0\\xa0return tokenizer \\nen_tokenizer = train_tokenizer(EN_TEXT) \\nfr_tokenizer = train_tokenizer(FR_TEXT)\\nData tensorization\\nThe text data is then tensorized, which involves converting the text data into tensor\\nformat. This step is crucial for preparing the data for training with PyTorch:\\n \\nimport torch \\nfrom torch.nn.utils.rnn import pad_sequence \\ndef tensorize_data(text_data, tokenizer): \\n\\xa0\\xa0\\xa0\\xa0numericalized_data = [ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.tensor(tokenizer.encode(text).ids) for text in \\ntext_data \\n\\xa0\\xa0\\xa0\\xa0] \\n\\xa0\\xa0\\xa0\\xa0padded_data = pad_sequence(numericalized_data, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0batch_first=True) \\n\\xa0\\xa0\\xa0\\xa0return padded_data \\nsrc_tensor = tensorize_data(EN_TEXT, en_tokenizer) \\ntgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)\\nDataset creation\\nA custom dataset class is created to handle the data. This class is essential for loading\\nthe data in batches during training:\\n \\nfrom torch.utils.data import Dataset, DataLoader \\nclass TextDataset(Dataset):\\n\\xa0\\xa0\\xa0\\xa0def __init__(self, src_data, tgt_data): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.src_data = src_data \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.tgt_data = tgt_data \\n\\xa0\\xa0\\xa0\\xa0def __len__(self): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return len(self.src_data)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 89, 'page_label': '90'}, page_content='def __getitem__(self, idx): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.src_data[idx], self.tgt_data[idx] \\ndataset = TextDataset(src_tensor, tgt_tensor)\\nEmbeddings layer\\nThe embeddings layer maps each token to a continuous vector space. This layer is\\ncrucial for the model to understand and process the text data:\\n \\nimport torch.nn as nn \\nclass Embeddings(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, vocab_size): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(Embeddings, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.embed = nn.Embedding(vocab_size, d_model) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.embed(x)\\nPositional encoding\\nThe positional encoding layer adds positional information to the embeddings, which\\nhelps the model understand the order of tokens in the sequence:\\n \\nimport math \\nclass PositionalEncoding(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, dropout=0.1, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0max_len=MAX_LEN \\n\\xa0\\xa0\\xa0\\xa0): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(PositionalEncoding, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.dropout = nn.Dropout(p=dropout) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pe = torch.zeros(max_len, d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0position = torch.arange(0.0, max_len).unsqueeze(1) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0div_term = torch.exp( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.arange(0.0, d_model, 2) * - \\\\ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0(math.log(10000.0) / d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pe[:, 0::2] = torch.sin(position * div_term) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pe[:, 1::2] = torch.cos(position * div_term) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0pe = pe.unsqueeze(0) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.register_buffer(\"pe\", pe) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x + self.pe[:, : x.size(1)] \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.dropout(x)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 90, 'page_label': '91'}, page_content='Multi-head self-attention\\nThe multi-head self-attention (MHSA) layer is a crucial part of the transformer\\narchitecture that allows the model to focus on different parts of the input sequence\\nwhen producing an output sequence:\\n \\nclass MultiHeadSelfAttention(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, nhead): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(MultiHeadSelfAttention, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.attention = nn.MultiheadAttention(d_model, nhead) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.attention(x, x, x)\\nFFN\\nThe FFN is a simple fully connected NN (FCNN) that operates independently on each\\nposition:\\n \\nclass FeedForward(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, d_ff): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(FeedForward, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.linear1 = nn.Linear(d_model, d_ff) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.dropout = nn.Dropout(0.1) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.linear2 = nn.Linear(d_ff, d_model) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return \\nself.linear2(self.dropout(torch.relu(self.linear1(x))))\\nEncoder layer\\nThe encoder layer consists of an MHSA mechanism and a simple FFNN. This\\nstructure is repeated in a stack to form the complete encoder:\\n \\nclass EncoderLayer(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, nhead, d_ff): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(EncoderLayer, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.self_attn = MultiHeadSelfAttention(d_model, nhead) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.feed_forward = FeedForward(d_model, d_ff) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.norm1 = nn.LayerNorm(d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.norm2 = nn.LayerNorm(d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.dropout = nn.Dropout(0.1) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x):'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 91, 'page_label': '92'}, page_content='x = x.transpose(0, 1) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0attn_output, _ = self.self_attn(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x + self.dropout(attn_output) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.norm1(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ff_output = self.feed_forward(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x + self.dropout(ff_output) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.norm2(x).transpose(0, 1)\\nEncoder\\nThe encoder is a stack of identical layers with an MHSA mechanism and an FFN:\\n \\nclass Encoder(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, nhead, d_ff, num_layers, \\nvocab_size): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(Encoder, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.embedding = Embeddings(d_model, vocab_size) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.pos_encoding = PositionalEncoding(d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.encoder_layers = nn.ModuleList( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[EncoderLayer(d_model, nhead, d_ff) for _ in range( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_layers)] \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.feed_forward = FeedForward(d_model, d_ff) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.embedding(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.pos_encoding(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for layer in self.encoder_layers: \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = layer(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return x\\nDecoder layer\\nSimilarly, the decoder layer consists of two MHA mechanisms—one self-attention and\\none cross-attention—followed by an FFN:\\n \\nclass DecoderLayer(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, nhead, d_ff): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(DecoderLayer, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.self_attn = MultiHeadSelfAttention(d_model, nhead) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.cross_attn = nn.MultiheadAttention(d_model, nhead) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.feed_forward = FeedForward(d_model, d_ff) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.norm1 = nn.LayerNorm(d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.norm2 = nn.LayerNorm(d_model)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 92, 'page_label': '93'}, page_content='self.norm3 = nn.LayerNorm(d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.dropout = nn.Dropout(0.1) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x, memory): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x.transpose(0, 1) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0memory = memory.transpose(0, 1) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0attn_output, _ = self.self_attn(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x + self.dropout(attn_output) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.norm1(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0attn_output, _ = self.cross_attn(x, memory, memory) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x + self.dropout(attn_output) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.norm2(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0ff_output = self.feed_forward(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = x + self.dropout(ff_output) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.norm3(x).transpose(0, 1)\\nDecoder\\nThe decoder is also a stack of identical layers. Each layer contains two MHA\\nmechanisms and an FFN:\\n \\nclass Decoder(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__(self, d_model, nhead, d_ff, num_layers, \\nvocab_size): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(Decoder, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.embedding = Embeddings(d_model, vocab_size) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.pos_encoding = PositionalEncoding(d_model) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.decoder_layers = nn.ModuleList( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0[DecoderLayer(d_model, nhead, d_ff) for _ in range( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_layers)] \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.linear = nn.Linear(d_model, vocab_size) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.softmax = nn.Softmax(dim=2) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, x, memory): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.embedding(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.pos_encoding(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for layer in self.decoder_layers: \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = layer(x, memory) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0x = self.linear(x) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return self.softmax(x)\\nThis stacking layer pattern continues to build the transformer architecture. Each block\\nhas a specific role in processing the input data and generating output translations.\\nComplete transformer'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 93, 'page_label': '94'}, page_content='The transformer model encapsulates the previously defined encoder and decoder\\nstructures. This is the primary class that will be used for training and translation tasks:\\n \\nclass Transformer(nn.Module): \\n\\xa0\\xa0\\xa0\\xa0def __init__( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_model, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0nhead, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0d_ff, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_encoder_layers,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_decoder_layers,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src_vocab_size, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_vocab_size, \\n\\xa0\\xa0\\xa0\\xa0): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0super(Transformer, self).__init__() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.encoder = Encoder(d_model, nhead, d_ff, \\\\ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_encoder_layers, src_vocab_size) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.decoder = Decoder(d_model, nhead, d_ff, \\\\ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0num_decoder_layers, tgt_vocab_size) \\n\\xa0\\xa0\\xa0\\xa0def forward(self, src, tgt): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0memory = self.encoder(src) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output = self.decoder(tgt, memory) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0return output\\nTraining function\\nThe train function iterates through the epochs and batches, calculates the loss, and\\nupdates the model parameters:\\n \\ndef train(model, loss_fn, optimizer, NUM_EPOCHS=10): \\n\\xa0\\xa0\\xa0\\xa0for epoch in range(NUM_EPOCHS): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.train() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_loss = 0 \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for batch in batch_iterator: \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0src, tgt = batch \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.zero_grad() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output = model(src, tgt) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss = loss_fn(output.view(-1, TGT_VOCAB_SIZE), \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt.view(-1)) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss.backward()\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0optimizer.step() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0total_loss += loss.item() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0print(f\"Epoch {epoch},  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Loss {total_loss / len(batch_iterator)}\")'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 94, 'page_label': '95'}, page_content='Translation function\\nThe translate function uses the trained model to translate a source text into the\\ntarget language. It generates a translation token by token and stops when an end-of-\\nsequence (EOS) token is generated or when the maximum target length is reached:\\n \\ndef translate(model, src_text, src_tokenizer, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_tokenizer, max_target_length=50 \\n): \\n\\xa0\\xa0\\xa0\\xa0model.eval() \\n\\xa0\\xa0\\xa0\\xa0src_tokens = src_tokenizer.encode(src_text).ids \\n\\xa0\\xa0\\xa0\\xa0src_tensor = torch.LongTensor(src_tokens).unsqueeze(0) \\n\\xa0\\xa0\\xa0\\xa0tgt_sos_idx = tgt_tokenizer.token_to_id(\"<sos>\") \\n\\xa0\\xa0\\xa0\\xa0tgt_eos_idx = tgt_tokenizer.token_to_id(\"<eos>\") \\n\\xa0\\xa0\\xa0\\xa0tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0) \\n\\xa0\\xa0\\xa0\\xa0for i in range(max_target_length): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0with torch.no_grad(): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0output = model(src_tensor, tgt_tensor) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0predicted_token_idx = output.argmax(dim=2)[0, -1].item() \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0if predicted_token_idx == tgt_eos_idx: \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0break \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tgt_tensor = torch.cat((tgt_tensor, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0torch.LongTensor([[predicted_token_idx]])), \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dim=1) \\n\\xa0\\xa0\\xa0\\xa0translated_token_ids = tgt_tensor[0, 1:].tolist() \\n\\xa0\\xa0\\xa0\\xa0translated_text = tgt_tokenizer.decode(translated_token_ids) \\n\\xa0\\xa0\\xa0\\xa0return translated_text\\nMain execution\\nIn the main block of the script, hyperparameters are defined, the tokenizer and model\\nare instantiated, and training and translation processes are initiated:\\n \\nif __name__ == \"__main__\": \\n\\xa0\\xa0\\xa0\\xa0NUM_ENCODER_LAYERS = 2 \\n\\xa0\\xa0\\xa0\\xa0NUM_DECODER_LAYERS = 2 \\n\\xa0\\xa0\\xa0\\xa0DROPOUT_RATE = 0.1 \\n\\xa0\\xa0\\xa0\\xa0EMBEDDING_DIM = 512 \\n\\xa0\\xa0\\xa0\\xa0NHEAD = 8 \\n\\xa0\\xa0\\xa0\\xa0FFN_HID_DIM = 2048 \\n\\xa0\\xa0\\xa0\\xa0BATCH_SIZE = 31 \\n\\xa0\\xa0\\xa0\\xa0LEARNING_RATE = 0.001 \\n\\xa0\\xa0\\xa0\\xa0en_tokenizer = train_tokenizer(EN_TEXT) \\n\\xa0\\xa0\\xa0\\xa0fr_tokenizer = train_tokenizer(FR_TEXT)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 95, 'page_label': '96'}, page_content='SRC_VOCAB_SIZE = len(en_tokenizer.get_vocab()) \\n\\xa0\\xa0\\xa0\\xa0TGT_VOCAB_SIZE = len(fr_tokenizer.get_vocab()) \\n\\xa0\\xa0\\xa0\\xa0src_tensor = tensorize_data(EN_TEXT, en_tokenizer) \\n\\xa0\\xa0\\xa0\\xa0tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer) \\n\\xa0\\xa0\\xa0\\xa0dataset = TextDataset(src_tensor, tgt_tensor) \\n\\xa0\\xa0\\xa0\\xa0model = Transformer( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0EMBEDDING_DIM, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NHEAD, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0FFN_HID_DIM, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NUM_ENCODER_LAYERS,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0NUM_DECODER_LAYERS,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0SRC_VOCAB_SIZE, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0TGT_VOCAB_SIZE, \\n\\xa0\\xa0\\xa0\\xa0) \\n\\xa0\\xa0\\xa0\\xa0loss_fn = nn.CrossEntropyLoss() \\n\\xa0\\xa0\\xa0\\xa0optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \\n\\xa0\\xa0\\xa0\\xa0batch_iterator = DataLoader( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0dataset, batch_size=BATCH_SIZE, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0shuffle=True, drop_last=True \\n\\xa0\\xa0\\xa0\\xa0) \\n\\xa0\\xa0\\xa0\\xa0train(model, loss_fn, optimizer, NUM_EPOCHS=10) \\n\\xa0\\xa0\\xa0\\xa0src_text = \"hello, how are you?\" \\n\\xa0\\xa0\\xa0\\xa0translated_text = translate( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model, src_text, en_tokenizer, fr_tokenizer) \\n\\xa0\\xa0\\xa0\\xa0print(translated_text)\\nThis script orchestrates a machine translation task from loading data to training a\\ntransformer model and eventually translating text from English to French. Initially, it\\nloads a dataset, processes the text, and establishes tokenizers to convert text to\\nnumerical data. Following this, it defines the architecture of a transformer model in\\nPyTorch, detailing each component from the embeddings’ self-attention mechanisms\\nto the encoder and decoder stacks.\\nThe script further organizes the data into batches, sets up a training loop, and defines a\\ntranslation function. Training the model on the provided English and French sentences\\nteaches it to map sequences from one language to another. Finally, it translates a\\nsample sentence from English to French to demonstrate the model’s capabilities.\\nSummary\\nThe advent of the transformer significantly propelled the field of NLP forward, serving\\nas the foundation for today’s cutting-edge generative language models. This chapter\\ndelineated the progression of NLP that paved the way for this pivotal innovation.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 96, 'page_label': '97'}, page_content='Initial statistical techniques such as count vectors and TF-IDF were adept at extracting\\nrudimentary word patterns, yet they fell short in grasping semantic nuances.\\nIncorporating neural language models marked a stride toward more profound\\nrepresentations through word embeddings. Nevertheless, recurrent networks\\nencountered hurdles in handling longer sequences. This inspired the emergence of\\nCNNs, which introduced computational efficacy via parallelism, albeit at the expense\\nof global contextual awareness.\\nThe inception of attention mechanisms emerged as a cornerstone. In 2017, Vaswani et\\nal. augmented these advancements, unveiling the transformer architecture. The\\nhallmark self-attention mechanism of the transformer facilitates contextual modeling\\nacross extensive sequences in a parallelized manner. The layered encoder-decoder\\nstructure meticulously refines representations to discern relationships indispensable for\\nendeavors such as translation.\\nThe transformer, with its parallelizable and scalable self-attention design, set new\\nbenchmarks in performance. Its core tenets are the architectural bedrock for\\ncontemporary high-achieving generative language models such as GPT.\\nIn the next chapter, we will discuss how to apply pre-trained generative models from\\nprototype to production.\\nReferences\\nThis reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nBahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly learning\\nto align and translate. arXiv preprint arXiv:1409.0473.\\nBengio, Y., Ducharme, R., and Vincent, P. (2003). A neural probabilistic language model. The\\nJournal of Machine Learning Research, 3, 1137-1155.\\nDadgar, S. M. H., Araghi, M. S., and Farahani, M. M. (2016). Improving text classification\\nperformance based on TFIDF and LSI index. 2016 IEEE International Conference on\\nEngineering & Technology (ICETECH).\\nElman, J. L. (1990). Finding structure in time. Cognitive science, 14(2), 179-211.\\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural computation,\\n9(8), 1735-1780.\\nKim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint\\narXiv:1408.5882.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 97, 'page_label': '98'}, page_content='Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed\\nrepresentations of words and phrases and their compositionality. Advances in neural\\ninformation processing systems, 26.\\nPennington, J., Socher, R., and Manning, C. (2014). GloVe: Global vectors for word\\nrepresentation. Proceedings of the 2014 conference on empirical methods in natural language\\nprocessing (EMNLP), 1532-1543.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... and\\nPolosukhin, I. (2017). Attention is all you need. Advances in neural information processing\\nsystems, 30.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 98, 'page_label': '99'}, page_content='4 \\nApplying Pretrained Generative Models: From\\nPrototype to Production\\nIn the preceding chapters, we explored the fundamentals of generative AI, explored\\nvarious generative models, such as generative adversarial networks (GANs),\\ndiffusers, and transformers, and learned about the transformative impact of natural\\nlanguage processing (NLP). As we transition into the practical aspects of applying\\ngenerative AI, we should ground our exploration in a practical example. This approach\\nwill provide a concrete context, making the technical aspects more relatable and the\\nlearning experience more engaging.\\nWe will introduce “StyleSprint,” a clothing shop looking to enhance its online\\npresence. One way to achieve this is by crafting unique and engaging product\\ndescriptions for its various products. However, manually creating captivating\\ndescriptions for a large inventory is challenging. This situation is prime opportunity for\\nthe application of generative AI. By leveraging a pretrained generative model,\\nStyleSprint can automate the crafting of compelling product descriptions, saving\\nconsiderable time and enriching the online shopping experience for its customers.\\nAs we step into the practical application of a pretrained generative large language\\nmodels (LLM), the first order of business is to set up a Python environment conducive\\nto prototyping with generative models. This setup is vital for transitioning the project\\nfrom a prototype to a production-ready state, setting the stage for StyleSprint to realize\\nits goal of automated content generation.\\nIn Chapters 2 and 3, we used Google Colab for prototyping due to its ease of use and\\naccessible GPU resources. It served as a great platform to test ideas quickly. However,\\nas we shift our focus toward deploying our generative model in a real-world setting, it\\nis essential to understand the transition from a prototyping environment such as\\nGoogle Colab to a more robust, production-ready setup. This transition will ensure our\\nsolution is scalable, reliable, and well-optimized for handling real-world traffic. In this\\nchapter, we will walk through the steps in setting up a production-ready Python\\nenvironment, underscoring the crucial considerations for a smooth transition from\\nprototype to production.\\nBy the end of this chapter, we will understand the process of taking a generative\\napplication from a prototyping environment to a production-ready setup. We will'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 99, 'page_label': '100'}, page_content='define a reliable and repeatable strategy for evaluating, monitoring, and deploying\\nmodels to production.\\nPrototyping environments\\nJupyter notebooks provide an interactive computing environment to combine code\\nexecution, text, mathematics, plots, and rich media into a single document. They are\\nideal for prototyping and interactive development, making them a popular choice\\namong data scientists, researchers, and engineers. Here is what they offer:\\nKernel: At the heart of a Jupyter notebook is a kernel, a computational engine that executes\\nthe code contained in the notebook. For Python, this is typically an IPython kernel. This kernel\\nremains active and maintains the state of your notebook’s computations while the notebook is\\nopen.\\nInteractive execution: Code cells allow you to write and execute code interactively, inspecting\\nthe results and tweaking the code as necessary.\\nDependency management: You can install and manage libraries and dependencies directly\\nwithin the notebook using pip or conda commands.\\nVisualization: You can embed plots, graphs, and other visualizations to explore data and\\nresults interactively.\\nDocumentation: Combining Markdown cells with code cells allows for well-documented,\\nself-contained notebooks that explain the code and its output.\\nA drawback to Jupyter notebooks is that they typically rely on the computational\\nresources of your personal computer. Most personal laptops and desktops are not\\noptimized or equipped to handle computationally intensive processes. Having adequate\\ncomputational resources is crucial for managing the computational complexity of\\nexperimenting with an LLM. Fortunately, we can extend the capabilities of a Jupyter\\nnotebook with cloud-based platforms that offer computational accelerators such as\\ngraphics processing units (GPUs) and tensor processing units (TPUs). For\\nexample, Google Colab instantly enhances Jupyter notebooks, making them conducive\\nto computationally intensive experimentation. Here are some of the key features of a\\ncloud-based notebook environment such as Google Colab:\\nGPU/TPU access: Provides free or affordable access to GPU and TPU resources for\\naccelerated computation, which is crucial when working with demanding machine learning\\nmodels\\nCollaboration: Permits easy sharing and real-time collaboration, similar to Google Docs\\nIntegration: Allows for easy storage and access to notebooks and data'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 100, 'page_label': '101'}, page_content='Let’s consider our StyleSprint scenario. We will want to explore a few different models\\nto generate product descriptions before deciding on one that best fits StyleSprint’s\\ngoals. We can set up a minimal working prototype in Google Colab to compare\\nmodels. Again, cloud-based platforms provide an optimal and accessible environment\\nfor initial testing, experimentation, and even some lightweight training of models.\\nHere is how we might initially set up a generative model to start experimenting with\\nautomated product description generation for StyleSprint:\\n \\n# In a Colab or Jupyter notebook \\n!pip install transformers \\n# Google Colab Jupyter notebook \\nfrom transformers import pipeline \\n# Initialize a text generation pipeline with a generative model, \\nsay GPT-Neo \\ntext_generator = pipeline( \\n\\xa0\\xa0\\xa0\\xa0\\'text-generation\\', model=\\'EleutherAI/gpt-neo-2.7B\\') \\n# Example prompt for product description generation \\nprompt = \"This high-tech running shoe with advanced cushioning \\nand support\" \\n# Generating the product description \\ngenerated_text = text_generator(prompt, max_length=100, \\ndo_sample=True) \\n# Printing the generated product description \\nprint(generated_text[0][\\'generated_text\\'])\\nOutput:\\n \\nThis high-tech running shoe with advanced cushioning and support \\ncombines the best of traditional running shoes and the latest \\ntechnologies.\\nIn this simple setup, we’re installing the transformers library, which offers a\\nconvenient interface to various pretrained models. We then initialize a text generation\\npipeline with an open source version of GPT-Neo, capable of generating coherent and\\ncontextually relevant text. This setup serves as a starting point for StyleSprint to\\nexperiment with generating creative product descriptions on a small scale.\\nLater in this chapter, we will expand our experiment to evaluate and compare multiple\\npretrained generative models to determine which best meets our needs. However,\\nbefore advancing further in our experimentation and prototyping, it is crucial to\\nstrategically pause and project forward. This deliberate forethought allows us to\\nconsider the necessary steps for effectively transitioning our experiment into a\\nproduction environment. By doing so, we ensure a comprehensive view of the project\\nfrom end to end, to align with long-term operational goals.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 101, 'page_label': '102'}, page_content='Figure 4.1: Moving from prototyping to production—the stages\\nTransitioning to production\\nAs we plan for a production setup, we should first understand the intrinsic benefits and\\nfeatures of the prototyping environment we will want to carry forward to a production\\nsetting. Many of the features of prototyping environments such as Google Colab are\\ndeeply integrated and can easily go unnoticed, so it is important to dissect and catalog\\nthe features we will need in production. For example, the following features are\\ninherent in Google Colab and will be critical in production:\\nPackage management: In Colab, installing necessary libraries is as straightforward as\\nexecuting a cell with !pip install library_name. In production, we will have to\\npreinstall libraries or make sure we can install them as needed. We must also ensure that\\nproject-specific libraries do not interfere with other projects.\\nDependency isolation: Google Colab automatically facilitates isolated dependencies, ensuring\\npackage installations and updates do not interfere with other projects. In production, we may\\nalso want to deploy various projects using the same infrastructure. Dependency isolation will\\nbe critical to prevent one project’s dependency updates from impacting other projects.\\nInteractive code execution: The interactive execution of code cells helps in testing individual\\ncode snippets, visualizing results, and debugging in real time. This convenience is not\\nnecessary in production but could be helpful for quick debugging.\\nResource accessibility: With Colab, access to GPUs and TPUs is simplified, which is crucial\\nfor running computation-intensive tasks. For production, we will want to examine our dynamic\\ncomputational needs and provision the appropriate infrastructure.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 102, 'page_label': '103'}, page_content='Data integration: Colab offers simple connectivity to data sources for analysis and modeling.\\nIn production, we can either bootstrap our environment with data (i.e., deploy data directly into\\nthe environment) or ensure connectivity to remote data sources as needed.\\nVersioning and collaboration: Tracking versions of your project code with Google Colab can\\neasily be accomplished using notebooks. Additionally, Colab is preconfigured to interact with\\nGit. Git is a distributed version control system that is widely used for tracking changes in\\nsource code during software development. In production, we will also want to integrate Git to\\nmanage our code and synchronize it with a remote code repository such as GitHub or\\nBitbucket. Remote versioning ensures that our production environment always reflects the\\nlatest changes and enables ongoing collaboration.\\nError handling and debugging: In Colab, we have direct access to the Python runtime and\\ncan typically see error messages and tracebacks in real time to help identify and resolve issues.\\nWe will want the same level of visibility in production via adequate logging of system errors.\\nIn total, we want to carry over the convenience and simplicity of our Google Colab prototyping\\nenvironment but provide the robustness and scalability required for production. To do so, we\\nwill map each of the key characteristics we laid out to a corresponding production solution.\\nThese key features should ensure a smooth transition for deploying StyleSprint’s generative\\nmodel for automated product description generation.\\nMapping features to production setup\\nTo ensure we can seamlessly transition our prototyping environment to production, we\\ncan leverage Docker, a leading containerization tool. Containerization tools package\\napplications with their dependencies for consistent performance across different\\nsystems. A containerized approach will help us replicate Google Colab’s isolated,\\nuniform environments, ensuring reliability and reducing potential compatibility issues\\nin production. The table that follows describes how we can map each of the benefits of\\nour prototyping environment to a production analog:\\nFeature Environment\\nPrototypingProduction\\nPackage\\nmanagement\\nInherent\\nthrough\\npreinstalled\\npackage\\nmanagers\\nDocker streamlines application deployment and\\nconsistency across environments including package\\nmanagers.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 103, 'page_label': '104'}, page_content='Feature Environment\\nPrototypingProduction\\nDependency\\nisolation\\nInherent\\nthrough\\nnotebooks\\nDocker can also ensure projects are cleanly isolated.\\nInteractive\\ncode\\nexecution\\nInherent\\nthrough\\nnotebooks\\nDocker helps to maintain versions of Python that\\nprovide interactive code execution by default.\\nHowever, we may want to connect an integrated\\ndevelopment environment (IDE) to our production\\nenvironment to interact with code remotely as\\nneeded.\\nResource\\naccessibility\\nInherent for\\ncloud-based\\nnotebooks\\nGPU-enabled Docker containers enhance production\\nby enabling structured GPU utilization, allowing\\nscalable, efficient model performance.\\nData\\nintegration\\nNot inherent,\\nand requires\\ncode-based\\nintegration\\nIntegrating Docker with a remote data source, such as\\nAWS S3 or Google Cloud Storage, provides secure\\nand scalable solutions for importing and exporting\\ndata.\\nVersioning\\nand\\ncollaboration\\nInherent\\nthrough\\nnotebooks\\nand\\npreconfigured\\nfor Git\\nIntegrating Docker with platforms such as GitHub or\\nGitLab enables code collaboration and\\ndocumentation.\\nError\\nhandling and\\ndebugging\\nInherent\\nthrough direct\\ninteractive\\naccess to\\nruntime\\nWe can embed Python libraries such as logging or\\nLoguru in Docker deployments for enhanced error\\ntracking in production.\\nTable 4.1: Transitioning features from Colab to production via Docker'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 104, 'page_label': '105'}, page_content='Having mapped out the features of our prototyping environment to corresponding tools\\nand practices for a production setup, we are now better prepared to implement a\\ngenerative model for StyleSprint in a production-ready environment. The transition\\nentails setting up a stable, scalable, and reproducible Python environment, a crucial\\nstep for deploying our generative model to automate the generation of product\\ndescriptions in a real-world setting. As discussed, we can leverage Docker in tandem\\nwith GitHub and its continuous integration/continuous deployment (CI/CD)\\ncapabilities, providing a robust framework for this production deployment. A CI\\npipeline automates the integration of code changes from multiple contributors into a\\nshared repository. We pair CI with CD to automate the deployment of our code to a\\nproduction environment.\\nSetting up a production-ready environment\\nSo far, we have discussed how to bridge the gap between prototyping and production\\nenvironments. Cloud-based environments such as Google Colab provide a wealth of\\nfeatures that are not inherently available in production. Now that we have a better\\nunderstanding of those characteristics, the next step is to implement a robust\\nproduction setup to ensure that our application can handle real-world traffic, scale as\\nneeded, and remain stable over time.\\nThe tools and practices in a production environment differ significantly from those in a\\nprototyping environment. In production, scalability, reliability, resource management,\\nand security become paramount, whereas, in a prototyping environment, the models\\nare only relied upon by a few users for experimentation. In production, we could\\nexpect large-scale consumption from divisions throughout the organization. For\\nexample, in the StyleSprint scenario, there may be multiple departments or sub-brands\\nhoping to automate their product descriptions.\\nIn the early stages of our StyleSprint project, we can use free and open source tools\\nsuch as Docker and GitHub for tasks such as containerization, version control, and CI.\\nThese tools are offered and managed by a community of users, giving us a cost-\\neffective solution. As StyleSprint expands, we might consider upgrading to paid or\\nenterprise editions that offer advanced features and professional support. For the\\nmoment, our focus is on leveraging the capabilities of the open source versions. Next,\\nwe will walk through the practical implementation of these tools step by step. By the\\nend, we will be ready to deploy a production-ready model-as-a-service (MaaS) for\\nautomatic product descriptions.\\nLocal development setup'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 105, 'page_label': '106'}, page_content='We begin by making sure we can connect to a production environment remotely. We\\ncan leverage an IDE, which is software that enables us to easily organize code and\\nremotely connect to the production environment.\\nVisual Studio Code\\nBegin by installing Visual Studio Code (VS Code), a free code editor by Microsoft. It\\nis preferred for its integrated Git control, terminal, and marketplace for extensions that\\nenhance its functionality. It provides a conducive environment for writing, testing, and\\ndebugging code.\\nProject initialization\\nNext, we set up a structured project directory to keep the code modular and organized.\\nWe will also initialize our working directory with Git, which enables us to synchronize\\ncode with a remote repository. As mentioned, we leverage Git to keep track of code\\nchanges and collaborate with others more seamlessly. Using the terminal window in\\nVisual Studio, we can initialize the project using three simple commands. We use\\nmkdir to create or “make” a directory. We use the cd command to change directories.\\nFinally, we use git init to initialize our project with Git. Keep in mind that this\\nassumes Git is installed. Instructions to install Git are made available on its website\\n(https://git-scm.com/).\\n \\nmkdir StyleSprint \\ncd StyleSprint \\ngit init\\nDocker setup\\nWe’ll now move on to setting up a Docker container. A Docker container is an isolated\\nenvironment that encapsulates an application and its dependencies, ensuring consistent\\noperation across different systems. For clarity, we can briefly describe the key aspects\\nof Docker as follows:\\nContainers: These are portable units comprising the application and its dependencies.\\nHost operating system’s kernel: When a Docker container is run on a host machine, it utilizes\\nthe kernel of the host’s operating system and resources to operate, but it does so in a way that\\nis isolated from both the host system and other containers.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 106, 'page_label': '107'}, page_content='Dockerfiles: These are scripts used to create container images. They serve as a blueprint\\ncontaining everything needed to run the application. This isolation and packaging method\\nprevents application conflicts and promotes efficient resource use, streamlining development\\nand deployment.\\nA containerized approach will help ensure consistency and portability. For example,\\nassume StyleSprint finds a cloud-based hosting provider that is more cost-effective;\\nmoving to the new provider is as simple as migrating a few configuration files.\\nWe can install Docker from the official website. Docker provides easy-to-follow\\ninstallation guides including support for various programming languages.\\nOnce Docker is installed, we can create a Dockerfile in the project directory to specify\\nthe environment setup. For GPU support, we will want to start from an NVIDIA\\nCUDA base image. Docker, like many other virtualized systems, operates using a\\nconcept called images. Images are a snapshot of a preconfigured environment that can\\nbe used as a starting point for a new project. In our case, we will want to start with a\\nsnapshot that integrates GPU support using the CUDA library, which is a parallel\\nprocessing library provided by NVIDIA. This library will enable the virtualized\\nenvironment (or container) to leverage any GPUs installed on the host machine.\\nLeveraging GPUs will accelerate model inferencing.\\nNow we can go ahead and create a Dockerfile with the specifications for our\\napplication:\\n \\n# Use an official NVIDIA CUDA runtime as a base image \\nFROM nvidia/cuda:11.0-base \\n# Set the working directory in the container to /app \\nWORKDIR /app \\n# Copy the current directory contents into the container at /app \\nCOPY . /app \\n# Install any needed packages specified in requirements.txt \\nRUN pip install --no-cache-dir -r requirements.txt \\n# Make port 80 available to the world outside this container \\nEXPOSE 80 \\n# Run app.py when the container launches \\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\\nThis Dockerfile serves as a blueprint that Docker follows to build our container. We\\ninitiate the process from an official NVIDIA CUDA base image to ensure GPU\\nsupport. The working directory in the container is set to /app, where we then copy the\\ncontents of our project. Following that, we install the necessary packages listed in the\\nrequirements.txt file. Port 80 is exposed for external access to our application.\\nLastly, we specify the command to launch our application, which is running app.py\\nusing the Python interpreter. This setup encapsulates all the necessary components,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 107, 'page_label': '108'}, page_content=\"including GPU support, to ensure our generative model operates efficiently in a\\nproduction-like environment.\\nRequirements file\\nWe also need a method for keeping track of our Python-specific dependencies. The\\ncontainer will include Python but will not have any indication as to what requirements\\nour Python application has. We can specify those dependencies explicitly by defining a\\nrequirements.txt file in our project directory to list all the necessary Python\\npackages:\\n \\nfastapi==0.65.2 \\ntorch==1.9.0 \\ntransformers==4.9.2 \\nuvicorn==0.14.0\\nApplication code\\nNow we can create an app.py file for our application code. This is where we will\\nwrite the code for our generative model, leveraging libraries such as PyTorch and\\nTransformers. To expose our model as a service, we will use FastAPI, a modern, high-\\nperformance framework for building web APIs. A web API is a protocol that enables\\ndifferent software applications to communicate and exchange data over the internet,\\nallowing them to use each other’s functions and services.\\nThe following snippet creates a minimal API that will serve the model responses\\nwhenever another application or software requests the /generate/ endpoint. This\\nwill enable StyleSprint to host its model as a web service. This means that other\\napplications (e.g., mobile apps, batch processes) can access the model using a simple\\nURL. We can also add exception handling to provide an informative error message\\nshould the model produce any kind of error:\\n \\nfrom fastapi import FastAPI, HTTPException \\nfrom pydantic import BaseModel \\nfrom transformers import pipeline \\n# Load the pre-trained model \\ngenerator = pipeline('text-generation',  \\n\\xa0\\xa0\\xa0\\xa0model='EleutherAI/gpt-neo-2.7B') \\n# Create the FastAPI app \\napp = FastAPI() \\n# Define the request body\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 108, 'page_label': '109'}, page_content='class GenerationInput(BaseModel): \\nprompt: str \\n# Define the endpoint \\n@app.post(\"/generate\") \\ndef generate_text(input: GenerationInput): \\ntry: \\n\\xa0\\xa0\\xa0\\xa0# Generate text based on the input prompt \\n\\xa0\\xa0\\xa0\\xa0generated_text = generator(input.prompt, max_length=150) \\n\\xa0\\xa0\\xa0\\xa0return {\"generated_text\": generated_text} \\nexcept: \\n\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=500, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0detail=\"Model failed to generate text\")\\nNow that we have a Docker setup, the next step is to deploy the application to the host\\nserver. We can streamline this process with a CI/CD pipeline. The goal is to fully\\nautomate all deployment steps, including a suite of tests to ensure that any code\\nchanges do not introduce any errors. We then leverage GitHub Actions to create a\\nworkflow that is directly integrated with a code repository.\\nCreating a code repository\\nBefore we can leverage the automation capabilities of GitHub, we will need a\\nrepository. Creating a GitHub repository is straightforward, following these steps:\\n1. Sign up/log in to GitHub: If you don’t have a GitHub account, sign up at github.com. If you\\nalready have an account, just log in.\\n2. Go to the repository creation page: Click the + icon in the top-right corner of the GitHub\\nhome page and select New repository.\\n3. Fill in the repository details:\\nRepository Name: Choose a name for your repository\\nDescription (optional): Add a brief description of your repository\\nVisibility: Select either Public (anyone can see this repository) or Private (only you\\nand the collaborators you invite can see it)\\n4. Initialize the repository with a README (optional):\\nCheck Initialize this repository with a README if you want to add a simple text\\nfile that can be updated later to provide instructions for collaborators.\\nWe can also add a .gitignore file or choose a license. A gitignore file allows\\nus to add paths or file types that should not be uploaded to the repository. For\\nexample, Python creates temporary files that are not critical to the application.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 109, 'page_label': '110'}, page_content='Adding `__pycache__/` to the gitignore file will automatically ignore all\\ncontents of that directory.\\n5. Create repository: Click the Create repository button.\\nWith our repository setup complete, we can move on to defining our CI/CD pipeline to\\nautomate our deployments.\\nCI/CD setup\\nTo create a pipeline, we will need a configuration file that outlines the stages of\\ndeployment and instructs the automation server to build and deploy our Docker\\ncontainer. Let’s look at the steps:\\n1. In our GitHub repository, we can create a new file in the .github/workflows directory\\nnamed ci-cd.yml. GitHub will automatically find any files in this directory to trigger\\ndeployments.\\n2. Open ci-cd.yml and define the following workflow:\\n \\nname: CI/CD Pipeline\\n \\non:\\n \\n\\xa0\\xa0push:\\n \\n\\xa0\\xa0\\xa0\\xa0branches:\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0- main\\n \\njobs:\\n \\n\\xa0\\xa0build-and-test:\\n \\n\\xa0\\xa0\\xa0\\xa0runs-on: ubuntu-latest\\n \\n\\xa0\\xa0steps:\\n \\n\\xa0\\xa0\\xa0\\xa0- name: Checkout code\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0uses: actions/checkout@v4'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 110, 'page_label': '111'}, page_content='- name: Build Docker image\\n \\n\\xa0\\xa0# assumes the Dockerfile is in the root (.)\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0run: docker build -t stylesprint .\\n \\n\\xa0\\xa0\\xa0\\xa0- name: Run tests\\n \\n\\xa0\\xa0# assumes a set of unit tests were defined\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0run: docker run stylesprint python -m unittest discover\\n \\ndeploy:\\n \\n\\xa0\\xa0needs: build-and-test\\n \\n\\xa0\\xa0runs-on: ubuntu-latest\\n \\n\\xa0\\xa0steps:\\n \\n\\xa0\\xa0\\xa0\\xa0- name: Checkout code\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0uses: actions/checkout@v4\\n \\n\\xa0\\xa0\\xa0\\xa0- name: Login to DockerHub\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0run: echo ${{ secrets.DOCKER_PASSWORD }} | docker login \\n-u ${{ secrets.DOCKER_USERNAME }} --password-stdin\\n \\n\\xa0\\xa0\\xa0\\xa0- name: Push Docker image\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0run: |\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0docker tag stylesprint:latest ${{ \\nsecrets.DOCKER_USERNAME }}/stylesprint:latest\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0docker push ${{ secrets.DOCKER_USERNAME \\n}}/stylesprint:latest\\nIn this setup, our workflow consists of two primary jobs: build-and-test and deploy.\\nThe build-and-test job is responsible for checking out the code from the repository,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 111, 'page_label': '112'}, page_content='building the Docker image, and executing any tests. On the other hand, the deploy job,\\nwhich relies on completing build-and-test, handles DockerHub login and pushes the\\nDocker image there. DockerHub, similar to GitHub, is a repository specifically for\\nDocker images.\\nFor authenticating with DockerHub, it is advised to securely store your DockerHub\\ncredentials in your GitHub repository. This can be done by navigating to your\\nrepository on GitHub, clicking on Settings, then Secrets, and adding\\nDOCKER_USERNAME and DOCKER_PASSWORD as new repository secrets.\\nNotice that we did not have to perform any additional steps to execute the pipeline.\\nThe workflow is designed to trigger automatically upon a push (or upload) to the main\\nbranch. Recall that the entire process relies on the Git pattern where new changes are\\nregistered through a commit or check-in of code and a push or upload of code\\nchanges. Whenever changes are pushed, we can directly observe the entire pipeline in\\naction within the Actions tab of the GitHub repository.\\nWe have now walked through all of the steps necessary to deploy our model to\\nproduction. With all of this critical setup behind us, we can now return to choosing the\\nbest model for our project. The goal is to find a model that can effectively generate\\ncaptivating product descriptions for StyleSprint. However, the variety of generative\\nmodels available requires a thoughtful choice based on our project’s needs and\\nconstraints.\\nMoreover, we want to choose the right evaluation metrics and discuss other\\nconsiderations that will guide us in making an informed decision for our project. This\\nexploration will equip us with the knowledge needed to select a model that not only\\nperforms well but also aligns with our project objectives and the technical\\ninfrastructure we have established.\\nModel selection – choosing the right pretrained\\ngenerative model\\nHaving established a minimal production environment in the previous section, we now\\nfocus on a pivotal aspect of our project – selecting the right generative model for\\ngenerating engaging product descriptions. The choice of model is crucial as it\\nsignificantly influences the effectiveness and efficiency of our solution. The objective\\nis to automate the generation of compelling and accurate product descriptions for\\nStyleSprint’s diverse range of retail products. By doing so, we aim to enrich the online\\nshopping experience for customers while alleviating the manual workload of crafting\\nunique product descriptions.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 112, 'page_label': '113'}, page_content='Our objective is to select a generative model that can adeptly handle nuanced and\\nsophisticated text generation to significantly expedite the process of creating unique,\\nengaging product descriptions, saving time and resources for StyleSprint.\\nIn selecting our model, it is important to thoroughly evaluate various factors\\ninfluencing its performance and suitability for the project.\\nMeeting project objectives\\nBefore we can select and apply evaluation methods to our model selection process, we\\nshould first make sure we understand the project objectives. This involves defining the\\nbusiness problem, identifying any technical constraints, identifying any risk associated\\nwith the model, including interpretation of model outcomes, and ascertaining\\nconsiderations for any potential disparate treatment or bias:\\nProblem definition: In our scenario, the goal is to create accurate and engaging descriptions\\nfor a wide range of retail clothing. As StyleSprint’s product range may expand, the system\\nshould scale seamlessly to accommodate a larger inventory without significantly increasing\\noperational costs. Performance expectations include compelling descriptions to attract potential\\ncustomers, accuracy to avoid misrepresentation, and prompt generation to maintain an up-to-\\ndate online catalog. Additionally, StyleSprint may apply personalized content descriptions\\nbased on a user’s shopping history. This implies that the model may have to provide product\\ndescriptions in near-real-time.\\nTechnical constraints: To maximize efficiency, there should not be any noticeable delay\\n(latency) in responses from the model API. The system should be capable of real-time updates\\nto the online catalog (as needed), and the hardware should support quick text generation\\nwithout compromising quality while remaining cost-effective, especially as the product range\\nexpands.\\nTransparency and openness: Generally, pretrained models from developers who disclose\\narchitectures and training data sources are preferred, as this level of transparency allows\\nStyleSprint to have a clear understanding of any risks or legal implications associated with\\nmodel use. Additionally, any usage restrictions imposed by using models provided as APIs,\\nsuch as request or token limitations, should be understood as they could hinder scalability for a\\ngrowing catalog.\\nBias and fairness: Identifying and mitigating biases in model outputs to ensure fair and\\nneutral representations is crucial, especially given StyleSprint’s diverse target audience.\\nEnsuring that the generated descriptions are culturally sensitive is of paramount importance.\\nFair representation ensures that the descriptions accurately and fairly represent the products to\\nall potential customers, irrespective of their individual characteristics or social backgrounds.\\nSuitability of pretraining: The underlying pretraining of generative models plays a significant\\nrole in their ability to generate meaningful and relevant text. Investigating the domains and\\ndata on which the models were pretrained or fine-tuned is important. A model pretrained on a\\nbroad dataset may be versatile but could lack domain-specific nuances. For StyleSprint, a'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 113, 'page_label': '114'}, page_content='model that is fine-tuned on fashion-related data or that has the ability to be fine-tuned on such\\ndata would be ideal to ensure the generated descriptions are relevant and appealing.\\nQuantitative metrics: Evaluating the quality of generated product descriptions for StyleSprint\\nnecessitates a combination of lexical and semantic metrics. Lexical overlap metrics measure\\nthe lexical similarity between generated and reference texts. Specifically, Bilingual\\nEvaluation Understudy (BLEU) emphasizes n-gram precision, Recall-Oriented Understudy\\nfor Gisting Evaluation (ROUGE) focuses on n-gram recall, and Metric for Evaluation of\\nTranslation with Explicit Ordering (METEOR) aims for a more balanced evaluation by\\nconsidering synonyms and stemming. For contextual and semantic evaluation, we use\\nsimilarity metrics to assess the semantic coherence and relevance of the generated descriptions,\\noften utilizing embeddings to represent text in a way that captures its meaning.\\nWe can further refine our assessment of the alignment between generated descriptions\\nand product images using models such as Contrastive Language-Image Pretraining\\n(CLIP). Recall that we used CLIP in Chapter 2 to score the compatibility between\\ncaptions and a synthesized image. In this case, we can apply CLIP to measure whether\\nour generated descriptions accurately reflect the visual aspects of the products.\\nCollectively, these evaluation techniques provide objective methods for assessing the\\nperformance of the generative model in creating effective product descriptions for\\nStyleSprint:\\nQualitative metrics: We introduce qualitative evaluation to measure nuances such as the\\nengaging and creative nature of descriptions. We also want to ensure we consider equity and\\ninclusivity in the generated content, which is critical to avoid biases or language that could\\nalienate or offend certain groups. Methods for engagement assessment could include customer\\nsurveys or A/B testing, a systematic method for testing two competing solutions. Additionally,\\nhaving a diverse group reviewing the content for equity and inclusivity could provide valuable\\ninsights. These steps help StyleSprint create captivating, respectful, and inclusive product\\ndescriptions, fostering a welcoming environment for all customers.\\nScalability: The computational resources required to run a model and the model’s ability to\\nscale with increasing data are vital considerations. Models that demand extensive\\ncomputational power may not be practical for real-time generation of product descriptions,\\nespecially as the product range expands. A balance between computational efficiency and\\noutput quality is essential to ensure cost-effectiveness and scalability for StyleSprint.\\nCustomization and fine-tuning capabilities: The ability to fine-tune or customize the model\\non domain-specific data is crucial for better aligning with brand-specific requirements.\\nExploring the availability and ease of fine-tuning can significantly impact the relevance and\\nquality of generated descriptions, ensuring that they resonate well with the brand identity and\\nproduct range of StyleSprint. In practice, some models are too large to fine-tune without\\nconsiderable resources, even when efficient methods are applied. We will explore fine-tuning\\nconsiderations in detail in the next chapter.\\nNow that we have carefully considered how we might align the model to the project’s\\ngoals, we are almost ready to evaluate our initial model selection against a few others'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 114, 'page_label': '115'}, page_content='to ensure we make the right choice. However, before benchmarking, we should\\ndedicate time to understanding one vital aspect of the model selection process: model\\nsize and computational complexity.\\nModel size and computational complexity\\nThe size of a generative model is often described by the number of parameters it has.\\nParameters in a model are the internal variables that are fine-tuned during the training\\nprocess based on the training data. In the context of neural networks used in generative\\nmodels, parameters typically refer to the weights and biases adjusted through training\\nto minimize the discrepancy between predicted outputs and actual targets.\\nMoreover, a model with more parameters can capture more complex patterns in the\\ndata, often leading to better performance on the task at hand. While larger models\\noften perform better in terms of the quality of the generated text, there’s a point of\\ndiminishing returns beyond which increasing model size yields marginal\\nimprovements. Moreover, the increased size comes with its own set of challenges:\\nComputational complexity: Larger models require more computational power and memory,\\nduring both training and inference (the phase where the model is used to make predictions or\\ngenerate new data based on the learned parameters). This can significantly increase the costs\\nand the time required to train and use the model, making it less suitable for real-time\\napplications or resource-constrained environments.\\nThe number of parameters significantly impacts the computational complexity of a\\nmodel. Each parameter in a model is a variable that must be stored in memory\\nduring computation, during both training and inference. Here are some specific\\nconsiderations for computational requirements:\\nMemory and storage: The total size of the model in memory is the product of the\\nnumber of parameters and the size of each parameter (typically a 32-bit or 64-bit\\nfloat). For instance, a model with 100 million parameters, each represented by a 32-\\nbit float, would require approximately 400 MB of memory (100 million * 32 bits =\\n400 million bits = 400 MB). Now consider a larger model, say with 10 billion\\nparameters; the memory requirement jumps to 40 GB (10 billion * 32 bits = 40\\nbillion bits = 40 GB). This requirement is just for the parameters and does not\\naccount for other data and overheads the model needs for its operations.\\nLoading into memory: When a model is used for inference, its parameters must be\\nloaded into the RAM of the machine it’s running on. For a large model with 10\\nbillion parameters, you would need a machine with enough RAM to accommodate\\nthe entire model, along with additional memory for the operational overhead, the\\ninput data, and the generated output. Suppose the model is too large to fit in memory.\\nIn that case, it may need to be sharded or distributed across multiple machines or'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 115, 'page_label': '116'}, page_content='loaded in parts, which can significantly complicate the deployment and operation of\\nthe model and also increase the latency of generating outputs.\\nSpecialized hardware requirements: Larger models require specialized hardware, such as\\npowerful GPUs or TPUs, which could increase the project costs. As discussed, models with a\\nlarge number of parameters require powerful computational resources for both training and\\ninference. Hardware accelerators such as GPUs and TPUs are often employed to meet these\\ndemands. These hardware accelerators are designed to handle the parallel computation\\ncapabilities needed for the matrix multiplications and other operations inherent in neural\\nnetwork computations, speeding up the processing significantly compared to traditional\\ncentral processing units (CPUs).\\nCloud-based infrastructure can alleviate the complexity of setup but often has\\nusage-based pricing. Understanding infrastructure costs on a granular level is vital\\nto ensuring that StyleSprint stays within its budget.\\nLatency: We’ve briefly discussed latency, but it is important to reiterate that larger models\\ntypically have higher latency, which could be a problem for applications that require real-time\\nresponses. In our case, we can process the descriptions as batches asynchronously. However,\\nStyleSprint may have projects that require fast turnarounds, requiring batches to be completed\\nin hours and not days.\\nIn the case of StyleSprint, the trade-off between model performance and size must be\\ncarefully evaluated to ensure the final model meets the project’s performance\\nrequirements while staying within budget and hardware constraints. StyleSprint was\\nhoping to have near-real-time responses to provide personalized descriptions, which\\ntypically translates to a smaller model with less computational complexity. However, it\\nwas also important that the model remains highly accurate and aligns with branding\\nstandards for tone and voice, which may require a larger model trained or fine-tuned\\non a larger dataset. In practice, we can evaluate the performance of models relative to\\nsize and complexity through benchmarking.\\nBenchmarking\\nBenchmarking is a systematic process used to evaluate the performance of different\\ngenerative models against predefined criteria. This process involves comparing the\\nmodels on various metrics to understand their strengths, weaknesses, and suitability\\nfor the project. It is an empirical method (based on observation) to obtain data on how\\nthe models perform under similar conditions, providing insights that can inform the\\ndecision-making process for model selection.\\nIn the StyleSprint scenario, benchmarking can be an invaluable exercise to navigate\\nthe trade-offs between model size, computational complexity, and the accuracy and\\ncreativity of generated descriptions.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 116, 'page_label': '117'}, page_content='For our benchmarking exercise, we can return to our Google Colab prototyping\\nenvironment to quickly load various generative models and run them through tests\\ndesigned to evaluate their performance based on the considerations outlined in the\\nprevious sections, such as computational efficiency and text generation quality. Once\\nwe have completed our evaluation and comparison, we can make a few simple changes\\nto our production application code and it will automatically redeploy. Benchmarking\\nwill be instrumental in measuring the quality of the descriptions relative to the model\\nsize and complexity. Recall that we will measure quality and overall model\\nperformance along several dimensions, including lexical and semantic similarity to a\\n“gold standard” of human-written descriptions, and a qualitative assessment performed\\nby a diverse group of reviewers.\\nThe next step is to revisit and adapt our original prototyping code to include a few\\nchallenger models and apply evaluation metrics.\\nUpdating the prototyping environment\\nFor our evaluation steps, there are a few key changes to our original experimentation\\nsetup in Google Colab. First, we will want to make sure we leverage performance\\nacceleration. Google Colab offers acceleration via GPU or TPU environments. For this\\nexperiment, we will leverage GPU. We will also want to transition from the\\nTransformers library to a slightly more versatile library such as Langchain, which\\nallows us to test both open source models such as GPT-Neo and commercial models\\nsuch as GPT-3.5.\\nGPU configuration\\nEnsure you have a GPU enabled for better performance. Returning to Google Colab,\\nwe can follow these steps to enable GPU acceleration:\\n1. Click on Runtime in the top menu (see Figure 4.2):'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 117, 'page_label': '118'}, page_content='Figure 4.2: Runtime drop-down menu\\n2. Select Change runtime type from the drop-down menu, as shown in the preceding screenshot.\\n3. In the pop-up window, select GPU from the Hardware accelerator drop-down menu (see\\nFigure 4.3):'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 118, 'page_label': '119'}, page_content='Figure 4.3: Select GPU and click on Save\\n4. Click on Save.\\nNow your notebook is set up to use a GPU to significantly speed up the computations\\nneeded for the benchmarking process. You can verify the GPU availability using the\\nfollowing code snippet:\\n \\n# Verify GPU is available \\nimport torch \\ntorch.cuda.is_available()\\nThis code snippet will return True if a GPU is available and False otherwise. This\\nsetup ensures that you have the necessary computational resources to benchmark\\nvarious generative models. The utilization of a GPU will be crucial when it comes to\\nhandling large models and extensive computations.\\nLoading pretrained models with LangChain\\nIn our first simple experiment, we relied on the Transformers library to load an open\\nsource version of GPT. However, for our benchmarking exercise, we want to evaluate'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 119, 'page_label': '120'}, page_content=\"the retail version of GPT-3 alongside open source models. We can leverage\\nLangChain, a versatile library that provides a streamlined interface, to access both\\nopen source models from providers such as Hugging Face and closed source models\\nsuch as OpenAI’s GPT-3.5. LangChain offers a unified API that simplifies\\nbenchmarking and comparison through standardization. Here are the steps to do it:\\n1. Install necessary libraries: We begin by installing the required libraries in our Colab\\nenvironment. LangChain simplifies the interaction with models hosted on OpenAI and\\nHugging Face.\\n \\n!pip -q install openai langchain huggingface_hub\\n2. Set up credentials: We obtain the credentials from OpenAI for accessing GPT-3, GPT-4, or\\nwhichever closed source model we select. We also provide credentials for the Hugging Face\\nHub, which hosts over 350,000 open source models. We must store these credentials securely\\nto prevent any unauthorized access, especially in the case where model usage has an associated\\ncost.\\n \\nimport os\\n \\nos.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'\\n \\nos.environ['HUGGINGFACEHUB_API_TOKEN'] = \\n \\n\\xa0\\xa0\\xa0\\xa0'your_huggingface_token_here'\\n3. Load models: With LangChain, we can quickly load models and generate responses. The\\nfollowing example demonstrates how to load GPT-3 and GPT-Neo from Hugging Face:\\n \\n!pip install openai langchain[llms] huggingface_hub\\n \\nfrom langchain.llms import OpenAI, HuggingFaceHub\\n \\n# Loading GPT-3\\n \\nllm_gpt3 = OpenAI(model_name='text-davinci-003',\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0temperature=0.9,\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0max_tokens = 256)\\n \\n# Loading Neo from Hugging Face\\n \\nllm_neo = HuggingFaceHub(repo_id=' EleutherAI/gpt-neo-2.7B',\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 120, 'page_label': '121'}, page_content='model_kwargs={\"temperature\":0.9}\\n \\n)\\nNotice that we have loaded two models that are significantly different in size. As the\\nmodel signature suggests, GPT-Neo was trained on 2.7 billion parameters. Meanwhile,\\naccording to information available from OpenAI, Davinci was trained on 175 billion\\nparameters. As discussed, a model that is significantly larger is expected to have\\ncaptured much more complex patterns and will likely outperform a smaller model.\\nHowever, these very large models are typically hosted by major providers and have\\nhigher usage costs. We will revisit cost considerations later. For now, we can continue\\nto the next step, which is to prepare our testing data. Our test data should provide a\\nbaseline for model performance that will inform the cost versus performance trade-off.\\nSetting up testing data\\nIn this context, testing data should comprise product attributes from the StyleSprint\\nwebsite (e.g., available colors, sizes, materials, etc.) and existing product descriptions\\nwritten by the StyleSprint team. The human-written descriptions serve as the “ground\\ntruth,” or the standard against which to compare the models’ generated descriptions.\\nWe can gather product data from existing datasets by scraping data from e-commerce\\nwebsites or using a pre-collected dataset from StyleSprint’s database. We should also\\nensure a varied collection of products to test a model’s capability across different\\ncategories and styles. The process of dividing data into distinct groups or segments\\nbased on shared characteristics is typically referred to as segmentation. Understanding\\na model’s behavior across segments should give us an indication of how well it can\\nperform across the entire family of products. For the purposes of this example, product\\ndata is made available in the GitHub companion to this book\\n(https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python).\\nLet’s see how we can extract relevant information for further processing:\\n \\nimport pandas as pd \\n# Assume `product_data.csv` is a CSV file with product data \\n# The CSV file has two columns: \\'product_image\\' and  \\n# \\'product_description\\'  \\n# Load the product data \\nproduct_data = pd.read_csv(\\'product_data.csv\\') \\n# Split the data into testing and reference sets \\ntest_data = product_data.sample(frac=0.2, random_state=42) \\nreference_data = product_data.drop(test_data.index)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 121, 'page_label': '122'}, page_content='# Checkpoint the testing and reference data \\ntest_data.to_csv(\\'test_data.csv\\', index=False) \\nreference_data.to_csv(\\'reference_data.csv\\', index=False) \\n# Extract reference descriptions and image file paths \\nreference_descriptions = / \\n\\xa0\\xa0\\xa0\\xa0reference_data[\\'product_description\\'].tolist() \\nproduct_images = reference_data[\\'product_image\\'].tolist()\\nWe must also format the product data in a way that makes it ready to be input into the\\nmodels for description generation. This could be just the product title or a combination\\nof product attributes:\\n \\n# Assume `product_metadata` is a column in the data that \\ncontains the collective information about the product including \\nthe title of the product and attributes. \\n# Format the input data for the models \\nmodel_input_data = reference_data[\\'product_metadata].tolist() \\nreference_descriptions = \\\\ \\n\\xa0\\xa0\\xa0\\xa0reference_data[\\'product_description\\'].tolist()\\nFinally, we will ask the model to generate a batch of product descriptions using each\\nmodel.\\n \\nfrom langchain import LLMChain, PromptTemplate \\nfrom tqdm.auto import tqdm \\ntemplate = \"\"\" \\nWrite a creative product description for the following product: \\n{product_metadata} \\n\"\"\" \\nPROMPT = PromptTemplate(template=template,  \\n\\xa0\\xa0\\xa0\\xa0input_variables=[\"product_metadata\"]) \\ndef generate_descriptions( \\n\\xa0\\xa0\\xa0\\xa0llm: object,  \\n\\xa0\\xa0\\xa0\\xa0prompt: PromptTemplate = PROMPT \\n) -> list: \\n\\xa0\\xa0\\xa0\\xa0# Initialize the LLM chain \\n\\xa0\\xa0\\xa0\\xa0llm_chain = LLMChain(prompt=prompt, llm=llm) \\n\\xa0\\xa0\\xa0\\xa0descriptions = [] \\n\\xa0\\xa0\\xa0\\xa0for i in tqdm(range(len(model_input_data))): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0description = llm_chain.run(model_input_data[i]) \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0descriptions.append(description) \\n\\xa0\\xa0\\xa0\\xa0return descriptions \\ngpt3_descriptions = generate_descriptions(llm_gpt3) \\ngptneo_descriptions = generate_descriptions(llm_neo)\\nNow, with the testing data set up, we have a structured dataset of product information,\\nreference descriptions, and images ready for use in the evaluation steps.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 122, 'page_label': '123'}, page_content=\"Quantitative metrics evaluation\\nNow that we have leveraged Langchain to load multiple models and prepared testing\\ndata, we are ready to begin applying evaluation metrics. These metrics capture\\naccuracy and alignment with product images and will help us assess how well the\\nmodels generate product descriptions compared to humans. As discussed, we focused\\non two categories of metrics, lexical and semantic similarity, which provide a measure\\nof how many of the same words were used and how much semantic information is\\ncommon to both the human and AI-generated product descriptions.\\nIn the following code block, we apply BLEU, ROUGE, and METEOR to evaluate the\\nlexical similarity between the generated text and the reference text. Each of these has a\\nreference-based assumption. This means that each metric assumes we are comparing\\nagainst a human reference. We have already set aside our reference descriptions (or\\ngold standard) for a diverse set of products to compare side-by-side with the generated\\ndescriptions.\\n \\n!pip install rouge sumeval nltk \\n# nltk requires an additional package \\nimport nltk \\nnltk.download('wordnet') \\n from nltk.translate.bleu_score import sentence_bleu \\nfrom rouge import Rouge \\nfrom sumeval.metrics.rouge import RougeCalculator \\nfrom nltk.translate.meteor_score import meteor_score \\ndef evaluate( \\n\\xa0\\xa0\\xa0\\xa0reference_descriptions: list,  \\n\\xa0\\xa0\\xa0\\xa0generated_descriptions: list \\n) -> tuple: \\n\\xa0\\xa0\\xa0\\xa0# Calculating BLEU score \\n\\xa0\\xa0\\xa0\\xa0bleu_scores = [ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0sentence_bleu([ref], gen)  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0for ref, gen in zip(reference_descriptions, \\ngenerated_descriptions) \\n\\xa0\\xa0\\xa0\\xa0] \\n\\xa0\\xa0\\xa0\\xa0average_bleu = sum(bleu_scores) / len(bleu_scores) \\n\\xa0\\xa0\\xa0\\xa0# Calculating ROUGE score \\n\\xa0\\xa0\\xa0\\xa0rouge = RougeCalculator() \\n\\xa0\\xa0\\xa0\\xa0rouge_scores = [rouge.rouge_n(gen, ref, 2) for ref, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0gen in zip(reference_descriptions, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0generated_descriptions)] \\n\\xa0\\xa0\\xa0\\xa0average_rouge = sum(rouge_scores) / len(rouge_scores) \\n\\xa0\\xa0\\xa0\\xa0# Calculating METEOR score \\n\\xa0\\xa0\\xa0\\xa0meteor_scores = [ meteor_score([ref.split() ], \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0gen.split()) for ref,\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 123, 'page_label': '124'}, page_content=\"gen in zip(reference_descriptions, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0generated_descriptions)] \\n\\xa0\\xa0\\xa0\\xa0average_meteor = sum(meteor_scores) / len(meteor_scores) \\n\\xa0\\xa0\\xa0\\xa0return average_bleu, average_rouge, average_meteor \\naverage_bleu_gpt3, average_rouge_gpt3, average_meteor_gpt3 = \\\\ \\n\\xa0\\xa0\\xa0\\xa0evaluate(reference_descriptions, gpt3_descriptions) \\nprint(average_bleu_gpt3, average_rouge_gpt3, \\naverage_meteor_gpt3) \\naverage_bleu_neo, average_rouge_neo, average_meteor_neo = \\\\ \\n\\xa0\\xa0\\xa0\\xa0evaluate(reference_descriptions, gptneo_descriptions) \\nprint(average_bleu_neo, average_rouge_neo, average_meteor_neo)\\nWe can evaluate the semantic coherence and relevance of the generated descriptions\\nusing sentence embeddings:\\n \\n!pip install sentence-transformers \\nfrom sentence_transformers import SentenceTransformer, util \\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2') \\ndef cosine_similarity(reference_descriptions, \\ngenerated_descriptions): \\n\\xa0\\xa0\\xa0\\xa0# Calculating cosine similarity for generated descriptions \\n\\xa0\\xa0\\xa0\\xa0cosine_scores = [util.pytorch_cos_sim( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model.encode(ref), model.encode(gen)) for ref, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0gen in zip(reference_descriptions, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0generated_descriptions)] \\n\\xa0\\xa0\\xa0\\xa0average_cosine = sum(cosine_scores) / len(cosine_scores) \\n\\xa0\\xa0\\xa0\\xa0return average_cosine \\naverage_cosine_gpt3 = cosine_similarity( \\n\\xa0\\xa0\\xa0\\xa0reference_descriptions, gpt3_descriptions) \\nprint(average_cosine_gpt3) \\naverage_cosine_neo = cosine_similarity( \\n\\xa0\\xa0\\xa0\\xa0reference_descriptions, gptneo_descriptions) \\nprint(average_cosine_neo)\\nAlignment with CLIP\\nWe again leverage the CLIP model to evaluate the alignment between generated\\nproduct descriptions and corresponding images, similar to our approach in Chapter 2.\\nThe CLIP model, adept at correlating visual and textual content, scores the congruence\\nbetween each product image and its associated generated and reference descriptions.\\nThe reference description serves as a human baseline for accuracy. These scores\\nprovide a quantitative measure of our generative model’s effectiveness at producing\\ndescriptions that correspond well to the product image. The following is a snippet from\\na component that processes the generated descriptions combined with corresponding\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 124, 'page_label': '125'}, page_content='images to generate a CLIP score. The full component code (including image pre-\\nprocessing) is available in the chapter 4 folder of this book’s GitHub repository at\\nhttps://github.com/PacktPublishing/Generative-AI-Foundations-in-Python).\\n \\nclip_model = \"openai/clip-vit-base-patch32\" \\ndef clip_scores(images, descriptions, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0model=clip_model, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0processor=clip_processor \\n): \\n\\xa0\\xa0\\xa0\\xa0scores = [] \\n\\xa0\\xa0\\xa0\\xa0# Process all images and descriptions together \\n\\xa0\\xa0\\xa0\\xa0inputs = process_inputs(processor, descriptions, images) \\n\\xa0\\xa0\\xa0\\xa0# Get model outputs \\n\\xa0\\xa0\\xa0\\xa0outputs = model(**inputs) \\n\\xa0\\xa0\\xa0\\xa0logits_per_image = outputs.logits_per_image # Image-to-text \\nlogits \\n\\xa0\\xa0\\xa0\\xa0# Diagonal of the matrix gives the scores for each image-\\ndescription pair \\n\\xa0\\xa0\\xa0\\xa0for i in range(logits_per_image.size(0)): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0score = logits_per_image[i, i].item() \\n\\xa0\\xa0\\xa0\\xa0scores.append(score) \\n\\xa0\\xa0\\xa0\\xa0return scores \\nreference_images = [ \\n\\xa0\\xa0\\xa0\\xa0load_image_from_path(image_path)  \\n\\xa0\\xa0\\xa0\\xa0for image_path in reference_data.product_image_path \\n] \\ngpt3_generated_scores = clip_scores( \\n\\xa0\\xa0\\xa0\\xa0reference_images, gpt3_descriptions \\n) \\nreference_scores = clip_scores( \\n\\xa0\\xa0\\xa0\\xa0reference_images, reference_descriptions \\n) \\n# Compare the scores \\nfor i, (gen_score, ref_score) in enumerate( \\n\\xa0\\xa0\\xa0\\xa0zip(gpt3_generated_scores, reference_scores) \\n): \\n\\xa0\\xa0\\xa0\\xa0print(f\"Image {i}: Generated Score = {gen_score:.2f},  \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Reference Score = {ref_score:.2f}\")\\nIn evaluating product descriptions using the CLIP model, the alignment scores\\ngenerated for each image-description pair are computed relative to other descriptions\\nin the batch. Essentially, CLIP assesses how well a specific description (either\\ngenerated or reference) aligns with a given image compared to other descriptions\\nwithin the same batch. For example, a score of 33.79 indicates that the description\\naligns with the image 33.79% better than the other descriptions in the batch align with'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 125, 'page_label': '126'}, page_content='that image. In comparing against the reference, we expect that the scores based on the\\ngenerated descriptions should align closely with the scores based on the reference\\ndescriptions.\\nNow that we have calculated lexical and semantic similarity to the reference scores,\\nand alignment between images and generated descriptions relative to reference\\ndescriptions, we can evaluate our models holistically and interpret the outcome of our\\nquantitative evaluation.\\nInterpreting outcomes\\nWe begin with lexical similarity, which gives us an indication of similarity in phrasing\\nand keywords between the reference and generated descriptions:\\nBLEU ROUGE METEOR\\nGPT-3.5 0.147 0.094 0.261\\nGPT-Neo 0.132 0.05 0.059\\nTable 4.2: Lexical similarity\\nIn evaluating text generated by GPT-3.5 and GPT-Neo models, we use several lexical\\nsimilarity metrics: BLEU, ROUGE, and METEOR. BLEU scores, which assess the\\nprecision of matching phrases, show GPT-3.5 (0.147) slightly outperforming GPT-Neo\\n(0.132). ROUGE scores, focusing on the recall of content, indicate that GPT-3.5\\n(0.094) better captures reference content than GPT-Neo (0.05). METEOR scores,\\ncombining both precision and recall with synonym matching, reveal a significant lead\\nfor GPT-3.5 (0.261) over GPT-Neo (0.059). Overall, these metrics suggest that GPT-\\n3.5’s generated text aligns more closely with reference standards, both in word choice\\nand content coverage, compared to that of GPT-Neo.\\nNext, we evaluate semantic similarity, which measures how closely the meanings of\\nthe generated text align with the reference text. This assessment goes beyond mere\\nword-to-word matching and considers the context and overall intent of the sentences.\\nSemantic similarity evaluates the extent to which the generated text captures the\\nnuances, concepts, and themes present in the reference text, providing insight into the\\nmodel’s ability to understand and replicate deeper semantic meanings:\\nModel Mean cosine similarity\\nGPT-3.5 0.8192'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 126, 'page_label': '127'}, page_content='GPT-Neo 0.2289\\nTable 4.3: Semantic similarity\\nThe mean cosine similarity scores reveal a stark contrast between the two models’\\nperformance in semantic similarity. GPT-3.5 shows a high degree of semantic\\nalignment with the reference text. GPT-Neo’s significantly lower score suggests a\\nrelatively poor performance, indicating that the generated descriptions were\\nfundamentally dissimilar to descriptions written by humans.\\nFinally, we review the CLIP scores, which tell us how well the generated descriptions\\nalign visually with the corresponding images. These scores, derived from a model\\ntrained to understand and correlate visual and textual data, provide a measure of the\\nrelevance and accuracy of the text in representing the visual content. High CLIP scores\\nindicate a strong correlation between the text and the image, suggesting that the\\ngenerated descriptions are not only textually coherent but also contextually appropriate\\nand visually descriptive:\\nModel Mean CLIP Reference delta\\nGPT-3.5 26.195 2.815\\nGPT-Neo 22.647 6.363\\nTable 4.4: Comparative CLIP score analysis for GPT-3.5 and GPT-Neo models\\nWe calculated the CLIP scores from the reference descriptions, which represent the\\naverage alignment score between a set of benchmark descriptions and the\\ncorresponding images. We then calculated CLIP scores for each model and analyzed\\nthe delta. In concert with our other metrics, GPT-3.5 has a clear advantage over GPT-\\nNeo, aligning more closely with the reference.\\nOverall, GPT-3.5 appears to significantly outperform GPT-Neo across all quantitative\\nmeasures. However, it is worth noting that GPT-3.5 incurs a higher cost and generally\\nhas a higher latency than GPT-Neo. In this case, the StyleSprint team would conduct a\\nqualitative analysis to accurately determine whether the GPT-Neo descriptions do not\\nalign with brand guidelines and expectations, therefore making the cost of using the\\nbetter model worthwhile. As discussed, the trade-off here is not clear-cut. StyleSprint\\nmust carefully consider that although using a commodity such as GPT-3.5 does not\\nincur computational costs directly, on-demand costs could increase significantly as\\nmodel usage rises.\\nThe contrasting strengths of the two models pose a decision-making challenge. While\\none clearly excels in performance metrics and alignment with CLIP, implying higher'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 127, 'page_label': '128'}, page_content='accuracy and semantic correctness, the other is significantly more resource-efficient\\nand scalable, which is crucial for cost-effectiveness. At this stage, it becomes critical\\nto assess model outcomes qualitatively and to engage stakeholders to help understand\\norganizational priorities.\\nWith these considerations in mind, we’ll revisit qualitative considerations such as\\ntransparency, bias, and fairness and how they play into the broader picture of\\ndeploying a responsible and effective AI system.\\nResponsible AI considerations\\nAddressing implicit or covert societal biases in AI systems is crucial to ensure\\nresponsible AI deployment. Although it may not seem obvious how a simple product\\ndescription could introduce bias, the language used can inadvertently reinforce\\nstereotypes or exclude certain groups. For instance, descriptions that consistently\\nassociate certain body types or skin tones with certain products or that unnecessarily\\ndefault to gendered language can unintentionally perpetuate societal biases. However,\\nwith a structured mitigation approach, including algorithmic audits, increased model\\ntransparency, and stakeholder engagement, StyleSprint can make sure its brand\\npromotes equity and inclusion.\\nAddressing and mitigating biases\\nWe present several considerations, as suggested by Costanza-Chock et al. in Who\\nAudits the Auditors? Recommendations from a field scan of the algorithmic auditing\\necosystem:\\nProfessional environment examination: Creating a supportive professional environment is\\ncrucial for addressing algorithmic fairness. Implementing whistleblower protections facilitates\\nthe safe reporting of biases and unfair practices while establishing processes for individuals to\\nreport harms to ensure these concerns are addressed proactively.\\nCustom versus standardized audit frameworks: While custom audit frameworks are\\nexpected, considering standardized methods may enhance rigor and transparency in bias\\nmitigation efforts. Engaging with external auditing entities could offer unbiased evaluations of\\nStyleSprint’s AI systems, aligning with the observations by Costanza-Chock et al. (2022).\\nFocusing on equity, not just equality: Equity notions acknowledge differing needs, essential\\nfor a comprehensive approach to fairness. Performing intersectional and small population\\nanalyses could help you to understand and address biases beyond legally protected classes.\\nDisclosure and transparency: Disclosing audit methods and outcomes can foster a culture of\\ntransparency and continuous improvement. Officially released audits could help you establish\\nbest practices and gain stakeholder trust.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 128, 'page_label': '129'}, page_content=\"Mixed methods analyses: As presented, a mix of technical and qualitative analyses could\\nprovide a holistic view of the system’s fairness. Engaging non-technical stakeholders could\\nemphasize qualitative analyses.\\nCommunity and stakeholder engagement: Again, involving diverse groups and domain\\nexperts in audits could ensure diverse perspectives are considered in bias mitigation efforts.\\nEstablishing feedback loops with stakeholders could facilitate continuous improvement.\\nContinuous learning and improvement: Staying updated on emerging standards and best\\npractices regarding AI fairness is crucial for continuous improvement. Fostering a culture of\\nlearning could help in adapting to evolving fairness challenges and regulatory landscapes, thus\\nensuring StyleSprint’s AI systems remain fair and responsible over time.\\nTransparency and explainability\\nGenerally, explainability in machine learning refers to the ability to understand the\\ninternal mechanics of a model, elucidating how it makes decisions or predictions based\\non given inputs. However, achieving explainability in generative models can be much\\nmore complex. As discussed, unlike discriminative machine learning models,\\ngenerative models do not have the objective of learning a decision boundary, nor do\\nthey reflect a clear notion of features or a direct mapping between input features and\\npredictions. This absence of feature-based decision-making makes traditional\\nexplainability techniques ineffective for generative foundational models such as GPT-\\n4.\\nAlternatively, we can adopt some pragmatic transparency practices, such as clear\\ndocumentation made accessible to all relevant stakeholders, to foster a shared\\nunderstanding and expectations regarding the model’s capabilities and usage.\\nThe topic of explainability is a critical space to watch, especially as generative models\\nbecome more complex and their outcomes become increasingly more difficult to\\nrationalize, which may present unknown risk implications.\\nPromising research from Anthropic, OpenAI, and others suggests that sparse\\nautoencoders—neural networks that activate only a few neurons at a time—could\\nfacilitate the identification of abstract and understandable patterns. This method could\\nhelp explain the network's behavior by highlighting features that align with human\\nconcepts.\\nFinal deployment\\nAssuming we have carefully gathered quantitative and qualitative feedback regarding\\nthe best model for the job, we can select our model and update our production\\nenvironment to deploy and serve it. We will continue to use FastAPI for creating a web\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 129, 'page_label': '130'}, page_content='server to serve our model, and Docker to containerize our application. However, now\\nthat we have been introduced to the simplicity of LangChain, we will continue to\\nleverage its simplified interface. Our existing CI/CD pipeline will ensure streamlined\\nautomatic deployment and continuous application monitoring. This means that\\ndeploying our model is as simple as checking-in our latest code. We begin with\\nupdating our dependencies list:\\n1. Update the requirements: Update the requirements.txt file in your project to include\\nthe necessary libraries:\\n \\nfastapi==0.68.0\\n \\nuvicorn==0.15.0\\n \\nopenai==0.27.0\\n \\nlangchain==0.1.0\\n2. Update the Dockerfile: Modify your Dockerfile to ensure it installs the updated requirements\\nand properly sets up the environment for running LangChain with FastAPI:\\n \\n# Use an official Python runtime as a base image\\n \\nFROM python:3.8-slim-buster\\n \\n# Set the working directory in the container to /app\\n \\nWORKDIR /app\\n \\n# Copy the current directory contents into the container at \\n/app\\n \\nCOPY . /app\\n \\n# Install any needed packages specified in requirements.txt\\n \\nRUN pip install --no-cache-dir -r requirements.txt\\n \\n# Make port 80 available to the world outside this container\\n \\nEXPOSE 80'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 130, 'page_label': '131'}, page_content='# Define environment variable\\n \\nENV NAME World\\n \\n# Run app.py when the container launches\\n \\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \\n\"80\"]\\n3. Update the FastAPI application: Modify your FastAPI application to utilize Langchain for\\ninteracting with GPT-3.5. Ensure your OpenAI API key is securely stored and accessible to\\nyour application:\\n \\nfrom fastapi import FastAPI, HTTPException, Request\\n \\nfrom langchain.llms import OpenAI\\n \\nimport os\\n \\n# Initialize FastAPI app\\n \\napp = FastAPI()\\n \\n# Setup Langchain with GPT-3.5\\n \\nllm = OpenAI(model_name=\\'text-davinci-003\\',\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0temperature=0.7,\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0max_tokens=256,\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0api_key=os.environ[\\'OPENAI_API_KEY\\'])\\n \\n@app.post(\"/generate/\")\\n \\nasync def generate_text(request: Request):\\n \\n\\xa0\\xa0\\xa0\\xa0data = await request.json()\\n \\n\\xa0\\xa0\\xa0\\xa0prompt = data.get(\\'prompt\\')'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 131, 'page_label': '132'}, page_content='if not prompt:\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0raise HTTPException(status_code=400,\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0detail=\"Prompt is required\")\\n \\n\\xa0\\xa0\\xa0\\xa0response = llm(prompt)\\n \\n\\xa0\\xa0\\xa0\\xa0return {\"generated_text\": response}\\nTesting and monitoring\\nOnce the model is deployed, perform necessary tests to ensure the setup works as\\nexpected. Continue to monitor the system’s performance, errors, and other critical\\nmetrics to ensure reliable operation.\\nBy this point, we have updated our production environment to deploy and serve GPT-\\n3.5, facilitating the generation of text based on the prompts received via the FastAPI\\napplication. This setup ensures a scalable, maintainable, and secure deployment of our\\nnew generative model. However, we should also explore some best practices regarding\\napplication reliability.\\nMaintenance and reliability\\nMaintaining reliability in our StyleSprint deployment is critical. As we employ\\nLangchain with FastAPI, Docker, and CI/CD, it’s essential to set up monitoring,\\nalerting, automatic remediation, and failover mechanisms. This section outlines a\\npossible approach to ensure continuous operation and robustness in our production\\nenvironment:\\nMonitoring tools: Integrate monitoring tools within the CI/CD pipeline to continuously track\\nsystem performance and model metrics. This step is fundamental for identifying and rectifying\\nissues proactively.\\nAlerting mechanisms: Establish alerting mechanisms to notify the maintenance team\\nwhenever anomalies or issues are detected. Tuning the alerting thresholds accurately is crucial\\nto catch issues early and minimize false alarms.\\nAutomatic remediation: Utilize Kubernetes’ self-healing features and custom scripts\\ntriggered by certain alerts for automatic remediation. This setup aims to resolve common\\nissues autonomously, reducing the need for human intervention.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 132, 'page_label': '133'}, page_content='Failover mechanisms: Implement a failover mechanism by setting up secondary servers and\\ndatabases. In case of primary server failure, these secondary setups take over to ensure\\ncontinuous service availability.\\nRegular updates via CI/CD: Employ the CI/CD pipeline for managing, testing, and deploying\\nupdates to LangChain, FastAPI, or other components of the stack. This process keeps the\\ndeployment updated and secure, reducing the maintenance burden significantly.\\nBy meticulously addressing each of these areas, you’ll be laying down a solid\\nfoundation for a reliable and maintainable StyleSprint deployment.\\nSummary\\nThis chapter outlined the process of transitioning the StyleSprint generative AI\\nprototype to a production-ready deployment for creating engaging product descriptions\\non an e-commerce platform. It started with setting up a robust Python environment\\nusing Docker, GitHub, and CI/CD pipelines for efficient dependency management,\\ntesting, and deployment. The focus then shifted to selecting a suitable pretrained\\nmodel, emphasizing alignment with project goals, computational considerations, and\\nresponsible AI practices. This selection relied on both quantitative benchmarking and\\nqualitative evaluation. We then outlined the deployment of the selected model using\\nFastAPI and LangChain, ensuring a scalable and reliable production environment.\\nFollowing the strategies outlined in this chapter will equip teams with the necessary\\ninsights and steps to successfully transition their generative AI prototype into a\\nmaintainable and value-adding production system. In the next chapter, we will explore\\nfine-tuning and its importance in LLMs. We will also weigh in on the decision-making\\nprocess, addressing when it is more beneficial to fine-tune versus zero or few-shot\\nprompting.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 133, 'page_label': '134'}, page_content='Part 2: Practical Applications of Generative AI\\nThis part focuses on the practical applications of generative AI, including fine-tuning\\nmodels for specific tasks, understanding domain adaptation, mastering prompt\\nengineering, and addressing ethical considerations. It aims to provide hands-on\\ninsights and methodologies for effectively implementing and leveraging generative AI\\nin various contexts with a focus on responsible adoption.\\nThis part contains the following chapters:\\nChapter 5, Fine-Tuning Generative Models for Specific Tasks\\nChapter 6, Understanding Domain Adaptation for Large Language Models\\nChapter 7, Mastering the Fundamentals of Prompt Engineering\\nChapter 8, Addressing Ethical Considerations and Charting a Path toward Trustworthy\\nGenerative AI'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 134, 'page_label': '135'}, page_content='5\\nFine-Tuning Generative Models for Specific\\nTasks\\nIn our narrative with StyleSprint, we described using a pre-trained generative AI\\nmodel for creating engaging product descriptions. While this model showed adeptness\\nin generating diverse content, StyleSprint’s evolving needs require a shift in focus. The\\nnew challenge is not just about producing content but also about engaging in specific,\\ntask-oriented interactions such as automatically answering customer’s specific\\nquestions about the products described.\\nIn this chapter, we introduce the concept of fine-tuning, a vital step in adapting a pre-\\ntrained model to perform specific downstream tasks. For StyleSprint, this means\\ntransforming the model from a versatile content generator to a specialized tool capable\\nof providing accurate and detailed responses to customer questions.\\nWe will explore and define a range of scalable fine-tuning techniques, comparing them\\nwith other approaches such as in-context learning. We will demonstrate advanced fine-\\ntuning methods, including parameter-efficient fine-tuning and prompt tuning, to\\ndemonstrate how they can fine-tune a model’s abilities for specific tasks such as Q&A.\\nBy the end of this chapter, we will have trained a language model to answer questions\\nand do so in a way that aligns with StyleSprint’s brand guidelines. However, before we\\nexplore the mechanics of fine-tuning and its importance in our application, we will\\nrevisit the history of fine-tuning in the context of LLMs.\\nFoundation and relevance – an introduction to\\nfine-tuning\\nFine-tuning is the process of leveraging a model pre-trained on a large dataset and\\ncontinuing the training process on a smaller, task-specific dataset to improve its\\nperformance on that task. It may also involve additional training that adapts a model to\\nthe nuances of a new domain. The latter is known as domain adaptation, which we will\\ncover in Chapter 6. The former is typically referred to as task-specific fine-tuning, and\\nit can be performed to accomplish several tasks, including Q&A, summarization,\\nclassification, and many others. For this chapter, we will focus on task-specific fine-\\ntuning to improve a general-purpose model’s performance when answering questions.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 135, 'page_label': '136'}, page_content='For StyleSprint, fine-tuning a model to handle a specific task such as answering\\ncustomer inquiries about products introduces unique challenges. Unlike generating\\nproduct descriptions, which primarily involves language generation using an out-of-\\nthe-box pre-trained model, answering customer questions requires the model to have\\nan extensive understanding of product-specific data and should have a brand-aware\\nvoice. Specifically, the model must accurately interpret and respond to questions about\\nproduct features, sizes, availability, user reviews, and many other details. It should also\\nproduce answers consistent with StyleSprint’s distinct brand tone. This task requires\\nboth generalized natural language proficiency (from pre-training) and robust\\nknowledge of product metadata and customer feedback, accomplished through fine-\\ntuning.\\nModels such as GPT initially learn to predict text through an unsupervised learning\\nprocess that involves being trained on wide-ranging and vast datasets. This pre-\\ntraining phase exposes the model to a diverse array of texts, enabling it to gain a broad\\nunderstanding of language, including syntax, grammar, and context, without any\\nspecific task-oriented guidance. However, fine-tuning applies task-oriented, supervised\\nlearning to refine the model’s capabilities to accomplish the specified task –\\nspecifically, semi-supervised learning, which, as described by Radford et al. (2018),\\ninvolves adapting the model to a specific supervised task by exposing it to a dataset\\ncomprising input sequences (x1, ..., xm) and corresponding labels (y).\\nThroughout the chapter, we will detail the fine-tuning process, including how to\\nselectively train the model on a curated dataset of product-related information and\\ncustomer interactions, enabling it to respond with the informed, brand-aligned\\nprecision that customers expect. However, fine-tuning an LLM, which could have\\nbillions of parameters, would typically require an enormous number of resources and\\ntime. This is where advanced techniques such as Parameter-Efficient Fine-Tuning\\n(PEFT) become particularly valuable in making fine-tuning accessible.\\nPEFT\\nTraditional fine-tuning methods become increasingly impractical as the model size\\ngrows due to the immense computational resources and time required to train and\\nupdate all model parameters. For most businesses, including larger organizations, a\\nclassical approach to fine-tuning is cost-prohibitive and, effectively, a non-starter.\\nAlternatively, PEFT methods modify only a small subset of a model’s parameters,\\nreducing the computational burden while still achieving state-of-the-art performance.\\nThis method is advantageous for adapting large models to specific tasks without\\nextensive retraining.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 136, 'page_label': '137'}, page_content='One such PEFT method is the Low-Rank Adaptation (LoRA) methodology,\\ndeveloped by Hu et al. (2021).\\nLoRA\\nThe LoRA method focuses on selectively fine-tuning specific components within the\\nTransformer architecture to enhance efficiency and effectiveness in LLMS. LoRA\\ntargets the weight matrices found in the self-attention module of the Transformer,\\nwhich, as discussed in Chapter 3, are key to its functionality and include four matrices:\\nw (query), w (key), w (value), and w (output). Although these matrices can be\\ndivided into multiple heads in a multi-head attention setting – where each head\\nrepresents one of several parallel attention mechanisms that process inputs\\nindependently – LoRA treats them as singular matrices, simplifying the adaptation\\nprocess.\\nLoRA’s approach involves adapting only the attention weights for downstream tasks,\\nwhile the weights in the other component of the Transformer, the feed-forward\\nnetwork (FFN), are unchanged. This decision to focus exclusively on the attention\\nweights and freeze the FFN is made for simplicity and parameter efficiency. By doing\\nso, LoRA ensures a more manageable and resource-efficient fine-tuning process,\\navoiding the complexities and demands of retraining the entire network.\\nThis selective fine-tuning strategy enables LoRA to effectively tailor the model for\\nspecific tasks while maintaining the overall structure and strengths of the pre-trained\\nmodel. This makes LoRA a practical solution for adapting LLMs to new tasks with a\\nreduced computational burden without requiring comprehensive parameter updates\\nacross the entire model (Liu et al., 2021).\\nBuilding upon the foundation of LoRA, Adaptive Low-Rank Adaptation\\n(AdaLoRA), as introduced in a study by Liu et al. (2022), represents a further\\nadvancement in PEFT methods. The key difference between LoRA and AdaLoRA lies\\nin (as the name suggests) its adaptiveness. While LoRA applies a consistent, low-rank\\napproach to fine-tuning across the model, AdaLoRA tailors the updates to the needs of\\neach layer, offering a more flexible and potentially more effective way to fine-tune\\nlarge models for specific tasks.\\nAdaLoRA\\nAdaLoRA’s key innovation lies in its adaptive allocation of the parameter budget\\namong the weight matrices of the pre-trained model. Many PEFT methods tend to\\ndistribute the parameter budget evenly across all pre-trained weight matrices,\\nq k v o'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 137, 'page_label': '138'}, page_content='potentially neglecting the varying importance of different weight parameters.\\nAdaLoRA overcomes this by assigning importance scores to these weight matrices and\\nallocating the parameter budget accordingly. Importance scores in the context of\\nAdaLoRA are metrics used to determine the significance (or importance) of different\\nweight parameters in a model, guiding the allocation of the parameter budget more\\neffectively during fine-tuning.\\nNOTE\\nParameter budget refers to the predefined limit on the number of additional parameters\\nthat can be introduced during the fine-tuning of a pre-trained model. This budget is set to\\nensure that the model’s complexity does not increase significantly, which can lead to\\nchallenges such as overfitting, increased computational costs, and longer training times.\\nAdditionally, AdaLoRA applies singular value decomposition (SVD) to efficiently\\norganize the incremental updates made during the model’s fine-tuning process. SVD\\nallows for the effective pruning of singular values associated with less critical updates,\\nreducing the overall parameter budget required for fine-tuning. It is important to note\\nthat this method also avoids the need for computationally intensive exact\\ncomputations, making the fine-tuning process more efficient.\\nAdaLoRA has been empirically tested across various domains, including natural\\nlanguage processing, question-answering, and natural language generation. Extensive\\nexperiments have demonstrated its effectiveness in improving model performance,\\nparticularly in question-answering tasks. The adaptability and efficiency of AdaLoRA\\nmake it an ideal choice for applications requiring precise and efficient model\\nadjustments for complex tasks.\\nIn the case of StyleSprint, AdaLoRA presents an opportunity to fine-tune its language\\nmodel for answering customer questions without the considerable overhead that would\\nbe incurred by traditional fine-tuning, which would require adjusting all of the model\\nparameters. By adopting AdaLoRA, StyleSprint can efficiently adapt its model to\\nhandle nuanced customer inquiries by adjusting significantly fewer parameters.\\nSpecifically, AdaLoRA’s adaptive allocation of parameter budgets means that\\nStyleSprint can optimize its model for the specific nuances of customer queries\\nwithout using extensive computational resources.\\nBy the end of this chapter, we will have fine-tuned an LLM using AdaLoRA for our\\nQ&A task. However, we should first decide whether fine-tuning is truly the right\\napproach. Prompt-based LLMs offer a viable alternative known as in-context learning,\\nwhere the model can learn from examples given in the prompt, meaning that the\\nprompt would contain the customer’s question paired with a few key historical\\nexamples of how other questions were answered. The model can infer from the\\nexamples how to answer the question at hand in a way that is consistent with the\\nexamples. In the next section, we will explore the benefits and drawbacks of in-context'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 138, 'page_label': '139'}, page_content='learning to help us determine whether fine-tuning is the best approach to enable a\\nmodel to answer very specific questions.\\nIn-context learning\\nIn-context learning is a technique where the model generates responses based on a few\\nexamples provided in the input prompt. This method leverages the model’s pre-trained\\nknowledge and the specific context or examples included in the prompt to perform\\ntasks without the need for parameter updates or retraining. The general approach,\\ndetailed in Language Models are Few-Shot Learners by Brown et al. (2020), describes\\nhow the extensive pre-training of these models enables them to perform tasks and\\ngenerate responses based on a limited set of examples paired with instructions\\nembedded within prompts. Unlike traditional methods that require fine-tuning for each\\nspecific task, in-context learning allows the model to adapt and respond based on the\\nadditional context provided at inference.\\nCentral to in-context learning is the concept of few-shot prompting, which is critical\\nfor enabling models to adapt to and perform tasks without additional training data,\\nrelying instead on their pre-trained knowledge and the context provided within input\\nprompts. For context, we’ll describe how an LLM typically works, which is known as\\nthe zero-shot approach, and contrast it to in-context learning, which uses the few-shot\\napproach:\\nZero-shot prompting: Models such as GPT respond to instruction based on their vast pre-\\ntraining and the specific task or instruction described in the input prompt. These models\\nestimate a conditional probability distribution over possible outputs for a given input sequence,\\nx. The model calculates the likelihood of a potential output sequence, y, expressed as\\nP(y|x). This computation is performed without prior examples specific to the task, relying\\nentirely on the model’s general pre-training. Meaning, the zero-shot approach has no specific\\ncontext apart from its general knowledge. For example, if we were to ask Are winter coats\\navailable in children’s sizes?, the model could not provide a specific answer about\\nStyleSprint’s inventory. It could only provide some generic answer.\\nFew-shot prompting: Using the few-shot approach, we provide the model with a prompt\\npaired with a few examples. These examples are concatenated to the prompt (represented as x)\\nto form an extended input sequence. So, our question Are winter coats available in children’s\\nsizes? might be paired with a few examples such as the following:\\nQ: Do you sell anything in children’s sizes?\\nA: Any items for children are specifically listed on the\\n“StyleSprint for Kids” page.\\nQ: What do you offer for kids?'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 139, 'page_label': '140'}, page_content='A: StyleSprint offers a variety of children’s fashions on its\\n“StyleSprint for Kids” page.\\nThe LLM then computes the probability of generating a specific output sequence, y,\\ngiven this extended input sequence, x. Mathematically, this can be conceptualized as\\nthe model estimating the joint probability distribution of y and x (where x includes\\nboth the prompt and the few-shot examples, as demonstrated previously). The model\\nuses this joint probability distribution to generate a response consistent with the\\ninstructions paired with the examples given in the input sequence.\\nIn both cases, the model’s ability to adapt its output based on the given context,\\nwhether with zero examples or a few, demonstrates the flexibility and sophistication of\\nits underlying architecture and training. However, the few-shot approach allows the\\nLLM to learn from the very specific examples provided.\\nLet’s consider how StyleSprint could apply in-context learning to answer customer\\nqueries. Performance using in-context learning (or the few-shot approach) consistently\\nreflects significant gains over zero-shot behavior (Brown et al., 2020). We can expand\\nour prior example to where a customer asks about the availability of a specific product.\\nAgain, the StyleSprint team could systematically append a few examples to each\\nprompt as follows.\\nHere is the prompt: Respond to the following {question} about product\\navailability.\\nThese are some examples:\\nExample 1:\\nCustomer query: Do you carry black leather handbags?\\nAI response: Give me a moment while I retrieve information\\nabout that particular item.\\nExample 2:\\nCustomer query: Do you have the silk scarves in blue?\\nAI response: Let me search our inventory for blue silk\\nscarves.\\nStyleSprint can provide examples that effectively help the model understand the nature\\nof the inquiry and generate a response that is informative and aligned with the\\ncompany’s policies and product offerings. In this example, we see that the responses\\nare intended to be paired with a search component. This is a common approach and\\ncan be accomplished using a technique called Retrieval Augmented Generation'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 140, 'page_label': '141'}, page_content='(RAG), which is a component that facilitates retrieval of real-time data to inform the\\ngenerated response. Combining a few-shot in-context learning approach with RAG\\ncould ensure that the system provides a logical and specific answer.\\nIn-context learning using a few-shot approach allows the model to rapidly adapt to\\nvarious customer queries using a limited set of examples. When augmented with RAG,\\nStyleSprint could potentially satisfy their use case and reduce the time and resources\\nneeded to fine-tune. However, this approach must be weighed against the depth of\\nspecialization and consistency of task-specific fine-tuning, which, as described, could\\nalso produce highly accurate answers that fit the brand tone.\\nIn the next section, we will formulate metrics that help us draw a direct comparison to\\nguide StyleSprint in making an informed decision that best suits its customer service\\nobjectives and operational framework.\\nFine-tuning versus in-context learning\\nWe learned how in-context learning could allow StyleSprint’s model to handle a\\ndiverse range of customer queries without requiring extensive retraining. Specifically,\\na few-shot approach combined with RAG could facilitate quick adaptation to new\\ninquiries, as the model can generate responses based on a few examples. However, the\\neffectiveness of in-context learning heavily relies on the quality and relevance of the\\nexamples provided in the prompts. Its success would also rely on the implementation\\nof RAG. Moreover, without fine-tuning, responses may lack consistency or may not\\nadhere as strictly to StyleSprint’s brand tone and customer service policies. Finally,\\ndepending entirely on a generative model without fine-tuning may inadvertently\\nintroduce bias, as discussed in Chapter 4.\\nIn practice, we have two very comparable and viable approaches. However, to make an\\ninformed decision, we should first perform a more in-depth comparison using\\nquantitative methods.\\nTo impartially assess the efficacy of in-context learning compared to fine-tuning, we\\ncan measure the quality and consistency of the generated responses. We can\\naccomplish this using established and reliable metrics to compare outcomes from each\\nof the approaches. Like prior evaluations, we will want to apply quantitative and\\nqualitative methods applied across the following key dimensions:\\nAlignment with human judgment: We can again apply semantic similarity to provide a\\nquantitative measure of how often the model’s responses are correct or relevant based on a\\nreference answer written by a human.\\nStyleSprint’s brand communication experts can review a subset of the responses to\\nprovide a qualitative evaluation of the response accuracy and alignment with brand'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 141, 'page_label': '142'}, page_content='tone and voice.\\nConsistency and stability: It is important to measure the degree to which questions are\\nanswered consistently each time despite minor variations in how the question is posed. Again,\\nwe can leverage semantic similarity to compare each new output to the prior when the input is\\nheld constant.\\nIn addition to evaluating the quality of model responses for each approach, we can also\\ndirectly compare the operational and computational overhead required for each.\\nFor fine-tuning, we will need to understand the overhead involved in training the\\nmodel. While the PEFT method will significantly reduce the training effort, there\\ncould be considerably more infrastructure-related costs compared to in-context\\nlearning, which requires no additional training. Alternatively, for in-context learning,\\ncommoditized models such as OpenAI’s GPT-4 have a per-token cost model.\\nStyleSprint must also consider the cost of tokens required to embed a sufficient\\nnumber of few-shot examples in the prompt.\\nIn both cases, StyleSprint will incur some operational costs to create best-in-class\\nexamples written by humans that can be used as a “gold standard” in either the few-\\nshot approach or for additional model training.\\nBy conducting these comparative tests and analyzing the results, StyleSprint will gain\\nvaluable insights into which approach – in-context learning or fine-tuning – best aligns\\nwith its operational goals and customer service standards. This data-driven evaluation\\nwill inform the decision on the optimal AI strategy for enhancing their customer\\nservice experience. We will implement these comparisons in the practice project that\\nfollows.\\nPractice project: Fine-tuning for Q&A using\\nPEFT\\nFor our practice project, we will experiment with AdaLoRA to efficiently fine-tune a\\nmodel for a customer query and compare it directly to the output of a state-of-the-art\\n(SOTA) model using in-context learning. Like the previous chapter, we can rely on a\\nprototyping environment such as Google Colab to complete the evaluation and\\ncomparison of the two approaches. We will demonstrate how to configure model\\ntraining to use AdaLoRA as our PEFT method.\\nBackground regarding question-answering fine-\\ntuning'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 142, 'page_label': '143'}, page_content='Our project utilizes the Hugging Face training pipeline library, a widely recognized\\nresource in the machine learning community. This library offers a variety of pre-built\\npipelines, including one for question-answering, which allows us to fine-tune pre-\\ntrained models with minimal setup. Hugging Face pipelines abstract much of the\\ncomplexity involved in model training, making it accessible for developers to\\nimplement advanced natural language processing tasks directly and efficiently In\\nparticular, this pipeline behaves as an interface to a transformer model with a specific\\nhead for question-answering tasks. Recall that when we fine-tune a transformer model,\\nwe keep the architecture of the model – including the self-attention mechanism and the\\ntransformer layers – but we train the model’s parameters on a specific task, which, in\\nthis case, results in a model refined specifically to answer questions. Recall our\\npractice project in Chapter 3 where the resulting model was a translator; we used a\\ntranslator head to accomplish translation from English to French. For this project, the\\n“head” is aligned to learn patterns in question-answering data.\\nHowever, when using a question-answer training pipeline, it is important to understand\\nthat the model does not simply memorize question-answer pairs, it learns the\\nconnection between questions and answers. Moreover, to answer appropriately, the\\nmodel cannot rely entirely on training. It also requires additional context as input to\\ncompose a relevant answer. To understand this further, we decompose the model\\ninferencing step as follows:\\n1. When feeding a question to a model, we must also include context relevant to the topic.\\n2. The model then determines the most relevant part of the context that answers the question. It\\ndoes this by assigning probability scores to each token (word or sub-word) in the context.\\n3. The model “thinks” of the context as a potential source for the answer and assigns each token\\ntwo scores: one score for being the start of the answer, and another for being the end of the\\nanswer.\\n4. The token with the highest “start” score and “end” score is then chosen to form the answer\\nspan. The span is what is presented to the user.\\nTo provide a concrete example, if we ask the model, Does StyleSprint have any\\nleather jackets? and provide a context of StyleSprint sells a variety of\\ncoats, jackets and outerwear, the model will process this context and identify\\nthat the most likely answer is something like Yes, StyleSprint sells a variety\\nof outerwear. However, if the answer to a question is not included in the provided\\ncontext, the model cannot generate a reliable answer. Additionally, if the context is too\\nunspecific, the model may provide a more generic answer. Like in-context learning,\\nthe fine-tuned approach for question-answering requires relevant context. This means\\nthat, in practice, the model must be integrated with a search component that can\\nretrieve additional context to pair with each question.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 143, 'page_label': '144'}, page_content='Consider our leather jacket example. When a question is received, the system could\\nperform a search of its knowledge base and retrieve any contextual information\\nrelevant to a leather jacket (e.g., a paragraph about outerwear). Again, since the model\\nwas trained to answer questions in a way that aligns with the brand tone, it will extract\\nthe relevant information from the context provided to formulate an appropriate answer.\\nNot only will integration with search provide the model with the context it needs but it\\nwill also allow the model to have up-to-date and real-time information.\\nAdditionally, we might incorporate a confidence threshold, where the model only gives\\nan answer if it assigns a high enough probability to the start and end tokens. If the\\nhighest probability is below this threshold, we might say the model does not know, or\\nrequest more information. Overall, the model efficacy relies heavily on the quality and\\nsize of the training data as well as the relevance of the context with regard to the\\nquestions posed.\\nNow that we have a better understanding of how fine-tuning for question-answering\\nworks and what to expect when using the question-answering pipeline from Hugging\\nFace, we can begin to write our implementation.\\nImplementation in Python\\nFirst and foremost, we install the required libraries:\\n \\n!pip install transformers peft sentence-transformers\\nThen, we import the question-answering modules from the transformers library. For\\nour project, we will use Google’s Flan T5 (small), which is considered a SOTA\\nalternative to GPT 3.5. As one of our goals continues to be to measure the\\nperformance versus efficiency trade-off, we begin with the smallest version of Flan T5,\\nwhich has 80M parameters. This will enable faster training and more rapid iteration.\\nHowever, please note that even a small model trained over a small number of epochs\\nwill require a high-RAM runtime environment:\\n \\nfrom transformers import ( \\n\\xa0\\xa0\\xa0\\xa0AutoModelForQuestionAnswering, AutoTokenizer) \\nmodel_name = \" google/flan-t5-small\" \\ntokenizer = AutoTokenizer.from_pretrained(model_name) \\nmodel = \\nAutoModelForQuestionAnswering.from_pretrained(model_name)\\nWith the pre-trained model instantiated, we can now configure the model to adapt its\\ntraining process to use AdaLoRA, which, as we’ve learned, is specifically designed to\\nallocate the parameter budget efficiently during the fine-tuning process:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 144, 'page_label': '145'}, page_content='from peft import AdaLoraConfig \\n# Example configuration; adjust parameters as needed \\nadapter_config = AdaLoraConfig(target_r=16) \\nmodel.add_adapter(adapter_config)\\nAs discussed, fine-tuning relies heavily on the quality and size of the training data. In\\nthe StyleSprint scenario, the company could aggregate question-answer pairs from its\\nFAQ page, social media, and customer service transcripts. For this exercise, we will\\nconstruct a simple dataset that looks similar to the following:\\n \\ndemo_data = [{ \\n\"question\": \"What are the latest streetwear trends available at \\nStylesprint?\", \\n\\xa0\\xa0\"answer\": \"Stylesprint\\'s latest streetwear collection includes \\nhoodies, and graphic tees, all inspired by the latest hip-hop \\nfashion trends.\" \\n... \\n}]\\nHowever, in order to integrate our dataset with the question-answer pipeline, we\\nshould first understand the Trainer class. The Trainer class in the Hugging Face\\ntransformers library expects the training and evaluation datasets to be in a specific\\nformat, usually as a PyTorch Dataset object, not just as simple lists of dictionaries.\\nFurther, each entry in the dataset needs to be tokenized and structured with the\\nnecessary fields such as input_ids, attention_mask, and, for question-answering\\ntasks, start_positions and end_positions. Let us explore these in more detail:\\ninput_ids: This is a sequence of integers that represent the input sentence in the model.\\nEach word or sub-word in the sentence is converted into a unique integer or ID. Recall from\\nearlier chapters that this process is known as tokenization. The words or tokens are looked up\\nin the vocabulary of the language model and the corresponding integer is then used in the\\nmodel. For example, a sentence such as I love Paris might be converted into something like\\n[101, 354, 2459].\\nattention_mask: An attention mask is a sequence of binary values where 1s indicate real\\ntokens and 0s indicate padding tokens. In other words, in the places where 1s are present, the\\nmodel will understand that those places need attention and the places with 0s will be ignored\\nby the model. This is crucial when dealing with sentences of varying lengths and dealing with\\nbatches of sentences in training models.\\nstart_positions and end_positions: These are for question-answering tasks. They\\nrepresent the indices of the start and end tokens of the answer in the tokenized form of the\\ncontext. For example, in the context Paris is the capital of France, if the question is What is\\nthe capital of France? and the answer given is Paris, after tokenization, start_position\\nand end_position will correspond to the index of Paris in the context.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 145, 'page_label': '146'}, page_content='With that understanding, we can create a class that adapts our dataset to meet the\\nexpectations of the trainer, as follows:\\n \\nfrom torch.utils.data import Dataset \\nclass StylesprintDataset(Dataset): \\n\\xa0\\xa0\\xa0def __init__(self, tokenizer, data): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenizer.pad_token = tokenizer.eos_token \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.tokenizer = tokenizer \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0self.data = data\\nFor the complete custom dataset class code, visit this book’s GitHub repository at\\nhttps://github.com/PacktPublishing/Generative-AI-Foundations-in-Python.\\nWith the training set prepared and our pipeline configured to apply the AdaLoRA\\nmethod, we can finally move to the training step. For this project, we will configure\\nthe training to run for just a few epochs, but in the StyleSprint scenario, a much more\\nrobust training process would be required:\\n \\nfrom transformers import Trainer, TrainingArguments \\n# Split the mock dataset into training and evaluation sets \\n(50/50) \\ntrain_data = StylesprintDataset( \\n\\xa0\\xa0\\xa0\\xa0tokenizer, demo_data[:len(demo_data)//2]) \\neval_data = StylesprintDataset( \\n\\xa0\\xa0\\xa0\\xa0tokenizer, demo_data[len(demo_data)//2:]) \\n# Training arguments \\ntraining_args = TrainingArguments( \\n\\xa0\\xa0\\xa0\\xa0output_dir=\"./results\",\\n\\xa0\\xa0\\xa0\\xa0num_train_epochs=10, \\n\\xa0\\xa0\\xa0\\xa0per_device_train_batch_size=16, \\n\\xa0\\xa0\\xa0\\xa0per_device_eval_batch_size=64, \\n\\xa0\\xa0\\xa0\\xa0warmup_steps=500, \\n\\xa0\\xa0\\xa0\\xa0weight_decay=0.01, \\n\\xa0\\xa0\\xa0\\xa0logging_dir=\"./logs\", \\n\\xa0\\xa0\\xa0\\xa0logging_steps=10, \\n) \\n# Initialize the Trainer \\ntrainer = Trainer( \\n\\xa0\\xa0\\xa0\\xa0model=model, \\n\\xa0\\xa0\\xa0\\xa0args=training_args, \\n\\xa0\\xa0\\xa0\\xa0train_dataset=train_data, \\n\\xa0\\xa0\\xa0\\xa0eval_dataset=eval_data \\n) \\n# Start training \\ntrainer.train()'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 146, 'page_label': '147'}, page_content='For our simple experiment, we do not expect a highly performant model; however, we\\ncan learn how to interpret the training output, which describes how well the model\\nperformed on the evaluation samples. The Trainer class will output a training\\nsummary that includes the loss metric.\\nTraining loss\\nTraining loss is a measure of how well the model is performing; a lower loss indicates\\nbetter performance. In many deep learning models, especially those dealing with\\ncomplex tasks such as language understanding, it’s common to start with a relatively\\nhigh loss. The expectation is that this value should decrease as training progresses.\\nIn the early stages of training, a high loss isn’t a cause for alarm as it commonly\\ndecreases as the model continues to learn. However, if the loss remains high, this\\nsignals that additional training may be needed. If the loss continues to be high after\\nprolonged training, the learning rate and other hyperparameters may require\\nadjustment, as an inappropriate learning rate can impact the model’s learning\\neffectiveness. Moreover, the quality and quantity of your training data should be\\nevaluated as insufficient data can hinder the training. For example, as we only use a\\nfew examples for the experiment, we expect a relatively high loss.\\nThe next step is to use our newly fine-tuned model to infer or predict. We should also\\nsecure our trained model parameters so we can reuse it without retraining:\\n \\nimport torch \\n# save parameters \\nmodel.save_pretrained(\"./stylesprint_qa_model\") \\ndef ask_question(model, question, context): \\n\\xa0\\xa0\\xa0# Tokenize the question and context \\n\\xa0\\xa0\\xa0inputs = tokenizer.encode_plus(question, context, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0add_special_tokens=True, return_tensors=\"pt\") \\n\\xa0\\xa0\\xa0# Get model predictions \\n\\xa0\\xa0\\xa0with torch.no_grad(): \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0outputs = model(**inputs) \\n\\xa0\\xa0\\xa0# Get the start and end positions \\n\\xa0\\xa0\\xa0answer_start_scores = outputs.start_logits \\n\\xa0\\xa0\\xa0answer_end_scores = outputs.end_logits \\n\\xa0\\xa0\\xa0# Find the tokens with the highest `start` and `end` scores \\n\\xa0\\xa0\\xa0answer_start = torch.argmax(answer_start_scores) \\n\\xa0\\xa0\\xa0answer_end = torch.argmax(answer_end_scores) + 1 \\n\\xa0\\xa0\\xa0# Convert the tokens to the answer string \\n\\xa0\\xa0\\xa0answer = tokenizer.convert_tokens_to_string( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0tokenizer.convert_ids_to_tokens( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0inputs[\"input_ids\"][0][answer_start:answer_end] \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0)'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 147, 'page_label': '148'}, page_content=') \\n\\xa0\\xa0\\xa0return answer \\nquestion = \"What is the return policy for online purchases?\" \\ncontext = \"\"\"Excerpt from return policy returned from search.\"\"\" \\nanswer = ask_question(model, question, context) \\nprint(answer)\\nAs discussed, we introduce context along with a question to the model, so that it can\\nidentify which fragment of the context responds most appropriately to the query.\\nConsequently, we may want to consider integrating a vector search system (such as\\nRAG) to automatically identify relevant documents from large datasets based on\\nsemantic similarities to a query. These search results may not provide specific answers,\\nbut the trained QA model can extract more precise answers from the results.\\nWith this hybrid approach, the vector search system first retrieves documents or text\\nsegments that are semantically related to the query. The QA model then analyzes this\\ncontext to identify the precise answer that aligns with StyleSprint’s guidelines and\\nexpectations.\\nEvaluation of results\\nTo evaluate our model outcomes, StyleSprint might apply the qualitative and\\nquantitative approaches we have discussed in the chapter already. For the purpose of\\nour experiment, we can measure the output of the model to a golden standard response\\nusing a simple measure for semantic similarity:\\n \\nfrom sentence_transformers import SentenceTransformer, util \\nimport pandas as pd \\n# Example of a gold standard answer written by a human \\ngs = \"Our policy at Stylesprint is to accept returns on online \\npurchases within 30 days, with the condition that the items are \\nunused and remain in their original condition.\" \\n# Example of answer using GPT 3.5 with in-context learning \\nreusing a relevant subset of the training data examples \\ngpt_35 = \"Stylesprint accepts returns within 30 days of \\npurchase, provided the items are unworn and in their original \\ncondition.\" \\n# Load your dataset \\ndataset = pd.DataFrame([ \\n\\xa0\\xa0\\xa0(gs, gpt_35, answer) \\n])# pd.read_csv(\"dataset.csv\") \\ndataset.columns = [\\'gold_standard_response\\', \\n\\xa0\\xa0\\xa0\\xa0\\'in_context_response\\', \\'fine_tuned_response\\'] \\n# Load a pre-trained sentence transformer model'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 148, 'page_label': '149'}, page_content='eval_model = SentenceTransformer(\\'all-MiniLM-L6-v2\\') \\n# Function to calculate semantic similarity \\ndef calculate_semantic_similarity(model, response, \\ngold_standard): \\n\\xa0\\xa0\\xa0\\xa0response_embedding = model.encode( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0response, convert_to_tensor=True) \\n\\xa0\\xa0\\xa0\\xa0gold_standard_embedding = model.encode(gold_standard, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0convert_to_tensor=True) \\n\\xa0\\xa0\\xa0\\xa0return util.pytorch_cos_sim(response_embedding, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0gold_standard_embedding).item() \\n# Measure semantic similarity \\ndataset[\\'in_context_similarity\\'] = dataset.apply( \\n\\xa0\\xa0\\xa0\\xa0lambda row:calculate_semantic_similarity( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0eval_model, row[\\'in_context_response\\'], \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0row[\\'gold_standard_response\\'] \\n\\xa0\\xa0\\xa0\\xa0), axis=1) \\ndataset[\\'fine_tuned_similarity\\'] = dataset.apply( \\n    lambda row:calculate_semantic_similarity( \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0eval_model, row[\\'fine_tuned_response\\'], \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0row[\\'gold_standard_response\\'] \\n\\xa0\\xa0\\xa0\\xa0), axis=1) \\n# Print semantic similarity\\nprint(\"Semantic similarity for in-context learning:\",  \\n\\xa0\\xa0\\xa0\\xa0dataset[\\'in_context_similarity\\']) \\nprint(\"Semantic similarity for fine-tuned model:\",  \\n\\xa0\\xa0\\xa0\\xa0dataset[\\'fine_tuned_similarity\\'])\\nThe results of our evaluation are as follows:\\nPEFT Flan T5 GPT 3.5T\\nFine-tuned In-context\\nSemantic Similarity 0.543 0.91\\nTable 5.1: Semantic similarity scores for fine-tuned Flan and GPT 3.5 Turbo,\\nrespectively\\nUndoubtedly, the in-context learning arrived at an answer that was much closer to our\\ngold standard reference. However, the fine-tuned model was not far behind. This tells\\nus that with a more robust training dataset and considerably more epochs, the fine-\\ntuned model could be comparable to GPT 3.5. With more iteration and\\nexperimentation, StyleSprint could have a very robust fine-tuned model to answer very\\nspecific questions for its customers.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 149, 'page_label': '150'}, page_content='Summary\\nIn this chapter, we focused on the strategic decision-making process between fine-\\ntuning and in-context learning for StyleSprint’s AI-driven customer service system.\\nWhile in-context learning, particularly few-shot learning, offers adaptability and\\nresource efficiency, it may not consistently align with StyleSprint’s brand tone and\\ncustomer service guidelines. This method relies heavily on the quality and relevance of\\nthe examples provided in the prompts, requiring careful crafting to ensure optimal\\noutcomes.\\nOn the other hand, PEFT methods such as AdaLoRA, offer a more focused approach\\nto adapt a pre-trained model to the specific demands of customer service queries.\\nPEFT methods modify only a small subset of a model’s parameters, reducing the\\ncomputational burden while still achieving high performance. This efficiency is crucial\\nfor real-world applications where computational resources and response accuracy are\\nboth key considerations.\\nUltimately, the choice between in-context learning and fine-tuning is not just a\\ntechnical decision but also a strategic one, deeply intertwined with the company’s\\noperational goals, resource allocation, and the desired customer experience. The\\nchapter suggests conducting comparative tests to assess the efficacy of both\\napproaches, evaluating outcomes at scale through reliable metrics. This data-driven\\nevaluation will inform StyleSprint’s decision on the optimal AI strategy for enhancing\\ntheir customer service experience.\\nIn summary, we now have a more complete understanding of the implications of fine-\\ntuning versus in-context learning in LLMs, specifically in the context of customer\\nservice. It highlights the need for a company like StyleSprint to make a well-informed\\nstrategic decision, balancing the depth of specialization and consistency offered by\\nfine-tuning against the adaptability and efficiency of in-context learning.\\nIn the next chapter, we will explore PEFT for domain adaptation where the outcome of\\nour training is a general-purpose model refined to understand a highly specific domain\\nlike finance or law.\\nReferences\\nThis reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving language\\nunderstanding by generative pre-training. OpenAI.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 150, 'page_label': '151'}, page_content='Hu, E. J., Shen, Y., Wallis, P., Li, Y., Wang, S., Wang, L., and Chen, W. (2021). LoRA: Low-\\nRank Adaptation of Large Language Models. ArXiv. /abs/2106.09685\\nZhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. (2023). Adaptive\\nBudget Allocation for Parameter-Efficient Fine-Tuning. ArXiv. /abs/2303.10512\\nBrown TB, Mann B, Ryder N, et al. 2020. Language Models are Few-Shot Learners.\\nArXiv:2005.14165.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 151, 'page_label': '152'}, page_content='6 \\nUnderstanding Domain Adaptation for Large\\nLanguage Models\\nIn the previous chapter, we examined how Parameter-Efficient Fine-Tuning (PEFT)\\nenhances large language models (LLMs) for specific tasks such as question-\\nanswering. In this chapter, we will be introduced to domain adaptation, a distinct fine-\\ntuning approach. Unlike task-specific tuning, domain adaptation equips models to\\ninterpret language that’s unique to specific industries or domains, addressing the gap in\\nLLMs’ understanding of specialized language.\\nTo illustrate this, we’ll introduce Proxima Investment Group, a hypothetical digital-\\nonly investment firm aiming to adapt an LLM to its specific financial language using\\ninternal data. We’ll demonstrate how modifying the LLM to process the specific\\nterminology and nuances typical in Proxima’s environment enhances the model’s\\nrelevance and effectiveness in the financial domain.\\nWe’ll also explore the practical steps Proxima might take, such as selecting relevant\\ninternal datasets for training, applying PEFT methods such as Low-Rank Adaptation\\n(LoRA) to adapt the model efficiently, and using masking techniques to refine the\\nmodel’s comprehension. Then, we’ll explore how Proxima can evaluate the success of\\nthis domain adaptation, assessing the model’s performance in tasks such as analyzing\\nfinancial trends, responding to client inquiries, and generating reports that align with\\nProxima’s internal standards and market position.\\nBy the end of this chapter, we will clearly understand the theoretical underpinnings of\\ndomain adaptation and its real-world application, particularly in a complex sector such\\nas finance, where the model’s depth of domain understanding can significantly impact\\nbusiness outcomes.\\nLet’s begin by demystifying the concept, exploring its technical underpinnings, and\\ndiscussing its importance in accomplishing domain-specific business objectives.\\nDemystifying domain adaptation – understanding\\nits history and importance\\nIn the context of generative LLMs, domain adaptation specifically tailors models such\\nas BLOOM, which have been pre-trained on extensive, generalized datasets (such as'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 152, 'page_label': '153'}, page_content='news articles and Wikipedia entries) for enhanced understanding of texts from targeted\\nsectors, including biomedical, legal, and financial fields. This type of refinement can\\nbe pivotal as LLMs, despite their vast pre-training, may not inherently capture the\\nintricate details and specialized terminology inherent to these domains. This adaptation\\ninvolves a deliberate process of realigning the model’s learned patterns to the linguistic\\ncharacteristics, terminologies, and contextual nuances prevalent in the target domain.\\nDomain adaptation operates within the ambit of transfer learning. In this broader\\nparadigm, a model’s learnings from one task are repurposed to improve its efficacy on\\na related yet distinct task. This approach capitalizes on the model’s pre-learned\\nfeatures to improve its efficiency and accuracy on the subsequent task, markedly\\nreducing its reliance on large volumes of domain-specific data and computational\\nresources. Specifically, we begin with a model that’s been trained on broad datasets\\nand use it as a starting point to adapt to specialized domains thereby augmenting their\\naccuracy, relevance, and applicability to more targeted use cases.\\nIn practice, several methodologies can be employed to tailor the model to specific\\ndomains, including the following:\\nContinued pre-training: The model undergoes additional pre-training on domain-specific\\ncorpora, allowing its parameters to be adapted incrementally to the target domain’s linguistic\\nfeatures, as highlighted in research by Gururangan et al. 2020.\\nIntermediate task training: Here, the model is trained on intermediate tasks, utilizing\\ndomain-specific data before being fine-tuned for downstream applications. This step facilitates\\na more robust adaptation to the domain (Pruksachatkun et al., 2020).\\nData augmentation: Techniques such as back translation (Xie et al., 2019) and token\\nreplacement (Anaby-Tavor et al., 2020) are leveraged to generate synthetic domain-specific\\ntraining examples from limited actual data:\\nBack translation entails translating an existing text from one language (for example,\\nEnglish) into another (for example, French) and then translating it back to the\\noriginal language. This process generates paraphrased versions of the original text\\nwhile preserving its semantics.\\nToken replacement involves altering individual words within a sentence to generate\\nnew sentences. This alteration usually aims to preserve the semantic meaning of the\\noriginal sentence while introducing variations.\\nMulti-task learning: This framework concurrently optimizes the model for both generic and\\ndomain-specific tasks during the adaptation phase, as demonstrated by Clark et al. 2019.\\nAs domain adaptation techniques evolve, they increasingly enhance model\\nperformance in specialized fields, even with reduced amounts of domain-specific data.\\nAs discussed in Chapter 4, more recent developments have focused on the\\ncomputational efficiency of these techniques. Adaptation methods such as LoRA'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 153, 'page_label': '154'}, page_content=\"facilitate significant model adjustments with minimal parameter changes without\\nrequiring comprehensive retraining. It is important to note that a model's performance\\nwill always vary based on various factors like the quality of the dataset, available\\ncomputational resources, and other implementation details.\\nNow that we have some insight into domain adaptation techniques and their focus on\\ncomputational efficiency, we can apply these concepts practically. Our practice project\\nwill leverage BLOOM, a state-of-the-art, open source LLM, to demonstrate domain\\nadaptation for the finance sector. Leveraging PEFT, we aim to fine-tune BLOOM with\\nminimal computational resources, illustrating the practical application of these\\nadvanced adaptation methods in enhancing model performance within the finance\\ndomain.\\nPractice project: Transfer learning for the\\nfinance domain\\nThis project aims to fine-tune BLOOM on a curated corpus of specific documents to\\nimbue it with the ability to interpret and articulate concepts specific to Proxima and its\\nproducts.\\nOur methodology is inspired by strategies for domain adaptation across various fields,\\nincluding biomedicine, finance, and law. A noteworthy study conducted by Cheng et\\nal. in 2023 called Adapting Large Language Models via Reading Comprehension\\npresents a novel approach for enhancing LLMs’ proficiency in domain-specific tasks.\\nThis approach repurposed extensive pre-training corpora into formats conducive to\\nreading comprehension tasks, significantly improving the models’ functionality in\\nspecialized domains. In our case, we will apply a similar but simplified approach to\\ncontinued pre-training by fine-tuning the pre-trained BLOOM model using a bespoke\\ndataset specific to Proxima, effectively continuing the model’s training. This process\\nadjusts the model parameters incrementally to ensure that it understands the language\\nunique to Proxima’s products and offerings better.\\nTraining methodologies for financial domain\\nadaptation\\nFour our continued training strategy, we’ll employ causal language modeling (CLM).\\nThis approach is part of a broader set of training methodologies that optimize model\\nperformance for various objectives. Before moving to implementation, let's try to\\ndisambiguate our chosen approach from other popular strategies to better understand\\nthe CLM methodology:\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 154, 'page_label': '155'}, page_content='Masked Language Modeling(MLM): A cornerstone of Transformer-based models such as\\nBERT, MLM randomly masks parts of the input text and challenges the model to predict the\\nmasked tokens. By considering the entire context around the mask (both before and after),\\nMLM enables a model to develop a bidirectional understanding of language, enriching its\\ngrasp of context and semantics.\\nNext-Sentence Prediction(NSP): This methodology further broadens a model’s narrative\\nunderstanding by training it to discern whether two sentences logically follow each other. NSP\\nis instrumental in teaching models about text structure and coherence, enabling them to\\nconstruct and comprehend logical sequences within larger bodies of text.\\nCLM: Our chosen path for BLOOM’s adaptation diverges here, embracing CLM for its\\nfocused, sequential prediction capabilities. Unlike MLM, which looks both ways (before and\\nafter the masked token), CLM adopts a unidirectional approach, predicting each subsequent\\ntoken based solely on the preceding context. This method is intrinsically aligned with natural\\nlanguage generation, making it especially suitable for crafting coherent, contextually rich\\nnarratives in the target domain.\\nIn selecting CLM for BLOOM’s adaptation, we’ll extend the model’s generative\\ncapabilities to produce text sequences that are not only logically structured but also\\ndeeply embedded with the nuance of the target domain. CLM’s unidirectional nature\\nensures that each token that’s generated is informed by a cohesive understanding of the\\npreceding text, enabling the model to generate detailed, accurate, and domain-specific\\ntexts.\\nOnce fine-tuning is complete, we can evaluate the efficacy of the domain-adapted\\nBLOOM model based on its proficiency in generating contextually relevant and\\ndomain-specific narratives. We’ll compare the adapted model’s performance against\\nthe original model with a special focus on the model’s fluency, accuracy, and overall\\ncomprehension of the target domain.\\nAs we’ve done previously, we’ll leverage Google Colab for our initial prototyping\\nphase. As Chapters 4 and 5 described, Google Colab offers a preconfigured\\nenvironment that simplifies the process of testing our methodologies before we\\nconsider promoting them to production environments. All the code in this chapter is\\navailable in the Chapter 6 folder of this book’s GitHub repository\\n(https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python).\\nWe’ll begin with the initial setup, which involves loading a smaller variation of\\nBLOOM-1b1 using the Transformers library. We’ll also import the methods that we’ll\\nneed to apply PEFT. For this example, we’ll rely on a few libraries that can be\\ninstalled as follows:\\n \\npip install sentence-transformers transformers peft datasets\\nOnce installed, we can begin importing:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 155, 'page_label': '156'}, page_content='from transformers import ( \\n\\xa0\\xa0\\xa0\\xa0AutoTokenizer, AutoModelForCausalLM) \\nfrom peft import AdaLoraConfig, get_peft_model\\nThe next step is to load the tokenizer and model:\\n \\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-\\n1b1\") \\nmodel = AutoModelForCausalLM.from_pretrained( \\n\\xa0\\xa0\\xa0\\xa0\"bigscience/bloom-1b1\")\\nAs discussed previously, we’re incorporating PEFT for efficient adaptation:\\n \\nadapter_config = AdaLoraConfig(target_r=16) \\nmodel.add_adapter(adapter_config)\\nThe PEFT technique, specifically through AdaLoraConfig, allows us to introduce a\\ncompact, efficient layer so that we can adapt the model to new contexts – here, the\\nfinance domain – with a significantly reduced number of trainable parameters:\\n \\nmodel = get_peft_model(model, adapter_config) \\nmodel.print_trainable_parameters()\\nWe must integrate the adapter to finalize the PEFT model setup, effectively creating a\\nmodel variant that’s optimized for our domain-specific training while focusing on\\nefficiency. We can quantify this by examining the number of trainable parameters our\\nmodel will use:\\n \\ntrainable params: 1,769,760 || all params: 1,067,084,088 || \\ntrainable%: 0.1658500974667331\\nThe preceding code provides us with the following information:\\nTrainable parameters: 1,769,760\\nTotal parameters in the model: 1,067,084,088\\nPercentage of trainable parameters: 0.166%\\nThis means that out of over 1 billion parameters in the BLOOM-1b1 model, only\\nabout 1.77 million parameters are being fine-tuned for the finance domain adaptation.\\nThis small percentage (0.166%) of trainable parameters highlights the efficiency of\\nPEFT, allowing significant model adaptability with minimal adjustments. This is\\ncrucial for practical applications as it reduces both computational costs and the time\\nrequired for training.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 156, 'page_label': '157'}, page_content='Next, we’ll move on to preparing the data. We’ll assume we have assembled texts\\nencompassing the breadth of knowledge about specialized Proxima products and\\nofferings such as the Proxima Passkey. CLM training requires distinct testing and\\ntraining phases to evaluate the model’s ability to accurately predict the next token in a\\nsequence. This ensures it generalizes well beyond the training data to unseen text.\\nDuring training, the loss calculation measures the difference between the model’s\\npredicted token probabilities and the actual tokens. It guides the model to adjust its\\nparameters to minimize this loss, improving its predictive accuracy over iterations. As\\nsuch, we must define training and testing texts as our dataset. An example dataset is\\nincluded in this book’s GitHub repository (linked earlier in the chapter).\\n \\ndataset = load_dataset(\"text\", \\n\\xa0\\xa0\\xa0\\xa0data_files={\"train\": \"./train.txt\", \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"test\": \"./test.txt\"} \\n\\xa0\\xa0\\xa0\\xa0)\\nNext, we must apply preprocessing and tokenization. Texts are cleaned, standardized,\\nand then converted into a numerical format (tokens) that the model can process. We\\nmust also truncate or pad texts to fit the model’s input size constraints and prepare\\nlabels for CLM training, where the model learns to predict each subsequent token.\\nTruncation and padding are preprocessing steps that are used to standardize the length\\nof input texts for machine learning models, particularly those with fixed input size\\nconstraints like many language models. Truncation removes parts of the text to\\nshorten inputs that exceed the model’s maximum length, ensuring they fit within the\\nspecified size limit. Padding adds filler values (often zeros) to shorter inputs to extend\\nthem to the required length, allowing for consistent input dimensions across the\\ndataset. Consistent input dimensions are necessary to ensure uniformity in matrix\\noperations and computations across the entire dataset since LLMs, like other models\\nthat rely on deep learning, process inputs through layers of functions that require\\nfixed-size vectors or matrices. In this case, we’ll set the sequence length to a maximum\\nof 512 tokens so that it aligns with the model’s architecture:\\n \\ndef preprocess_function(examples): \\n\\xa0\\xa0\\xa0\\xa0inputs = tokenizer(examples[\"text\"], truncation=True, \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0padding=\"max_length\", max_length=512) \\n\\xa0\\xa0\\xa0\\xa0inputs[\"labels\"] = inputs[\"input_ids\"].copy() \\n\\xa0\\xa0\\xa0\\xa0return inputs\\nThe TrainingArguments class configures the training process, setting parameters\\nsuch as the batch size, number of epochs, and the directory for saving model\\ncheckpoints. This configuration is crucial for efficient learning and model evaluation.\\nMeanwhile, the Trainer class orchestrates the model’s training process. Again,'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 157, 'page_label': '158'}, page_content='continued training gradually adapts the model’s parameters to generate and understand\\ntext related to the Proxima Passkey:\\n \\nfrom transformers import Trainer, TrainingArguments \\ntraining_args = TrainingArguments( \\n\\xa0\\xa0\\xa0\\xa0output_dir=\"./model_output\", \\n\\xa0\\xa0\\xa0\\xa0per_device_train_batch_size=2, \\n\\xa0\\xa0\\xa0\\xa0num_train_epochs=5, \\n\\xa0\\xa0\\xa0\\xa0logging_dir=\\'./logs\\', \\n\\xa0\\xa0\\xa0\\xa0logging_steps=10, \\n\\xa0\\xa0\\xa0\\xa0load_best_model_at_end=True, \\n\\xa0\\xa0\\xa0\\xa0prediction_loss_only=True, \\n) \\ntrainer = Trainer( \\n\\xa0\\xa0\\xa0\\xa0model=model, \\n\\xa0\\xa0\\xa0\\xa0args=training_args, \\n\\xa0\\xa0\\xa0\\xa0train_dataset=tokenized_datasets[\"train\"], \\n\\xa0\\xa0\\xa0\\xa0eval_dataset=tokenized_datasets[\"test\"], \\n) \\ntrainer.train() \\nmodel.save_pretrained(\"./proxima_da_model\")\\nGenerally, our configuration specifies the training parameters and initializes the\\nTrainer class while focusing on domain adaptation. The TrainingArguments class\\nis tailored to manage the training process efficiently, including logging and model-\\nsaving strategies. Remember that the batch size we choose for training the model\\nbalances the GPU’s memory capacity and how quickly the model learns from the\\ndataset. A larger batch size allows more data to be processed at once, speeding up\\ntraining but requiring more memory, which can be a limitation if the GPU has\\nrestricted capacity. Conversely, a smaller batch size means the model updates its\\nweights more frequently with fewer samples, which can benefit learning but results in\\nslower overall progress through the dataset.\\nWith training complete, we can use the adapted model to generate text based on\\nprompts related to the Proxima Passkey. The model considers the prompt, generates a\\nsequence of tokens representing the continuation, and then decodes this sequence back\\ninto human-readable text:\\n \\ndef predict(model, prompt=\"The Proxima Passkey is\"): \\n\\xa0\\xa0\\xa0\\xa0inputs = tokenizer(prompt, return_tensors=\"pt\") \\n\\xa0\\xa0\\xa0\\xa0output = model.generate(**inputs, max_length=50) \\n\\xa0\\xa0\\xa0\\xa0return tokenizer.decode(output[0], skip_special_tokens=True)\\nNotice the model.generate() function, which takes tokenized input and produces a\\nsequence of tokens as output. These tokens are then decoded into text.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 158, 'page_label': '159'}, page_content='In this example, we adapted the BLOOM language model so that it specializes in the\\nfinance domain. This involved loading the pre-trained model, applying a PEFT adapter\\nfor efficient domain adaptation, and preparing a financial dataset for model training\\nthrough standardization and tokenization. After fine-tuning BLOOM with this domain-\\nspecific data, we used the model to generate text relevant to the finance sector. The\\nfinal step is to evaluate this adapted model’s performance compared to the original\\npre-trained version, focusing on its effectiveness in accurately handling financial\\nlanguage and concepts.\\nEvaluation and outcome analysis – the ROUGE\\nmetric\\nQuantitative and qualitative evaluations are essential to assess the adapted BLOOM\\nmodel against the original, especially in the context of Proxima’s language.\\nQuantitatively, the model’s output is compared against a reference dataset that mirrors\\nProxima’s product language using the ROUGE metric. This comparison helps\\nmeasure the overlap in key terms and styles. Additionally, it’s beneficial to develop\\nspecific metrics for evaluating the model’s proficiency in terms of financial\\nterminology and concepts relevant to Proxima:\\n \\nfrom rouge import Rouge \\n# Example reference text (what we expect the model to generate \\nafter training on a complete dataset) \\nreference = \"Proxima\\'s Passkey enables seamless integration of \\ndiverse financial portfolios, offering unparalleled access to \\nglobal investment opportunities and streamlined asset \\nmanagement.\" \\n# Example predicted model output \\npredicted = \"The Proxima Passkey provides a unified platform for \\nmanaging various investment portfolios, granting access to \\nworldwide investment options and efficient asset control.\" \\n# Initialize the Rouge metric \\nrouge = Rouge() \\n# Compute the Rouge scores \\nscores = rouge.get_scores(predicted, reference) \\nprint(scores)\\nThe ROUGE score would be calculated by comparing the two texts in this example.\\nThe score measures the overlap between the predicted output and the reference text in\\nterms of n-grams (sequences of words). For instance, ROUGE-N (where N can be 1,\\n2, or L) calculates the overlap of n-grams between the predicted and reference texts:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 159, 'page_label': '160'}, page_content='ROUGE-1 evaluates the overlap of unigrams (individual words) between the predicted and\\nreference texts\\nROUGE-2 assesses the overlap of bigrams (two-word phrases) between the texts\\nROUGE-L focuses on the longest common subsequence, which is useful for evaluating\\nsentence-level structure similarity\\nThe ROUGE scores range from 0 to 1 and quantify the similarity between the\\npredicted text and a reference text, providing insights into how well a model’s output\\nmatches the expected content. Scores closer to 1 indicate higher similarity or overlap,\\nwhile scores near 0 suggest little to no commonality. These scores are divided into\\nthree key components – precision, recall, and the F1 score:\\nPrecision measures the proportion of words in the predicted text that are also found in the\\nreference text. A high precision score indicates that most of the words generated by the model\\nare relevant and appear in the reference, signifying accuracy in the model’s output.\\nRecall assesses the proportion of words from the reference text that are captured in the model’s\\nprediction. High recall implies that the model effectively includes most of the relevant content\\nfrom the reference in its output, indicating comprehensiveness.\\nThe F1 score is the harmonic mean of precision and recall, balancing the two. It is especially\\nuseful for understanding the model’s overall accuracy in generating text that is both relevant\\n(precision) and comprehensive (recall). The F1 score is crucial when equal importance is given\\nto precision and recall in evaluating the model’s performance.\\nHere’s the output:\\nMetric Recall (r) Precision (p) F1 Score (f)\\nROUGE-1 0.35 0.333 0.341\\nROUGE-2 0.053 0.048 0.05\\nROUGE-L 0.35 0.333 0.341\\nTable 6.1: ROUGE metric outcomes\\nThese scores indicate a moderate level of unigram overlap (ROUGE-1) between the\\ntexts but a significantly lower bigram overlap (ROUGE-2). The similarity between the\\nROUGE-1 and ROUGE-L scores suggests the model captures individual key terms to\\nsome extent but may struggle with longer phrase structures, pointing to areas for\\nmodel improvement.\\nOverall, while the model demonstrates a basic grasp of key individual terms (as shown\\nby ROUGE-1 and ROUGE-L), its ability to replicate more complex structures or'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 160, 'page_label': '161'}, page_content='phrases from the reference text (as indicated by ROUGE-2) is quite limited. This\\nsuggests that while the model has some understanding of the domain-specific\\nlanguage, further fine-tuning is required for it to effectively replicate the more nuanced\\nand structured aspects of the reference texts. Keep in mind that, as we have seen in\\nother chapters, semantic similarity is also a good measure of domain-specific language\\nunderstanding and does not rely on lexical overlap the way ROUGE does.\\nQualitatively, domain experts should review the model’s outputs to judge their\\nrelevance and accuracy in the context of Proxima’s products and institutional\\nlanguage. These experts can provide insights into the nuances of the model’s\\nperformance, which might not be captured by quantitative metrics alone. Comparing\\ntheir feedback on the outputs from both the original and adapted models will highlight\\nhow well the adaptation has aligned BLOOM with Proxima’s specific communication\\nneeds. This dual approach ensures a comprehensive evaluation, blending statistical\\nanalysis with real-world applicability and relevance.\\nSummary\\nIn this chapter, we explored the domain adaptation process for the BLOOM LLM,\\nwhich is specifically tailored to enhance its proficiency in the financial sector,\\nparticularly in understanding and generating content related to Proxima’s product\\nofferings. We began by introducing the concept of domain adaptation within the\\nbroader scope of transfer learning, emphasizing its significance in fine-tuning general-\\npurpose models to grasp the intricacies of specialized fields.\\nThe adaptation process involved integrating PEFT techniques into BLOOM and\\npreprocessing a financial dataset for model training. This included standardizing text\\nlengths through truncation and padding and tokenizing the texts for consistency in\\nmodel input. The adapted model’s performance was then quantitatively assessed\\nagainst a reference dataset using the ROUGE metric, providing insights into its ability\\nto capture key financial terminologies and phrases. Qualitative evaluation by domain\\nexperts was also suggested as a complementary method to gauge the model’s practical\\neffectiveness in real-world scenarios.\\nOverall, this chapter detailed a common approach to refining an LLM for a specific\\ndomain, illustrating both the methodology and the importance of a nuanced evaluation\\nto ascertain the success of such adaptations. In the next chapter, we will explore how\\nto adapt an LLM without fine-tuning using prompt engineering. We will discover how\\nto contextualize and guide model outputs to produce similar results comparable to\\nfine-tuned models.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 161, 'page_label': '162'}, page_content='References\\nThis reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nGururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., & Smith, N.\\nA. (2020). Don’t stop pretraining: Adapt language models to domains and tasks. In arXiv\\n[cs.CL]. http://arxiv.org/abs/2004.10964/.\\nPruksachatkun, Y., Phang, J., Liu, H., Htut, P. M., Zhang, X., Pang, R. Y., Vania, C., Kann, K.,\\n& Bowman, S. R. (2020a). Intermediate-task transfer learning with pretrained language\\nmodels: When and why does it work? Proceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics.\\nXie, Q., Dai, Z., Hovy, E., Luong, M.-T., & Le, Q. V. (n.d.). Unsupervised Data Augmentation\\nfor Consistency Training. Arxiv.org. Retrieved March 16, 2024, from\\nhttp://arxiv.org/abs/1904.12848.\\nAnaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A., Kour, G., Shlomov, S., Tepper, N.,\\n& Zwerdling, N. (2020). Do not have enough data? Deep learning to the rescue! Proceedings\\nof the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial\\nIntelligence, 34(05), 7383–7390. https://doi.org/10.1609/aaai.v34i05.6233.\\nClark, K., Luong, M.-T., Khandelwal, U., Manning, C. D., & Le, Q. V. (2019). BAM! Born-\\nagain multi-task networks for natural language understanding. In arXiv [cs.CL].\\nhttp://arxiv.org/abs/1907.04829.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 162, 'page_label': '163'}, page_content='7 \\nMastering the Fundamentals of Prompt\\nEngineering\\nIn Chapter 5, we briefly evaluated a fine-tuned Large Language Model (LLM)\\nagainst a general-purpose model using in-context learning or the few-shot prompting\\napproach. In this chapter, we will revisit and explore prompting techniques to examine\\nhow well we can adapt a general-purpose LLM without fine-tuning. We explore\\nvarious prompting strategies that leverage the model’s inherent capabilities to produce\\ntargeted and contextually relevant outputs. We will start by examining the shift toward\\nprompt-based language models. Then, we will revisit zero- and few-shot methods,\\nexplain prompt-chaining, and discuss various strategies, including more advanced\\ntechniques such as Retrieval Augmented Generation (RAG). At the end of the\\nchapter, we will apply what we have learned and design a prompting strategy with the\\naim of consistently eliciting factual, accurate, and consistent responses that accomplish\\na specific business task.\\nBefore diving into specific prompt engineering techniques, we will review a few\\nbreakthroughs that pioneered State-of-the-Art (SOTA) prompt-based models.\\nResearch from early 2018 demonstrated how pretraining LLMs could enable few-shot\\ngeneralization – accurate performance on new tasks given only a prompt statement and\\na few demonstrations. Follow-up work further tailored model architectures and\\ntraining specifically for excelling at prompt-based inference across many text-specific\\ntasks. More recent methods optimized model efficiency and stability, enabling accurate\\nand reliable and efficient prompt completion. These innovations laid the groundwork\\nfor prompt engineering, demonstrating the remarkable versatility of prompt-based\\nmodels with minimal input data. Now, prompt design is becoming its own subfield of\\nresearch – unlocking SOTA performance for an ever-expanding range of tasks. Let’s\\nget started.\\nThe shift to prompt-based approaches\\nAs discussed in prior chapters, the development of the original GPT marked a\\nsignificant advance in natural language generation, introducing the use of prompts to\\ninstruct the model. This method allowed models such as GPT to perform tasks such as\\ntranslations – converting text such as “Hello, how are you?” to “Bonjour, comment ça\\nva?” – without task-specific training, leveraging deeply contextualized semantic'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 163, 'page_label': '164'}, page_content='patterns learned during pretraining. This concept of interacting with language models\\nvia natural language prompts was significantly expanded with OpenAI’s GPT-3 in\\n2020. Unlike its predecessors, GPT-3 showcased remarkable capabilities in\\nunderstanding and responding to prompts in zero- and few-shot learning scenarios, a\\nstark contrast to earlier models that weren’t as adept at such direct interactions. The\\nmethodologies, including the specific training strategies and datasets used for\\nachieving GPT-3’s advanced performance, remain largely undisclosed. Nonetheless, it\\nis inferred from OpenAI’s public research that the model learned to follow instructions\\nbased on its vast training corpus, and not explicit instruction-tuning. GPT-3’s success\\nin performing tasks based on simple and direct prompting highlighted the potential for\\nlanguage models to understand and execute a wide range of tasks without requiring\\nexplicit task-specific training data for each new task. This led to a new paradigm in\\nNLP research and applications, focusing on how effectively a model could be\\nprompted with instructions to perform tasks such as summarization, translation,\\ncontent generation, and more.\\nAfter the release of GPT-3, OpenAI was among the first to introduce specialized fine-\\ntuning to respond more accurately to instructions in their release of InstructGPT\\n(Ouyang et al., 2022). The researchers aimed to teach the model to closely follow\\ninstructions using two novel approaches. The first was Supervised Fine-Tuning\\n(SFT), which involved fine-tuning using datasets carefully crafted from prompts and\\nresponse pairs. These demonstration datasets were then used to perform SFT on top of\\nthe GPT-3 pretrained model, refining it to provide responses more closely aligned with\\nhuman responses. Figure 7.1 provides an example of a prompt and response pair.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 164, 'page_label': '165'}, page_content='Figure 7.1: InstructGPT SFT instruction and output pairs\\nThe second approach involved additional refinement using Reinforcement Learning\\nfrom Human Feedback (RLHF). Reinforcement Learning (RL), established\\ndecades ago, aims to enhance autonomous agents’ decision-making capabilities. It\\ndoes this by teaching them to optimize their actions based on the trade-off between\\nrisk and reward. The policy captures the guidelines for the agent’s behavior,\\ndynamically updating as new insights and feedback are learned to refine decisions\\nfurther. RL is the exact technology used in many robotic applications and is most\\nfamously applied to autonomous driving.\\nRLHF is a variation of traditional RL, incorporating human feedback alongside the\\nusual risk/reward signals to direct LLM behavior toward better alignment with human\\njudgment. In practice, human labelers would provide preference ratings on model\\noutputs from various prompts, and these ratings would be used to update the model\\npolicy, steering the LLM to generate responses that better conform to expected user\\nintent across a range of tasks. In effect, this technique helped to reduce the model’s\\ntendency to generate inappropriate, biased, harmful, or otherwise undesirable content.\\nAlthough RLHF is not a perfect solution in this regard, it represents a significant step\\ntoward models that better understand and align with human values.\\nLater that year, following OpenAI’s introduction of InstructGPT, Google unveiled\\nFine-tuned Language Net or FLAN (Wei et al., 2021). FLAN represented another\\nleap toward prompt-based LLMs, employing explicit instruction tuning. Google’s\\napproach relied on formatting existing datasets into instructions, enabling the model to\\nunderstand various tasks. Specifically, the authors of FLAN merged multiple NLP\\ndatasets across different categories, such as translation and question answering,\\ncreating distinct instruction templates for each dataset to frame them as instruction-\\nfollowing tasks. For example, the FLAN team leveraged ANLI challenges (Nie et al.,\\n2020) to construct question-answer pairs explicitly designed to test the model’s\\nunderstanding of complex textual relationships and reasoning. By framing these\\nchallenges as question-answer pairs, the FLAN team could directly measure a model’s\\nproficiency in deducing these relationships under a unified instruction-following\\nframework. Through this innovative approach, FLAN effectively broadened the scope\\nof tasks a model can learn from, enhancing its overall performance and adaptability\\nacross a diverse set of NLU benchmarks. Figure 7.2 presents a theoretical example of\\nquestion-answer pairs based on ANLI.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 165, 'page_label': '166'}, page_content='Figure 7.2: Training templates based on the ANLI dataset\\nAgain, the central idea behind FLAN was that each benchmark dataset (e.g., ANLI)\\ncould be translated into an intuitive instruction format, yielding a broad mixture of\\ninstructional data and natural language tasks.\\nThese advancements, among others, represent a significant evolution in the capabilities\\nof LLMs, transitioning from models that required specific training for each task to\\nthose that can intuitively follow instructions and adapt to a multitude of tasks with a\\nsimple prompt. This shift has not only broadened the scope of tasks these models can\\nperform but also demonstrated the potential for AI to process and generate human\\nlanguage in complex ways with unprecedented precision.\\nWith this insight, we can shift our focus to prompt engineering. This discipline\\ncombines technical skill, creativity, and human psychology to maximize how models\\ncomprehend and respond, appropriately and accurately, to instructions. We will learn\\nprompting techniques that increasingly influence the model’s behavior toward\\nprecision.\\nBasic prompting – guiding principles, types, and\\nstructures\\nIn Chapter 5, we introduced the concept of zero- and few-shot learning, providing the\\nmodel either a direct instruction, or a direct instruction paired with examples specific\\nto the task. In this section, we will focus on zero-shot learning, where prompting'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 166, 'page_label': '167'}, page_content='becomes a critical tool for guiding the model to perform specific tasks without prior\\nexplicit training on those tasks. This section explores elements of a prompt and how to\\nstructure it effectively for zero-shot learning. However, we will first establish some\\ncritical guiding principles to help us understand expected model behavior.\\nGuiding principles for model interaction\\nIt is absolutely critical to understand that LLMs, despite their unprecedented SOTA\\nperformance on natural language tasks, have significant inherent limitations,\\nweaknesses, and susceptibilities. As described in Chapter 1, LLMs cannot establish\\nrationale or perform logical operations natively. Our interactions with LLMs are\\ntypically supplemented by a highly sophisticated application layer that enables the raw\\nmodel to carry on an extended exchange, integrate with systems that perform\\ncomputations, and retrieve additional information and knowledge not intrinsic to the\\nmodel itself. Independent of supplemental integrations, many LLMs are prone to\\nerratic behavior. The most common of these is often referred to as hallucination,\\nwhere the model generates a plausible output that is not entirely factual. As such, we\\nshould approach the general use of LLMs with the following guidelines in mind:\\nApply domain knowledge and subject-matter expertise: As SOTA LLMs are prone to\\ngenerating inaccuracies that sound plausible, in use cases where factuality and precision are\\nessential (e.g., code generation, technical writing, or academic research), users must have a\\nfirm grasp of the subject matter to detect potential inaccuracies. For example, suppose a user\\nwithout medical expertise were to prompt a model for healthcare advice. In that case, the\\nmodel may confuse, conflate, or simply invent information that could result in misleading or\\npotentially dangerous advice. A mitigant for this behavior could be to provide the model with\\ninformation from a reputable health journal and instruct it to generate its answers explicitly\\nfrom the passages provided. This technique is often called grounding, and we will cover it in\\ndepth later. However, even when supplementing the model’s knowledge with verified\\ninformation, the model can still misrepresent facts. Without expertise in the specific domain in\\nquestion, we may never detect misinformation. Consequently, we should generally avoid using\\nLLMs when we cannot verify the model output. Moreover, we should avoid using LLMs in\\nhigh-stake scenarios where erroneous output could have profound implications.\\nAcknowledge bias, underrepresentation, and toxicity: We have described how LLMs are\\ntrained at an enormous scale and often on uncurated datasets. Inevitably, LLMs will learn,\\nexhibit, and amplify societal biases. The model will propagate stereotypes, reflect biased\\nassumptions, and generate toxic and harmful content. Moreover, LLMs can overrepresent\\ncertain populations and grossly underrepresent others, leading to a skewed or warped\\nsociological perspective. These notions of bias can manifest in many ways. We will explore\\nthis topic, and other ethical implications of LLM use, in detail in Chapter 8.\\nAvoid ambiguity and lack of clarity: Since LLMs were trained to synthesize information\\nresembling human responses, they can often exhibit notions of creativity. In practice, if\\nprompting is ambiguous or lacks clarity, the model will likely use its vast contextualized'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 167, 'page_label': '168'}, page_content='knowledge to “assume” or “infer” the meaning or objective of a given prompt or instruction. It\\nmay apply some context from its training instead of responding with a clarifying question. As\\nwe will describe in the next section, it is crucial to provide clarity by contextualizing input in\\nmost cases.\\nNow that we have established a few overarching principles to help navigate\\ninteractions and keep us within the boundaries of appropriate use, we can deconstruct\\nthe various elements of a prompt.\\nPrompt elements and structure\\nGenerally, a prompt acts as a guide, directing the model’s response toward the desired\\noutcome. It typically comprises key elements that frame the task at hand, providing\\nclarity and direction for the model’s generative capabilities. The following table\\npresents the essential elements of a zero-shot prompt.\\nInstructionA clear, concise statement describing what you want the model to do.\\nThis could be a direct command, a question, or a statement that implies\\na task.\\nContext Relevant information or background is needed to understand the\\ninstruction or the task. This could include definitions or clarifications.\\nInput Following the instructions, the model should work with specific data\\nor content. This could be a piece of text, a question, or any information\\nrelevant to the task.\\nOutput\\ncue\\nAn indication of how the model’s response is to be structured. This can\\nbe part of the instruction or implied through the prompt’s formatting.\\nTable 7.1: Basic elements of a zero-shot prompt\\nWe can then structure these elements to maximize the zero-shot approach, whereby the\\nmodel relies entirely on the prompt to understand and execute a task. In this context,\\nwe use the term task to describe a specific natural language task, such as\\nsummarization or translation. However, we will also encounter the term task applied\\nmore broadly to refer to the output the model should provide. Let’s explore a few\\nconcrete examples of various tasks. In this case, we will be referring to specific NLP\\ntasks and applying a standard structure combining the key elements we’ve described:\\nExample 1: Summarization task\\nInstruction: Summarize the following text in one sentence.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 168, 'page_label': '169'}, page_content='Context: The text provides an overview of the benefits of renewable energy.\\nInput: Renewable energy sources like solar and wind power offer\\nsustainable alternatives to fossil fuels, reducing greenhouse\\ngas emissions and promoting environmental conservation...\\nOutput Cue: Renewable energy sources, such as\\nExample Outcome: \"Renewable energy sources, such as solar and\\nwind, play a crucial role in reducing emissions and conserving\\nthe environment.\"\\nExample 2: Translation task\\nInstruction: Translate the following sentence from English to Spanish.\\nContext: The sentence is a greeting.\\nInput: \"Hello, how are you?\"\\nOutput Cue: This translates to\\nExample Outcome: This translates to \"Hola, ¿cómo estás?\"\\nThe structured templates help us to efficiently and reliably prompt the model for a\\nwide range of inputs, while maintaining a structure that the model has learned to\\nrecognize and respond to. In fact, we can take this a step further by asking the\\nmodel to provide a specific format in its output. Using the output cue, we can\\ninstruct the model to provide a specified format such as Markdown.\\nExample 3: Code generation task\\nInstruction: Generate a Python function that calculates the square of a number.\\nContext: The function should take a single integer argument and return its square.\\nInput: \"Please write a Python function to calculate the square of\\na number.\"\\nOutput Cue: By using the Markdown format in the output cue, the model knows\\nto provide this format and returns the following:\\n \\ndef square(number):\\n \\n\\xa0\\xa0\\xa0\\xa0return number ** 2\\nUsing LangChain to produce JSON-formatted output, we can leverage the same\\napproach. Specifically, LangChain’s PromptTemplate provides a flexible way to\\ndynamically define a structure for our prompts and insert elements:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 169, 'page_label': '170'}, page_content='from langchain.prompts import PromptTemplate\\n \\nfrom langchain.llms import OpenAI\\n \\n# Define a prompt template requesting JSON formatted output\\n \\nprompt_structure = PromptTemplate(\\n \\n\\xa0\\xa0\\xa0\\xa0template=\"\"\"\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Context: {context}\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Instruction: {instruction}\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Text: {text_to_process}\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Output Cue: Format the response in JSON with one \\nelement called summary.\\n \\n\\xa0\\xa0\\xa0\\xa0\"\"\",\\n \\n\\xa0\\xa0\\xa0\\xa0input_variables=[\"context,\" \"instruction\",\\n \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"text_to_process\"]\\n \\n)\\n \\n# Dynamic elements for the prompt\\n \\ncontext = \"Summarizing long text passages.\"\\n \\ninstruction = \"Summarize the key points from the following \\ntext in JSON format.\"\\n \\ntext_to_process = \"\"\"\\n \\nMars is the fourth planet from the Sun. The surface of Mars \\nis orange-red because…\\n \\n\"\"\"'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 170, 'page_label': '171'}, page_content='formatted_prompt = prompt_structure.format_prompt(\\n \\n\\xa0\\xa0\\xa0\\xa0context=context,\\n \\n\\xa0\\xa0\\xa0\\xa0instruction=instruction,\\n \\n\\xa0\\xa0\\xa0\\xa0text_to_process=text_to_process\\n \\n)\\n \\nllm = OpenAI(model_name=\\'gpt-3.5-turbo-instruct\\',\\n \\n\\xa0\\xa0\\xa0\\xa0temperature=0.9, max_tokens = 256)\\n \\nresponse = llm.invoke(formatted_prompt)\\n \\nprint(response)\\nThis produces the following:\\n \\n{\\n \\n\\xa0\\xa0\\xa0\\xa0\"summary\": \"Mars is the fourth planet from the Sun, known \\nfor its orange-red surface and high-contrast features that \\nmake it a popular object for telescope viewing.\"\\n \\n}\\nCrafting effective prompts for zero-shot learning with LLMs requires a clear\\nunderstanding of the task, thoughtful structuring of the prompt, and consideration of\\nhow the model interprets and responds to different elements within the prompt. By\\napplying these principles, we can guide models to perform various tasks accurately\\nand effectively. Subsequently, we will explore methods to guide models’ behavior\\nthrough positive affirmations, emotional engagement, and other cognitive-behavioral\\ntechniques.\\nElevating prompts – iteration and influencing\\nmodel behaviors'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 171, 'page_label': '172'}, page_content='In this section, we will introduce techniques for enhancing AI model interactions\\ninspired by cognitive-behavioral research. Behavioral prompting can guide models\\ntoward more accurate and nuanced responses. For example, LLM performance can be\\nimproved by providing the model with positive emotional stimuli, asking the model to\\nassume a persona or character, or using situational prompting (i.e., role-play).\\nHowever, it is crucial to recognize that these techniques can also be misused or used to\\ninadvertently introduce stereotypes, as they rely on assumptions and generalizations\\nthat may not accurately reflect individual experiences or diverse perspectives. Without\\ncareful consideration and monitoring, there is a risk of reinforcing existing biases or\\ncreating new ones, potentially leading to skewed or harmful output. Given these\\nchallenges, we will explore a responsible approach to employing cognitive-behavioral\\ntechniques in AI interactions, aiming to harness their benefits while minimizing risks\\nand ensuring inclusivity and fairness.\\nLLMs respond to emotional cues\\nResearch conducted by Microsoft in collaboration with various institutions, including\\nthe Beijing Normal University psychology department, suggests that LLMs can mimic\\nand display some aspects of human emotional intelligence. This can lead to improved\\ntask performance when prompts are infused with emotional stimuli. In particular, the\\nresearchers hypothesize that emphasizing positive words can trigger more constructive\\nand effective responses. The phenomenon is not well understood, but the effect is that\\npositive emotional cues seem to improve model performance on various tasks\\nconsistently (Li et al., 2023). For example, the researchers input phrases encouraging\\nconfidence and positive outcomes, such as This is very important to my\\ncareer or Believe in your abilities and strive for excellence. Your\\nhard work will yield remarkable results. These types of psychological\\nelements not only positively influenced the output quality but also made interactions\\nmore effective and nuanced.\\nEffect of personas\\nResearch has shown that the effect of personas enables LLMs to better simulate\\nhuman-like interactions by adopting varied characteristics, perspectives, and response\\nstyles tailored to specific user profiles. This approach enhances the model’s capability\\nfor engaging in more personalized, contextually relevant conversations. Incorporating\\npersonas into LLM prompts effectively shifts the distribution of possible outputs,\\ndirecting the model’s generation process toward responses that resonate with specific\\ncharacter traits or perspectives. Thanks to the model’s extensive training on diverse'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 172, 'page_label': '173'}, page_content='datasets, it can replicate patterns aligning with the described characteristics of\\npersonas, focusing its attention and selection process on a more defined set of outputs.\\nThis enhances relevance and specificity.\\nFor clarity, let’s reconsider our hypothetical business StyleSprint, a fashion brand. If\\nwe prompt an LLM with “As a creative and bold fashion brand, how\\nwould StyleSprint describe its upcoming summer collection?”, the\\nmodel, adopting the brand’s persona, might generate a response such as the following:\\n“This summer, StyleSprint dares you to stand out! Our collection\\nis a vibrant explosion of colors and patterns, designed for the\\nfearless fashionista. Think bold prints, daring cuts, and a\\nstatement that screams confidence. From sun-drenched beaches to\\nchic city streets, our pieces promise to make every moment\\nunforgettable. Embrace your boldness with StyleSprint this\\nsummer.”.\\nIn this example, we can see how personas guide the model’s outputs to reflect specific\\nbrand tones, such as creativity and boldness in this case, making the marketing\\nmaterial more engaging and aligned with the brand’s identity. Furthermore, using a\\nfew-shot approach, we could imbue model responses with a very specific branded\\ntone. We will discuss few-shot in the sections that follow.\\nHowever, as discussed, personas should be used with caution. Personas can perpetuate\\nstereotypes and biases, particularly against marginalized groups. A study conducted by\\nresearchers at Stanford University found that generating personas based on\\nintersectional demographic groups often yields higher rates of racial stereotypes and\\npatterns of othering, or portraying someone or a group as fundamentally different or\\nalien, compared to human-written texts. In some cases, model outputs could amplify\\nnarratives and tropes (Cheng, Durmus, & Jurafsky, 2023).\\nSituational prompting or role-play\\nRole-play in LLMs, in the same way as personas, involves adopting specific identities\\nor characteristics. However, the two serve different purposes and are applied in distinct\\ncontexts. Personas are predefined sets of traits or characteristics that an LLM mimics\\nto tailor its responses, focusing on consistency with those traits. As we have\\ndemonstrated with our StyleSprint example, this is useful for creating content with a\\nspecific tone or perspective.\\nConversely, role-play extends beyond adopting a set of traits to engage in a scenario or\\nnarrative dynamically. It involves the LLM taking on a character within a simulated\\nenvironment or story, responding to inputs in a manner that aligns with both a persona'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 173, 'page_label': '174'}, page_content=\"and the evolving context of the role-play scenario. This can be especially useful in\\ncomplex simulations where the LLM must navigate and contribute to ongoing\\nnarratives or dialogues that require understanding and adapting to new information or\\nchanging circumstances in real time.\\nFigure 7.3: Persona versus role-play\\nRevisiting our real-world scenario, role-play could be particularly useful for creating\\ninteractive and engaging customer service experiences. For example, StyleSprint could\\ndesign a role-play scenario where the LLM acts as a virtual personal stylist. In this\\nrole, the model would engage customers with prompts such as I'm your personal\\nstylist for today! What's the occasion you're dressing for?. Based\\non the customer’s response, the LLM could ask follow-up questions to narrow down\\npreferences, such as Do you prefer bold colors or pastel shades?. Finally,\\nit could recommend outfits from StyleSprint’s collection that match the customer’s\\nneeds, saying something such as For a summer wedding, I recommend our\\nFloral Maxi Dress paired with the Vintage Sun Hat. It's elegant,\\nyet perfect for an outdoor setting!.\\nIn this case, we leverage the LLM’s ability to dynamically adapt its dialogue based on\\ncustomer inputs to create an advanced recommender system that facilitates a highly\\npersonalized shopping experience. It not only helps in providing tailored fashion\\nadvice but also engages customers in a novel way.\\nHaving examined how behavior-inspired techniques, such as personas and role-play,\\ninfluence model behavior through zero-shot learning, let’s now turn our attention to\\nfew-shot learning. This is also known as in-context learning, which we described in\\nChapter 5. Recall that the few-shot approach can enhance the consistency, stability,\\nand reliability of model responses. By providing the model with a few examples of the\\ndesired output within the prompt itself, few-shot learning effectively teaches the model\\nthe specific task at hand, leading to more predictable and accurate outputs.\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 174, 'page_label': '175'}, page_content='Advanced prompting in action – few-shot\\nlearning and prompt chaining\\nIn few-shot settings, the LLM is presented with a small number of examples of a task\\nwithin the input prompt, guiding the model to generate responses that align with these\\nexamples. As discussed in the prior chapter, this method significantly reduces the need\\nfor fine-tuning on large, task-specific datasets. Instead, it leverages the model’s pre-\\nexisting knowledge and ability to infer context from the examples provided. In\\nChapter 5, we saw how this approach was particularly useful for StyleSprint by\\nenabling the model to answer specific questions after being provided with just a few\\nexamples, enhancing consistency and creativity in brand messaging.\\nThis method typically involves using between 10 and 100 examples, depending on the\\nmodel’s context window. Recall that the context window is the limit of tokens a\\nlanguage model can process in one turn. The primary benefit of the few-shot approach\\nis that it minimizes the risk of the model learning a too-narrow distribution from a\\nspecific dataset through fine-tuning. Although the performance of few-shot may not\\nalways match its fine-tuned counterpart, few-shot learning often outperforms both one-\\nshot and zero-shot learning, showing significant improvements in task adaptation and\\naccuracy. This is especially true as more examples are added to the context window\\n(Brown et al., 2020).\\nApplications such as LangChain provide a simple and convenient pattern for few-shot\\nimplementation. Consider a scenario in which StyleSprint would like to generate\\ntaglines for its seasonal collections. In this case, we can provide the model with\\nexamples written by the content team to guide the model toward consistency with the\\nbrand tone:\\n \\nexamples = [ \\n\\xa0\\xa0\\xa0\\xa0{ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"prompt\": \"Describe the new summer collection in a bold \\nand adventurous tone.\", \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"response\": \"Dive into summer with StyleSprint\\'s latest \\ncollection! Featuring daring designs and vibrant colors, it\\'s \\nall about making bold statements. Perfect for the fearless \\nfashionista ready to conquer the heat.\" \\n\\xa0\\xa0\\xa0\\xa0}, \\n\\xa0\\xa0\\xa0\\xa0{ \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"prompt\": \"How would you introduce our eco-friendly line \\nto environmentally conscious customers?\", \\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\"response\": \"Embrace sustainable style with \\nStyleSprint\\'s eco-friendly line. Crafted from recycled \\nmaterials, each piece combines fashion with responsibility, \\ndesigned for the eco-conscious and trendy.\"'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 175, 'page_label': '176'}, page_content='} \\n]\\nThe LangChain API offers FewShotPromptTemplate to format the examples\\nconsistently:\\n \\nfrom langchain.prompts.few_shot import FewShotPromptTemplate \\nfrom langchain.prompts.prompt import PromptTemplate \\n# Create a formatter \\nprompt_format = PromptTemplate( \\n\\xa0\\xa0\\xa0\\xa0input_variables=[\"prompt\", \"response\"], \\n\\xa0\\xa0\\xa0\\xa0template=\"Prompt: {prompt}\\\\nResponse: {response}\") \\n# Create the FewShotPromptTemplate \\nfew_shot_prompt = FewShotPromptTemplate( \\n\\xa0\\xa0\\xa0\\xa0examples=examples, example_prompt=prompt_format, \\n\\xa0\\xa0\\xa0\\xa0suffix=\"Prompt: {input}\", input_variables=[\"input\"])\\nWe can now apply the template to an LLM to generate a response that we can expect\\nwill closely align with the tone and style of our examples:\\n \\nfrom langchain import LLMChain, OpenAI \\n# Setup the LLM and LLMChain \\nllm = OpenAI(temperature=0)\\nllm_chain = LLMChain(llm=llm, prompt=few_shot_prompt) \\n# Define the input prompt \\ninput_prompt = \"Create a catchy tagline for our winter \\ncollection.\" \\n# Invoke the chain to generate output \\nresponse = llm_chain.run(input_prompt) \\n# Extract and print the generated slogan \\ngenerated_slogan = response\\nprint(generated_slogan)  \\n\\xa0\\xa0\\xa0\\xa0# => Response: \"Stay warm, \\n\\xa0\\xa0\\xa0\\xa0stay stylish, \\n\\xa0\\xa0\\xa0\\xa0stay ahead with StyleSprint\\'s winter collection!\"\\nNow that we have a consistent and programmatic method for providing the model with\\nexamples, we can iterate over the model responses using prompt chaining. A prompt\\nchain generally refers to chaining together multiple prompts and LLM interactions to\\nhave a conversation with the model and iteratively build on the results. Remember, the\\nmodel itself cannot store information and effectively has no memory or prior inputs\\nand outputs. Instead, the application layer stores prior inputs and outputs, which are\\nthen provided to the model with each exchange. For example, you might start with an\\ninitial prompt such as the following:\\n \\n\"Write a slogan for a winter clothing line\"'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 176, 'page_label': '177'}, page_content='The LLM might generate the following:\\n \\n\"Be warm, be cozy, be you\"\\nYou could then construct a follow-up prompt using the following:\\n \\n\"Modify the slogan to be more specific about the quality of the \\nclothing\"\\nYou could then keep iterating to improve the output.\\nChaining facilitates guiding and interactively refining the text generated rather than\\nrelying purely on the given examples. Notice that our prior few-shot code had already\\nestablished a chain, which we can now use to iterate as follows:\\n \\nresponse = llm_chain.run(\"Rewrite the last tag to something \\nabout embracing the winter\") \\nResponse #  \\n=> Response: Embrace the winter wonderland with StyleSprint\\'s \\nlatest collection. From cozy knits to chic outerwear, our pieces \\nwill keep you stylish and warm all season long.\\nThe model is now working from both the examples we provided and any additional\\ninstructions we want to include as part of the chain. Prompt chaining, combined with\\nfew-shot learning, provides a powerful framework for iteratively guiding language\\nmodel outputs. By leveraging application state to maintain conversation context, we\\ncan steer the model toward desired responses in line with our provided examples. This\\napproach balances harnessing the model’s inferential capabilities and retaining control\\nto align its creative outputs.\\nNext, we will dive into our practice project, which implements RAG. RAG augments\\nmodel responses by retrieving and incorporating external data sources. This technique\\nmitigates hallucination risks by grounding AI-generated text in real data. For example,\\nStyleSprint may leverage past customer survey results or catalog data to enhance\\nproduct descriptions. By combining retrieval with prompt chaining, RAG provides a\\nscalable method for balancing creativity with accuracy.\\nPractice project: Implementing RAG with\\nLlamaIndex using Python\\nFor our practice project, we will shift from LangChain to exploring another library that\\nfacilitates the RAG approach. LlamaIndex is an open source library that is specifically\\ndesigned for RAG-based applications. LlamaIndex simplifies ingestion and indexing'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 177, 'page_label': '178'}, page_content='across various data sources. However, before we dive into implementation, we will\\nexplain the underlying methods and approach behind RAG.\\nAs discussed, the key premise of RAG is to enhance LLM outputs by supplying\\nrelevant context from external data sources. These sources should provide specific and\\nverified information to ground model outputs. Moreover, RAG can optionally leverage\\nthe few-shot approach by retrieving few-shot examples at inference time to guide\\ngeneration. This approach alleviates the need to store examples in the prompt chain\\nand only retrieves relevant examples when needed. In essence, the RAG approach is a\\nculmination of many of the prompt engineering techniques we have already discussed.\\nIt provides structure, chaining, few-shot learning, and grounding.\\nAt a high level, the RAG pipeline can be described as follows:\\n1. The RAG component ingests and indexes domain-specific data sources using vector\\nembeddings to encode semantics. As we learned in Chapter 3, these embeddings are imbued\\nwith deeply contextualized, rich semantic information that the component uses later to perform\\na semantic search.\\n2. The component then uses the initial prompt as a search query. The query is input to retrieval\\nsystems, which find the most relevant snippets from the indexed data based on vector\\nsimilarity. Similar to how we applied semantic similarity in prior chapters, RAG leverages a\\nsimilarity metric to rank results by semantic relevance.\\n3. Lastly, the original prompt is augmented with information from the retrieved contexts, and the\\naugmented prompt is passed to the LLM to generate a response grounded in the external data.\\nRAG introduces two major benefits. First, like the chaining approach, the indexed\\nexternal data acts as a form of memory, overcoming the LLM’s statelessness. Second,\\nthis memory can rapidly scale beyond model context window limitations, since\\nexamples are curated and only provided at the time of the request as needed.\\nUltimately, RAG unlocks otherwise unattainable capabilities in reliable and factual\\ntext generation.\\nIn our practice project, we revisited the StyleSprint product descriptions. This time, we\\nwant to leverage RAG to retrieve detailed information about the product to produce\\nvery specific descriptions. For the purpose of keeping this project accessible, we will\\nimplement an in-memory vector store (Faiss) instead of an external database. We begin\\nwith installing the necessary libraries. We will leverage LlamaIndex’s integrated\\nsupport for Faiss:\\n \\npip install llama-index faiss-cpu llama-index-vector-stores-faiss\\nWe will then import the necessary libraries, load the data, and create the index. This\\nvector store will rely on OpenAI’s embeddings, so we must also define\\nOPENAI_API_KEY using a valid key:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 178, 'page_label': '179'}, page_content='assert os.getenv(\"OPENAI_API_KEY\") is not None,  \\n\\xa0\\xa0\\xa0\\xa0\"Please set OPENAI_API_KEY\" \\n# load document vectors \\ndocuments = SimpleDirectoryReader(\"products/\").load_data() \\n# load faiss index \\nd = 1536 # dimension of the vectors \\nfaiss_index = faiss.IndexFlatL2(d) \\n# create vector store \\nvector_store = FaissVectorStore(faiss_index=faiss_index) \\n# initialize storage context \\nstorage_context = StorageContext.from_defaults( \\n\\xa0\\xa0\\xa0\\xa0vector_store=vector_store) \\n# create index \\nindex = VectorStoreIndex.from_documents( \\n\\xa0\\xa0\\xa0\\xa0documents,storage_context=storage_context)\\nWe now have a vector store that the model can rely on to retrieve our very specific\\nproduct data. This means we can query for very specific responses augmented by our\\ndata:\\n \\n# query the index \\nquery_engine = index.as_query_engine() \\nresponse = query_engine.query(\"describe summer dress with \\nprice\") \\nprint(response)  \\n=> A lightweight summer dress with a vibrant floral print is \\npriced at 59.99.\\nThe result is a response that not only provides an accurate description of the summer\\ndress but also includes specific details, such as the price. This level of detail enriches\\nthe customer’s shopping experience, providing relevant and real-time information for\\ncustomers to consider when making a purchase.\\nThe next step is to evaluate our RAG implementation to ensure that the answer is\\nrelevant, faithful to the source text, reflective of contextual accuracy, and not in any\\nway harmful or inappropriate. We can apply an open source evaluation framework\\n(RAGAS), which provides implementation of the following metrics:\\nFaithfulness assesses the degree to which the generated response is faithful or true to the\\noriginal context\\nAnswer relevance evaluates how relevant the generated answer is to the given question\\nContext precision measures the precision of the context used to generate the answer\\nContext recall measures the recall of the context used to generate the answer\\nContext relevancy assesses the relevancy of the context used to generate the answer'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 179, 'page_label': '180'}, page_content='Harmfulness evaluates whether a submission (or answer) contains anything that could\\npotentially cause harm to individuals, groups, or society at large\\nThis suite of metrics provides an objective measure of RAG application performance\\nbased on a comparison to ground truth. In our case, we can use responses generated\\nfrom our product data, along with context and ground truth derived from the original\\ndataset, to construct an evaluation dataset and perform a comprehensive evaluation\\nusing the metrics described.\\nThe following is a simplified code snippet implementing the RAGAS evaluation for\\nour generated product descriptions. A complete working implementation is available\\nin the Chapter 7 folder of the GitHub companion to this book\\n(https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python).\\n \\n# Define the evaluation data \\neval_data: Dict[str, Any] = { \\n\\xa0\\xa0\\xa0\"question\": questions, # list of sampled questions \\n\\xa0\\xa0\\xa0\"answer\": engine_responses, # responses from RAG application \\n\\xa0\\xa0\\xa0\"contexts\": contexts, # product metadata \\n\"ground_truth\": ground_truth, # corresponding descriptions \\nwritten by a human \\n} \\n# Create a dataset from the evaluation data \\ndataset: Dataset = Dataset.from_dict(eval_data) \\n# Define the evaluation metrics \\nmetrics: List[Callable] = [\\n\\xa0\\xa0\\xa0\\xa0faithfulness, \\n\\xa0\\xa0\\xa0\\xa0answer_relevancy, \\n\\xa0\\xa0\\xa0\\xa0context_precision, \\n\\xa0\\xa0\\xa0\\xa0context_recall, \\n\\xa0\\xa0\\xa0\\xa0context_relevancy, \\n\\xa0\\xa0\\xa0\\xa0harmfulness, \\n] \\n# Evaluate the model using the defined metrics \\nresult: Dict[str, float] = evaluate(dataset, metrics=metrics) \\nprint(result)\\nOur evaluation program should produce the following:\\n \\n{\\'faithfulness\\': 0.9167, \\'answer_relevancy\\': 0.9961, \\n\\'context_precision\\': 0.5000, \\'context_recall\\': 0.7500, \\n\\'harmfulness\\': 0.0000}\\nWe can observe that the system performs well in generating accurate and relevant\\nanswers, as evidenced by high faithfulness and answer relevancy scores. While context\\nprecision shows room for improvement, half of the relevant information is correctly'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 180, 'page_label': '181'}, page_content='identified. Context recall is effective, retrieving most of the relevant context. The\\nabsence of harmful content ensures safe interactions. Overall, the system displays\\nrobust performance in answering accurately and contextually, but could benefit from\\nrefinements in pinpointing the most pertinent context snippets.\\nAs discussed in Chapters 5 and 6, the evaluation of LLMs often requires the additional\\noperational burden of collecting ground-truth data. However, doing so makes it\\npossible to perform a robust evaluation of model and application performance.\\nSummary\\nIn this chapter, we explored the intricacies of prompt engineering. We also explored\\nadvanced strategies to elicit precise and consistent responses from LLMs, offering a\\nversatile alternative to fine-tuning. We traced the evolution of instruction-based\\nmodels, highlighting how they’ve shifted the paradigm toward an intuitive\\nunderstanding and adaptation to tasks through simple prompts. We expanded on the\\nadaptability of LLMs with techniques such as few-shot learning and retrieval\\naugmentation, which allow for dynamic model guidance across diverse tasks with\\nminimal explicit training. The chapter further explored the structuring of effective\\nprompts, and the use of personas and situational prompting to tailor model responses\\nmore closely to specific interaction contexts, enhancing the model’s applicability and\\ninteraction quality. We also addressed the nuanced aspects of prompt engineering,\\nincluding the influence of emotional cues on model performance and the\\nimplementation of RLHF to refine model outputs. These discussions underscored the\\npotential of LLMs to exhibit some level of emotional intelligence, leading to more\\neffective and nuanced interactions. However, alongside these technological strides, we\\nstressed the paramount importance of ethical considerations. We highlighted the need\\nfor responsible adoption and vigilance to mitigate potential harm and biases associated\\nwith these techniques, ensuring fairness, integrity, and the prevention of misuse.\\nLastly, we learned how to implement and evaluate the RAG approach to ground the\\nLLM in contextual information from trusted sources and produce answers that are\\nrelevant and faithful to the source text. In the next chapter, we will look more closely\\nat the role of individuals in advancing generative AI while emphasizing the dual\\nresponsibility of developers and researchers to navigate this rapidly evolving field with\\na conscientious approach, balancing innovation with ethical imperatives and societal\\nimpacts.\\nReferences'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 181, 'page_label': '182'}, page_content='This reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C.,\\nAgarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M.,\\nAskell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language\\nmodels to follow instructions with human feedback. In arXiv [cs.CL].\\nhttp://arxiv.org/abs/2203.02155\\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q.\\nV. (2021). Finetuned language models are zero-shot learners. In arXiv [cs.CL].\\nhttp://arxiv.org/abs/2109.01652\\nNie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., & Kiela, D. (2020). Adversarial NLI:\\nA new benchmark for natural language understanding. Arxiv.org.\\nLi, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q., & Xie, X. (2023).\\nLarge Language Models understand and can be enhanced by emotional stimuli. In arXiv\\n[cs.CL]. http://arxiv.org/abs/2307.11760\\nCheng, M., Durmus, E., & Jurafsky, D. (2023). Marked personas: Using natural language\\nprompts to measure stereotypes in language models. Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers).\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,\\nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T.,\\nChild, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language\\nModels are Few-Shot Learners. In arXiv [cs.CL]. http://arxiv.org/abs/2005.14165'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 182, 'page_label': '183'}, page_content='8\\nAddressing Ethical Considerations and Charting\\na Path Toward Trustworthy Generative AI\\nAs generative AI advances, it will extend beyond basic language tasks, integrating into\\ndaily life and impacting almost every sector. The inevitability of its widespread\\nadoption highlights the need to address its ethical implications. The promise of this\\ntechnology to revolutionize industries, enhance creativity, and solve complex problems\\nmust be coupled with the responsibility to navigate its ethical landscape diligently.\\nThis chapter will explore these ethical considerations, dissect the intricacies of biases\\nentangled in these models, and look at strategies for cultivating trust in general-\\npurpose AI systems. Through thorough examination and reflection, we can begin to\\noutline a path toward responsible use, helping to ensure that advancements in\\ngenerative AI are leveraged for the greater good while minimizing harm.\\nTo ground our discussion, we will first identify some ethical norms and universal\\nvalues relevant to generative AI. While this chapter cannot be exhaustive, it aims to\\nintroduce key ethical considerations.\\nEthical norms and values in the context of\\ngenerative AI\\nThe ethical norms and values guiding the development and deployment of generative\\nAI are rooted in transparency, equity, accountability, privacy, consent, security, and\\ninclusivity. These principles can serve as a foundation for developing and adopting\\nsystems aligned with societal values and supporting the greater good. Let’s explore\\nthese in detail:\\nTransparency involves clearly explaining the methodologies, data sources, and processes\\nbehind large language model (LLM) construction. This practice builds trust by enabling\\nstakeholders to understand the technology’s reliability and limits. For example, a company\\ncould publish a detailed report on the types of data trained on their LLM and the steps taken to\\nensure data privacy and bias mitigation.\\nEquity in the context of LLMs ensures fair treatment and outcomes for all users by actively\\npreventing biases in models. This requires thorough analysis and correction of training data\\nand continuous monitoring of exchanges to reduce discrimination. One measure a firm might\\napply is a routine review of LLM performance across various demographic groups to identify\\nand address unintended biases.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 183, 'page_label': '184'}, page_content='Accountability establishes that developers and users of LLMs are responsible for model\\noutputs and impacts. It includes transparent and accessible mechanisms for reporting and\\naddressing negative consequences or ethical violations. In practice, this could manifest as the\\nestablishment of an independent review board that oversees AI projects and intervenes in cases\\nof ethical misconduct.\\nPrivacy and consent, in principle, involves ensuring that individual privacy and consent are\\nrespected and preserved during the use of personal data as input to LLMs. In practice,\\ndevelopers should avoid using personal data for training without explicit permission and\\nimplement strong data protection measures. For example, a developer might use data\\nanonymization or privacy-preserving techniques to train models, ensuring that personal\\nidentifiers and sensitive information are removed before data processing.\\nSecurity involves protecting LLM-integrated systems and their data from unauthorized access\\nand cyber threats. In practice, setting up LLM-specific red teams (or teams that test defenses\\nby simulating attacks) can help safeguard AI systems against potential breaches.\\nInclusivity involves the deliberate effort to include diverse voices and perspectives in the\\ndevelopment process of LLMs, ensuring the technology is accessible and beneficial to a broad\\nspectrum of users. In practice, it is vital to collaborate with socio-technical subject-matter\\nexperts who can guide appropriate actions to promote and preserve inclusion.\\nThis set of principles is not comprehensive but may help to form a conceptual\\nfoundation for ethical LLM development and adoption with the universal goal of\\nadvancing the technology in ways that avoid harm.\\nAdditionally, various leading authorities have published guidance regarding\\nresponsible AI, inclusive of ethical implications. These include the US Department of\\nCommerce’s National Institute of Standards and Technology (NIST), Stanford\\nUniversity’s Institute for Human-Centered Artificial Intelligence (HAI), and the\\nDistributed AI Research Institute (DAIR), to name a few.\\nInvestigating and minimizing bias in generative\\nLLMs and generative image models\\nBias in generative AI models, including both LLMs and generative image models, is a\\ncomplex issue that requires careful investigation and mitigation strategies. Bias can\\nmanifest as unintended stereotypes, inaccuracies, and exclusions in the generated\\noutputs, often stemming from biased datasets and model architectures. Recognizing\\nand addressing these biases is crucial to creating equitable and trustworthy AI systems.\\nAt its core, algorithmic or model bias refers to systematic errors that lead to\\npreferential treatment or unfair outcomes for certain groups. In generative AI, this can\\nappear as gender, racial, or socioeconomic biases in outputs, often mirroring societal'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 184, 'page_label': '185'}, page_content='stereotypes. For example, an LLM may produce content that reinforces these biases,\\nreflecting the historical and societal biases present in its training data.\\nLet us again revisit our hypothetical fashion retailer, StyleSprint. Consider a situation\\nwhere StyleSprint experimented with using a multimodal generative LLM model to\\ngenerate promotional images and captions for its latest sneaker line. It finds that the\\nmodel predominantly generates sneakers in urban, graffiti-laden backgrounds,\\nunintentionally drawing an association that relies on stereotypes. Moreover, the team\\nbegins noticing that the captions are also laden with language that perpetuates\\nstereotypes. This realization prompts a reevaluation of the imagery and text, first with\\nan investigation of how the problem surfaced.\\nInvestigating bias involves various techniques, from analyzing the diversity and\\nrepresentativeness of training datasets to implementing testing protocols that\\nspecifically look for biased outputs across different demographics and scenarios.\\nStatistical analysis can reveal disparities in model outcomes, while comparative\\nstudies and user feedback can help identify biases in the generated content.\\nIn this case, let us assume that StyleSprint was using an LLM-provider without the\\nability to influence its training data or development process. To mitigate the risk of\\nbias, the team might employ the following:\\nPost-processing adjustments to diversify the imagery, ensuring a broader representation of\\nbackgrounds that resonate with its customer base\\nThe institution of a manual review process, enlisting team members to scrutinize and curate\\nAI-generated images and captions before publishing (i.e., “human-in-the-loop”), ensuring that\\nevery piece of content aligns with the brand’s commitment to diversity and inclusion\\nAs is true for other kinds of evaluation of generative AI, evaluating bias demands both\\nquantitative and qualitative methods. Statistical analysis can uncover performance\\ndisparities across groups, and comparative studies can detect biases in outputs.\\nGathering feedback from diverse users aids the understanding of real-world bias\\nimpacts, while independent audits and research are essential for identifying issues that\\ninternal evaluations may miss.\\nWith a better understanding of how we might investigate and evaluate model outcomes\\nfor societal bias, we can explore technical methods for guiding model outcomes\\ntoward reliability, equity, and general trustworthiness to curb biased or inequitable\\noutcomes during inference.\\nConstrained generation and eliciting trustworthy\\noutcomes'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 185, 'page_label': '186'}, page_content='In practice, it is possible to constrain model generation and guide outcomes toward\\nfactuality and equitable outcomes. As discussed, guiding models toward trustworthy\\noutcomes can be done through continued training and fine-tuning, or during inference.\\nFor example, methodologies such as reinforcement learning from human feedback\\n(RLHF) and direct preference optimization (DPO) increasingly refine model\\noutputs to align model outcomes with human judgment. Additionally, as discussed in\\nChapter 7, various grounding techniques help to ensure that model outputs reflect\\nverified data, continuously guiding the model toward responsible and accurate content\\ngeneration.\\nConstrained generation with fine-tuning\\nRefinement strategies such as RLHF integrate human judgments into the model\\ntraining process, steering the AI toward behavior that aligns with ethical and truthful\\nstandards. By incorporating human feedback loops, RLHF ensures that the AI’s\\noutputs meet technical accuracy and societal norms.\\nSimilarly, DPO refines model outputs based on explicit human preferences, providing\\nprecise control to ensure outcomes adhere to ethical standards and human values. This\\ntechnique exemplifies the shift toward more ethically aligned content generation by\\ndirectly incorporating human values into the optimization process.\\nConstrained generation through prompt engineering\\nAs we discovered in Chapter 7, we can guide model responses by grounding the LLM\\nwith factual information. This can be achieved directly using the context window or\\nretrieval approach (e.g., Retrieval Augmented Generation (RAG)). Just as we can\\napply these methods to induce factual responses, we can apply the same technique to\\nguide the model toward equitable and inclusive outcomes.\\nFor example, consider an online news outlet looking to use an LLM to review article\\ncontent for grammar and readability. The model does an excellent job of reviewing and\\nrevising its drafts. However, during peer review, it realizes some of the language is\\nculturally insensitive or lacks inclusivity. As discussed, qualitative evaluation and\\nhuman oversight are critical to ensuring that model output aligns with human\\njudgment. Notwithstanding, the writing team can guide the model toward alignment\\nwith company values using a set of general guidelines for inclusive and debiased\\nlanguage. For example, it could ground the model with excerpts from its internal\\npolicy documents or content from its unconscious bias training guides.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 186, 'page_label': '187'}, page_content='Employing methodologies such as RLHF and DPO, alongside grounding techniques,\\nensures that LLMs generate content that is not only factual but also ethically aligned,\\ndemonstrating the potential of generative AI to adhere to high standards of truthfulness\\nand inclusivity. Although we cannot underestimate or deemphasize the importance of\\nhuman judgment in shaping model outputs, we can apply practical supplemental\\nmethods such as grounding to reduce the likelihood of harmful or biased model\\noutputs.\\nIn the next section, we’ll explore the risks and ethical dilemmas posed by attempts to\\ncircumvent the constraints we have just discussed, highlighting the ongoing challenge\\nof balancing the rapid adoption of generative LLMs with appropriate safeguards\\nagainst misuse.\\nUnderstanding jailbreaking and harmful\\nbehaviors\\nIn the context of generative LLMs, the term jailbreaking describes techniques and\\nstrategies that intend to manipulate models to override any ethical safeguards or\\ncontent restrictions, thereby enabling the generation of restricted or harmful content.\\nJailbreaking exploits models through sophisticated adversarial prompting that can\\ninduce unexpected or harmful responses. For example, an attacker might try to instruct\\nan LLM to explain how to generate explicit content or express discriminatory views.\\nUnderstanding this susceptibility is crucial for developers and stakeholders to\\nsafeguard applied generative AI against misuse and minimize potential harm.\\nThese jailbreaking attacks exploit the fact that LLMs are trained to interpret and\\nrespond to instructions. Despite sophisticated efforts to defend against misuse,\\nattackers can take advantage of the complex and expansive knowledge embedded in\\nLLMs to find gaps in their safety precautions. In particular, models that have been\\ntrained on uncurated datasets are the most susceptible, as the universe of possible\\noutputs that the models sample from can include harmful and toxic content. Moreover,\\nLLMs are multilingual and can accept various encodings as input. For example, an\\nencoding such as base64, which can be used to translate plain text into binary format,\\ncould be applied to obfuscate a harmful instruction. In this case, safety filters may\\nperform inconsistently, failing to detect some languages or alternative inputs.\\nDespite this inherent weakness in LLMs, developers and practitioners can take several\\npractical steps to mitigate jailbreaking risks. Remember, these cannot be exhaustive as\\nnew adversarial techniques are often uncovered:\\nPreprocessing and safety filtering: Implement robust content filtering to detect and block\\nunsafe semantic patterns across languages and input types. For example, a firm might apply'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 187, 'page_label': '188'}, page_content='machine learning techniques to analyze prompts for adversarial patterns and block suspicious\\ninputs before passing them to the LLM.\\nPostprocessing and output screening: Apply a specialized classifier or other sophisticated\\ntechnique to screen LLM outputs for inappropriate content before returning them.\\nSafety-focused fine-tuning: Provide additional safety-focused fine-tuning to the LLM to\\nreinforce and expand its safety knowledge. Focus on known jailbreaking tactics.\\nMonitoring and iterating: Actively monitor for jailbreaking or policy violation attempts in\\nproduction, analyze them to identify gaps, and continually update defense measures to stay\\nahead of creative attackers.\\nWhile eliminating all possible jailbreaking attempts is infeasible, a multi-layered\\ndefense and operational best practices can significantly mitigate the risk.\\nIn the next section, we will apply a real-time defense mechanism for jailbreaking, all\\nwhile reducing the likelihood of biased and harmful output.\\nPractice project: Minimizing harmful behaviors\\nwith filtering\\nFor this project, we will use response filtering to try to minimize misuse and curb\\nunwanted LLM output. Again, we’ll consider our hypothetical business, StyleSprint.\\nAfter successfully using an LLM to generate product descriptions and fine-tuning it to\\nanswer FAQs, StyleSprint now wants to attempt to use a general-purpose LLM\\n(without fine-tuning) to refine its website search. However, giving its customers direct\\naccess to the LLM poses the risk of misuse. Bad actors may attempt to use the LLM\\nsearch to produce harmful content with the intention of harming StyleSprint’s\\nreputation. To prevent this behavior, we can revisit our RAG implementation from\\nChapter 7, applying a filter that evaluates whether queries deviate from the appropriate\\nuse.\\nReusing our previous implementation from the last chapter (found in the GitHub\\nrepository: https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python),\\nwhich applied RAG to answer specific product-related questions, we can evaluate how\\nthe model would respond to questions outside the desired scope. Recall that RAG is\\nsimply a vector search engine combined with an LLM to produce coherent and more\\nprecise responses, contextualized by a specific data source. We will directly reuse that\\nimplementation and the same product data for simplicity, but this time, we’ll input a\\ncompletely unrelated query instead of asking about products:\\n \\n# random query \\nresponse = query_engine.query(\"describe a giraffe\")'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 188, 'page_label': '189'}, page_content='print(response)  \\n=> A giraffe is a tall mammal with a long neck, distinctive \\nspotted coat, and long legs. They are known for their unique \\nappearance and are the tallest land animals in the world.\\nAs we can see, the model did not attempt to constrain its answer to the contents of the\\nsearch index. It returned an answer based on its vast training. This is precisely the\\nbehavior we want to avoid. Imagine that a bad actor induced the model to produce\\nexplicit content or some other unwanted output. Moreover, consider a sophisticated\\nattacker that could induce the model to leak training data or expose sensitive\\ninformation accidentally memorized during training procedures (Carlini et al., 2018;\\nHu et al., 2022). In either case, StyleSprint could face material risk and exposure.\\nTo prevent this, we can leverage a filter to constrain the output to provide answers\\nrelevant to a given question explicitly. The implementation is already built into the\\nLlamaIndex RAG interface. It is a feature they call Structured Answer Filtering:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 189, 'page_label': '190'}, page_content='With structured_answer_filtering set to True, our refine module is able to filter\\nout any input nodes that are not relevant to the question being asked. This is\\nparticularly useful for RAG-based Q&A systems that involve retrieving chunks of\\ntext from external vector store for a given user query. (LlamaIndex)\\nIn short, this functionality gives us fine-grained control to restrict the context we\\nprovide to the LLM for synthesis, ensuring that only the most relevant results are\\nincluded. Filtering out irrelevant content before synthesizing responses ensures that\\nonly information related to the user’s question is used. This approach helps avoid\\nanswers that are off-topic or outside the intended subject matter. We can quickly\\nreimplement our RAG approach, applying minor changes that enable the feature.\\nNOTE\\nThis functionality is most reliable when using an LLM that can support function calling.\\nLet’s see how this functionality can be implemented.\\n \\nfrom llama_index.core import get_response_synthesizer \\nfrom llama_index.core.retrievers import VectorIndexRetriever \\nfrom llama_index.core.query_engine import RetrieverQueryEngine \\n# Configure retriever \\nretriever = VectorIndexRetriever(index=index,similarity_top_k=1) \\n# Configure response synthesizer \\nresponse_synthesizer = get_response_synthesizer( \\n\\xa0\\xa0\\xa0\\xa0structured_answer_filtering=True, \\n\\xa0\\xa0\\xa0\\xa0response_mode=\"refine\" \\n) \\n# Assemble query engine \\nsafe_query_engine = RetrieverQueryEngine( \\n\\xa0\\xa0\\xa0\\xa0retriever=retriever, \\n\\xa0\\xa0\\xa0\\xa0response_synthesizer=response_synthesizer \\n) \\n# Execute query and evaluate response \\nprint(safe_query_engine.query(\"describe a summer dress with \\nprice\")) \\n# => A lightweight summer dress with a vibrant floral print, \\nperfect for sunny days, priced at 59.99. \\nprint(safe_query_engine.query(\"describe a horse\")) \\n# => Empty Response\\nUsing this approach, the model returns a response to the standard question but no\\nresponse to the irrelevant question. In fact, we can take this further and compound this\\nfiltering with additional instructions in the prompt template. For example, if we revise\\nresponse_synthesizer, we can promote a stricter response from the LLM:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 190, 'page_label': '191'}, page_content='QA_PROMPT_TMPL = ( \\n\\xa0\\xa0\\xa0\\xa0\"Context information is below.\\\\n\" \\n\\xa0\\xa0\\xa0\\xa0\"---------------------\\\\n\" \\n\\xa0\\xa0\\xa0\\xa0\"{context_str}\\\\n\" \\n\\xa0\\xa0\\xa0\\xa0\"---------------------\\\\n\" \\n\\xa0\\xa0\\xa0\\xa0\"Given only the context information and no prior knowledge, \\n\" \\n\\xa0\\xa0\\xa0\\xa0\"answer the query.\\\\n\" \\n\\xa0\\xa0\\xa0\\xa0\"Query: {query_str}\\\\n\" \\n\\xa0\\xa0\\xa0\\xa0\"Answer: \" \\n\\xa0\\xa0\\xa0\\xa0\"Otherwise, state: I cannot answer.\" \\n) \\nSTRICT_QA_PROMPT = PromptTemplate( \\n\\xa0\\xa0\\xa0\\xa0QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER \\n) \\n# Configure response synthesizer \\nresponse_synthesizer = get_response_synthesizer( \\n\\xa0\\xa0\\xa0\\xa0structured_answer_filtering=True, \\n\\xa0\\xa0\\xa0\\xa0response_mode=\"refine\",\\n\\xa0\\xa0\\xa0\\xa0text_qa_template=STRICT_QA_PROMPT \\n)\\nThis time, the model responded explicitly, I cannot answer. Using a prompt\\ntemplate, StyleSprint could return a message it deems appropriate in response to inputs\\nunrelated to the search index and, as a side effect, ignore queries that do not adhere to\\nits policies. Although not entirely a perfect solution, combining RAG with more strict\\nanswer filtering can help deter or defend against harmful instructions or adversarial\\nprompting. Additionally, as explored in Chapter 7, we can apply RAG-specific\\nevaluation techniques such as RAGAS to measure factual consistency and answer\\nrelevancy.\\nSummary\\nIn this section, we recognized the increasing prominence of generative AI and\\nexplored the ethical considerations that should steer its progress. We outlined key\\nconcepts such as transparency, fairness, accountability, respect for privacy, informed\\nconsent, security, and inclusivity, which are essential to the responsible development\\nand use of these technologies.\\nWe reviewed strategies to attempt to counter these biases, including human-aligned\\ntraining techniques and practical application-level measures against susceptibilities\\nsuch as jailbreaking. In sum, we explored a multidimensional and human-centered\\napproach to generative AI adoption.'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 191, 'page_label': '192'}, page_content='Having completed our foundational exploration of generative AI, we can now reflect\\non our journey. We began by laying the groundwork, examining foundational\\ngenerative architectures such as generative adversarial networks (GANs), diffusion\\nmodels, and transformers.\\nChapters 2 and 3 guided us through the evolution of language models, with a\\nparticular focus on autoregressive transformers. We explored how these models have\\nsignificantly advanced the capabilities of generative AI, pushing the boundaries of\\nmachine understanding and the generation of human-like language.\\nChapter 4 provided us with practical experience in production-ready environments. In\\nChapter 5, we explored the fine-tuning of LLMs for specific tasks, a technique that\\nenhances their performance and adaptability to specific applications. Chapter 6\\nfocused on the concept of domain adaptation, demonstrating how tailoring AI models\\nto understand domain-specific nuances can greatly improve their utility in specialized\\nfields such as finance, law, and healthcare.\\nChapters 7 and 8 centered on prompt engineering and constrained generation,\\naddressing techniques to ensure that AI-generated content remains trustworthy and\\naligned with ethical guidelines.\\nThis book has aimed to provide a solid foundation in generative AI, preparing\\nprofessionals across disciplines and sectors with the necessary theoretical knowledge\\nand practical skills to effectively engage with this transformative technology. The\\npotential of generative AI is significant, and with our deeper understanding of its\\ntechnologies, coupled with a thoughtful approach to ethical and societal\\nconsiderations, we are ready to responsibly leverage its advantages.\\nReferences\\nThis reference section serves as a repository of sources referenced within this book;\\nyou can explore these resources to further enhance your understanding and knowledge\\nof the subject matter:\\nSun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W., Zhang, Y., Li,\\nX., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong, C., Xiao, C., Li, C., Xing, E., . .\\n. Zhao, Y. (2024). TrustLLM: Trustworthiness in Large Language Models. ArXiv.\\n/abs/2401.05561\\nCarlini, N., Liu, C., Erlingsson, Ú., Kos, J., & Song, D. (2018). The secret sharer: Evaluating\\nand testing unintended memorization in neural networks. In arXiv [cs.LG].\\nhttp://arxiv.org/abs/1802.08232\\nHu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., & Zhang, X. (2022). Membership inference\\nattacks on machine learning: A survey. ACM Computing Surveys, 54(11s), 1–37.\\nhttps://doi.org/10.1145/3523273'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 192, 'page_label': '193'}, page_content='LlamaIndex. (n.d.). Response synthesizers. In LlamaIndex Documentation (stable version).\\nRetrieved March 12, 2024.\\nhttps://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 193, 'page_label': '194'}, page_content=\"I n d e x \\nAs this ebook edition doesn't have fixed pagination, the page numbers below are\\nhyperlinked for reference only, based on the printed edition of this book.\\nA \\nAdaptive Low-Rank Adaptation (AdaLoRA) 107, 108\\nAI model interactions 138\\neffect of personas 139\\nLLMs 138\\nrole play 139, 140\\nsituational prompting 139, 140\\nAI models\\nversus generative AI 4\\nalgorithmic/model bias 151\\napplication code 79, 80\\nartificial intelligence (AI) 3\\nAR Transformer\\nrole, in GenAI 57-59\\nautoregressive (AR) 39\\nautoregressive modeling 26\\nautoregressive transformers 5\\nB \\nback translation 122\\nbase64 153\\nbenchmarking 87\\nbias\\ninvestigating and minimizing, in generative image models 151, 152\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 194, 'page_label': '195'}, page_content='investigating and minimizing, in generative LLMs 151, 152\\nBidirectional Encoder Representations from Transformers (BERT) 9, 25, 57\\nBigGANs 20\\nBilingual Evaluation Understudy (BLEU) 84\\nBLOOM 122\\nC \\ncausal language modeling (CLM) 124\\ncentral processing units (CPUs) 86\\ncloud-based notebook environment\\nfeatures 72\\ncode repository\\ncreating 80, 81\\nconditional distribution 22\\nconditional GANs (cGANs) 20\\nconstrained generation\\nthrough prompt engineering 152\\nwith fine-tuning 152\\ncontainerization tools 75\\ncontinued pre-training 122\\ncontinuous integration/continuous deployment (CI/CD) 76\\nsetting up 81-83\\ncontinuous vector space 41\\nContrastive Language-Image Pretraining (CLIP) 24, 84\\nalignment with 94-96\\nencoder-only image, captioning with 27\\nscoring with 33-36\\nconvolutional neural networks (CNNs) 39\\nrise 44, 45'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 195, 'page_label': '196'}, page_content='scenario, considerations 44\\ncount vectors 40\\nCycleGAN 20\\nD \\nDALL-E\\nencoder-decoder image generation 27\\ndata augmentation 122\\ndecoder-only approach 26\\ndecoder stack 47\\nDeep Convolutional GANs (DCGANs) 20\\ndeep learning (DL) 39\\ndiffusers\\nused, for image generation 29, 30\\ndiffusion models 5, 21, 22\\nadvancement 23, 24\\nlimitations and challenges 24\\ndirect preference optimization (DPO) 152\\ndiscriminative model\\nversus generative AI 7, 8\\nDistributed AI Research Institute (DAIR) 151\\ndistributed representation 40-42\\nDocker container\\nsetting up 78, 79\\ndomain adaptation 105, 122, 123\\nE \\nembeddings 47\\nencoder-decoder image generation'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 196, 'page_label': '197'}, page_content='with DALL-E 27\\nencoder-only approach 26\\nencoder stack 46\\nend-of-sequence (EOS) 66\\nend-to-end architecture flow\\nsynopsis 56\\nethical norms and values\\nin context, of generative AI 150, 151\\nF \\nfeature-based representation 41\\nfeedforward networks (FNNs) 43, 107\\nfew-shot prompting 109\\nfiltering\\nused, for minimizing harmful behaviors 154-156\\nfinance domain\\ntraining methodologies 124-127\\ntransfer learning 123\\nFine-tuned Language Net (FLAN) 133\\nfine-tuning 105, 106\\nused, for constrained generation 152\\nversus in-context learning 110, 111\\nfine-tuning, for Q&A with PEFT 111\\nbackground 112, 113\\nimplementation, in Python 113-115\\nmodel outcomes, evaluating 117, 118\\ntraining loss 116, 117\\nfully connected NN (FCNN) 63'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 197, 'page_label': '198'}, page_content='G \\nGeneral Artificial Intelligence (GAI)\\napplying 29, 30\\nmethods, deconstructing 19\\ntypes 18, 19\\nGenerative Adversarial Networks (GANs) 3, 5, 17, 19, 39, 71\\nadvancement 20\\nlimitations and challenges 21\\ntraining 21\\nused, for image generation 29, 30\\ngenerative AI, evolution\\nGPT-4, development and impact 10\\ntraditional methods in NLP, overview 8, 9\\ntransformer-based models, evolution 9, 10\\ngenerative AI (GenAI) 4, 39\\napproaches, surveying 5, 6\\nAR Transformer, role in 57-59\\nevolution 8\\nfuture 13, 14\\nprinciples 150\\nrisks and implications 11, 12\\nuse cases 12, 13\\nversus AI models 4\\nversus discriminative model 7, 8\\ngenerative image models\\nbias, investigating and minimizing 151, 152\\ngenerative LLMs\\nbias, investigating and minimizing 151, 152\\ngenerative methods'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 198, 'page_label': '199'}, page_content='versus traditional ML methods 6, 7\\ngenerative modeling paradigms\\nwith transformers 25, 26\\nGenerative Pre-trained Transformer 4 (GPT-4) 39\\ndevelopment and impact 10\\nmultimodal image generation with 28\\nGenerative Pretrained Transformer (GPT) 17\\ngenerative transformers 24\\nadvancement 27\\narchitecture, overview 25\\nbias and ethics 29\\ndecoder-only approach 26\\nencoder-decoder image, generation with DALL-E 27\\nencoder-only approach 26\\nencoder-only image, captioning with CLIP 27\\ngenerative modeling paradigms with 25, 26\\nimage fidelity, improving with scaled transformers (DALL-E 2) 27, 28\\nmultimodal image generation, with GPT-4 28\\ntransformer-based approaches, limitations and challenges 28\\nGlobal Vectors (GloVe) 40\\nGoogle Colab\\nfeatures, inherent 74, 75\\nworking with 30\\ngraphics processing units (GPUs) 72\\nconfiguration 88, 89\\ngrounding 135\\nguiding principles for model interaction 134, 135\\nprompt elements and structure 135-138'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 199, 'page_label': '200'}, page_content='H \\nhallucination 134\\nharmful behaviors\\nminimizing, with filtering 154-156\\nhidden Markov models (HMMs) 8\\nhyperparameters 53\\nI \\nimage generation\\nusing, diffusers 29, 30\\nusing, GANs 29, 30\\nusing, transformers 29, 30\\nimportance scores 107\\nin-context learning 108-110\\nversus fine-tuning 110, 111\\ninference stage 56\\nInstitute for Human-Centered Artificial Intelligence (HAI) 151\\nintermediate task training 122\\nJ \\njailbreaking 153\\nrisks, mitigating 153, 154\\nJupyter Notebook\\nworking with 30\\nL \\nLangchain\\npretrained models, loading with 89, 90\\nlanguage models\\nevolving 57-59'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 200, 'page_label': '201'}, page_content='large language models (LLMs) 4, 39, 71, 121\\nLatent Diffusion Models (LDMs) 23\\nlinear transformations 50\\nLlamaIndex 143\\nused, for implementing RAG 143-146\\nlong short-term memory (LSTM) 9, 43, 44\\nloss function 55, 56\\nLow-Rank Adaptation (LoRA) 107, 121\\nM \\nmachine learning (ML) 3\\nMasked Language Modeling (MLM) 25, 123\\nmasked self-attention 25\\nMetric for Evaluation of Translation with Explicit Ordering (METEOR) 84\\nmodel-as-a-service (MaaS) 77\\nmodel size and computational complexity\\nchallenges 85, 86\\nmulti-head attention (MHA) 25, 49\\nmulti-head self-attention (MHSA) 62\\nmultimodal image generation\\nwith GPT-4 28\\nmulti-task learning 122\\nN \\nNational Institute of Standards and Technology (NIST) 151\\nnatural language generation (NLG) 4\\nnatural language (NL) 50\\nnatural language processing (NLP) 25, 39, 71\\napproaches 40'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 201, 'page_label': '202'}, page_content='traditional methods 8, 9\\nnatural language processing (NLP), approaches\\nadvent, of neural language models 40\\nadvent, of NNs 43\\ndistributed representation 40-42\\ntransfer learning (TL) 42\\nneural language models\\nadvent 40\\nNeural Network Language Model (NNLM) 40\\nneural network (NN) 40\\nNext-Sentence Prediction(NSP) 124\\nn-grams 128\\nNLP tasks\\ncode generation task 136\\nsummarization task 136\\ntranslation task 136\\nNNs, in NLP\\nadvent of 43\\nmodeling, with RNNs 43, 44\\nrise, of CNNs 44, 45\\nO \\noriginal transformer architecture\\ncomplete transformer 65\\ndata loading and preparation 59\\ndataset creation 61\\ndata tensorization 60\\ndecoder 65\\ndecoder layer 64'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 202, 'page_label': '203'}, page_content='embeddings layer 61\\nencoder 63\\nencoder layer 63\\nFFN 63\\nimplementing 59\\nmain block execution 67, 68\\nmulti-head self-attention (MHSA) layer 62\\npositional encoding layer 62\\ntokenization 60\\ntrain function 66\\ntranslate function 66\\nP \\npadding 126\\nparameter budget 107\\nParameter-Efficient Fine-Tuning (PEFT) 106, 121\\nAdaLoRA 107, 108\\nLow-Rank Adaptation (LoRA) 107\\npositional encoding 25, 47\\nposition-wise 50\\nposition-wise FFNs 50, 51\\ncomponents 50\\npretrained generative model\\nbenchmarking 87\\nproject objectives, meeting 83-85\\nselecting 83\\nsize and computational complexity 85-87\\nproduction environment\\nlocal development setup 77'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 203, 'page_label': '204'}, page_content='production environment, local development setup\\napplication code 79, 80\\nCI/CD setup 81-83\\ncode repository, creating 80, 81\\nDocker setup 78, 79\\nproject initialization 77\\nrequirements file 79\\nVisual Studio Code (VS Code) 77\\nproduction-ready environment\\nsetting up 76\\nproduction setup\\nfeatures, mapping to 75, 76\\ntransitioning to 74, 75\\nprompt chaining 141-143\\nprompt elements and structure 135-138\\nprompt engineering\\nconstrained generation through 152\\nprompting techniques 134\\nguiding principles, for model interaction 134, 135\\nprototyping environment 72-74\\nGPU configuration 88, 89\\npretrained models, loading with Langchain 89, 90\\ntesting data, setting up 90-92\\nupdating 87\\nProxima Passkey 125\\nPython\\nimplementation 113-115\\nQ'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 204, 'page_label': '205'}, page_content='quantitative metrics evaluation 92-94\\nalignment, with CLIP model 94-96\\noutcomes, interpreting 96-98\\nR \\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) 84\\nrectified linear unit (ReLU) 50\\nrecurrent neural network (RNN) 8, 39\\nmodeling with 43, 44\\nregularization techniques 54, 55\\nReinforcement Learning from Human Feedback (RLHF) 10, 132, 152\\nReinforcement Learning (RL) 132\\nresponsible AI deployment\\nbiases, addressing and mitigating 98, 99\\nconsiderations 98\\ntransparency and explainability 99\\nRetrieval Augmented Generation (RAG) 110, 131\\nimplementing, with LlamaIndex 143-146\\nROUGE metric 127-129\\nROUGE-N 128\\nROUGE scores 128\\nF1 score 128\\nprecision 128\\nrecall 128\\nS \\nscaled transformers (DALL-E 2)\\nimage fidelity, improving with 27\\nself-attention mechanism 25, 45, 48, 49'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 205, 'page_label': '206'}, page_content='sequence-to-sequence (Seq2Seq) 26, 51, 52\\nchoice of optimizer 54\\nhyperparameters 53, 54\\ninference 56, 57\\nloss function 55, 56\\nmasking 49\\nmodel training 49, 52\\nregularization techniques 54, 55\\nshift to prompt-based approach 131-134\\nshot learning 141-143\\nsingular value decomposition (SVD) 108\\nstable diffusion transformer 30-33\\nstate-of-the-art (SOTA) model 111, 131\\nStochastic Differential Equations (SDEs) 22\\nStochastic Gradient Descent (SGD) 22\\nStyleSprint deployment 99, 100\\nmaintenance and reliability 101, 102\\ntesting and monitoring 101\\nSupervised Fine-Tuning (SFT) 132\\nsupervised learning (SL) 51\\nT \\ntask-specific fine-tuning 105\\ntensor processing units (TPUs) 72\\nTerm Frequency-Inverse Document Frequency (TF-IDF) 40\\ntesting data\\nsetting up 90-92\\ntokenization 25, 114\\ntoken replacement 122'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 206, 'page_label': '207'}, page_content='tokens 25\\ntraditional ML methods\\nversus generative methods 6, 7\\ntrain function 66\\ntraining loss 116, 117\\ntransfer learning (TL) 42, 122\\ntransformer architecture\\ncomponents 46\\nin advanced language models 45, 46\\ntransformer architecture, components\\ndecoder stack 47\\nencoder stack 46\\npositional encoding 47\\nposition-wise FFNs 50, 51\\nself-attention mechanism 48, 49\\ntransformer-based models\\nevolution 9, 10\\ntransformers\\nused, for image generation 29, 30\\ntranslate function 66\\ntruncation 126\\nV \\nvanishing gradient 43\\nVariational Autoencoders (VAEs) 5, 17\\nVisual Studio Code (VS Code) 77\\nW \\nWasserstein GANs (WGANs) 20'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 207, 'page_label': '208'}, page_content='Word2Vec 40\\nZ \\nzero-shot approach 109\\nzero-shot prompting 109\\nzero-sum game 5'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 208, 'page_label': '209'}, page_content='packtpub.com\\nSubscribe to our online digital library for full access to over 7,000 books and videos,\\nas well as industry leading tools to help you plan your personal development and\\nadvance your career. For more information, please visit our website.\\nWhy subscribe?\\nSpend less time learning and more time coding with practical eBooks and Videos from over\\n4,000 industry professionals\\nImprove your learning with Skill Plans built especially for you\\nGet a free eBook or video every month\\nFully searchable for easy access to vital information\\nCopy and paste, print, and bookmark content\\nDid you know that Packt offers eBook versions of every book published, with PDF\\nand ePub files available? You can upgrade to the eBook version at packtpub.com and\\nas a print book customer, you are entitled to a discount on the eBook copy. Get in\\ntouch with us at customercare@packtpub.com for more details.\\nAt www.packtpub.com, you can also read a collection of free technical articles, sign up\\nfor a range of free newsletters, and receive exclusive discounts and offers on Packt\\nbooks and eBooks.\\nOther Books You May Enjoy\\nIf you enjoyed this book, you may be interested in these other books by Packt:'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 209, 'page_label': '210'}, page_content='Mastering NLP from Foundations to LLMs\\nLior Gazit, Meysam Ghaffari\\nISBN: 978-1-80461-918-6'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 210, 'page_label': '211'}, page_content='Master the mathematical foundations of machine learning and NLP Implement advanced\\ntechniques for preprocessing text data and analysis Design ML-NLP systems in Python\\nModel and classify text using traditional machine learning and deep learning methods\\nUnderstand the theory and design of LLMs and their implementation for various applications\\nin AI\\nExplore NLP insights, trends, and expert opinions on its future direction and potential'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 211, 'page_label': '212'}, page_content='OpenAI API Cookbook\\nHenry Habib\\nISBN: 978-1-80512-135-0'),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 212, 'page_label': '213'}, page_content=\"Grasp the fundamentals of the OpenAI API\\nNavigate the API’s capabilities and limitations of the API\\nSet up the OpenAI API with step-by-step instructions, from obtaining your API key to making\\nyour first call\\nExplore advanced features such as system messages, fine-tuning, and the effects of different\\nparameters\\nIntegrate the OpenAI API into existing applications and workflows to enhance their\\nfunctionality with AI\\nDesign and build applications that fully harness the power of ChatGPT\\nPackt is searching for authors like you\\nIf you're interested in becoming an author for Packt, please visit authors.packtpub.com\\nand apply today. We have worked with thousands of developers and tech professionals,\\njust like you, to help them share their insight with the global tech community. You can\\nmake a general application, apply for a specific hot topic that we are recruiting an\\nauthor for, or submit your own idea.\\nShare Your Thoughts\\nNow you’ve finished Generative AI Foundations in Python, we’d love to hear your\\nthoughts! If you purchased the book from Amazon, please click here to go straight to\\nthe Amazon review page for this book and share your feedback or leave a review on\\nthe site that you purchased it from.\\nYour review is important to us and the tech community and will help us make sure\\nwe’re delivering excellent quality content.\\nDownload a free PDF copy of this book\\nThanks for purchasing this book!\\nDo you like to read on the go but are unable to carry your print books everywhere?\\nIs your eBook purchase not compatible with the device of your choice?\\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that\\nbook at no cost.\\nRead anywhere, any place, on any device. Search, copy, and paste code from your\\nfavorite technical books directly into your application.\"),\n",
       " Document(metadata={'producer': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creator': 'calibre (6.13.0) [http://calibre-ebook.com]', 'creationdate': '2024-07-28T10:46:07+00:00', 'author': 'Carlos Rodriguez', 'moddate': '2024-07-28T12:46:07+02:00', 'title': 'Generative AI Foundations in Python', 'source': 'data/Generative AI Foundations in Python.pdf', 'total_pages': 214, 'page': 213, 'page_label': '214'}, page_content='The perks don’t stop there, you can get exclusive access to discounts, newsletters, and\\ngreat free content in your inbox daily\\nFollow these simple steps to get the benefits:\\n1. Scan the QR code or visit the link below\\nhttps://packt.link/free-ebook/9781835460825\\n2. Submit your proof of purchase\\n3. That’s it! We’ll send your free PDF and other benefits to your email directly'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='ByteTrack: Multi-Object Tracking by Associating Every Detection Box\\nYifu Zhang1, Peize Sun 2, Yi Jiang 3, Dongdong Yu 3, Fucheng Weng 1,\\nZehuan Yuan3, Ping Luo 2, Wenyu Liu 1, Xinggang Wang 1†\\n1Huazhong University of Science and Technology 2The University of Hong Kong 3ByteDance Inc.\\nAbstract\\nMulti-object tracking (MOT) aims at estimating bound-\\ning boxes and identities of objects in videos. Most meth-\\nods obtain identities by associating detection boxes whose\\nscores are higher than a threshold. The objects with low\\ndetection scores, e.g. occluded objects, are simply thrown\\naway, which brings non-negligible true object missing and\\nfragmented trajectories. To solve this problem, we present\\na simple, effective and generic association method, tracking\\nby associating almost every detection box instead of only\\nthe high score ones. For the low score detection boxes, we\\nutilize their similarities with tracklets to recover true ob-\\njects and ﬁlter out the background detections. When ap-\\nplied to 9 different state-of-the-art trackers, our method\\nachieves consistent improvement on IDF1 score ranging\\nfrom 1 to 10 points. To put forwards the state-of-the-\\nart performance of MOT, we design a simple and strong\\ntracker, named ByteTrack. For the ﬁrst time, we achieve\\n80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set\\nof MOT17 with 30 FPS running speed on a single V100\\nGPU. ByteTrack also achieves state-of-the-art performance\\non MOT20, HiEve and BDD100K tracking benchmarks.\\nThe source code, pre-trained models with deploy versions\\nand tutorials of applying to other trackers are released at\\nhttps://github.com/ifzhang/ByteTrack.\\n1. Introduction\\nWas vern¨unftig ist, das ist wirklich; und was wirklich ist,\\ndas ist vern¨unftig.\\n—— G. W. F . Hegel\\nTracking-by-detection is the most effective paradigm for\\nmulti-object tracking (MOT) in current. Due to the com-\\nplex scenarios in videos, detectors are prone to make im-\\nperfect predictions. State-of-the-art MOT methods [1–3, 6,\\n12, 18, 45, 59, 70, 72, 85] need to deal with true positive /\\n†Corresponding author.\\nPart of this work was performed while Yifu Zhang worked as an intern\\nat ByteDance.\\nFigure 1. MOTA-IDF1-FPS comparisons of different trackers on\\nthe test set of MOT17. The horizontal axis is FPS (running speed),\\nthe vertical axis is MOTA, and the radius of circle is IDF1. Our\\nByteTrack achieves 80.3 MOTA, 77.3 IDF1 on MOT17 test set\\nwith 30 FPS running speed, outperforming all previous trackers.\\nDetails are given in Table 4.\\nfalse positive trade-off in detection boxes to eliminate low\\nconﬁdence detection boxes [4, 40]. However, is it the right\\nway to eliminate all low conﬁdence detection boxes? Our\\nanswer is NO: as Hegel said “What is reasonable is real;\\nthat which is real is reasonable.” Low conﬁdence detection\\nboxes sometimes indicate the existence of objects, e.g. the\\noccluded objects. Filtering out these objects causes irre-\\nversible errors for MOT and brings non-negligible missing\\ndetection and fragmented trajectories.\\nFigure 2 (a) and (b) show this problem. In frame t1,\\nwe initialize three different tracklets as their scores are\\nall higher than 0.5. However, in frame t2 and frame t3\\nwhen occlusion happens, red tracklet’s corresponding de-\\ntection score becomes lower i.e. 0.8 to 0.4 and then 0.4 to\\narXiv:2110.06864v3  [cs.CV]  7 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='0.90.80.90.10.90.40.80.10.90.10.80.1\\n0.4 0.1\\n(a) detection boxes\\n(b) trackletsby associating high score detection boxes\\n(c) trackletsby associating every detection box\\nFramet1 Frame t2 Frame t3\\nFigure 2. Examples of our method which associates every detec-\\ntion box. (a) shows all the detection boxes with their scores. (b)\\nshows the tracklets obtained by previous methods which associates\\ndetection boxes whose scores are higher than a threshold, i.e. 0.5.\\nThe same box color represents the same identity. (c) shows the\\ntracklets obtained by our method. The dashed boxes represent the\\npredicted box of the previous tracklets using Kalman Filter. The\\ntwo low score detection boxes are correctly matched to the previ-\\nous tracklets based on the large IoU.\\n0.1. These detection boxes are eliminated by the threshold-\\ning mechanism and the red tracklet disappears accordingly.\\nNevertheless, if we take every detection box into consider-\\nation, more false positives will be introduced immediately,\\ne.g., the most right box in frame t3 of Figure 2 (a). To the\\nbest of our knowledge, very few methods [30, 63] in MOT\\nare able to handle this detection dilemma.\\nIn this paper, we identify that the similarity with tracklets\\nprovides a strong cue to distinguish the objects and back-\\nground in low score detection boxes. As shown in Figure 2\\n(c), two low score detection boxes are matched to the track-\\nlets by the motion model’s predicted boxes, and thus the\\nobjects are correctly recovered. At the same time, the back-\\nground box is removed since it has no matched tracklet.\\nFor making full use of detection boxes from high scores\\nto low ones in the matching process, we present a simple\\nand effective association method BYTE, named for each de-\\ntection box is a basic unit of the tracklet, as byte in computer\\nprogram, and our tracking method values every detailed de-\\ntection box. We ﬁrst match the high score detection boxes\\nto the tracklets based on motion similarity or appearance\\nsimilarity. Similar to [6], we adopt Kalman ﬁlter [29] to\\npredict the location of the tracklets in the new frame. The\\nsimilarity can be computed by the IoU or Re-ID feature dis-\\ntance of the predicted box and the detection box. Figure 2\\n(b) is exactly the results after the ﬁrst matching. Then, we\\nperform the second matching between the unmatched track-\\nlets, i.e. the tracklet in red box, and the low score detection\\nboxes using the same motion similarity. Figure 2 (c) shows\\nthe results after the second matching. The occluded person\\nwith low detection scores is matched correctly to the pre-\\nvious tracklet and the background (in the right part of the\\nimage) is removed.\\nAs the integrating topic of object detection and asso-\\nciation, a desirable solution to MOT is never a detector\\nand the following association; besides, well-designed of\\ntheir junction area is also important. The innovation of\\nBYTE lies in the junction area of detection and associa-\\ntion, where low score detection boxes are bridges to boost\\nboth of them. Beneﬁting from this integration innovation,\\nwhen BYTE is applied to 9 different state-of-the-art track-\\ners, including the Re-ID-based ones [33,47,69,85], motion-\\nbased ones [71, 89], chain-based one [48] and attention-\\nbased ones [59, 80], notable improvements are achieved on\\nalmost all the metrics including MOTA, IDF1 score and ID\\nswitches. For example, we increase the MOTA of Center-\\nTrack [89] from 66.1 to 67.4, IDF1 from 64.2 to 74.0 and\\ndecrease the IDs from 528 to 144 on the half validation set\\nof MOT17.\\nTowards pushing forwards the state-of-the-art perfor-\\nmance of MOT, we propose a simple and strong tracker,\\nnamed ByteTrack. We adopt a recent high-performance\\ndetector YOLOX [24] to obtain the detection boxes and\\nassociate them with our proposed BYTE. On the MOT\\nchallenges, ByteTrack ranks 1st on both MOT17 [44] and\\nMOT20 [17], achieving 80.3 MOTA, 77.3 IDF1 and 63.1\\nHOTA with 30 FPS running speed on V100 GPU on\\nMOT17 and 77.8 MOTA, 75.2 IDF1 and 61.3 HOTA on\\nmuch more crowded MOT20. ByteTrack also achieves\\nstate-of-the-art performance on HiEve [37] and BDD100K\\n[79] tracking benchmarks. We hope the efﬁciency and sim-\\nplicity of ByteTrack could make it attractive in real applica-\\ntions such as social computing.\\n2. Related Work\\n2.1. Object Detection in MOT\\nObject detection is one of the most active topics in\\ncomputer vision and it is the basis of multi-object track-\\ning. The MOT17 dataset [44] provides detection results\\nobtained by popular detectors such as DPM [22], Faster\\nR-CNN [50] and SDP [77]. A large number of methods\\n[3,9,12,14,28,74,91] focus on improving the tracking per-\\nformance based on these given detection results.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='Tracking by detection.With the rapid development of ob-\\nject detection [10, 23, 26, 35, 49, 50, 58, 60], more and more\\nmethods begin to utilize more powerful detectors to obtain\\nhigher tracking performance. The one-stage object detec-\\ntor RetinaNet [35] begin to be adopted by several methods\\nsuch as [39, 48]. CenterNet [90] is the most popular de-\\ntector adopted by most methods [63, 65, 67, 71, 85, 87, 89]\\nfor its simplicity and efﬁciency. The YOLO series detec-\\ntors [8, 49] are also adopted by a large number of meth-\\nods [15, 33, 34, 69] for its excellent balance of accuracy and\\nspeed. Most of these methods directly use the detection\\nboxes on a single image for tracking.\\nHowever, the number of missing detections and very low\\nscoring detections begin to increase when occlusion or mo-\\ntion blur happens in the video sequence, as is pointed out\\nby video object detection methods [41, 62]. Therefore, the\\ninformation of the previous frames are usually leveraged to\\nenhance the video detection performance.\\nDetection by tracking. Tracking can also adopted to\\nhelp obtain more accurate detection boxes. Some meth-\\nods [12–15, 53, 91] utilize single object tracking (SOT) [5]\\nor Kalman ﬁlter [29] to predict the location of the tracklets\\nin the following frame and fuse the predicted boxes with\\nthe detection boxes to enhance the detection results. Other\\nmethods [34, 86] leverage tracked boxes in the previous\\nframes to enhance feature representation of the following\\nframe. Recently, Transformer-based [20, 38, 64, 66] detec-\\ntors [11, 92] are adopted by several methods [42, 59, 80] for\\nits strong ability to propagate boxes between frames. Our\\nmethod also utilize the similarity with tracklets to strength\\nthe reliability of detection boxes.\\nAfter obtaining the detection boxes by various detectors,\\nmost MOT methods [33, 39, 47, 59, 69, 71, 85] only keep\\nthe high score detection boxes by a threshold, i.e. 0.5, and\\nuse those boxes as the input of data association. This is\\nbecause the low score detection boxes contain many back-\\ngrounds which harm the tracking performance. However,\\nwe observe that many occluded objects can be correctly de-\\ntected but have low scores. To reduce missing detections\\nand keep the persistence of trajectories, we keep all the de-\\ntection boxes and associate across every of them.\\n2.2. Data Association\\nData association is the core of multi-object tracking,\\nwhich ﬁrst computes the similarity between tracklets and\\ndetection boxes and leverage different strategies to match\\nthem according to the similarity.\\nSimilarity metrics. Location, motion and appearance are\\nuseful cues for association. SORT [6] combines location\\nand motion cues in a very simple way. It ﬁrst adopts Kalman\\nﬁlter [29] to predict the location of the tracklets in the new\\nframe and then computes the IoU between the detection\\nboxes and the predicted boxes as the similarity. Some re-\\ncent methods [59, 71, 89] design networks to learn object\\nmotions and achieve more robust results in cases of large\\ncamera motion or low frame rate. Location and motion\\nsimilarity are accurate in the short-range matching. Ap-\\npearance similarity are helpful in the long-range matching.\\nAn object can be re-identiﬁed using appearance similarity\\nafter being occluded for a long period of time. Appear-\\nance similarity can be measured by the cosine similarity of\\nthe Re-ID features. DeepSORT [70] adopts a stand-alone\\nRe-ID model to extract appearance features from the de-\\ntection boxes. Recently, joint detection and Re-ID mod-\\nels [33, 39, 47, 69, 84, 85] becomes more and more popular\\nbecause of their simplicity and efﬁciency.\\nMatching strategy. After similarity computation, match-\\ning strategy assigns identities to the objects. This can be\\ndone by Hungarian Algorithm [31] or greedy assignment\\n[89]. SORT [6] matches the detection boxes to the track-\\nlets by once matching. DeepSORT [70] proposes a cas-\\ncaded matching strategy which ﬁrst matches the detection\\nboxes to the most recent tracklets and then to the lost ones.\\nMOTDT [12] ﬁrst utilizes appearance similarity to match\\nand then utilize the IoU similarity to match the unmatched\\ntracklets. QDTrack [47] turns the appearance similarity into\\nprobability by a bi-directional softmax operation and adopts\\na nearest neighbor search to accomplish matching. Atten-\\ntion mechanism [64] can directly propagate boxes between\\nframes and perform association implicitly. Recent methods\\nsuch as [42,80] propose track queries to ﬁnd the location of\\nthe tracked objects in the following frames. The matching\\nis implicitly performed in the attention interaction process\\nwithout using Hungarian Algorithm.\\nAll these methods focus on how to design better associ-\\nation methods. However, we argue that the way detection\\nboxes are utilized determines the upper bound of data asso-\\nciation and we focus on how to make full use of detection\\nboxes from high scores to low ones in the matching process.\\n3. BYTE\\nWe propose a simple, effective and generic data asso-\\nciation method, BYTE. Different from previous methods\\n[33, 47, 69, 85] which only keep the high score detection\\nboxes, we keep almost every detection box and separate\\nthem into high score ones and low score ones. We ﬁrst asso-\\nciate the high score detection boxes to the tracklets. Some\\ntracklets get unmatched because they do not match to an ap-\\npropriate high score detection box, which usually happens\\nwhen occlusion, motion blur or size changing occurs. We\\nthen associate the low score detection boxes and these un-\\nmatched tracklets to recover the objects in low score detec-\\ntion boxes and ﬁlter out background, simultaneously. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='Algorithm 1:Pseudo-code of BYTE.\\nInput: A video sequence V; object detector Det; detection score\\nthreshold τ\\nOutput: Tracks Tof the video\\n1 Initialization: T ←∅\\n2 for frame fk in V do\\n/* Figure 2(a) */\\n/* predict detection boxes & scores */\\n3 Dk ←Det(fk)\\n4 Dhigh ←∅\\n5 Dlow ←∅\\n6 for din Dk do\\n7 if d.score>τ then\\n8 Dhigh ←Dhigh ∪{d}\\n9 end\\n10 else\\n11 Dlow ←Dlow ∪{d}\\n12 end\\n13 end\\n/* predict new locations of tracks */\\n14 for tin Tdo\\n15 t←KalmanFilter(t)\\n16 end\\n/* Figure 2(b) */\\n/* first association */\\n17 Associate Tand Dhigh using Similarity#1\\n18 Dremain ←remaining object boxes from Dhigh\\n19 Tremain ←remaining tracks from T\\n/* Figure 2(c) */\\n/* second association */\\n20 Associate Tremain and Dlow using similarity#2\\n21 Tre−remain ←remaining tracks from Tremain\\n/* delete unmatched tracks */\\n22 T ←T\\\\Tre−remain\\n/* initialize new tracks */\\n23 for din Dremain do\\n24 T ←T∪{d}\\n25 end\\n26 end\\n27 Return: T\\nTrack rebirth [70,89] is not shown in the algorithm for simplicity. In green\\nis the key of our method.\\npseudo-code of BYTE is shown in Algorithm 1.\\nThe input of BYTE is a video sequence V, along with an\\nobject detector Det. We also set a detection score threshold\\nτ. The output of BYTE is the tracksTof the video and each\\ntrack contains the bounding box and identity of the object\\nin each frame.\\nFor each frame in the video, we predict the detection\\nboxes and scores using the detector Det. We separate all\\nthe detection boxes into two parts Dhigh and Dlow accord-\\ning to the detection score threshold τ. For the detection\\nboxes whose scores are higher than τ, we put them into the\\nhigh score detection boxes Dhigh. For the detection boxes\\nwhose scores are lower than τ, we put them into the low\\nscore detection boxes Dlow (line 3 to 13 in Algorithm 1).\\nAfter separating the low score detection boxes and the\\nhigh score detection boxes, we adopt Kalman ﬁlter to pre-\\ndict the new locations in the current frame of each track in\\nT (line 14 to 16 in Algorithm 1).\\nThe ﬁrst association is performed between the high score\\ndetection boxes Dhigh and all the tracks T (including the\\nlost tracks Tlost). Similarity#1 can be computed by\\neither by the IoU or the Re-ID feature distances between\\nthe detection boxes Dhigh and the predicted box of tracks\\nT. Then, we adopt Hungarian Algorithm [31] to ﬁnish the\\nmatching based on the similarity. We keep the unmatched\\ndetections in Dremain and the unmatched tracks in Tremain\\n(line 17 to 19 in Algorithm 1).\\nBYTE is highly ﬂexible and can be compatible to other\\ndifferent association methods. For example, when BYTE is\\ncombined with FairMOT [85], Re-ID feature is added into\\n* first association * in Algorithm 1, others are\\nthe same. In the experiments, we apply BYTE to 9 different\\nstate-of-the-art trackers and achieve notable improvements\\non almost all the metrics.\\nThe second association is performed between the low\\nscore detection boxes Dlow and the remaining tracks\\nTremain after the ﬁrst association. We keep the unmatched\\ntracks in Tre−remain and just delete all the unmatched low\\nscore detection boxes, since we view them as background.\\n(line 20 to 21 in Algorithm 1). We ﬁnd it important to use\\nIoU alone as the Similarity#2 in the second associa-\\ntion because the low score detection boxes usually contains\\nsevere occlusion or motion blur and appearance features are\\nnot reliable. Thus, when apply BYTE to other Re-ID based\\ntrackers [47, 69, 85], we do not adopt appearance similarity\\nin the second association.\\nAfter the association, the unmatched tracks will be\\ndeleted from the tracklets. We do not list the procedure of\\ntrack rebirth [12, 70, 89] in Algorithm 1 for simplicity. Ac-\\ntually, it is necessary for the long-range association to pre-\\nserve the identity of the tracks. For the unmatched tracks\\nTre−remain after the second association, we put them into\\nTlost. For each track in Tlost, only when it exists for more\\nthan a certain number of frames, i.e. 30, we delete it from\\nthe tracks T. Otherwise, we remain the lost tracks Tlost in\\nT(line 22 in Algorithm 1).Finally, we initialize new tracks\\nfrom the unmatched high score detection boxesDremain af-\\nter the ﬁrst association. (line 23 to 27 in Algorithm 1).The\\noutput of each individual frame is the bounding boxes and\\nidentities of the tracks T in the current frame. Note that we\\ndo not output the boxes and identities of Tlost.\\nTo put forwards the state-of-the-art performance of\\nMOT, we design a simple and strong tracker, named\\nByteTrack, by equipping the high-performance detector\\nYOLOX [24] with our association method BYTE.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='4. Experiments\\n4.1. Setting\\nDatasets. We evaluate BYTE and ByteTrack on MOT17\\n[44] and MOT20 [17] datasets under the “private detection”\\nprotocol. Both datasets contain training sets and test sets,\\nwithout validation sets. For ablation studies, we use the ﬁrst\\nhalf of each video in the training set of MOT17 for training\\nand the last half for validation following [89]. We train on\\nthe combination of CrowdHuman dataset [55] and MOT17\\nhalf training set following [59, 71, 80, 89]. We add Cityper-\\nson [82] and ETHZ [21] for training following [33, 69, 85]\\nwhen testing on the test set of MOT17. We also test Byte-\\nTrack on HiEve [37] and BDD100K [79] datasets. HiEve\\nis a large scale human-centric dataset focusing on crowded\\nand complex events. BDD100K is the largest driving video\\ndataset and the dataset splits of the MOT task are 1400\\nvideos for training, 200 videos for validation and 400 videos\\nfor testing. It needs to track objects of 8 classes and contains\\ncases of large camera motion.\\nMetrics. We use the CLEAR metrics [4], including MOTA,\\nFP, FN, IDs,etc., IDF1 [51] and HOTA [40] to evaluate dif-\\nferent aspects of the tracking performance. MOTA is com-\\nputed based on FP, FN and IDs. Considering the amount\\nof FP and FN are larger than IDs, MOTA focuses more\\non the detection performance. IDF1 evaluates the identity\\npreservation ability and focus more on the association per-\\nformance. HOTA is a very recently proposed metric which\\nexplicitly balances the effect of performing accurate detec-\\ntion, association and localization. For BDD100K dataset,\\nthere are some multi-class metrics such as mMOTA and\\nmIDF1. mMOTA / mIDF1 is computed by averaging the\\nMOTA / IDF1 of all the classes.\\nImplementation details. For BYTE, the default detection\\nscore threshold τ is 0.6, unless otherwise speciﬁed. For\\nthe benchmark evaluation of MOT17, MOT20 and HiEve,\\nwe only use IoU as the similarity metrics. In the linear as-\\nsignment step, if the IoU between the detection box and\\nthe tracklet box is smaller than 0.2, the matching will be re-\\njected. For the lost tracklets, we keep it for 30 frames in case\\nit appears again. For BDD100K, we use UniTrack [68] as\\nthe Re-ID model. In ablation study, we use FastReID [27]\\nto extract Re-ID features for MOT17.\\nFor ByteTrack, the detector is YOLOX [24] with\\nYOLOX-X as the backbone and COCO-pretrained model\\n[36] as the initialized weights. For MOT17, the train-\\ning schedule is 80 epochs on the combination of MOT17,\\nCrowdHuman, Cityperson and ETHZ. For MOT20 and\\nHiEve, we only add CrowdHuman as additional training\\ndata. For BDD100K, we do not use additional training data\\nand only train 50 epochs. The input image size is 1440\\n×800 and the shortest side ranges from 576 to 1024 during\\nmulti-scale training. The data augmentation includes Mo-\\nsaic [8] and Mixup [81]. The model is trained on 8 NVIDIA\\nTesla V100 GPU with batch size of 48. The optimizer is\\nSGD with weight decay of 5 ×10−4 and momentum of\\n0.9. The initial learning rate is 10−3 with 1 epoch warm-\\nup and cosine annealing schedule. The total training time\\nis about 12 hours. Following [24], FPS is measured with\\nFP16-precision [43] and batch size of 1 on a single GPU.\\n4.2. Ablation Studies on BYTE\\nSimilarity analysis. We choose different types of sim-\\nilarity for the ﬁrst association and the second association\\nof BYTE. The results are shown in Table 1. We can\\nsee that either IoU or Re-ID can be a good choice for\\nSimilarity#1 on MOT17. IoU achieves better MOTA\\nand IDs while Re-ID achieves higher IDF1. On BDD100K,\\nRe-ID achieves much better results than IoU in the ﬁrst as-\\nsociation. This is because BDD100K contains large camera\\nmotion and the annotations are in low frame rate, which\\ncauses failure of motion cues. It is important to utilize\\nIoU as Similarity#2 in the second association on both\\ndatasets because the low score detection boxes usually con-\\ntains severe occlusion or motion blur and thus Re-ID fea-\\ntures are not reliable. From Table 1 we can ﬁnd that using\\nIoU as Similarity#2 increases about 1.0 MOTA com-\\npared to Re-ID, which indicates that Re-ID features of the\\nlow score detection boxes are not reliable.\\nComparisons with other association methods.We com-\\npare BYTE with other popular association methods includ-\\ning SORT [6], DeepSORT [70] and MOTDT [12] on the\\nvalidation set of MOT17 and BDD100K. The results are\\nshown in Table 2.\\nSORT can be seen as our baseline method because both\\nmethods only adopt Kalman ﬁlter to predict the object mo-\\ntion. We can ﬁnd that BYTE improves the MOTA metric\\nof SORT from 74.6 to 76.6, IDF1 from 76.9 to 79.3 and\\ndecreases IDs from 291 to 159. This highlights the impor-\\ntance of the low score detection boxes and proves the ability\\nof BYTE to recover object boxes from low score one.\\nDeepSORT utilizes additional Re-ID models to enhance\\nthe long-range association. We surprisingly ﬁnd BYTE also\\nhas additional gains compared with DeepSORT. This sug-\\ngests a simple Kalman ﬁlter can perform long-range asso-\\nciation and achieve better IDF1 and IDs when the detection\\nboxes are accurate enough. We note that in severe occlusion\\ncases, Re-ID features are vulnerable and may lead to iden-\\ntity switches, instead, motion model behaves more reliably.\\nMOTDT integrates motion-guided box propagation re-\\nsults along with detection results to associate unreliable de-\\ntection results with tracklets. Although sharing the simi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='MOT17 BDD100K\\nSimilarity#1 Similarity#2 MOTA ↑ IDF1↑ IDs↓ mMOTA↑ mIDF1↑ IDs↓\\nIoU Re-ID 75.8 77.5 231 39.2 48.3 29172\\nIoU IoU 76.6 79.3 159 39.4 48.9 27902\\nRe-ID Re-ID 75.2 78.7 276 45.0 53.4 10425\\nRe-ID IoU 76.3 80.5 216 45.5 54.8 9140\\nTable 1. Comparison of different type of similarity metrics used in the ﬁrst association and the second association of BYTE on MOT17\\nand BDD100K validation set. The best results are shown in bold.\\nMOT17 BDD100K\\nMethod w/ Re-ID MOTA ↑ IDF1↑ IDs↓ mMOTA↑ mIDF1↑ IDs↓ FPS\\nSORT 74.6 76.9 291 30.9 41.3 10067 30.1\\nDeepSORT ✓ 75.4 77.2 239 24.5 38.2 10720 13.5\\nMOTDT ✓ 75.8 77.6 273 26.7 39.8 14520 11.1\\nBYTE (ours) 76.6 79.3 159 39.4 48.9 27902 29.6\\nBYTE (ours) ✓ 76.3 80.5 216 45.5 54.8 9140 11.8\\nTable 2. Comparison of different data association methods on MOT17 and BDD100K validation set. The best results are shown in bold.\\nFigure 3. Comparison of the performances of BYTE and SORT\\nunder different detection score thresholds. The results are from\\nthe validation set of MOT17.\\nlar motivation, MOTDT is behind BYTE by a large mar-\\ngin. We explain that MOTDT uses propagated boxes as\\ntracklet boxes, which may lead to locating drifts in track-\\ning. Instead, BYTE uses low-score detection boxes to\\nre-associate those unmatched tracklets, therefore, tracklet\\nboxes are more accurate.\\nTable 2 also shows the results on BDD100K dataset.\\nBYTE also outperforms other association methods by a\\nlarge margin. Kalman ﬁlter fails in autonomous driving\\nscenes and it is the main reason for the low performance\\nof SORT, DeepSORT and MOTDT. Thus, we do not use\\nKalman ﬁlter on BDD100K. Additional off-the-shelf Re-\\nID models greatly improve the performance of BYTE on\\nBDD100K.\\nRobustness to detection score threshold.The detection\\nscore threshold τhigh is a sensitive hyper-parameter and\\nneeds to be carefully tuned in the task of multi-object track-\\ning. We change it from 0.2 to 0.8 and compare the MOTA\\nFigure 4. Comparison of the number of TPs and FPs in all low\\nscore detection boxes and the low score tracked boxes obtained by\\nBYTE. The results are from the validation set of MOT17.\\nand IDF1 score of BYTE and SORT. The results are shown\\nin Figure 3. From the results we can see that BYTE is more\\nrobust to the detection score threshold than SORT. This is\\nbecause the second association in BYTE recovers the ob-\\njects whose scores are lower than τhigh, and thus consid-\\ners almost every detection box regardless of the change of\\nτhigh.\\nAnalysis on low score detection boxes.To prove the ef-\\nfectiveness of BYTE, we collect the number of TPs and\\nFPs in the low score boxes obtained by BYTE. We use the\\nhalf training set of MOT17 and CrowdHuman for training\\nand evaluate on the half validation set of MOT17. First,\\nwe keep all the low score detection boxes whose scores\\nrange from τlow to τhigh and classify the TPs and FPs us-\\ning ground truth annotations. Then, we select the tracking\\nresults obtained by BYTE from low score detection boxes.\\nThe results of each sequence are shown in Figure 4. We\\ncan see that BYTE obtains notably more TPs than FPs from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='the low score detection boxes even though some sequences\\n(i.e. MOT17-02) have much more FPs in all the detection\\nboxes. The obtained TPs notably increases MOTA from\\n74.6 to 76.6 as is shown in Table 2.\\nApplications on other trackers. We apply BYTE on\\n9 different state-of-the-arts trackers, including JDE [69],\\nCSTrack [33], FairMOT [85], TraDes [71], QDTrack [47],\\nCenterTrack [89], Chained-Tracker [48], TransTrack [59]\\nand MOTR [80]. Among these trackers, JDE, CSTrack,\\nFairMOT, TraDes adopt a combination of motion and Re-\\nID similarity. QDTrack adopts Re-ID similarity alone.\\nCenterTrack and TraDes predict the motion similarity by\\nthe learned networks. Chained-Tracker adopts the chain\\nstructure and outputs the results of two consecutive frames\\nsimultaneously and associate in the same frame by IoU.\\nTransTrack and MOTR adopt the attention mechanism to\\npropagate boxes among frames. Their results are shown in\\nthe ﬁrst line of each tracker in Table 3.To evaluate the effec-\\ntiveness of BYTE, we design two different modes to apply\\nBYTE to these trackers.\\n• The ﬁrst mode is to insert BYTE into the original asso-\\nciation methods of different trackers, as is shown in the\\nsecond line of the results of each tracker in Table 3. Take\\nFairMOT [85] for example, after the original association\\nis done, we select all the unmatched tracklets and asso-\\nciate them with the low score detection boxes follow-\\ning the * second association * in Algorithm 1.\\nNote that for the low score objects, the Re-ID features are\\nnot reliable so we only adopt the IoU between the detec-\\ntion boxes and the tracklet boxes after motion prediction\\nas the similarity. We do not apply the ﬁrst mode of BYTE\\nto Chained-Tracker because we ﬁnd it is difﬁcult to im-\\nplement in the chain structure.\\n• The second mode is to directly use the detection boxes of\\nthese trackers and associate using the whole procedure in\\nAlgorithm 1, as is shown in the third line of the results of\\neach tracker in Table 3.\\nWe can see that in both modes, BYTE can bring stable\\nimprovements over almost all the metrics including MOTA,\\nIDF1 and IDs. For example, BYTE increases CenterTrack\\nby 1.3 MOTA and 9.8 IDF1, Chained-Tracker by 1.9 MOTA\\nand 5.8 IDF1, TransTrack by 1.2 MOTA and 4.1 IDF1. The\\nresults in Table 3 indicate that BYTE has strong generaliza-\\ntion ability and can be easily applied to existing trackers to\\nobtain performance gain.\\n4.3. Benchmark Evaluation\\nWe compare ByteTrack with the state-of-the-art track-\\ners on the test set of MOT17, MOT20 and HiEve under the\\nprivate detection protocol in Table 4, Table 5 and Table 6,\\nMethod Similarity w/ BYTEMOTA↑ IDF1↑ IDs↓\\nJDE [69] Motion(K) + Re-ID 60.0 63.6 473\\nMotion(K) + Re-ID✓ 60.3 (+0.3) 64.1 (+0.5) 418\\nMotion(K) ✓ 60.6 (+0.6) 66.0 (+2.4) 360\\nCSTrack [33]Motion(K) + Re-ID 68.0 72.3 325\\nMotion(K) + Re-ID✓ 69.2 (+1.2) 73.9 (+1.6) 285\\nMotion(K) ✓ 69.3 (+1.3) 71.7 (-0.6) 279\\nFairMOT [85]Motion(K) + Re-ID 69.1 72.8 299\\nMotion(K) + Re-ID✓ 70.4 (+1.3) 74.2 (+1.4) 232\\nMotion(K) ✓ 70.3 (+1.2) 73.2 (+0.4) 236\\nTraDes [71] Motion + Re-ID 68.2 71.7 285\\nMotion + Re-ID ✓ 68.6 (+0.4) 71.1 (-0.6) 259\\nMotion(K) ✓ 67.9 (-0.3) 72.0 (+0.3) 178\\nQuasiDense [47] Re-ID 67.3 67.8 377\\nMotion(K) + Re-ID✓ 67.7 (+0.4) 72.0 (+4.2) 281\\nMotion(K) ✓ 67.9 (+0.6) 70.9 (+3.1) 258\\nCenterTrack [89] Motion 66.1 64.2 528\\nMotion ✓ 66.3 (+0.2) 64.8 (+0.6) 334\\nMotion(K) ✓ 67.4 (+1.3) 74.0 (+9.8) 144\\nCTracker [48] Chain 63.1 60.9 755\\nMotion(K) ✓ 65.0 (+1.9) 66.7 (+5.8) 346\\nTransTrack [59] Attention 67.1 68.3 254\\nAttention ✓ 68.6 (+1.5) 69.0 (+0.7) 232\\nMotion(K) ✓ 68.3 (+1.2) 72.4 (+4.1) 181\\nMOTR [80] Attention 64.7 67.2 346\\nAttention ✓ 64.3 (-0.4) 69.3 (+2.1) 263\\nMotion(K) ✓ 65.7 (+1.0) 68.4 (+1.2) 260\\nTable 3. Results of applying BYTE to 9 different state-of-the-art\\ntrackers on the MOT17 validation set. “K” is short for Kalman\\nFilter. In green are the improvements of at least +1.0 point.\\nrespectively. All the results are directly obtained from the\\nofﬁcial MOT Challenge evaluation server and the Human in\\nEvents server.\\nMOT17. ByteTrack ranks 1st among all the trackers on the\\nleaderboard of MOT17. Not only does it achieve the best\\naccuracy (i.e. 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA), but\\nalso runs with highest running speed (30 FPS). It outper-\\nforms the second-performance tracker [76] by a large mar-\\ngin (i.e. +3.3 MOTA, +5.3 IDF1 and +3.4 HOTA). Also, we\\nuse less training data than many high performance methods\\nsuch as [33, 34, 54, 65, 85] (29K images vs. 73K images). It\\nis worth noting that we only leverage the simplest similarity\\ncomputation method Kalman ﬁlter in the association step\\ncompared to other methods [33,47,59,67,80,85] which ad-\\nditionally adopt Re-ID similarity or attention mechanisms.\\nAll these indicate that ByteTrack is a simple and strong\\ntracker.\\nMOT20. Compared with MOT17, MOT20 has much more\\ncrowded scenarios and occlusion cases. The average num-\\nber of pedestrians in an image is 170 in the test set of\\nMOT20. ByteTrack also ranks 1st among all the trackers on\\nhttps://motchallenge.net\\nhttp://humaninevents.org'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='Tracker MOTA↑IDF1↑HOTA↑ FP↓ FN↓ IDs↓FPS↑\\nDAN [61] 52.4 49.5 39.3 25423 234592 8431 <3.9\\nTubeTK [46] 63.0 58.6 48.0 27060 177483 4137 3.0\\nMOTR [80] 65.1 66.4 - 45486 149307 2049 -\\nCTracker [48] 66.6 57.4 49.0 22284 160491 5529 6.8\\nCenterTrack [89] 67.8 64.7 52.2 18498160332 3039 17.5\\nQuasiDense [47] 68.7 66.3 53.9 26589 146643 3378 20.3\\nTraDes [71] 69.1 63.9 52.7 20892 150060 3555 17.5\\nMAT [25] 69.5 63.1 53.8 30660 138741 2844 9.0\\nSOTMOT [87] 71.0 71.9 - 39537 118983 5184 16.0\\nTransCenter [75] 73.2 62.2 54.5 23112 123738 4614 1.0\\nGSDT [67] 73.2 66.5 55.2 26397 120666 3891 4.9\\nSemi-TCL [32] 73.3 73.2 59.8 22944 124980 2790 -\\nFairMOT [85] 73.7 72.3 59.3 27507 117477 3303 25.9\\nRelationTrack [78]73.8 74.7 61.0 27999 118623 1374 8.5\\nPermaTrackPr [63]73.8 68.9 55.5 28998 115104 3699 11.9\\nCSTrack [33] 74.9 72.6 59.3 23847 114303 3567 15.8\\nTransTrack [59] 75.2 63.5 54.1 50157 86442 3603 10.0\\nFUFET [54] 76.2 68.0 57.9 32796 98475 3237 6.8\\nSiamMOT [34] 76.3 72.3 - - - - 12.8\\nCorrTracker [65] 76.5 73.6 60.7 29808 99510 3369 15.6\\nTransMOT [15] 76.7 75.1 61.7 36231 93150 2346 9.6\\nReMOT [76] 77.0 72.0 59.7 33204 93612 2853 1.8\\nByteTrack (ours)80.3 77.3 63.1 2549183721219629.6\\nTable 4. Comparison of the state-of-the-art methods under the\\n“private detector” protocol on MOT17 test set. The best results\\nare shown in bold. MOT17 contains rich scenes and half of the\\nsequences are captured with camera motion. ByteTrack ranks 1st\\namong all the trackers on the leaderboard of MOT17 and outper-\\nforms the second one ReMOT by a large margin on almost all the\\nmetrics. It also has the highest running speed among all trackers.\\nTracker MOTA↑IDF1↑HOTA↑ FP↓ FN↓ IDs↓FPS↑\\nMLT [83] 48.9 54.6 43.2 45660 216803 2187 3.7\\nFairMOT [85] 61.8 67.3 54.6 103440 88901 5243 13.2\\nTransCenter [75]61.9 50.4 - 45895 146347 4653 1.0\\nTransTrack [59] 65.0 59.4 48.5 27197 150197 3608 7.2\\nCorrTracker [65]65.2 69.1 - 79429 95855 5183 8.5\\nSemi-TCL [32] 65.2 70.1 55.3 61209 114709 4139 -\\nCSTrack [33] 66.6 68.6 54.0 25404144358 3196 4.5\\nGSDT [67] 67.1 67.5 53.6 31913 135409 3131 0.9\\nSiamMOT [34] 67.1 69.1 - - - - 4.3\\nRelationTrack [78]67.2 70.5 56.5 61134 104597 4243 2.7\\nSOTMOT [87] 68.6 71.4 - 57064 101154 4209 8.5\\nByteTrack (ours)77.8 75.2 61.3 26249 87594 1223 17.5\\nTable 5. Comparison of the state-of-the-art methods under the\\n“private detector” protocol on MOT20 test set. The best results\\nare shown in bold. The scenes in MOT20 are much more crowded\\nthan those in MOT17. ByteTrack ranks 1st among all the track-\\ners on the leaderboard of MOT20 and outperforms the second one\\nSOTMOT by a large margin on all the metrics. It also has the\\nhighest running speed among all trackers.\\nthe leaderboard of MOT20 and outperforms other trackers\\nby a large margin on almost all the metrics. For example,\\nit increases MOTA from 68.6 to 77.8, IDF1 from 71.4 to\\n75.2 and decreases IDs by 71% from 4209 to 1223. It is\\nworth noting that ByteTrack achieves extremely low iden-\\nTracker MOTA↑IDF1↑ MT↑ ML↓ FP↓ FN↓ IDs↓\\nDeepSORT [70] 27.1 28.6 8.5% 41.5% 5894 42668 2220\\nMOTDT [12] 26.1 32.9 8.7% 54.6% 6318 43577 1599\\nIOUtracker [7] 38.6 38.6 28.3% 27.6% 9640 28993 4153\\nJDE [69] 33.1 36.0 15.1% 24.1% 9526 33327 3747\\nFairMOT [85] 35.0 46.7 16.3% 44.2% 6523 37750 995\\nCenterTrack [89] 40.9 45.1 10.8% 32.2% 3208 36414 1568\\nByteTrack (Ours)61.7 63.1 38.3% 21.6% 2822 228521031\\nTable 6. Comparison of the state-of-the-art methods under the\\n“private detector” protocol on HiEve test set. The best results are\\nshown in bold. HiEve has more complex events than MOT17 and\\nMOT20. ByteTrackranks 1st among all the trackers on the leader-\\nboard of HiEve and outperforms the second one CenterTrack by a\\nlarge margin on all the metrics.\\ntity switches, which further indicates that associating every\\ndetection boxes is very effective under occlusion cases.\\nHuman in Events.Compared with MOT17 and MOT20,\\nHiEve contains more complex events and more diverse\\ncamera views. We train ByteTrack on CrowdHuman dataset\\nand the training set of HiEve. ByteTrack also ranks 1st\\namong all the trackers on the leaderboard of HiEve and\\noutperforms other state-of-the-art trackers by a large mar-\\ngin. For example, it increases MOTA from 40.9 to 61.3 and\\nIDF1 from 45.1 to 62.9. The superior results indicate that\\nByteTrack is robust to complex scenes.\\nBDD100K. BDD100K is multiple categories tracking\\ndataset in autonomous driving scenes. The challenges in-\\nclude low frame rate and large camera motion. We utilize a\\nsimple ResNet-50 ImageNet classiﬁcation model from Uni-\\nTrack [68] to extract Re-ID features and compute appear-\\nance similarity. ByteTrack ranks ﬁrst on the leaderboard\\nof BDD100K. It increases mMOTA from 36.6 to 45.5 on\\nthe validation set and 35.5 to 40.1 on the test set, which\\nindicates that ByteTrack can also handle the challenges in\\nautonomous driving scenes.\\n5. Conclusion\\nWe present a simple yet effective data association\\nmethod BYTE for multi-object tracking. BYTE can be eas-\\nily applied to existing trackers and achieve consistent im-\\nprovements. We also propose a strong tracker ByteTrack,\\nwhich achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA\\non MOT17 test set with 30 FPS, ranking 1st among all\\nthe trackers on the leaderboard. ByteTrack is very robust\\nto occlusion for its accurate detection performance and the\\nhelp of associating low score detection boxes. It also sheds\\nlight on how to make the best use of detection results to\\nenhance multi-object tracking. We hope the high accuracy,\\nfast speed and simplicity of ByteTrack can make it attractive\\nin real applications.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='Tracker split mMOTA ↑ mIDF1↑ MOTA↑ IDF1↑ FN↓ FP↓ IDs↓ MT↑ ML↓\\nYuet al. [79] val 25.9 44.5 56.9 66.8 122406 52372 8315 8396 3795\\nQDTrack [47] val 36.6 50.8 63.5 71.5 108614 46621 6262 9481 3034\\nByteTrack(Ours) val 45.5 54.8 69.1 70.4 92805 34998 9140 9626 3005\\nYuet al. [79] test 26.3 44.7 58.3 68.2 213220 100230 14674 16299 6017\\nDeepBlueAI test 31.6 38.7 56.9 56.0 292063 35401 25186 10296 12266\\nmadamada test 33.6 43.0 59.8 55.7 209339 76612 42901 16774 5004\\nQDTrack [47] test 35.5 52.3 64.3 72.3 201041 80054 10790 17353 5167\\nByteTrack(Ours)test 40.1 55.8 69.6 71.3 169073 63869 15466 18057 5107\\nTable 7. Comparison of the state-of-the-art methods on BDD100K test set. The best results are shown inbold. ByteTrack ranks 1st among\\nall the trackers on the leaderboard of BDD100K and outperforms the second one QDTrack by a large margin on most metrics.\\nA. Bounding box annotations\\nWe note MOT17 [44] requires the bounding boxes [89]\\ncovering the whole body, even though the object is occluded\\nor partly out of the image. However, the default implemen-\\ntation of YOLOX clips the detection boxes inside the im-\\nage area. To avoid the wrong detection results around the\\nimage boundary, we modify YOLOX in terms of data pre-\\nprocessing and label assignment. We do not clip the bound-\\ning boxes inside the image during the data pre-processing\\nand data augmentation procedure. We only delete the boxes\\nwhich are fully outside the image after data augmentation.\\nIn the SimOTA label assignment strategy, the positive sam-\\nples need to be around the center of the object, while the\\ncenter of the whole body boxes may lie out of the image, so\\nwe clip the center of the object inside the image.\\nMOT20 [17], HiEve [37] and BDD100K clip the bound-\\ning box annotations inside the image in and thus we just use\\nthe original setting of YOLOX.\\nB. Tracking performance of light models\\nWe compare BYTE and DeepSORT [70] using light de-\\ntection models. We use YOLOX [24] with different back-\\nbones as our detector. All models are trained on CrowdHu-\\nman and the half training set of MOT17. The input image\\nsize is 1088 ×608 and the shortest side ranges from 384\\nto 832 during multi-scale training. The results are shown\\nin Table 8. We can see that BYTE brings stable improve-\\nments on MOTA and IDF1 compared to DeepSORT, which\\nindicates that BYTE is robust to detection performance. It\\nis worth noting that when using YOLOX-Nano as back-\\nbone, BYTE brings 3 points higher MOTA than DeepSORT,\\nwhich makes it more appealing in real applications.\\nC. Ablation Studies on ByteTrack\\nSpeed v.s. accuracy.We evaluate the speed and accuracy\\nof ByteTrack using different size of input images during in-\\nference. All experiments use the same multi-scale training.\\nThe results are shown in Table 9. The input size during in-\\nference ranges from 512 ×928 to 800 ×1440. The running\\nBackbone Params GFLOPsTracker MOTA↑IDF1↑IDs↓\\nYOLOX-M 25.3 M 118.7 DeepSORT 74.5 76.2 197\\nYOLOX-M 25.3 M 118.7 BYTE 75.3 77.5 200\\nYOLOX-S 8.9 M 43.0 DeepSORT 69.6 71.5 205\\nYOLOX-S 8.9 M 43.0 BYTE 71.1 73.6 224\\nYOLOX-Tiny 5.0 M 24.5 DeepSORT 68.6 72.0 224\\nYOLOX-Tiny 5.0 M 24.5 BYTE 70.5 72.1 222\\nYOLOX-Nano 0.9 M 4.0 DeepSORT 61.4 66.8 212\\nYOLOX-Nano 0.9 M 4.0 BYTE 64.4 68.4 161\\nTable 8. Comparison of BYTE and DeepSORT using light detec-\\ntion models on the MOT17 validation set.\\nInput size MOTA↑ IDF1↑ IDs↓ Time (ms)\\n512 ×928 75.0 77.6 200 17.9+4.0\\n608 ×1088 75.6 76.4 212 21.8+4.0\\n736 ×1280 76.2 77.4 188 26.2+4.2\\n800 ×1440 76.6 79.3 159 29.6+4.2\\nTable 9. Comparison of different input sizes on the MOT17 vali-\\ndation set. The total running time is a combination of the detection\\ntime and the association time. The best results are shown in bold.\\nTraining data Images MOTA↑ IDF1↑ IDs↓\\nMOT17 2.7K 75.8 76.5 205\\nMOT17 + CH 22.0K 76.6 79.3 159\\nMOT17 + CH + CE 26.6K 76.7 79.7 183\\nTable 10. Comparison of different training data on the MOT17\\nvalidation set. “MOT17” is short for the MOT17 half training set.\\n“CH” is short for the CrowdHuman dataset. “CE” is short for the\\nCityperson and ETHZ datasets. The best results are shown inbold.\\ntime of the detector ranges from 17.9 ms to 30.0 ms and\\nthe association time is all around 4.0 ms. ByteTrack can\\nachieve 75.0 MOTA with 45.7 FPS running speed and 76.6\\nMOTA with 29.6 FPS running speed, which has advantages\\nin practical applications.\\nTraining data. We evaluate ByteTrack on the half valida-\\ntion set of MOT17 using different combinations of training\\ndata. The results are shown in Table 10. When only using\\nthe half training set of MOT17, the performance achieves\\n75.8 MOTA, which already outperforms most methods.\\nThis is because we use strong augmentations such as Mo-\\nsaic [8] and Mixup [81]. When further adding CrowdHu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-08T00:31:37+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-08T00:31:37+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/ObectTracking.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='Interval MOTA↑ IDF1↑ FP↓ FN↓ IDs↓\\nNo 76.6 79.3 3358 9081 159\\n10 77.4 79.7 3638 8403 150\\n20 78.3 80.2 3941 7606 146\\n30 78.3 80.2 4237 7337 147\\nTable 11. Comparison of different interpolation intervals on the\\nMOT17 validation set. The best results are shown in bold.\\nman, Cityperson and ETHZ for training, we can achieve\\n76.7 MOTA and 79.7 IDF1. The big improvement of IDF1\\narises from that the CrowdHuman dataset can boost the de-\\ntector to recognize occluded person, therefore, making the\\nKalman Filter generate smoother predictions and enhance\\nthe association ability of the tracker.\\nThe experiments on training data suggest that ByteTrack\\nis not data hungry. This is a big advantage for real applica-\\ntions, comparing with previous methods [33,34,65,85] that\\nrequire more than 7 data sources [19, 21, 44, 55, 73, 82, 88]\\nto achieve high performance.\\nD. Tracklet interpolation\\nWe notice that there are some fully-occluded pedestrians\\nin MOT17, whose visible ratio is 0 in the ground truth an-\\nnotations. Since it is almost impossible to detect them by\\nvisual cues, we obtain these objects by tracklet interpola-\\ntion.\\nSuppose we have a tracklet T, its tracklet box is lost due\\nto occlusion from frame t1 to t2. The tracklet box of T at\\nframe t1 is Bt1 ∈R4 which contains the top left and bottom\\nright coordinate of the bounding box. Let Bt2 represent the\\ntracklet box of T at frame t2. We set a hyper-parameter σ\\nrepresenting the max interval we perform tracklet interpola-\\ntion, which means tracklet interpolation is performed when\\nt2 −t1 ≤σ, . The interpolated box of tracklet T at frame t\\ncan be computed as follows:\\nBt = Bt1 + (Bt2 −Bt1 ) t−t1\\nt2 −t1\\n, (1)\\nwhere t1 <t<t 2.\\nAs shown in Table 11, tracklet interpolation can im-\\nprove MOTA from 76.6 to 78.3 and IDF1 from 79.3 to\\n80.2, when σ is 20. Tracklet interpolation is an effective\\npost-processing method to obtain the boxes of those fully-\\noccluded objects. We use tracklet interpolation in the test\\nsets of MOT17 [44], MOT20 [17] and HiEve [37] under the\\nprivate detection protocol.\\nE. Public detection results on MOTChallenge\\nWe evaluate ByteTrack on the test set of MOT17 [44]\\nand MOT20 [17] under the public detection protocol. Fol-\\nlowing the public detection ﬁltering strategy in Tracktor [3]\\nTracker MOTA↑IDF1↑HOTA↑ FP↓ FN↓ IDs↓\\nSTRN [74] 50.9 56.0 42.6 25295 249365 2397\\nFAMNet [14] 52.0 48.7 - 14138 253616 3072\\nTracktor++v2 [3] 56.3 55.1 44.8 8866 235449 1987\\nMPNTrack [9] 58.8 61.7 49.0 17413 213594 1185\\nLPCMOT [16] 59.0 66.8 51.5 23102 206948 1122\\nLifT [28] 60.5 65.6 51.1 14966 206619 1189\\nCenterTrack [89] 61.5 59.6 48.2 14076 200672 2583\\nTMOH [57] 62.1 62.8 50.4 10951 201195 1897\\nArTISTC [52] 62.3 59.7 48.9 19611 191207 2062\\nQDTrack [47] 64.6 65.1 - 14103 182998 2652\\nSiamMOT [56] 65.9 63.3 - 18098 170955 3040\\nByteTrack (ours)67.4 70.0 56.1 9939 1726361331\\nTable 12. Comparison of the state-of-the-art methods under the\\n“public detector” protocol on MOT17 test set. The best results are\\nshown in bold.\\nTracker MOTA↑IDF1↑HOTA↑ FP↓ FN↓ IDs↓\\nSORT [6] 42.7 45.1 36.1 27521 264694 4470\\nTracktor++v2 [3] 52.6 52.7 42.1 6930 236680 1648\\nArTISTC [52] 53.6 51.0 41.6 7765 230576 1531\\nLPCMOT [16] 56.3 62.5 49.0 11726 213056 1562\\nMPNTrack [9] 57.6 59.1 46.8 16953 201384 1210\\nTMOH [57] 60.1 61.2 48.9 38043 165899 2342\\nByteTrack (ours)67.0 70.2 56.4 9685 160303 680\\nTable 13. Comparison of the state-of-the-art methods under the\\n“public detector” protocol on MOT20 test set. The best results are\\nshown in bold.\\nand CenterTrack [89], we only initialize a new trajectory\\nwhen its IoU with a public detection box is larger than 0.8.\\nWe do not use tracklet interpolation under the public detec-\\ntion protocol. As is shown in Table 12, ByteTrack outper-\\nforms other methods by a large margin on MOT17. For ex-\\nample, it outperforms SiamMOT by 1.5 points on MOTA\\nand 6.7 points on IDF1. Table 13 shows the results on\\nMOT20. ByteTrack also outperforms existing results by\\na large margin. For example, it outperforms TMOH [57]\\nby 6.9 points on MOTA, 9.0 points on IDF1, 7.5 points on\\nHOTA and reduce the identity switches by three quarters.\\nThe results under public detection protocol further indicate\\nthe effectiveness of our association method BYTE.\\nF. Visualization results.\\nWe show some visualization results of difﬁcult cases\\nwhich ByteTrack is able to handle in Figure 5. The dif-\\nﬁcult cases include occlusion ( i.e. MOT17-02, MOT17-\\n04, MOT17-05, MOT17-09, MOT17-13), motion blur ( i.e.\\nMOT17-10, MOT17-13) and small objects ( i.e. MOT17-\\n13). The pedestrian in the middle frame with red triangle\\nhas low detection score, which is obtained by our associa-\\ntion method BYTE. The low score boxes not only decrease\\nthe number of missing detection, but also play an impor-\\ntant role for long-range association. As we can see from all\\nthese difﬁcult cases, ByteTrack does not bring any identity\\nswitch and preserve the identity effectively.'),\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43928847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f42888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e011589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3816 chunks\n"
     ]
    }
   ],
   "source": [
    "doc_splits = splitter.split_documents(docs)\n",
    "print(f\"Split into {len(doc_splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f41138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0613d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75194520",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    doc_splits,\n",
    "    OpenAIEmbeddings(),\n",
    "    persist_directory=\"./rag_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16023c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b267024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate,PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a822711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    metadata={\"description\": \"A prompt for answering questions based on context.\"},\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=[\"context\", \"question\"],\n",
    "                template = \"You are a helpful assistant. Use the following context to answer the question.\\\n",
    "                If you dont't know the answer,just say i could not say your answer.\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "               \n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8edc80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ae9a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7361a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\":retriever | format_docs,\"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a077dd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An encoder is a component of a neural network that maps input data into an embedding space, creating a representation of the input data that can be used for further processing or analysis.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"what is  encoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484ecbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_portal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
